{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['id','Unnamed: 32'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"diagnosis\"] = df[\"diagnosis\"].replace('B',0)\n",
    "df[\"diagnosis\"] = df[\"diagnosis\"].replace('M',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"diagnosis\",axis=1)\n",
    "Y = df[\"diagnosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcl = svm.SVC(kernel=\"linear\")\n",
    "lcl.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lcl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nX = df[[\"radius_mean\",\"concave points_mean\"]]\n",
    "nY = df[\"diagnosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(nX,nY,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcl2 = svm.SVC(kernel=\"linear\")\n",
    "lcl2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lcl2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0296483039857525 3.9712876465451097 -1.2618195840825894 3.9279296603616616\n"
     ]
    }
   ],
   "source": [
    "sc_x = scaler.fit_transform(df['radius_mean'].values.reshape(-1,1))\n",
    "sc_y = scaler.fit_transform(df['concave points_mean'].values.reshape(-1,1))\n",
    "xmin = np.min(sc_x)\n",
    "xmax = np.max(sc_x)\n",
    "ymin = np.min(sc_y)\n",
    "ymax = np.max(sc_y)\n",
    "h=0.01\n",
    "print(xmin,xmax,ymin,ymax)\n",
    "xx,yy = np.meshgrid(np.arange(xmin,xmax,h),np.arange(ymin,ymax,h))\n",
    "xx_row = xx.ravel()\n",
    "yy_row = yy.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz_row=lcl2.predict(np.c_[xx_row,yy_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TEST\\anaconda\\lib\\site-packages\\IPython\\core\\pylabtools.py:132: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_df=pd.DataFrame(np.c_[xx_row,yy_row,zz_row],columns=['x','y','label'])\n",
    "plt.figure(figsize=[5,5])\n",
    "sns.scatterplot(data=tmp_df,x='x',y='y',hue='label')\n",
    "plt.show()\n",
    "plt.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 569\n"
     ]
    }
   ],
   "source": [
    "attri = scaler.fit_transform(np.array(df[[\"radius_mean\",\"concave points_mean\"]]))\n",
    "label = df[\"diagnosis\"].values\n",
    "num_labels = np.unique(label).shape[0]\n",
    "num_rec = attri.shape[0]\n",
    "num_attri = attri.shape[1]\n",
    "print(num_labels,num_attri,num_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossFunction(data,label,weights,delta):\n",
    "    num_rec = data.shape[0]\n",
    "    scores = weights @ data.T \n",
    "    margins = np.maximum(0,scores-scores[label,np.arange(num_rec)]+delta)\n",
    "    margins[label,np.arange(num_rec)] = 0 \n",
    "    loss = np.sum(margins)/num_rec\n",
    "    return loss \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49164636708394405\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.random([num_labels,num_attri])\n",
    "print(LossFunction(attri,label,weights=weights,delta=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossFunctionGradRegularization(data,label,weights,delta,eps=1E-3,lam=0.1):\n",
    "    \n",
    "    num_rec = data.shape[0]\n",
    "    scores = weights @ data.T \n",
    "    margins = np.maximum(0,scores-scores[label,np.arange(num_rec)]+delta)\n",
    "    margins[label,np.arange(num_rec)] = 0 \n",
    "    loss = (np.sum(margins)/num_rec)\n",
    "    regularization = (lam*(np.sum(weights**2)))\n",
    "    \n",
    "    num_pixels = weights.shape[1]  \n",
    "    num_classes = weights.shape[0]\n",
    "  \n",
    "    gradient = np.zeros(weights.shape)\n",
    "    for jind in range(num_pixels): \n",
    "        pert = eps*data.T[jind,:]\n",
    "        for iind in range(num_classes):    \n",
    "            newScores = scores.copy()\n",
    "            newScores[iind,:] = scores[iind,:]+pert\n",
    "            newmargins = np.maximum(0,newScores-newScores[label,np.arange(num_rec)]+delta)\n",
    "            newmargins[label,np.arange(num_rec)]=0\n",
    "            newloss = np.sum(newmargins)/num_rec\n",
    "            gradient[iind,jind]=((newloss - loss)/eps)\n",
    "    gradient = gradient + (2*lam*weights)\n",
    "    loss = loss + regularization\n",
    "    \n",
    "    return loss,gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.70652826 0.47785395]\n",
      " [0.11526788 0.52357783]]\n",
      "Step - 1, Loss - 1.4953227524656385, Learning Rate - 0.05, magnitude of gradient - 1.6254754409355912\n",
      "Step - 2, Loss - 1.4102192307662507, Learning Rate - 0.05, magnitude of gradient - 1.5496636679122329\n",
      "Step - 3, Loss - 1.3904395212040466, Learning Rate - 0.05, magnitude of gradient - 1.6885474383188719\n",
      "Step - 4, Loss - 1.2739912778936024, Learning Rate - 0.05, magnitude of gradient - 1.5731390829060492\n",
      "Step - 5, Loss - 1.1949267604444922, Learning Rate - 0.05, magnitude of gradient - 1.3187371307268732\n",
      "Step - 6, Loss - 1.1088958936819941, Learning Rate - 0.05, magnitude of gradient - 1.4881469542081136\n",
      "Step - 7, Loss - 1.0498102873984476, Learning Rate - 0.05, magnitude of gradient - 1.2987548884188627\n",
      "Step - 8, Loss - 0.9914825979327004, Learning Rate - 0.05, magnitude of gradient - 1.3785914170238105\n",
      "Step - 9, Loss - 0.8416805862603746, Learning Rate - 0.05, magnitude of gradient - 1.5403208147956204\n",
      "Step - 10, Loss - 0.8175308906615415, Learning Rate - 0.05, magnitude of gradient - 1.254401879321775\n",
      "Step - 11, Loss - 0.726117956225552, Learning Rate - 0.05, magnitude of gradient - 1.2666950706232876\n",
      "Step - 12, Loss - 0.7593884482867076, Learning Rate - 0.05, magnitude of gradient - 1.1360706275586179\n",
      "Step - 13, Loss - 0.6190661854947839, Learning Rate - 0.05, magnitude of gradient - 1.1822673330943443\n",
      "Step - 14, Loss - 0.5615458423355323, Learning Rate - 0.05, magnitude of gradient - 1.143177979516269\n",
      "Step - 15, Loss - 0.5097291461988283, Learning Rate - 0.05, magnitude of gradient - 0.7481357566635585\n",
      "Step - 16, Loss - 0.5237360017085192, Learning Rate - 0.05, magnitude of gradient - 0.7832132182294976\n",
      "Step - 17, Loss - 0.4998658828215582, Learning Rate - 0.05, magnitude of gradient - 0.5065588224633983\n",
      "Step - 18, Loss - 0.47458182796827064, Learning Rate - 0.05, magnitude of gradient - 0.4767086439220916\n",
      "Step - 19, Loss - 0.3978912952613545, Learning Rate - 0.05, magnitude of gradient - 0.3842675277159521\n",
      "Step - 20, Loss - 0.4091501872700728, Learning Rate - 0.05, magnitude of gradient - 0.2832973701302379\n",
      "Step - 21, Loss - 0.3718773503203631, Learning Rate - 0.05, magnitude of gradient - 0.33827118690918767\n",
      "Step - 22, Loss - 0.3421293935475892, Learning Rate - 0.05, magnitude of gradient - 0.18308642409890652\n",
      "Step - 23, Loss - 0.326448338618357, Learning Rate - 0.05, magnitude of gradient - 0.22816030123198697\n",
      "Step - 24, Loss - 0.3962169651576176, Learning Rate - 0.05, magnitude of gradient - 0.1656181694124735\n",
      "Step - 25, Loss - 0.3335481229776872, Learning Rate - 0.05, magnitude of gradient - 0.1273713331474422\n",
      "Step - 26, Loss - 0.3790454504172431, Learning Rate - 0.05, magnitude of gradient - 0.13163336345288068\n",
      "Step - 27, Loss - 0.4323004517692904, Learning Rate - 0.05, magnitude of gradient - 0.14212296414209388\n",
      "Step - 28, Loss - 0.338715080079726, Learning Rate - 0.05, magnitude of gradient - 0.13423111260933596\n",
      "Step - 29, Loss - 0.38176746718626603, Learning Rate - 0.05, magnitude of gradient - 0.11080448261287186\n",
      "Step - 30, Loss - 0.39269743569154125, Learning Rate - 0.05, magnitude of gradient - 0.11130692344579578\n",
      "Step - 31, Loss - 0.28149120017607837, Learning Rate - 0.05, magnitude of gradient - 0.11386775846675036\n",
      "Step - 32, Loss - 0.4415412672869717, Learning Rate - 0.05, magnitude of gradient - 0.10226164545938549\n",
      "Step - 33, Loss - 0.3533515838487368, Learning Rate - 0.05, magnitude of gradient - 0.08171992940093159\n",
      "Step - 34, Loss - 0.3035520187827801, Learning Rate - 0.05, magnitude of gradient - 0.14535166286527493\n",
      "Step - 35, Loss - 0.36145212098338475, Learning Rate - 0.05, magnitude of gradient - 0.05674253074204671\n",
      "Step - 36, Loss - 0.26625826521422596, Learning Rate - 0.05, magnitude of gradient - 0.06313896594712748\n",
      "Step - 37, Loss - 0.3344850366253825, Learning Rate - 0.05, magnitude of gradient - 0.038295577832065705\n",
      "Step - 38, Loss - 0.4048886814021776, Learning Rate - 0.05, magnitude of gradient - 0.089731146383052\n",
      "Step - 39, Loss - 0.3184425374452154, Learning Rate - 0.05, magnitude of gradient - 0.08466454854603633\n",
      "Step - 40, Loss - 0.34725580249334564, Learning Rate - 0.05, magnitude of gradient - 0.03439981670209282\n",
      "Step - 41, Loss - 0.323763851661157, Learning Rate - 0.05, magnitude of gradient - 0.07113547452567161\n",
      "Step - 42, Loss - 0.2965994615078642, Learning Rate - 0.05, magnitude of gradient - 0.08589255569747546\n",
      "Step - 43, Loss - 0.30512677877778516, Learning Rate - 0.05, magnitude of gradient - 0.07987675725641051\n",
      "Step - 44, Loss - 0.3551843208437684, Learning Rate - 0.05, magnitude of gradient - 0.022088201968593967\n",
      "Step - 45, Loss - 0.2875428964521088, Learning Rate - 0.05, magnitude of gradient - 0.04880566102029701\n",
      "Step - 46, Loss - 0.3255290970018478, Learning Rate - 0.05, magnitude of gradient - 0.10529323944337197\n",
      "Step - 47, Loss - 0.3486677174105201, Learning Rate - 0.05, magnitude of gradient - 0.07365308208891799\n",
      "Step - 48, Loss - 0.26109398637311, Learning Rate - 0.05, magnitude of gradient - 0.036647189896953335\n",
      "Step - 49, Loss - 0.27364032292355933, Learning Rate - 0.05, magnitude of gradient - 0.04267714868578895\n",
      "Step - 50, Loss - 0.34162347878554317, Learning Rate - 0.05, magnitude of gradient - 0.039567612606973564\n",
      "Step - 51, Loss - 0.2952111112209847, Learning Rate - 0.05, magnitude of gradient - 0.10371758382422969\n",
      "Step - 52, Loss - 0.3915128310622286, Learning Rate - 0.05, magnitude of gradient - 0.02205106415611823\n",
      "Step - 53, Loss - 0.31158910769997394, Learning Rate - 0.05, magnitude of gradient - 0.09412345838067934\n",
      "Step - 54, Loss - 0.3861645260865065, Learning Rate - 0.05, magnitude of gradient - 0.004744864603116598\n",
      "Step - 55, Loss - 0.42432874226164796, Learning Rate - 0.05, magnitude of gradient - 0.055983831274115776\n",
      "Step - 56, Loss - 0.2612716458631912, Learning Rate - 0.05, magnitude of gradient - 0.07905893510403146\n",
      "Step - 57, Loss - 0.3192996181261242, Learning Rate - 0.05, magnitude of gradient - 0.029231765778111762\n",
      "Step - 58, Loss - 0.26365104912038534, Learning Rate - 0.05, magnitude of gradient - 0.04393108218141441\n",
      "Step - 59, Loss - 0.279795569614687, Learning Rate - 0.05, magnitude of gradient - 0.05817439644687977\n",
      "Step - 60, Loss - 0.2758705875462357, Learning Rate - 0.05, magnitude of gradient - 0.027962509943438744\n",
      "Step - 61, Loss - 0.2559511040009237, Learning Rate - 0.05, magnitude of gradient - 0.09761337608553969\n",
      "Step - 62, Loss - 0.36339622320327636, Learning Rate - 0.05, magnitude of gradient - 0.09397121255947162\n",
      "Step - 63, Loss - 0.3203074223876037, Learning Rate - 0.05, magnitude of gradient - 0.052609006353794514\n",
      "Step - 64, Loss - 0.3202965484232238, Learning Rate - 0.05, magnitude of gradient - 0.01966997302306027\n",
      "Step - 65, Loss - 0.29988171508459316, Learning Rate - 0.05, magnitude of gradient - 0.06212840864863564\n",
      "Step - 66, Loss - 0.3416346343125173, Learning Rate - 0.05, magnitude of gradient - 0.03636232074945525\n",
      "Step - 67, Loss - 0.34267230063515397, Learning Rate - 0.05, magnitude of gradient - 0.05112199724958579\n",
      "Step - 68, Loss - 0.2873661982329406, Learning Rate - 0.05, magnitude of gradient - 0.10343694840638672\n",
      "Step - 69, Loss - 0.2825066604719358, Learning Rate - 0.05, magnitude of gradient - 0.05065245466188248\n",
      "Step - 70, Loss - 0.2261640764202241, Learning Rate - 0.05, magnitude of gradient - 0.0934960463757743\n",
      "Step - 71, Loss - 0.3459725745836665, Learning Rate - 0.05, magnitude of gradient - 0.06488251975800467\n",
      "Step - 72, Loss - 0.34396817004484226, Learning Rate - 0.05, magnitude of gradient - 0.10280782212358983\n",
      "Step - 73, Loss - 0.41282267186775423, Learning Rate - 0.05, magnitude of gradient - 0.029200739968464326\n",
      "Step - 74, Loss - 0.28025404548100946, Learning Rate - 0.05, magnitude of gradient - 0.03814513625787985\n",
      "Step - 75, Loss - 0.32052400110546037, Learning Rate - 0.05, magnitude of gradient - 0.06866805143041801\n",
      "Step - 76, Loss - 0.34086354508903177, Learning Rate - 0.05, magnitude of gradient - 0.05396043559220499\n",
      "Step - 77, Loss - 0.41281557893178, Learning Rate - 0.05, magnitude of gradient - 0.033326695487208\n",
      "Step - 78, Loss - 0.24035866685466784, Learning Rate - 0.05, magnitude of gradient - 0.024762315428539614\n",
      "Step - 79, Loss - 0.32160288812970306, Learning Rate - 0.05, magnitude of gradient - 0.0024121910618576693\n",
      "Step - 80, Loss - 0.31093767271939443, Learning Rate - 0.05, magnitude of gradient - 0.075196552584961\n",
      "Step - 81, Loss - 0.36087782699746435, Learning Rate - 0.05, magnitude of gradient - 0.08609865082032964\n",
      "Step - 82, Loss - 0.3486852657376987, Learning Rate - 0.05, magnitude of gradient - 0.09745858273439748\n",
      "Step - 83, Loss - 0.30724329780578075, Learning Rate - 0.05, magnitude of gradient - 0.012496630152569703\n",
      "Step - 84, Loss - 0.30965578129640414, Learning Rate - 0.05, magnitude of gradient - 0.07848629303617151\n",
      "Step - 85, Loss - 0.3522124710920413, Learning Rate - 0.05, magnitude of gradient - 0.035753055683122564\n",
      "Step - 86, Loss - 0.4032531970451396, Learning Rate - 0.05, magnitude of gradient - 0.04795285510189802\n",
      "Step - 87, Loss - 0.3319096486454284, Learning Rate - 0.05, magnitude of gradient - 0.19702649629453642\n",
      "Step - 88, Loss - 0.29805122299552517, Learning Rate - 0.05, magnitude of gradient - 0.07330099199455097\n",
      "Step - 89, Loss - 0.2563171993481751, Learning Rate - 0.05, magnitude of gradient - 0.058337557363312316\n",
      "Step - 90, Loss - 0.3582250566762667, Learning Rate - 0.05, magnitude of gradient - 0.024579001684457927\n",
      "Step - 91, Loss - 0.45842873855192046, Learning Rate - 0.05, magnitude of gradient - 0.07058074030972931\n",
      "Step - 92, Loss - 0.349076897451574, Learning Rate - 0.05, magnitude of gradient - 0.04437555488842583\n",
      "Step - 93, Loss - 0.3343692274046709, Learning Rate - 0.05, magnitude of gradient - 0.09982094611917093\n",
      "Step - 94, Loss - 0.3802180921600549, Learning Rate - 0.05, magnitude of gradient - 0.09213661778876439\n",
      "Step - 95, Loss - 0.3394706367515329, Learning Rate - 0.05, magnitude of gradient - 0.04941790792072193\n",
      "Step - 96, Loss - 0.3771527620096842, Learning Rate - 0.05, magnitude of gradient - 0.1586201998571234\n",
      "Step - 97, Loss - 0.27272048321342035, Learning Rate - 0.05, magnitude of gradient - 0.08891393014891896\n",
      "Step - 98, Loss - 0.32138247513672574, Learning Rate - 0.05, magnitude of gradient - 0.03363409544043468\n",
      "Step - 99, Loss - 0.3048262122006481, Learning Rate - 0.05, magnitude of gradient - 0.06348884786755012\n",
      "Step - 100, Loss - 0.3358878303375028, Learning Rate - 0.05, magnitude of gradient - 0.06280268950336815\n",
      "Step - 101, Loss - 0.3051400275215307, Learning Rate - 0.05, magnitude of gradient - 0.05828174062171442\n",
      "Step - 102, Loss - 0.4004809469894202, Learning Rate - 0.05, magnitude of gradient - 0.058734577301894464\n",
      "Step - 103, Loss - 0.3585033757607353, Learning Rate - 0.05, magnitude of gradient - 0.08486328064814151\n",
      "Step - 104, Loss - 0.2843562891833652, Learning Rate - 0.05, magnitude of gradient - 0.05266281473756938\n",
      "Step - 105, Loss - 0.279035639522643, Learning Rate - 0.05, magnitude of gradient - 0.09492542562091118\n",
      "Step - 106, Loss - 0.36893570823180577, Learning Rate - 0.05, magnitude of gradient - 0.013521717130059459\n",
      "Step - 107, Loss - 0.2879022075651156, Learning Rate - 0.05, magnitude of gradient - 0.026562698066307077\n",
      "Step - 108, Loss - 0.3966046407210668, Learning Rate - 0.05, magnitude of gradient - 0.06326908748485563\n",
      "Step - 109, Loss - 0.3307707647598497, Learning Rate - 0.05, magnitude of gradient - 0.028660060732547097\n",
      "Step - 110, Loss - 0.3012858868329956, Learning Rate - 0.05, magnitude of gradient - 0.025903681458953196\n",
      "Step - 111, Loss - 0.4620076806637484, Learning Rate - 0.05, magnitude of gradient - 0.05135927602942542\n",
      "Step - 112, Loss - 0.4101348376171572, Learning Rate - 0.05, magnitude of gradient - 0.03403982401709535\n",
      "Step - 113, Loss - 0.3389188818684071, Learning Rate - 0.05, magnitude of gradient - 0.03517041404343726\n",
      "Step - 114, Loss - 0.24469022449223848, Learning Rate - 0.05, magnitude of gradient - 0.08152051301589143\n",
      "Step - 115, Loss - 0.392814365048428, Learning Rate - 0.05, magnitude of gradient - 0.036667916943507795\n",
      "Step - 116, Loss - 0.3073870216260368, Learning Rate - 0.05, magnitude of gradient - 0.1011366410191532\n",
      "Step - 117, Loss - 0.23546597403129532, Learning Rate - 0.05, magnitude of gradient - 0.06464233659303693\n",
      "Step - 118, Loss - 0.2813305595138182, Learning Rate - 0.05, magnitude of gradient - 0.0435902212574414\n",
      "Step - 119, Loss - 0.31362303683654086, Learning Rate - 0.05, magnitude of gradient - 0.11556085523201542\n",
      "Step - 120, Loss - 0.30965449048508886, Learning Rate - 0.05, magnitude of gradient - 0.034594404201368875\n",
      "Step - 121, Loss - 0.2181275956965623, Learning Rate - 0.05, magnitude of gradient - 0.057238720370292935\n",
      "Step - 122, Loss - 0.4256655992255704, Learning Rate - 0.05, magnitude of gradient - 0.04857302543114026\n",
      "Step - 123, Loss - 0.2848377789965188, Learning Rate - 0.05, magnitude of gradient - 0.07850610671010934\n",
      "Step - 124, Loss - 0.38286799308360386, Learning Rate - 0.05, magnitude of gradient - 0.11190504643959215\n",
      "Step - 125, Loss - 0.33224524053318505, Learning Rate - 0.05, magnitude of gradient - 0.06921346669816095\n",
      "Step - 126, Loss - 0.3629686372332589, Learning Rate - 0.05, magnitude of gradient - 0.05664005501941492\n",
      "Step - 127, Loss - 0.29069680771714546, Learning Rate - 0.05, magnitude of gradient - 0.055128563869620444\n",
      "Step - 128, Loss - 0.2996959895229695, Learning Rate - 0.05, magnitude of gradient - 0.06878568937726394\n",
      "Step - 129, Loss - 0.2818735917925407, Learning Rate - 0.05, magnitude of gradient - 0.07178323416047316\n",
      "Step - 130, Loss - 0.3072610553547621, Learning Rate - 0.05, magnitude of gradient - 0.014071728473647172\n",
      "Step - 131, Loss - 0.21571350851010007, Learning Rate - 0.05, magnitude of gradient - 0.052539082311251754\n",
      "Step - 132, Loss - 0.37044506711885783, Learning Rate - 0.05, magnitude of gradient - 0.06379974604538555\n",
      "Step - 133, Loss - 0.28051732009709174, Learning Rate - 0.05, magnitude of gradient - 0.02373639772195408\n",
      "Step - 134, Loss - 0.29210725366475593, Learning Rate - 0.05, magnitude of gradient - 0.030091985597752745\n",
      "Step - 135, Loss - 0.22134708652403168, Learning Rate - 0.05, magnitude of gradient - 0.10389661406012705\n",
      "Step - 136, Loss - 0.285435605591488, Learning Rate - 0.05, magnitude of gradient - 0.05623949397879241\n",
      "Step - 137, Loss - 0.27600001072312996, Learning Rate - 0.05, magnitude of gradient - 0.026219195953635945\n",
      "Step - 138, Loss - 0.28108440936604207, Learning Rate - 0.05, magnitude of gradient - 0.052588606999572504\n",
      "Step - 139, Loss - 0.30838875398007576, Learning Rate - 0.05, magnitude of gradient - 0.03762416081916226\n",
      "Step - 140, Loss - 0.3449254899369296, Learning Rate - 0.05, magnitude of gradient - 0.06337918094384268\n",
      "Step - 141, Loss - 0.30241236957842854, Learning Rate - 0.05, magnitude of gradient - 0.06792463615162643\n",
      "Step - 142, Loss - 0.3074995842651475, Learning Rate - 0.05, magnitude of gradient - 0.02967482224180407\n",
      "Step - 143, Loss - 0.3496913990920621, Learning Rate - 0.05, magnitude of gradient - 0.01294827932721751\n",
      "Step - 144, Loss - 0.38614574413697383, Learning Rate - 0.05, magnitude of gradient - 0.10553344181161563\n",
      "Step - 145, Loss - 0.32598286570436447, Learning Rate - 0.05, magnitude of gradient - 0.08169518015409383\n",
      "Step - 146, Loss - 0.2890685983021336, Learning Rate - 0.05, magnitude of gradient - 0.03778149417223604\n",
      "Step - 147, Loss - 0.3622972246054855, Learning Rate - 0.05, magnitude of gradient - 0.06240843282339077\n",
      "Step - 148, Loss - 0.25505632829226843, Learning Rate - 0.05, magnitude of gradient - 0.02591127629073142\n",
      "Step - 149, Loss - 0.30298425049355604, Learning Rate - 0.05, magnitude of gradient - 0.025896803263943186\n",
      "Step - 150, Loss - 0.36700160942746274, Learning Rate - 0.05, magnitude of gradient - 0.02364040073843315\n",
      "Step - 151, Loss - 0.2850019388068935, Learning Rate - 0.05, magnitude of gradient - 0.06792626897984477\n",
      "Step - 152, Loss - 0.32716823286460434, Learning Rate - 0.05, magnitude of gradient - 0.04661701636136313\n",
      "Step - 153, Loss - 0.3766646051464129, Learning Rate - 0.05, magnitude of gradient - 0.028166609757710744\n",
      "Step - 154, Loss - 0.3251914092842283, Learning Rate - 0.05, magnitude of gradient - 0.08943789727534235\n",
      "Step - 155, Loss - 0.3103087642697804, Learning Rate - 0.05, magnitude of gradient - 0.056592960272556406\n",
      "Step - 156, Loss - 0.4509409461237357, Learning Rate - 0.05, magnitude of gradient - 0.03350180917489523\n",
      "Step - 157, Loss - 0.3389257426896497, Learning Rate - 0.05, magnitude of gradient - 0.09561409152832759\n",
      "Step - 158, Loss - 0.3121385174101859, Learning Rate - 0.05, magnitude of gradient - 0.014189862736223541\n",
      "Step - 159, Loss - 0.33018346891710076, Learning Rate - 0.05, magnitude of gradient - 0.03791411029569416\n",
      "Step - 160, Loss - 0.29495180163346846, Learning Rate - 0.05, magnitude of gradient - 0.07551566162016288\n",
      "Step - 161, Loss - 0.34416262561316724, Learning Rate - 0.05, magnitude of gradient - 0.047908743174630564\n",
      "Step - 162, Loss - 0.3348506757675316, Learning Rate - 0.05, magnitude of gradient - 0.0680986635857067\n",
      "Step - 163, Loss - 0.2568457830032272, Learning Rate - 0.05, magnitude of gradient - 0.029868643093818238\n",
      "Step - 164, Loss - 0.27884536099989354, Learning Rate - 0.05, magnitude of gradient - 0.045023005794745594\n",
      "Step - 165, Loss - 0.3854664406166525, Learning Rate - 0.05, magnitude of gradient - 0.02466777701656951\n",
      "Step - 166, Loss - 0.2803711260258396, Learning Rate - 0.05, magnitude of gradient - 0.115753883830864\n",
      "Step - 167, Loss - 0.3039526759723243, Learning Rate - 0.05, magnitude of gradient - 0.05372852974480218\n",
      "Step - 168, Loss - 0.28304557866520835, Learning Rate - 0.05, magnitude of gradient - 0.047698798516477584\n",
      "Step - 169, Loss - 0.3042738824042679, Learning Rate - 0.05, magnitude of gradient - 0.04346484580571068\n",
      "Step - 170, Loss - 0.24160278521870132, Learning Rate - 0.05, magnitude of gradient - 0.03292267568734808\n",
      "Step - 171, Loss - 0.4003786383199752, Learning Rate - 0.05, magnitude of gradient - 0.05761649432124562\n",
      "Step - 172, Loss - 0.31318468149060175, Learning Rate - 0.05, magnitude of gradient - 0.11230828110855394\n",
      "Step - 173, Loss - 0.36051578551933705, Learning Rate - 0.05, magnitude of gradient - 0.02046544859514075\n",
      "Step - 174, Loss - 0.3112198265925932, Learning Rate - 0.05, magnitude of gradient - 0.08690993399299177\n",
      "Step - 175, Loss - 0.30577098828963234, Learning Rate - 0.05, magnitude of gradient - 0.04203489311261076\n",
      "Step - 176, Loss - 0.2626990853611867, Learning Rate - 0.05, magnitude of gradient - 0.015971093773575282\n",
      "Step - 177, Loss - 0.3623614464103714, Learning Rate - 0.05, magnitude of gradient - 0.04773910908011003\n",
      "Step - 178, Loss - 0.28981589844172084, Learning Rate - 0.05, magnitude of gradient - 0.04770419669062067\n",
      "Step - 179, Loss - 0.31844769707396364, Learning Rate - 0.05, magnitude of gradient - 0.040815210549423865\n",
      "Step - 180, Loss - 0.29097132435215944, Learning Rate - 0.05, magnitude of gradient - 0.07508713566230259\n",
      "Step - 181, Loss - 0.30426607791802635, Learning Rate - 0.05, magnitude of gradient - 0.022600363742696767\n",
      "Step - 182, Loss - 0.3240214569709857, Learning Rate - 0.05, magnitude of gradient - 0.043044891350720295\n",
      "Step - 183, Loss - 0.3201110267667958, Learning Rate - 0.05, magnitude of gradient - 0.06759434495544248\n",
      "Step - 184, Loss - 0.25162600019583836, Learning Rate - 0.05, magnitude of gradient - 0.0060181333465335095\n",
      "Step - 185, Loss - 0.29409951726757994, Learning Rate - 0.05, magnitude of gradient - 0.012228377760858203\n",
      "Step - 186, Loss - 0.3493430383842646, Learning Rate - 0.05, magnitude of gradient - 0.07633681224987823\n",
      "Step - 187, Loss - 0.2758224857705462, Learning Rate - 0.05, magnitude of gradient - 0.07222454514388663\n",
      "Step - 188, Loss - 0.3753311687311902, Learning Rate - 0.05, magnitude of gradient - 0.02515909550092319\n",
      "Step - 189, Loss - 0.38657880295633396, Learning Rate - 0.05, magnitude of gradient - 0.024825140792647908\n",
      "Step - 190, Loss - 0.31126554516977345, Learning Rate - 0.05, magnitude of gradient - 0.01513358231247727\n",
      "Step - 191, Loss - 0.3357146538099651, Learning Rate - 0.05, magnitude of gradient - 0.025586588464732316\n",
      "Step - 192, Loss - 0.436737609846594, Learning Rate - 0.05, magnitude of gradient - 0.016015082651649585\n",
      "Step - 193, Loss - 0.39004962715218183, Learning Rate - 0.05, magnitude of gradient - 0.025081233652840172\n",
      "Step - 194, Loss - 0.3017111096745016, Learning Rate - 0.05, magnitude of gradient - 0.041563872241177205\n",
      "Step - 195, Loss - 0.29479563772123446, Learning Rate - 0.05, magnitude of gradient - 0.16941510122220496\n",
      "Step - 196, Loss - 0.3074169939432681, Learning Rate - 0.05, magnitude of gradient - 0.07337331591708077\n",
      "Step - 197, Loss - 0.30296790019216485, Learning Rate - 0.05, magnitude of gradient - 0.07265303701121345\n",
      "Step - 198, Loss - 0.2877093891042869, Learning Rate - 0.05, magnitude of gradient - 0.12759442781231012\n",
      "Step - 199, Loss - 0.2941913504761788, Learning Rate - 0.05, magnitude of gradient - 0.032192905764794895\n",
      "Step - 200, Loss - 0.32290572875406215, Learning Rate - 0.05, magnitude of gradient - 0.1376225298589361\n",
      "Step - 201, Loss - 0.3356695038757835, Learning Rate - 0.05, magnitude of gradient - 0.03399916077124541\n",
      "Step - 202, Loss - 0.33615403885181727, Learning Rate - 0.05, magnitude of gradient - 0.07384147166226697\n",
      "Step - 203, Loss - 0.31665859459019763, Learning Rate - 0.05, magnitude of gradient - 0.051975090285191365\n",
      "Step - 204, Loss - 0.34537329311945963, Learning Rate - 0.05, magnitude of gradient - 0.153422019675264\n",
      "Step - 205, Loss - 0.3451385767384124, Learning Rate - 0.05, magnitude of gradient - 0.054873833619638546\n",
      "Step - 206, Loss - 0.321758395127956, Learning Rate - 0.05, magnitude of gradient - 0.03846721209920006\n",
      "Step - 207, Loss - 0.2737049875421552, Learning Rate - 0.05, magnitude of gradient - 0.0518677452014728\n",
      "Step - 208, Loss - 0.32081372530461755, Learning Rate - 0.05, magnitude of gradient - 0.049585896189539264\n",
      "Step - 209, Loss - 0.271158439845091, Learning Rate - 0.05, magnitude of gradient - 0.03215566752238063\n",
      "Step - 210, Loss - 0.33852603361397754, Learning Rate - 0.05, magnitude of gradient - 0.03153635856258055\n",
      "Step - 211, Loss - 0.2735730902943432, Learning Rate - 0.05, magnitude of gradient - 0.08281811203591989\n",
      "Step - 212, Loss - 0.2757269358005343, Learning Rate - 0.05, magnitude of gradient - 0.07501539378911694\n",
      "Step - 213, Loss - 0.2504446540551267, Learning Rate - 0.05, magnitude of gradient - 0.015555144575537475\n",
      "Step - 214, Loss - 0.30803166876332877, Learning Rate - 0.05, magnitude of gradient - 0.048739378437341954\n",
      "Step - 215, Loss - 0.376579268673476, Learning Rate - 0.05, magnitude of gradient - 0.01988238888377287\n",
      "Step - 216, Loss - 0.35073291231824155, Learning Rate - 0.05, magnitude of gradient - 0.043937645518017994\n",
      "Step - 217, Loss - 0.2893748103737485, Learning Rate - 0.05, magnitude of gradient - 0.0873322433694077\n",
      "Step - 218, Loss - 0.299703370976758, Learning Rate - 0.05, magnitude of gradient - 0.0957856191297553\n",
      "Step - 219, Loss - 0.2924392459045968, Learning Rate - 0.05, magnitude of gradient - 0.014716810451538456\n",
      "Step - 220, Loss - 0.31545300810380683, Learning Rate - 0.05, magnitude of gradient - 0.04603759393436336\n",
      "Step - 221, Loss - 0.2778394860307569, Learning Rate - 0.05, magnitude of gradient - 0.006567675131794671\n",
      "Step - 222, Loss - 0.30397425255032684, Learning Rate - 0.05, magnitude of gradient - 0.06098047392635736\n",
      "Step - 223, Loss - 0.3632528674274905, Learning Rate - 0.05, magnitude of gradient - 0.11637679435437968\n",
      "Step - 224, Loss - 0.3227060447233415, Learning Rate - 0.05, magnitude of gradient - 0.021461757093483268\n",
      "Step - 225, Loss - 0.3469697218765589, Learning Rate - 0.05, magnitude of gradient - 0.038335857867649065\n",
      "Step - 226, Loss - 0.3294530425178431, Learning Rate - 0.05, magnitude of gradient - 0.015898796576911456\n",
      "Step - 227, Loss - 0.35636055238349323, Learning Rate - 0.05, magnitude of gradient - 0.032752424040869435\n",
      "Step - 228, Loss - 0.3207510603766513, Learning Rate - 0.05, magnitude of gradient - 0.1062441904122236\n",
      "Step - 229, Loss - 0.34438027577719516, Learning Rate - 0.05, magnitude of gradient - 0.04142017083035504\n",
      "Step - 230, Loss - 0.3256764381902265, Learning Rate - 0.05, magnitude of gradient - 0.03975246689171277\n",
      "Step - 231, Loss - 0.3167391115807156, Learning Rate - 0.05, magnitude of gradient - 0.015614639103312183\n",
      "Step - 232, Loss - 0.30537002226323406, Learning Rate - 0.05, magnitude of gradient - 0.12700077547006206\n",
      "Step - 233, Loss - 0.3025196780921455, Learning Rate - 0.05, magnitude of gradient - 0.12837619562148275\n",
      "Step - 234, Loss - 0.3430979441146831, Learning Rate - 0.05, magnitude of gradient - 0.08793378056750448\n",
      "Step - 235, Loss - 0.39644785724665554, Learning Rate - 0.05, magnitude of gradient - 0.08663158187962562\n",
      "Step - 236, Loss - 0.28868048269440305, Learning Rate - 0.05, magnitude of gradient - 0.12283466945532534\n",
      "Step - 237, Loss - 0.3077360035833012, Learning Rate - 0.05, magnitude of gradient - 0.11021500171107283\n",
      "Step - 238, Loss - 0.2834231405460188, Learning Rate - 0.05, magnitude of gradient - 0.03610954254889966\n",
      "Step - 239, Loss - 0.34478220251517383, Learning Rate - 0.05, magnitude of gradient - 0.04320812065622303\n",
      "Step - 240, Loss - 0.4082546456314889, Learning Rate - 0.05, magnitude of gradient - 0.08828800794996564\n",
      "Step - 241, Loss - 0.28352338704545565, Learning Rate - 0.05, magnitude of gradient - 0.07991772690778687\n",
      "Step - 242, Loss - 0.394558854606244, Learning Rate - 0.05, magnitude of gradient - 0.05731169067591551\n",
      "Step - 243, Loss - 0.3610655004384022, Learning Rate - 0.05, magnitude of gradient - 0.08134706370568308\n",
      "Step - 244, Loss - 0.3363838953520098, Learning Rate - 0.05, magnitude of gradient - 0.040835664485835636\n",
      "Step - 245, Loss - 0.3507515663936316, Learning Rate - 0.05, magnitude of gradient - 0.07660313686643162\n",
      "Step - 246, Loss - 0.3313748643712282, Learning Rate - 0.05, magnitude of gradient - 0.08701040148819945\n",
      "Step - 247, Loss - 0.3103109693541243, Learning Rate - 0.05, magnitude of gradient - 0.02450611488602109\n",
      "Step - 248, Loss - 0.2497632799730877, Learning Rate - 0.05, magnitude of gradient - 0.028374887857514342\n",
      "Step - 249, Loss - 0.25139649359064264, Learning Rate - 0.05, magnitude of gradient - 0.11269919460580123\n",
      "Step - 250, Loss - 0.27347985036414546, Learning Rate - 0.05, magnitude of gradient - 0.049083935867195\n",
      "Step - 251, Loss - 0.2760301674231897, Learning Rate - 0.05, magnitude of gradient - 0.0669715320836993\n",
      "Step - 252, Loss - 0.3073769450531321, Learning Rate - 0.05, magnitude of gradient - 0.01861729431296664\n",
      "Step - 253, Loss - 0.2596696448440146, Learning Rate - 0.05, magnitude of gradient - 0.055934912104375366\n",
      "Step - 254, Loss - 0.3642694732110415, Learning Rate - 0.05, magnitude of gradient - 0.07130757593680352\n",
      "Step - 255, Loss - 0.23358709269877595, Learning Rate - 0.05, magnitude of gradient - 0.08144999393039538\n",
      "Step - 256, Loss - 0.30981786419258306, Learning Rate - 0.05, magnitude of gradient - 0.035451906377871864\n",
      "Step - 257, Loss - 0.3683532775723634, Learning Rate - 0.05, magnitude of gradient - 0.07428977253607531\n",
      "Step - 258, Loss - 0.29747255211373486, Learning Rate - 0.05, magnitude of gradient - 0.06393440854261315\n",
      "Step - 259, Loss - 0.32058574044543064, Learning Rate - 0.05, magnitude of gradient - 0.045646023656444774\n",
      "Step - 260, Loss - 0.3233729244869622, Learning Rate - 0.05, magnitude of gradient - 0.02409670427587352\n",
      "Step - 261, Loss - 0.29539159438290674, Learning Rate - 0.05, magnitude of gradient - 0.09952961695184495\n",
      "Step - 262, Loss - 0.3466758855413493, Learning Rate - 0.05, magnitude of gradient - 0.016324818116255205\n",
      "Step - 263, Loss - 0.2353211899460908, Learning Rate - 0.05, magnitude of gradient - 0.02717886383918028\n",
      "Step - 264, Loss - 0.28764272034152727, Learning Rate - 0.05, magnitude of gradient - 0.16760216316891677\n",
      "Step - 265, Loss - 0.3869803780647268, Learning Rate - 0.05, magnitude of gradient - 0.05302613674893275\n",
      "Step - 266, Loss - 0.3259367699431742, Learning Rate - 0.05, magnitude of gradient - 0.04207626010364372\n",
      "Step - 267, Loss - 0.3476819759006102, Learning Rate - 0.05, magnitude of gradient - 0.0834476340392209\n",
      "Step - 268, Loss - 0.3413381930539796, Learning Rate - 0.05, magnitude of gradient - 0.070718832196057\n",
      "Step - 269, Loss - 0.33406923807447386, Learning Rate - 0.05, magnitude of gradient - 0.04124971769712171\n",
      "Step - 270, Loss - 0.37161698977559454, Learning Rate - 0.05, magnitude of gradient - 0.00877681892084087\n",
      "Step - 271, Loss - 0.30050691178025507, Learning Rate - 0.05, magnitude of gradient - 0.08622597629718963\n",
      "Step - 272, Loss - 0.3568124064670517, Learning Rate - 0.05, magnitude of gradient - 0.04846852114367008\n",
      "Step - 273, Loss - 0.28634799457442817, Learning Rate - 0.05, magnitude of gradient - 0.03646950545203429\n",
      "Step - 274, Loss - 0.38246639233757473, Learning Rate - 0.05, magnitude of gradient - 0.0637067534224362\n",
      "Step - 275, Loss - 0.3338802643516736, Learning Rate - 0.05, magnitude of gradient - 0.017432894032473886\n",
      "Step - 276, Loss - 0.36182199900826495, Learning Rate - 0.05, magnitude of gradient - 0.03325190787834993\n",
      "Step - 277, Loss - 0.28776210966149957, Learning Rate - 0.05, magnitude of gradient - 0.05630482119402346\n",
      "Step - 278, Loss - 0.32210692725325973, Learning Rate - 0.05, magnitude of gradient - 0.07821921368368587\n",
      "Step - 279, Loss - 0.25727824128024135, Learning Rate - 0.05, magnitude of gradient - 0.05415530618561086\n",
      "Step - 280, Loss - 0.3764330029039301, Learning Rate - 0.05, magnitude of gradient - 0.13063989681943394\n",
      "Step - 281, Loss - 0.27340735672371236, Learning Rate - 0.05, magnitude of gradient - 0.00982978591859802\n",
      "Step - 282, Loss - 0.3245048862548081, Learning Rate - 0.05, magnitude of gradient - 0.08849465713695208\n",
      "Step - 283, Loss - 0.3334912651323891, Learning Rate - 0.05, magnitude of gradient - 0.1316451804038474\n",
      "Step - 284, Loss - 0.30602370085447284, Learning Rate - 0.05, magnitude of gradient - 0.0993471945058485\n",
      "Step - 285, Loss - 0.35845716029021657, Learning Rate - 0.05, magnitude of gradient - 0.09950098595565597\n",
      "Step - 286, Loss - 0.32475095291672984, Learning Rate - 0.05, magnitude of gradient - 0.12334170249621831\n",
      "Step - 287, Loss - 0.2737662877586878, Learning Rate - 0.05, magnitude of gradient - 0.04884938584525141\n",
      "Step - 288, Loss - 0.38403144915747306, Learning Rate - 0.05, magnitude of gradient - 0.11627969091660116\n",
      "Step - 289, Loss - 0.28763355929097995, Learning Rate - 0.05, magnitude of gradient - 0.029727870157742226\n",
      "Step - 290, Loss - 0.35807337242253334, Learning Rate - 0.05, magnitude of gradient - 0.0621948356061672\n",
      "Step - 291, Loss - 0.3847293254467177, Learning Rate - 0.05, magnitude of gradient - 0.08906721453922123\n",
      "Step - 292, Loss - 0.3045692882012531, Learning Rate - 0.05, magnitude of gradient - 0.06296914121946678\n",
      "Step - 293, Loss - 0.4047588183701376, Learning Rate - 0.05, magnitude of gradient - 0.061631442701472326\n",
      "Step - 294, Loss - 0.3366801672298888, Learning Rate - 0.05, magnitude of gradient - 0.08248044524776026\n",
      "Step - 295, Loss - 0.3555329685035607, Learning Rate - 0.05, magnitude of gradient - 0.0564193406313044\n",
      "Step - 296, Loss - 0.2707805566195953, Learning Rate - 0.05, magnitude of gradient - 0.04287968197255826\n",
      "Step - 297, Loss - 0.33609604392587755, Learning Rate - 0.05, magnitude of gradient - 0.04900295520587088\n",
      "Step - 298, Loss - 0.3013604623739995, Learning Rate - 0.05, magnitude of gradient - 0.0851528739443927\n",
      "Step - 299, Loss - 0.33979909165023603, Learning Rate - 0.05, magnitude of gradient - 0.06525268254774888\n",
      "Step - 300, Loss - 0.3425296128522894, Learning Rate - 0.05, magnitude of gradient - 0.08902971650640847\n",
      "Step - 301, Loss - 0.33026056894045897, Learning Rate - 0.05, magnitude of gradient - 0.11849086670830775\n",
      "Step - 302, Loss - 0.4053921020828885, Learning Rate - 0.05, magnitude of gradient - 0.04122661828177149\n",
      "Step - 303, Loss - 0.27486472167393095, Learning Rate - 0.05, magnitude of gradient - 0.04272030034071993\n",
      "Step - 304, Loss - 0.3106381893984873, Learning Rate - 0.05, magnitude of gradient - 0.09407297687049507\n",
      "Step - 305, Loss - 0.43134115003132023, Learning Rate - 0.05, magnitude of gradient - 0.0866523771757873\n",
      "Step - 306, Loss - 0.28706933293037135, Learning Rate - 0.05, magnitude of gradient - 0.12349429027764652\n",
      "Step - 307, Loss - 0.34493960081246, Learning Rate - 0.05, magnitude of gradient - 0.0756494635989287\n",
      "Step - 308, Loss - 0.3230419608113834, Learning Rate - 0.05, magnitude of gradient - 0.05627825327619636\n",
      "Step - 309, Loss - 0.3578944300433194, Learning Rate - 0.05, magnitude of gradient - 0.026027864161589385\n",
      "Step - 310, Loss - 0.44414651526946564, Learning Rate - 0.05, magnitude of gradient - 0.04548952684751481\n",
      "Step - 311, Loss - 0.3476879958562241, Learning Rate - 0.05, magnitude of gradient - 0.17521824095768673\n",
      "Step - 312, Loss - 0.2703374757839476, Learning Rate - 0.05, magnitude of gradient - 0.0409593795108678\n",
      "Step - 313, Loss - 0.291702230914952, Learning Rate - 0.05, magnitude of gradient - 0.03412738429876311\n",
      "Step - 314, Loss - 0.3436169571712274, Learning Rate - 0.05, magnitude of gradient - 0.03888743103687941\n",
      "Step - 315, Loss - 0.41687271284232835, Learning Rate - 0.05, magnitude of gradient - 0.04297995943233105\n",
      "Step - 316, Loss - 0.40870653380046307, Learning Rate - 0.05, magnitude of gradient - 0.09297814941923994\n",
      "Step - 317, Loss - 0.31552447050890925, Learning Rate - 0.05, magnitude of gradient - 0.04148068272400803\n",
      "Step - 318, Loss - 0.3211019734164164, Learning Rate - 0.05, magnitude of gradient - 0.0883945740197593\n",
      "Step - 319, Loss - 0.3161788113393936, Learning Rate - 0.05, magnitude of gradient - 0.02571047050739068\n",
      "Step - 320, Loss - 0.2784783523326105, Learning Rate - 0.05, magnitude of gradient - 0.10810774725548036\n",
      "Step - 321, Loss - 0.34412035590974854, Learning Rate - 0.05, magnitude of gradient - 0.07051308618337257\n",
      "Step - 322, Loss - 0.314493442069535, Learning Rate - 0.05, magnitude of gradient - 0.05946482186978445\n",
      "Step - 323, Loss - 0.29215191368212956, Learning Rate - 0.05, magnitude of gradient - 0.08213681655725133\n",
      "Step - 324, Loss - 0.3939400339223286, Learning Rate - 0.05, magnitude of gradient - 0.07223615807126321\n",
      "Step - 325, Loss - 0.3638924080987913, Learning Rate - 0.05, magnitude of gradient - 0.08895861623940722\n",
      "Step - 326, Loss - 0.3256134240782911, Learning Rate - 0.05, magnitude of gradient - 0.043366704796731326\n",
      "Step - 327, Loss - 0.2921796469443966, Learning Rate - 0.05, magnitude of gradient - 0.11698404329656485\n",
      "Step - 328, Loss - 0.30679342269147464, Learning Rate - 0.05, magnitude of gradient - 0.11915314556100674\n",
      "Step - 329, Loss - 0.2752726263556369, Learning Rate - 0.05, magnitude of gradient - 0.020344969622907463\n",
      "Step - 330, Loss - 0.5316872737726179, Learning Rate - 0.05, magnitude of gradient - 0.12051600865289197\n",
      "Step - 331, Loss - 0.25394836368610074, Learning Rate - 0.05, magnitude of gradient - 0.1263789445310861\n",
      "Step - 332, Loss - 0.2883845185134246, Learning Rate - 0.05, magnitude of gradient - 0.06284994386226894\n",
      "Step - 333, Loss - 0.3266362835067511, Learning Rate - 0.05, magnitude of gradient - 0.05419048867701592\n",
      "Step - 334, Loss - 0.2668702423333728, Learning Rate - 0.05, magnitude of gradient - 0.08752612401343488\n",
      "Step - 335, Loss - 0.30710536735756977, Learning Rate - 0.05, magnitude of gradient - 0.049019635798460325\n",
      "Step - 336, Loss - 0.3268857037805807, Learning Rate - 0.05, magnitude of gradient - 0.10511778224901228\n",
      "Step - 337, Loss - 0.34325516716766624, Learning Rate - 0.05, magnitude of gradient - 0.07894390020288668\n",
      "Step - 338, Loss - 0.2960901053174427, Learning Rate - 0.05, magnitude of gradient - 0.035146395518270625\n",
      "Step - 339, Loss - 0.316087902347381, Learning Rate - 0.05, magnitude of gradient - 0.04632496433105856\n",
      "Step - 340, Loss - 0.36720314191682774, Learning Rate - 0.05, magnitude of gradient - 0.012859481231716013\n",
      "Step - 341, Loss - 0.38811734648897445, Learning Rate - 0.05, magnitude of gradient - 0.06100887473218211\n",
      "Step - 342, Loss - 0.3649596994152622, Learning Rate - 0.05, magnitude of gradient - 0.09600785242395157\n",
      "Step - 343, Loss - 0.33688616395692983, Learning Rate - 0.05, magnitude of gradient - 0.06074468033843174\n",
      "Step - 344, Loss - 0.31353377650645, Learning Rate - 0.05, magnitude of gradient - 0.04247935390711924\n",
      "Step - 345, Loss - 0.36820503510363656, Learning Rate - 0.05, magnitude of gradient - 0.04593895810325033\n",
      "Step - 346, Loss - 0.2947510434260341, Learning Rate - 0.05, magnitude of gradient - 0.08643344618589899\n",
      "Step - 347, Loss - 0.32764528405845933, Learning Rate - 0.05, magnitude of gradient - 0.05239652185006684\n",
      "Step - 348, Loss - 0.37813883133397574, Learning Rate - 0.05, magnitude of gradient - 0.04679450565770327\n",
      "Step - 349, Loss - 0.2555156072518196, Learning Rate - 0.05, magnitude of gradient - 0.08493093977222069\n",
      "Step - 350, Loss - 0.2756707560104622, Learning Rate - 0.05, magnitude of gradient - 0.004945020318699228\n",
      "Step - 351, Loss - 0.33008214833618577, Learning Rate - 0.05, magnitude of gradient - 0.07103325662895375\n",
      "Step - 352, Loss - 0.3004146316873022, Learning Rate - 0.05, magnitude of gradient - 0.004692524826483993\n",
      "Step - 353, Loss - 0.3978505603914346, Learning Rate - 0.05, magnitude of gradient - 0.02651736591167641\n",
      "Step - 354, Loss - 0.3315746120051566, Learning Rate - 0.05, magnitude of gradient - 0.07610662302557039\n",
      "Step - 355, Loss - 0.34932190001062524, Learning Rate - 0.05, magnitude of gradient - 0.04685989775583771\n",
      "Step - 356, Loss - 0.3002732364862655, Learning Rate - 0.05, magnitude of gradient - 0.021523229270036925\n",
      "Step - 357, Loss - 0.2610779520645735, Learning Rate - 0.05, magnitude of gradient - 0.013260215705942392\n",
      "Step - 358, Loss - 0.2833801812674335, Learning Rate - 0.05, magnitude of gradient - 0.010902644188571462\n",
      "Step - 359, Loss - 0.278854653744866, Learning Rate - 0.05, magnitude of gradient - 0.03750504669702992\n",
      "Step - 360, Loss - 0.4265823621194294, Learning Rate - 0.05, magnitude of gradient - 0.02460039580503385\n",
      "Step - 361, Loss - 0.3675464735879247, Learning Rate - 0.05, magnitude of gradient - 0.05273101456672533\n",
      "Step - 362, Loss - 0.27932293121818286, Learning Rate - 0.05, magnitude of gradient - 0.041557837456248845\n",
      "Step - 363, Loss - 0.29298742669622, Learning Rate - 0.05, magnitude of gradient - 0.04523245697359947\n",
      "Step - 364, Loss - 0.3640463234702408, Learning Rate - 0.05, magnitude of gradient - 0.029175925821018987\n",
      "Step - 365, Loss - 0.3717472489467106, Learning Rate - 0.05, magnitude of gradient - 0.0791586324225433\n",
      "Step - 366, Loss - 0.30650119984521706, Learning Rate - 0.05, magnitude of gradient - 0.037246923743597844\n",
      "Step - 367, Loss - 0.29298593386306193, Learning Rate - 0.05, magnitude of gradient - 0.011643656687657034\n",
      "Step - 368, Loss - 0.37619300683296997, Learning Rate - 0.05, magnitude of gradient - 0.07907544389993437\n",
      "Step - 369, Loss - 0.32053259880803175, Learning Rate - 0.05, magnitude of gradient - 0.061816111537773194\n",
      "Step - 370, Loss - 0.2558468211350489, Learning Rate - 0.05, magnitude of gradient - 0.07609444883867941\n",
      "Step - 371, Loss - 0.3386599672934063, Learning Rate - 0.05, magnitude of gradient - 0.07094942435606101\n",
      "Step - 372, Loss - 0.38572274745095547, Learning Rate - 0.05, magnitude of gradient - 0.0653514218133284\n",
      "Step - 373, Loss - 0.30981422517803137, Learning Rate - 0.05, magnitude of gradient - 0.05036818038125971\n",
      "Step - 374, Loss - 0.3145606548811263, Learning Rate - 0.05, magnitude of gradient - 0.06166424563024678\n",
      "Step - 375, Loss - 0.31727682351525927, Learning Rate - 0.05, magnitude of gradient - 0.02623830592923302\n",
      "Step - 376, Loss - 0.3454657936456864, Learning Rate - 0.05, magnitude of gradient - 0.072100780759982\n",
      "Step - 377, Loss - 0.3445687727684533, Learning Rate - 0.05, magnitude of gradient - 0.03642805580672364\n",
      "Step - 378, Loss - 0.32636516003663135, Learning Rate - 0.05, magnitude of gradient - 0.022026174553753903\n",
      "Step - 379, Loss - 0.26382891920277873, Learning Rate - 0.05, magnitude of gradient - 0.041585793655153275\n",
      "Step - 380, Loss - 0.3375748227589003, Learning Rate - 0.05, magnitude of gradient - 0.029644005075534687\n",
      "Step - 381, Loss - 0.3165743812966848, Learning Rate - 0.05, magnitude of gradient - 0.033939070641356185\n",
      "Step - 382, Loss - 0.28534024729635765, Learning Rate - 0.05, magnitude of gradient - 0.048954445936055066\n",
      "Step - 383, Loss - 0.2724844009767192, Learning Rate - 0.05, magnitude of gradient - 0.04903525510805144\n",
      "Step - 384, Loss - 0.3456449577676518, Learning Rate - 0.05, magnitude of gradient - 0.0892447729162854\n",
      "Step - 385, Loss - 0.2559438758433711, Learning Rate - 0.05, magnitude of gradient - 0.07976203790842673\n",
      "Step - 386, Loss - 0.32518724393808884, Learning Rate - 0.05, magnitude of gradient - 0.050350973543209165\n",
      "Step - 387, Loss - 0.3369103335548226, Learning Rate - 0.05, magnitude of gradient - 0.10952049198498456\n",
      "Step - 388, Loss - 0.3107226244384529, Learning Rate - 0.05, magnitude of gradient - 0.038558101182957605\n",
      "Step - 389, Loss - 0.29628094262085913, Learning Rate - 0.05, magnitude of gradient - 0.055467440945902884\n",
      "Step - 390, Loss - 0.4098707040567135, Learning Rate - 0.05, magnitude of gradient - 0.12926490356509432\n",
      "Step - 391, Loss - 0.328457708584767, Learning Rate - 0.05, magnitude of gradient - 0.06494602213503464\n",
      "Step - 392, Loss - 0.27595036942010603, Learning Rate - 0.05, magnitude of gradient - 0.04674005315369229\n",
      "Step - 393, Loss - 0.32491643676339294, Learning Rate - 0.05, magnitude of gradient - 0.06086889790615277\n",
      "Step - 394, Loss - 0.3219169569561128, Learning Rate - 0.05, magnitude of gradient - 0.06246019536928091\n",
      "Step - 395, Loss - 0.3684619052984721, Learning Rate - 0.05, magnitude of gradient - 0.038068492653135194\n",
      "Step - 396, Loss - 0.3570182488013026, Learning Rate - 0.05, magnitude of gradient - 0.07676703240534737\n",
      "Step - 397, Loss - 0.3555826496716313, Learning Rate - 0.05, magnitude of gradient - 0.053019871758324295\n",
      "Step - 398, Loss - 0.2322507150076384, Learning Rate - 0.05, magnitude of gradient - 0.04229271038344311\n",
      "Step - 399, Loss - 0.3044732695923727, Learning Rate - 0.05, magnitude of gradient - 0.05202098873257031\n",
      "Step - 400, Loss - 0.37084526933819784, Learning Rate - 0.05, magnitude of gradient - 0.08186018556772373\n",
      "Step - 401, Loss - 0.2760087132205974, Learning Rate - 0.05, magnitude of gradient - 0.11415787111774107\n",
      "Step - 402, Loss - 0.3124682422068268, Learning Rate - 0.05, magnitude of gradient - 0.059952874410610875\n",
      "Step - 403, Loss - 0.35734699770672185, Learning Rate - 0.05, magnitude of gradient - 0.028770345792171546\n",
      "Step - 404, Loss - 0.31631809963203694, Learning Rate - 0.05, magnitude of gradient - 0.06428514887295007\n",
      "Step - 405, Loss - 0.36609115868558206, Learning Rate - 0.05, magnitude of gradient - 0.12119460193328474\n",
      "Step - 406, Loss - 0.32431858019446924, Learning Rate - 0.05, magnitude of gradient - 0.06428831804779434\n",
      "Step - 407, Loss - 0.2963695507436066, Learning Rate - 0.05, magnitude of gradient - 0.05883027152930444\n",
      "Step - 408, Loss - 0.45185985431108683, Learning Rate - 0.05, magnitude of gradient - 0.04443236651315411\n",
      "Step - 409, Loss - 0.38561574754366507, Learning Rate - 0.05, magnitude of gradient - 0.04649254368487926\n",
      "Step - 410, Loss - 0.30256973344065424, Learning Rate - 0.05, magnitude of gradient - 0.15322494110612986\n",
      "Step - 411, Loss - 0.27724659754001524, Learning Rate - 0.05, magnitude of gradient - 0.041362982912681666\n",
      "Step - 412, Loss - 0.2621017896610668, Learning Rate - 0.05, magnitude of gradient - 0.030192462835119147\n",
      "Step - 413, Loss - 0.3418791752066849, Learning Rate - 0.05, magnitude of gradient - 0.057501485649246016\n",
      "Step - 414, Loss - 0.4234444269229247, Learning Rate - 0.05, magnitude of gradient - 0.06836729741470925\n",
      "Step - 415, Loss - 0.3672029186685899, Learning Rate - 0.05, magnitude of gradient - 0.025738710704395422\n",
      "Step - 416, Loss - 0.3075876086267668, Learning Rate - 0.05, magnitude of gradient - 0.12341645270842744\n",
      "Step - 417, Loss - 0.3310862294807437, Learning Rate - 0.05, magnitude of gradient - 0.023613719431833154\n",
      "Step - 418, Loss - 0.319876611125216, Learning Rate - 0.05, magnitude of gradient - 0.014669405663074266\n",
      "Step - 419, Loss - 0.27747833706076586, Learning Rate - 0.05, magnitude of gradient - 0.05380518440074392\n",
      "Step - 420, Loss - 0.3035573665578912, Learning Rate - 0.05, magnitude of gradient - 0.13936972221637792\n",
      "Step - 421, Loss - 0.35276045118685906, Learning Rate - 0.05, magnitude of gradient - 0.003188735680348095\n",
      "Step - 422, Loss - 0.30999303369381115, Learning Rate - 0.05, magnitude of gradient - 0.04290965695931091\n",
      "Step - 423, Loss - 0.3381476027568179, Learning Rate - 0.05, magnitude of gradient - 0.0728420873744381\n",
      "Step - 424, Loss - 0.35728869644902783, Learning Rate - 0.05, magnitude of gradient - 0.01654979888169131\n",
      "Step - 425, Loss - 0.25986800176505376, Learning Rate - 0.05, magnitude of gradient - 0.024022920784136894\n",
      "Step - 426, Loss - 0.30637899142276903, Learning Rate - 0.05, magnitude of gradient - 0.10309177525019408\n",
      "Step - 427, Loss - 0.30139314551001134, Learning Rate - 0.05, magnitude of gradient - 0.08555276101612541\n",
      "Step - 428, Loss - 0.2914809980112584, Learning Rate - 0.05, magnitude of gradient - 0.11472560933954049\n",
      "Step - 429, Loss - 0.2888149750599746, Learning Rate - 0.05, magnitude of gradient - 0.07705451502359775\n",
      "Step - 430, Loss - 0.28750848395383255, Learning Rate - 0.05, magnitude of gradient - 0.047908657994662464\n",
      "Step - 431, Loss - 0.33837744480895815, Learning Rate - 0.05, magnitude of gradient - 0.01128993862699079\n",
      "Step - 432, Loss - 0.3051856181437513, Learning Rate - 0.05, magnitude of gradient - 0.05081597702379555\n",
      "Step - 433, Loss - 0.2779182199493637, Learning Rate - 0.05, magnitude of gradient - 0.021982300555110062\n",
      "Step - 434, Loss - 0.32650686766574044, Learning Rate - 0.05, magnitude of gradient - 0.08624654155056768\n",
      "Step - 435, Loss - 0.27007666384906315, Learning Rate - 0.05, magnitude of gradient - 0.04624645386902017\n",
      "Step - 436, Loss - 0.2569899648585076, Learning Rate - 0.05, magnitude of gradient - 0.058287506640105034\n",
      "Step - 437, Loss - 0.2977356790732948, Learning Rate - 0.05, magnitude of gradient - 0.04256935058861589\n",
      "Step - 438, Loss - 0.3316048378771188, Learning Rate - 0.05, magnitude of gradient - 0.06590574048740157\n",
      "Step - 439, Loss - 0.3007091161635672, Learning Rate - 0.05, magnitude of gradient - 0.04909038272821301\n",
      "Step - 440, Loss - 0.3585936982308582, Learning Rate - 0.05, magnitude of gradient - 0.06739763053804788\n",
      "Step - 441, Loss - 0.37174125567358707, Learning Rate - 0.05, magnitude of gradient - 0.0422850808455472\n",
      "Step - 442, Loss - 0.3114892100152158, Learning Rate - 0.05, magnitude of gradient - 0.06449378961811295\n",
      "Step - 443, Loss - 0.2911702832246586, Learning Rate - 0.05, magnitude of gradient - 0.041747091638385365\n",
      "Step - 444, Loss - 0.32678738238264016, Learning Rate - 0.05, magnitude of gradient - 0.05367412891932007\n",
      "Step - 445, Loss - 0.3479875382142529, Learning Rate - 0.05, magnitude of gradient - 0.037738651536467924\n",
      "Step - 446, Loss - 0.27932846366616715, Learning Rate - 0.05, magnitude of gradient - 0.06750371217322616\n",
      "Step - 447, Loss - 0.33256875947930475, Learning Rate - 0.05, magnitude of gradient - 0.04017312727503954\n",
      "Step - 448, Loss - 0.35706807106385785, Learning Rate - 0.05, magnitude of gradient - 0.027018874216825615\n",
      "Step - 449, Loss - 0.43573673649079747, Learning Rate - 0.05, magnitude of gradient - 0.1303661649455504\n",
      "Step - 450, Loss - 0.31658245902932425, Learning Rate - 0.05, magnitude of gradient - 0.04382499468664931\n",
      "Step - 451, Loss - 0.31807463043744855, Learning Rate - 0.05, magnitude of gradient - 0.02387148385102772\n",
      "Step - 452, Loss - 0.258889812713982, Learning Rate - 0.05, magnitude of gradient - 0.02347128230638823\n",
      "Step - 453, Loss - 0.3167402902493151, Learning Rate - 0.05, magnitude of gradient - 0.03609019116716176\n",
      "Step - 454, Loss - 0.3299175100772711, Learning Rate - 0.05, magnitude of gradient - 0.03470096694215349\n",
      "Step - 455, Loss - 0.322712983380457, Learning Rate - 0.05, magnitude of gradient - 0.051465154385978686\n",
      "Step - 456, Loss - 0.3246697993901595, Learning Rate - 0.05, magnitude of gradient - 0.16304802277353156\n",
      "Step - 457, Loss - 0.28606245214687825, Learning Rate - 0.05, magnitude of gradient - 0.1147010359098486\n",
      "Step - 458, Loss - 0.344637096252508, Learning Rate - 0.05, magnitude of gradient - 0.026178920163197957\n",
      "Step - 459, Loss - 0.28680603015788037, Learning Rate - 0.05, magnitude of gradient - 0.07362679754557636\n",
      "Step - 460, Loss - 0.24221387719643786, Learning Rate - 0.05, magnitude of gradient - 0.07462127618450323\n",
      "Step - 461, Loss - 0.32383567603510277, Learning Rate - 0.05, magnitude of gradient - 0.0961816231548266\n",
      "Step - 462, Loss - 0.3181286460379784, Learning Rate - 0.05, magnitude of gradient - 0.04345225880625522\n",
      "Step - 463, Loss - 0.2123957325349046, Learning Rate - 0.05, magnitude of gradient - 0.029470172208779784\n",
      "Step - 464, Loss - 0.3130589656114997, Learning Rate - 0.05, magnitude of gradient - 0.07612915450808763\n",
      "Step - 465, Loss - 0.26410903530808527, Learning Rate - 0.05, magnitude of gradient - 0.039776591328142395\n",
      "Step - 466, Loss - 0.34161865948612957, Learning Rate - 0.05, magnitude of gradient - 0.10226382890315881\n",
      "Step - 467, Loss - 0.30003207009599936, Learning Rate - 0.05, magnitude of gradient - 0.09823873519107768\n",
      "Step - 468, Loss - 0.3045606570535901, Learning Rate - 0.05, magnitude of gradient - 0.01841646496922909\n",
      "Step - 469, Loss - 0.30410600500137286, Learning Rate - 0.05, magnitude of gradient - 0.12514177643544927\n",
      "Step - 470, Loss - 0.27592821427142566, Learning Rate - 0.05, magnitude of gradient - 0.09639689131089767\n",
      "Step - 471, Loss - 0.30791207208448057, Learning Rate - 0.05, magnitude of gradient - 0.03915652467450944\n",
      "Step - 472, Loss - 0.2972726053150771, Learning Rate - 0.05, magnitude of gradient - 0.03127774968034957\n",
      "Step - 473, Loss - 0.2984752385342206, Learning Rate - 0.05, magnitude of gradient - 0.02453925188604125\n",
      "Step - 474, Loss - 0.2831029726631031, Learning Rate - 0.05, magnitude of gradient - 0.07792079537103674\n",
      "Step - 475, Loss - 0.28673307653757774, Learning Rate - 0.05, magnitude of gradient - 0.11741900590945009\n",
      "Step - 476, Loss - 0.34326752521501047, Learning Rate - 0.05, magnitude of gradient - 0.06812712529590943\n",
      "Step - 477, Loss - 0.3368729991238783, Learning Rate - 0.05, magnitude of gradient - 0.08310034188162133\n",
      "Step - 478, Loss - 0.36540662747011077, Learning Rate - 0.05, magnitude of gradient - 0.05546827104254981\n",
      "Step - 479, Loss - 0.30618549673995166, Learning Rate - 0.05, magnitude of gradient - 0.03937287073655793\n",
      "Step - 480, Loss - 0.33230584882891734, Learning Rate - 0.05, magnitude of gradient - 0.06117998937160767\n",
      "Step - 481, Loss - 0.3194331776483248, Learning Rate - 0.05, magnitude of gradient - 0.07445405169831532\n",
      "Step - 482, Loss - 0.3772840310341612, Learning Rate - 0.05, magnitude of gradient - 0.09110333223151748\n",
      "Step - 483, Loss - 0.36082491954490004, Learning Rate - 0.05, magnitude of gradient - 0.06933222696263339\n",
      "Step - 484, Loss - 0.2937517069042908, Learning Rate - 0.05, magnitude of gradient - 0.11902132090731199\n",
      "Step - 485, Loss - 0.381875667200939, Learning Rate - 0.05, magnitude of gradient - 0.11706328491595473\n",
      "Step - 486, Loss - 0.25759981320701203, Learning Rate - 0.05, magnitude of gradient - 0.05472165491982927\n",
      "Step - 487, Loss - 0.30197978753429855, Learning Rate - 0.05, magnitude of gradient - 0.025203192528792686\n",
      "Step - 488, Loss - 0.29550024242307893, Learning Rate - 0.05, magnitude of gradient - 0.06684416590923488\n",
      "Step - 489, Loss - 0.27080754736828905, Learning Rate - 0.05, magnitude of gradient - 0.12078349026957186\n",
      "Step - 490, Loss - 0.3005540845483272, Learning Rate - 0.05, magnitude of gradient - 0.04861028661942271\n",
      "Step - 491, Loss - 0.336515963840961, Learning Rate - 0.05, magnitude of gradient - 0.057291933604140914\n",
      "Step - 492, Loss - 0.2937248593523706, Learning Rate - 0.05, magnitude of gradient - 0.05906042099770234\n",
      "Step - 493, Loss - 0.2816809113608875, Learning Rate - 0.05, magnitude of gradient - 0.07974087330194891\n",
      "Step - 494, Loss - 0.25658940259220514, Learning Rate - 0.05, magnitude of gradient - 0.08888218656777894\n",
      "Step - 495, Loss - 0.35318660710482075, Learning Rate - 0.05, magnitude of gradient - 0.0882352257141172\n",
      "Step - 496, Loss - 0.4331601416827963, Learning Rate - 0.05, magnitude of gradient - 0.0767391314087671\n",
      "Step - 497, Loss - 0.31530893082008804, Learning Rate - 0.05, magnitude of gradient - 0.06714411457647863\n",
      "Step - 498, Loss - 0.28463981993101345, Learning Rate - 0.05, magnitude of gradient - 0.06558865457959012\n",
      "Step - 499, Loss - 0.312355773753591, Learning Rate - 0.05, magnitude of gradient - 0.06054862469791172\n",
      "Step - 500, Loss - 0.2701284066091775, Learning Rate - 0.05, magnitude of gradient - 0.10212155720530741\n",
      "Step - 501, Loss - 0.3204737388860047, Learning Rate - 0.05, magnitude of gradient - 0.05380192931592376\n",
      "Step - 502, Loss - 0.34061704269180765, Learning Rate - 0.05, magnitude of gradient - 0.034637242491786784\n",
      "Step - 503, Loss - 0.25395694934576957, Learning Rate - 0.05, magnitude of gradient - 0.025638159374948337\n",
      "Step - 504, Loss - 0.34716004210670276, Learning Rate - 0.05, magnitude of gradient - 0.050186321231237105\n",
      "Step - 505, Loss - 0.40324985238198185, Learning Rate - 0.05, magnitude of gradient - 0.057709269891358765\n",
      "Step - 506, Loss - 0.3097327843101552, Learning Rate - 0.05, magnitude of gradient - 0.052316098521223785\n",
      "Step - 507, Loss - 0.34657198210358175, Learning Rate - 0.05, magnitude of gradient - 0.03510777247082706\n",
      "Step - 508, Loss - 0.27753148984314635, Learning Rate - 0.05, magnitude of gradient - 0.02539576516553159\n",
      "Step - 509, Loss - 0.24737137262046083, Learning Rate - 0.05, magnitude of gradient - 0.021860877552047017\n",
      "Step - 510, Loss - 0.3545688616428385, Learning Rate - 0.05, magnitude of gradient - 0.04444490109819151\n",
      "Step - 511, Loss - 0.2923083933209179, Learning Rate - 0.05, magnitude of gradient - 0.0491091723746395\n",
      "Step - 512, Loss - 0.3354432656842959, Learning Rate - 0.05, magnitude of gradient - 0.030479895977730702\n",
      "Step - 513, Loss - 0.28482101905536855, Learning Rate - 0.05, magnitude of gradient - 0.02455388430460078\n",
      "Step - 514, Loss - 0.2763418047823236, Learning Rate - 0.05, magnitude of gradient - 0.08838533586027005\n",
      "Step - 515, Loss - 0.2918116811975522, Learning Rate - 0.05, magnitude of gradient - 0.026564133240215674\n",
      "Step - 516, Loss - 0.3574830381660793, Learning Rate - 0.05, magnitude of gradient - 0.06292410658261458\n",
      "Step - 517, Loss - 0.3319251843847866, Learning Rate - 0.05, magnitude of gradient - 0.0693792628101064\n",
      "Step - 518, Loss - 0.3900900087648412, Learning Rate - 0.05, magnitude of gradient - 0.030518885761350346\n",
      "Step - 519, Loss - 0.4100494709165449, Learning Rate - 0.05, magnitude of gradient - 0.08924663103928908\n",
      "Step - 520, Loss - 0.37460164828030074, Learning Rate - 0.05, magnitude of gradient - 0.09080117984325567\n",
      "Step - 521, Loss - 0.3110912639327421, Learning Rate - 0.05, magnitude of gradient - 0.03202048457200408\n",
      "Step - 522, Loss - 0.29179244521956815, Learning Rate - 0.05, magnitude of gradient - 0.03451870937750155\n",
      "Step - 523, Loss - 0.2570954491130272, Learning Rate - 0.05, magnitude of gradient - 0.02394633243410697\n",
      "Step - 524, Loss - 0.32462806230552943, Learning Rate - 0.05, magnitude of gradient - 0.023822223680341054\n",
      "Step - 525, Loss - 0.4682127802435894, Learning Rate - 0.05, magnitude of gradient - 0.1412037634494713\n",
      "Step - 526, Loss - 0.32672345999152086, Learning Rate - 0.05, magnitude of gradient - 0.11948948917523679\n",
      "Step - 527, Loss - 0.39134323324288955, Learning Rate - 0.05, magnitude of gradient - 0.04802910639361448\n",
      "Step - 528, Loss - 0.3318736257205473, Learning Rate - 0.05, magnitude of gradient - 0.06386018560648671\n",
      "Step - 529, Loss - 0.28026341388721765, Learning Rate - 0.05, magnitude of gradient - 0.019492352235736126\n",
      "Step - 530, Loss - 0.319213371209228, Learning Rate - 0.05, magnitude of gradient - 0.059927805581465966\n",
      "Step - 531, Loss - 0.3116692670490755, Learning Rate - 0.05, magnitude of gradient - 0.06910839619187359\n",
      "Step - 532, Loss - 0.3199704041754933, Learning Rate - 0.05, magnitude of gradient - 0.03285460113462264\n",
      "Step - 533, Loss - 0.3211192035481826, Learning Rate - 0.05, magnitude of gradient - 0.07057788232858124\n",
      "Step - 534, Loss - 0.3444063201466592, Learning Rate - 0.05, magnitude of gradient - 0.03381031527022878\n",
      "Step - 535, Loss - 0.34513008075779955, Learning Rate - 0.05, magnitude of gradient - 0.07151598400956093\n",
      "Step - 536, Loss - 0.36333292366059233, Learning Rate - 0.05, magnitude of gradient - 0.017806871943087187\n",
      "Step - 537, Loss - 0.286490646834976, Learning Rate - 0.05, magnitude of gradient - 0.014005817259575266\n",
      "Step - 538, Loss - 0.33520188112538346, Learning Rate - 0.05, magnitude of gradient - 0.07613061219348721\n",
      "Step - 539, Loss - 0.3524958109236071, Learning Rate - 0.05, magnitude of gradient - 0.023271209401199804\n",
      "Step - 540, Loss - 0.42123775356875864, Learning Rate - 0.05, magnitude of gradient - 0.09614816539434823\n",
      "Step - 541, Loss - 0.3224067557486574, Learning Rate - 0.05, magnitude of gradient - 0.02086701442972324\n",
      "Step - 542, Loss - 0.3778870763319385, Learning Rate - 0.05, magnitude of gradient - 0.10354129029088645\n",
      "Step - 543, Loss - 0.4136765199629795, Learning Rate - 0.05, magnitude of gradient - 0.07045883713479947\n",
      "Step - 544, Loss - 0.35261853692163986, Learning Rate - 0.05, magnitude of gradient - 0.06678512902414502\n",
      "Step - 545, Loss - 0.260484514210519, Learning Rate - 0.05, magnitude of gradient - 0.030391039079477045\n",
      "Step - 546, Loss - 0.28690812524877585, Learning Rate - 0.05, magnitude of gradient - 0.06312735874153455\n",
      "Step - 547, Loss - 0.22786594168719795, Learning Rate - 0.05, magnitude of gradient - 0.06093192450829497\n",
      "Step - 548, Loss - 0.2788981110601636, Learning Rate - 0.05, magnitude of gradient - 0.03339708034892081\n",
      "Step - 549, Loss - 0.30078330522332347, Learning Rate - 0.05, magnitude of gradient - 0.11319085414793421\n",
      "Step - 550, Loss - 0.36144670001757717, Learning Rate - 0.05, magnitude of gradient - 0.02243268435639074\n",
      "Step - 551, Loss - 0.32451400713812406, Learning Rate - 0.05, magnitude of gradient - 0.04099539829089049\n",
      "Step - 552, Loss - 0.3295751204559536, Learning Rate - 0.05, magnitude of gradient - 0.06454413895688527\n",
      "Step - 553, Loss - 0.3327372103233399, Learning Rate - 0.05, magnitude of gradient - 0.0245065544371023\n",
      "Step - 554, Loss - 0.3101630200817109, Learning Rate - 0.05, magnitude of gradient - 0.08204417892006384\n",
      "Step - 555, Loss - 0.2998889751663428, Learning Rate - 0.05, magnitude of gradient - 0.06522416136886407\n",
      "Step - 556, Loss - 0.32235301327303223, Learning Rate - 0.05, magnitude of gradient - 0.08246106306007467\n",
      "Step - 557, Loss - 0.2277284045426501, Learning Rate - 0.05, magnitude of gradient - 0.08129627694580906\n",
      "Step - 558, Loss - 0.2897528796861174, Learning Rate - 0.05, magnitude of gradient - 0.027875832309767618\n",
      "Step - 559, Loss - 0.3759409143776635, Learning Rate - 0.05, magnitude of gradient - 0.05886576051393882\n",
      "Step - 560, Loss - 0.3199201207590339, Learning Rate - 0.05, magnitude of gradient - 0.04406268325942973\n",
      "Step - 561, Loss - 0.2568301509104282, Learning Rate - 0.05, magnitude of gradient - 0.0578914731230847\n",
      "Step - 562, Loss - 0.29871211739387404, Learning Rate - 0.05, magnitude of gradient - 0.0481232443086709\n",
      "Step - 563, Loss - 0.37181459876219186, Learning Rate - 0.05, magnitude of gradient - 0.0060930571552634295\n",
      "Step - 564, Loss - 0.41290757943428846, Learning Rate - 0.05, magnitude of gradient - 0.037869787340821526\n",
      "Step - 565, Loss - 0.3023028503228205, Learning Rate - 0.05, magnitude of gradient - 0.0457404248208754\n",
      "Step - 566, Loss - 0.3070110631801639, Learning Rate - 0.05, magnitude of gradient - 0.016833459642984206\n",
      "Step - 567, Loss - 0.379080355855245, Learning Rate - 0.05, magnitude of gradient - 0.048412064055441704\n",
      "Step - 568, Loss - 0.34530160543904953, Learning Rate - 0.05, magnitude of gradient - 0.08031263104743068\n",
      "Step - 569, Loss - 0.3229851553313512, Learning Rate - 0.05, magnitude of gradient - 0.01207257512957963\n",
      "Step - 570, Loss - 0.3090267373868341, Learning Rate - 0.05, magnitude of gradient - 0.07341600929731727\n",
      "Step - 571, Loss - 0.30566835255274494, Learning Rate - 0.05, magnitude of gradient - 0.07697241292049947\n",
      "Step - 572, Loss - 0.2922063493383198, Learning Rate - 0.05, magnitude of gradient - 0.1708206316751133\n",
      "Step - 573, Loss - 0.3809011543906317, Learning Rate - 0.05, magnitude of gradient - 0.030787218226038887\n",
      "Step - 574, Loss - 0.2670911602930306, Learning Rate - 0.05, magnitude of gradient - 0.06872896067438215\n",
      "Step - 575, Loss - 0.3674434979531861, Learning Rate - 0.05, magnitude of gradient - 0.018343193875098474\n",
      "Step - 576, Loss - 0.3517787273326822, Learning Rate - 0.05, magnitude of gradient - 0.041123212338798815\n",
      "Step - 577, Loss - 0.3213639905060982, Learning Rate - 0.05, magnitude of gradient - 0.02473535714234976\n",
      "Step - 578, Loss - 0.3695777124929921, Learning Rate - 0.05, magnitude of gradient - 0.04208429059535345\n",
      "Step - 579, Loss - 0.20437929435020116, Learning Rate - 0.05, magnitude of gradient - 0.017010560465792187\n",
      "Step - 580, Loss - 0.367072917196375, Learning Rate - 0.05, magnitude of gradient - 0.05676331999536182\n",
      "Step - 581, Loss - 0.29326060587003144, Learning Rate - 0.05, magnitude of gradient - 0.05419757771599486\n",
      "Step - 582, Loss - 0.40342865334764133, Learning Rate - 0.05, magnitude of gradient - 0.03557420848349163\n",
      "Step - 583, Loss - 0.34162487018081794, Learning Rate - 0.05, magnitude of gradient - 0.03947924405634715\n",
      "Step - 584, Loss - 0.3294088950550436, Learning Rate - 0.05, magnitude of gradient - 0.10344293760896917\n",
      "Step - 585, Loss - 0.3250424136483147, Learning Rate - 0.05, magnitude of gradient - 0.029103212113842487\n",
      "Step - 586, Loss - 0.32252151017487574, Learning Rate - 0.05, magnitude of gradient - 0.1100406531386451\n",
      "Step - 587, Loss - 0.30651941819703, Learning Rate - 0.05, magnitude of gradient - 0.0502513124429018\n",
      "Step - 588, Loss - 0.3125027655366016, Learning Rate - 0.05, magnitude of gradient - 0.05027301521924382\n",
      "Step - 589, Loss - 0.3796292401453657, Learning Rate - 0.05, magnitude of gradient - 0.0486249478595405\n",
      "Step - 590, Loss - 0.33179857222893566, Learning Rate - 0.05, magnitude of gradient - 0.10689490867115406\n",
      "Step - 591, Loss - 0.3139437684709941, Learning Rate - 0.05, magnitude of gradient - 0.03619161270584619\n",
      "Step - 592, Loss - 0.337845441549798, Learning Rate - 0.05, magnitude of gradient - 0.02237118898957083\n",
      "Step - 593, Loss - 0.36989215191368047, Learning Rate - 0.05, magnitude of gradient - 0.11950105603733602\n",
      "Step - 594, Loss - 0.30116014340369335, Learning Rate - 0.05, magnitude of gradient - 0.04286202908819947\n",
      "Step - 595, Loss - 0.3764660409127073, Learning Rate - 0.05, magnitude of gradient - 0.09604649002110131\n",
      "Step - 596, Loss - 0.28102303450095234, Learning Rate - 0.05, magnitude of gradient - 0.030446997320482574\n",
      "Step - 597, Loss - 0.3301370338150421, Learning Rate - 0.05, magnitude of gradient - 0.036695679171551386\n",
      "Step - 598, Loss - 0.2955245504022106, Learning Rate - 0.05, magnitude of gradient - 0.028171019792563236\n",
      "Step - 599, Loss - 0.3431658946968801, Learning Rate - 0.05, magnitude of gradient - 0.047998781111454905\n",
      "Step - 600, Loss - 0.30233873704993186, Learning Rate - 0.05, magnitude of gradient - 0.055661765297821866\n",
      "Step - 601, Loss - 0.35060883431520407, Learning Rate - 0.05, magnitude of gradient - 0.0440374932136461\n",
      "Step - 602, Loss - 0.25353121171501924, Learning Rate - 0.05, magnitude of gradient - 0.04965147195820552\n",
      "Step - 603, Loss - 0.2741000761665184, Learning Rate - 0.05, magnitude of gradient - 0.020596705037241363\n",
      "Step - 604, Loss - 0.3756428687323535, Learning Rate - 0.05, magnitude of gradient - 0.01863326635935076\n",
      "Step - 605, Loss - 0.33257896562476374, Learning Rate - 0.05, magnitude of gradient - 0.01838603999129394\n",
      "Step - 606, Loss - 0.250763278215795, Learning Rate - 0.05, magnitude of gradient - 0.054197518033724704\n",
      "Step - 607, Loss - 0.31812899131311234, Learning Rate - 0.05, magnitude of gradient - 0.09244283037096253\n",
      "Step - 608, Loss - 0.28369638552815013, Learning Rate - 0.05, magnitude of gradient - 0.01573179984023488\n",
      "Step - 609, Loss - 0.31862570231129556, Learning Rate - 0.05, magnitude of gradient - 0.06879963593289222\n",
      "Step - 610, Loss - 0.2737686532917472, Learning Rate - 0.05, magnitude of gradient - 0.02306550146397472\n",
      "Step - 611, Loss - 0.31830641802961046, Learning Rate - 0.05, magnitude of gradient - 0.04265792327647262\n",
      "Step - 612, Loss - 0.324470279485655, Learning Rate - 0.05, magnitude of gradient - 0.044253059233651124\n",
      "Step - 613, Loss - 0.28697325093150367, Learning Rate - 0.05, magnitude of gradient - 0.0513102070114792\n",
      "Step - 614, Loss - 0.3073712741489264, Learning Rate - 0.05, magnitude of gradient - 0.0796811823647363\n",
      "Step - 615, Loss - 0.25692068363268655, Learning Rate - 0.05, magnitude of gradient - 0.06308147272291528\n",
      "Step - 616, Loss - 0.28832068474429345, Learning Rate - 0.05, magnitude of gradient - 0.040974981873914684\n",
      "Step - 617, Loss - 0.2554651035257842, Learning Rate - 0.05, magnitude of gradient - 0.0319627963921296\n",
      "Step - 618, Loss - 0.39254994145247146, Learning Rate - 0.05, magnitude of gradient - 0.10513083165371148\n",
      "Step - 619, Loss - 0.23479338846042103, Learning Rate - 0.05, magnitude of gradient - 0.06156012429892392\n",
      "Step - 620, Loss - 0.25461730088838674, Learning Rate - 0.05, magnitude of gradient - 0.057882767650149075\n",
      "Step - 621, Loss - 0.32734007019953637, Learning Rate - 0.05, magnitude of gradient - 0.04395817409725912\n",
      "Step - 622, Loss - 0.359843961975975, Learning Rate - 0.05, magnitude of gradient - 0.03178921338861862\n",
      "Step - 623, Loss - 0.3608549239121472, Learning Rate - 0.05, magnitude of gradient - 0.05432204484916774\n",
      "Step - 624, Loss - 0.32619884998409715, Learning Rate - 0.05, magnitude of gradient - 0.0680192637822148\n",
      "Step - 625, Loss - 0.3036179071856311, Learning Rate - 0.05, magnitude of gradient - 0.11148379848988108\n",
      "Step - 626, Loss - 0.2500450485809539, Learning Rate - 0.05, magnitude of gradient - 0.0032140015379491663\n",
      "Step - 627, Loss - 0.27209062310944154, Learning Rate - 0.05, magnitude of gradient - 0.05627445833747053\n",
      "Step - 628, Loss - 0.24840589094786292, Learning Rate - 0.05, magnitude of gradient - 0.022657092504430022\n",
      "Step - 629, Loss - 0.37522197693558346, Learning Rate - 0.05, magnitude of gradient - 0.05952299057039051\n",
      "Step - 630, Loss - 0.4310187869681096, Learning Rate - 0.05, magnitude of gradient - 0.04918142094419101\n",
      "Step - 631, Loss - 0.29544996267191426, Learning Rate - 0.05, magnitude of gradient - 0.09124600127093391\n",
      "Step - 632, Loss - 0.31706476675498574, Learning Rate - 0.05, magnitude of gradient - 0.06980987051824047\n",
      "Step - 633, Loss - 0.37560449548176333, Learning Rate - 0.05, magnitude of gradient - 0.06393116925767\n",
      "Step - 634, Loss - 0.3411403945098062, Learning Rate - 0.05, magnitude of gradient - 0.030414401657379447\n",
      "Step - 635, Loss - 0.3685130262674439, Learning Rate - 0.05, magnitude of gradient - 0.05973169122700181\n",
      "Step - 636, Loss - 0.31977684456947686, Learning Rate - 0.05, magnitude of gradient - 0.07562079106630887\n",
      "Step - 637, Loss - 0.33120702246832884, Learning Rate - 0.05, magnitude of gradient - 0.03761873199605505\n",
      "Step - 638, Loss - 0.3375599399125342, Learning Rate - 0.05, magnitude of gradient - 0.04010990458122899\n",
      "Step - 639, Loss - 0.34309955680363063, Learning Rate - 0.05, magnitude of gradient - 0.025735552397899056\n",
      "Step - 640, Loss - 0.33726753868154025, Learning Rate - 0.05, magnitude of gradient - 0.08530123773315566\n",
      "Step - 641, Loss - 0.29433674530387527, Learning Rate - 0.05, magnitude of gradient - 0.10203276577744333\n",
      "Step - 642, Loss - 0.3131226711334033, Learning Rate - 0.05, magnitude of gradient - 0.02033628674541031\n",
      "Step - 643, Loss - 0.25543858615309756, Learning Rate - 0.05, magnitude of gradient - 0.04784488903435815\n",
      "Step - 644, Loss - 0.3131243473657356, Learning Rate - 0.05, magnitude of gradient - 0.05004098350653738\n",
      "Step - 645, Loss - 0.3420070245303187, Learning Rate - 0.05, magnitude of gradient - 0.0463959450059706\n",
      "Step - 646, Loss - 0.37801054734796935, Learning Rate - 0.05, magnitude of gradient - 0.0552627338852834\n",
      "Step - 647, Loss - 0.2656577475678956, Learning Rate - 0.05, magnitude of gradient - 0.058234749540176815\n",
      "Step - 648, Loss - 0.3979609367102035, Learning Rate - 0.05, magnitude of gradient - 0.12602842224635072\n",
      "Step - 649, Loss - 0.31571531115113827, Learning Rate - 0.05, magnitude of gradient - 0.04708780366166012\n",
      "Step - 650, Loss - 0.37619832591154634, Learning Rate - 0.05, magnitude of gradient - 0.06003484672552129\n",
      "Step - 651, Loss - 0.35924030106673027, Learning Rate - 0.05, magnitude of gradient - 0.08536470859719243\n",
      "Step - 652, Loss - 0.29890779839564663, Learning Rate - 0.05, magnitude of gradient - 0.0478970081772353\n",
      "Step - 653, Loss - 0.26034068023630325, Learning Rate - 0.05, magnitude of gradient - 0.06401889960622\n",
      "Step - 654, Loss - 0.35485066267238063, Learning Rate - 0.05, magnitude of gradient - 0.07685116445888585\n",
      "Step - 655, Loss - 0.3824975555416482, Learning Rate - 0.05, magnitude of gradient - 0.028948064159707028\n",
      "Step - 656, Loss - 0.3102991082847517, Learning Rate - 0.05, magnitude of gradient - 0.034900834464810335\n",
      "Step - 657, Loss - 0.26811748372370225, Learning Rate - 0.05, magnitude of gradient - 0.06494464643932736\n",
      "Step - 658, Loss - 0.35638891425410907, Learning Rate - 0.05, magnitude of gradient - 0.10028676377146642\n",
      "Step - 659, Loss - 0.3058182443777391, Learning Rate - 0.05, magnitude of gradient - 0.02059137699315352\n",
      "Step - 660, Loss - 0.35246223460888654, Learning Rate - 0.05, magnitude of gradient - 0.06167847745087793\n",
      "Step - 661, Loss - 0.4079224807896179, Learning Rate - 0.05, magnitude of gradient - 0.09932454955261381\n",
      "Step - 662, Loss - 0.2629512582669521, Learning Rate - 0.05, magnitude of gradient - 0.051625904508346566\n",
      "Step - 663, Loss - 0.25585698725423456, Learning Rate - 0.05, magnitude of gradient - 0.03723169727487032\n",
      "Step - 664, Loss - 0.37854751171665285, Learning Rate - 0.05, magnitude of gradient - 0.08686761791738992\n",
      "Step - 665, Loss - 0.35697188789001, Learning Rate - 0.05, magnitude of gradient - 0.08548427561265662\n",
      "Step - 666, Loss - 0.3770226101137065, Learning Rate - 0.05, magnitude of gradient - 0.07543775592003402\n",
      "Step - 667, Loss - 0.34020653416864194, Learning Rate - 0.05, magnitude of gradient - 0.06144765854998709\n",
      "Step - 668, Loss - 0.37351395421818656, Learning Rate - 0.05, magnitude of gradient - 0.06127133009030442\n",
      "Step - 669, Loss - 0.401835225942496, Learning Rate - 0.05, magnitude of gradient - 0.032779678665402295\n",
      "Step - 670, Loss - 0.331791569857927, Learning Rate - 0.05, magnitude of gradient - 0.038046053197698666\n",
      "Step - 671, Loss - 0.2913782882416695, Learning Rate - 0.05, magnitude of gradient - 0.043451311246779825\n",
      "Step - 672, Loss - 0.32142308059150143, Learning Rate - 0.05, magnitude of gradient - 0.02496187320413063\n",
      "Step - 673, Loss - 0.46367831050997343, Learning Rate - 0.05, magnitude of gradient - 0.08151122300500374\n",
      "Step - 674, Loss - 0.3185950775753525, Learning Rate - 0.05, magnitude of gradient - 0.09293698785002845\n",
      "Step - 675, Loss - 0.30629529995707394, Learning Rate - 0.05, magnitude of gradient - 0.03403605291810914\n",
      "Step - 676, Loss - 0.23701113753123695, Learning Rate - 0.05, magnitude of gradient - 0.022456102925553237\n",
      "Step - 677, Loss - 0.30635531883443184, Learning Rate - 0.05, magnitude of gradient - 0.14108957718658793\n",
      "Step - 678, Loss - 0.37815375047493205, Learning Rate - 0.05, magnitude of gradient - 0.046512747503608766\n",
      "Step - 679, Loss - 0.2841111806798438, Learning Rate - 0.05, magnitude of gradient - 0.018300509156958283\n",
      "Step - 680, Loss - 0.329181069382755, Learning Rate - 0.05, magnitude of gradient - 0.07612909322672133\n",
      "Step - 681, Loss - 0.31965853592786736, Learning Rate - 0.05, magnitude of gradient - 0.06335621905587448\n",
      "Step - 682, Loss - 0.3106961700360957, Learning Rate - 0.05, magnitude of gradient - 0.10061857954419219\n",
      "Step - 683, Loss - 0.3127117523025091, Learning Rate - 0.05, magnitude of gradient - 0.04704775222459372\n",
      "Step - 684, Loss - 0.34576662580694667, Learning Rate - 0.05, magnitude of gradient - 0.07867961122981841\n",
      "Step - 685, Loss - 0.28365177060673874, Learning Rate - 0.05, magnitude of gradient - 0.033268163558055557\n",
      "Step - 686, Loss - 0.31636179039893736, Learning Rate - 0.05, magnitude of gradient - 0.0994511256286042\n",
      "Step - 687, Loss - 0.3369535595306719, Learning Rate - 0.05, magnitude of gradient - 0.041628393769612755\n",
      "Step - 688, Loss - 0.3135132205739912, Learning Rate - 0.05, magnitude of gradient - 0.050748307234267674\n",
      "Step - 689, Loss - 0.31596375121984743, Learning Rate - 0.05, magnitude of gradient - 0.12514938871830558\n",
      "Step - 690, Loss - 0.32306036294295926, Learning Rate - 0.05, magnitude of gradient - 0.025022196482700633\n",
      "Step - 691, Loss - 0.2995830475078134, Learning Rate - 0.05, magnitude of gradient - 0.07459743472973487\n",
      "Step - 692, Loss - 0.3174289809093841, Learning Rate - 0.05, magnitude of gradient - 0.10470940741024923\n",
      "Step - 693, Loss - 0.2739593720573587, Learning Rate - 0.05, magnitude of gradient - 0.0656972719667712\n",
      "Step - 694, Loss - 0.2550342707935728, Learning Rate - 0.05, magnitude of gradient - 0.076652708801619\n",
      "Step - 695, Loss - 0.2667327702919271, Learning Rate - 0.05, magnitude of gradient - 0.05555535944540238\n",
      "Step - 696, Loss - 0.33586647989486695, Learning Rate - 0.05, magnitude of gradient - 0.02855744137745617\n",
      "Step - 697, Loss - 0.272906724140898, Learning Rate - 0.05, magnitude of gradient - 0.04430046328339434\n",
      "Step - 698, Loss - 0.34225243722133025, Learning Rate - 0.05, magnitude of gradient - 0.03016573164112132\n",
      "Step - 699, Loss - 0.32751660771157337, Learning Rate - 0.05, magnitude of gradient - 0.08232316069139417\n",
      "Step - 700, Loss - 0.3373653374797296, Learning Rate - 0.05, magnitude of gradient - 0.04803897372903043\n",
      "Step - 701, Loss - 0.3135384351544953, Learning Rate - 0.05, magnitude of gradient - 0.06884551605470746\n",
      "Step - 702, Loss - 0.3742731297431194, Learning Rate - 0.05, magnitude of gradient - 0.10231625040881974\n",
      "Step - 703, Loss - 0.3524803584251635, Learning Rate - 0.05, magnitude of gradient - 0.05603074844469969\n",
      "Step - 704, Loss - 0.380728833804455, Learning Rate - 0.05, magnitude of gradient - 0.09670850612732021\n",
      "Step - 705, Loss - 0.3337615703767792, Learning Rate - 0.05, magnitude of gradient - 0.05824087366718131\n",
      "Step - 706, Loss - 0.29604420057692615, Learning Rate - 0.05, magnitude of gradient - 0.15856897951721013\n",
      "Step - 707, Loss - 0.2776466113724121, Learning Rate - 0.05, magnitude of gradient - 0.016674640563200185\n",
      "Step - 708, Loss - 0.31929152112469233, Learning Rate - 0.05, magnitude of gradient - 0.036009041431195946\n",
      "Step - 709, Loss - 0.2757266868976514, Learning Rate - 0.05, magnitude of gradient - 0.09974548491984593\n",
      "Step - 710, Loss - 0.3542061444624249, Learning Rate - 0.05, magnitude of gradient - 0.10523748323641764\n",
      "Step - 711, Loss - 0.2346948755185273, Learning Rate - 0.05, magnitude of gradient - 0.06945217565245726\n",
      "Step - 712, Loss - 0.3146545940713764, Learning Rate - 0.05, magnitude of gradient - 0.04715708640441535\n",
      "Step - 713, Loss - 0.3615487177372489, Learning Rate - 0.05, magnitude of gradient - 0.10322612300833613\n",
      "Step - 714, Loss - 0.32018564680327455, Learning Rate - 0.05, magnitude of gradient - 0.09670567606285457\n",
      "Step - 715, Loss - 0.30719902853436587, Learning Rate - 0.05, magnitude of gradient - 0.06813212700555668\n",
      "Step - 716, Loss - 0.30750990811627577, Learning Rate - 0.05, magnitude of gradient - 0.05981836597647637\n",
      "Step - 717, Loss - 0.30228128138114313, Learning Rate - 0.05, magnitude of gradient - 0.045577826079099694\n",
      "Step - 718, Loss - 0.35990002968516244, Learning Rate - 0.05, magnitude of gradient - 0.06132797278702395\n",
      "Step - 719, Loss - 0.2692131693368789, Learning Rate - 0.05, magnitude of gradient - 0.03132345796528595\n",
      "Step - 720, Loss - 0.27617492722255216, Learning Rate - 0.05, magnitude of gradient - 0.07223344352931695\n",
      "Step - 721, Loss - 0.3053840328879514, Learning Rate - 0.05, magnitude of gradient - 0.13130807226147873\n",
      "Step - 722, Loss - 0.3292315100808088, Learning Rate - 0.05, magnitude of gradient - 0.058701191418627154\n",
      "Step - 723, Loss - 0.2966413934881093, Learning Rate - 0.05, magnitude of gradient - 0.10612770696154238\n",
      "Step - 724, Loss - 0.34688712130095173, Learning Rate - 0.05, magnitude of gradient - 0.08980344241325862\n",
      "Step - 725, Loss - 0.3254613604607586, Learning Rate - 0.05, magnitude of gradient - 0.044877866911333025\n",
      "Step - 726, Loss - 0.4323063623237927, Learning Rate - 0.05, magnitude of gradient - 0.030772520353342353\n",
      "Step - 727, Loss - 0.36049020020750905, Learning Rate - 0.05, magnitude of gradient - 0.05677639333691656\n",
      "Step - 728, Loss - 0.3125339114758949, Learning Rate - 0.05, magnitude of gradient - 0.048823847860983774\n",
      "Step - 729, Loss - 0.3177865025961138, Learning Rate - 0.05, magnitude of gradient - 0.05848047540025972\n",
      "Step - 730, Loss - 0.3231627303745005, Learning Rate - 0.05, magnitude of gradient - 0.010423225353593923\n",
      "Step - 731, Loss - 0.3153984313776835, Learning Rate - 0.05, magnitude of gradient - 0.0728415492993687\n",
      "Step - 732, Loss - 0.28224777065231926, Learning Rate - 0.05, magnitude of gradient - 0.055903997286429606\n",
      "Step - 733, Loss - 0.2927390280279629, Learning Rate - 0.05, magnitude of gradient - 0.022605493261534507\n",
      "Step - 734, Loss - 0.28839791768040063, Learning Rate - 0.05, magnitude of gradient - 0.07838512095670037\n",
      "Step - 735, Loss - 0.3140280221634626, Learning Rate - 0.05, magnitude of gradient - 0.027369126142815996\n",
      "Step - 736, Loss - 0.4125836540691301, Learning Rate - 0.05, magnitude of gradient - 0.08044567084492114\n",
      "Step - 737, Loss - 0.4222162399113109, Learning Rate - 0.05, magnitude of gradient - 0.043666760879632914\n",
      "Step - 738, Loss - 0.3323491667505395, Learning Rate - 0.05, magnitude of gradient - 0.03162395037899783\n",
      "Step - 739, Loss - 0.28995222840246226, Learning Rate - 0.05, magnitude of gradient - 0.01494282205454674\n",
      "Step - 740, Loss - 0.2926375631204532, Learning Rate - 0.05, magnitude of gradient - 0.055409015065302686\n",
      "Step - 741, Loss - 0.2779433176157291, Learning Rate - 0.05, magnitude of gradient - 0.04586961360742904\n",
      "Step - 742, Loss - 0.39136655218075395, Learning Rate - 0.05, magnitude of gradient - 0.06734967392701731\n",
      "Step - 743, Loss - 0.3226899771344957, Learning Rate - 0.05, magnitude of gradient - 0.048272273365831786\n",
      "Step - 744, Loss - 0.29478884275257744, Learning Rate - 0.05, magnitude of gradient - 0.020133758579020635\n",
      "Step - 745, Loss - 0.22838741090234413, Learning Rate - 0.05, magnitude of gradient - 0.052548625732619446\n",
      "Step - 746, Loss - 0.26052610441107693, Learning Rate - 0.05, magnitude of gradient - 0.03953536106450081\n",
      "Step - 747, Loss - 0.35054424954011215, Learning Rate - 0.05, magnitude of gradient - 0.03911192792790312\n",
      "Step - 748, Loss - 0.28962236668203306, Learning Rate - 0.05, magnitude of gradient - 0.05053048501769713\n",
      "Step - 749, Loss - 0.3254145539613693, Learning Rate - 0.05, magnitude of gradient - 0.05454774819092415\n",
      "Step - 750, Loss - 0.357996547452059, Learning Rate - 0.05, magnitude of gradient - 0.023512592657167136\n",
      "Step - 751, Loss - 0.39752059473987444, Learning Rate - 0.05, magnitude of gradient - 0.034200119276306486\n",
      "Step - 752, Loss - 0.37716922096067523, Learning Rate - 0.05, magnitude of gradient - 0.044089431739153716\n",
      "Step - 753, Loss - 0.30570468713568144, Learning Rate - 0.05, magnitude of gradient - 0.02681789859462384\n",
      "Step - 754, Loss - 0.3163811653285322, Learning Rate - 0.05, magnitude of gradient - 0.055669408663610045\n",
      "Step - 755, Loss - 0.3478077492826061, Learning Rate - 0.05, magnitude of gradient - 0.05301000564738789\n",
      "Step - 756, Loss - 0.3636175725396365, Learning Rate - 0.05, magnitude of gradient - 0.1358478816929646\n",
      "Step - 757, Loss - 0.24717078865253134, Learning Rate - 0.05, magnitude of gradient - 0.025309210479566347\n",
      "Step - 758, Loss - 0.3220167595639615, Learning Rate - 0.05, magnitude of gradient - 0.045764507031585605\n",
      "Step - 759, Loss - 0.42922463489850393, Learning Rate - 0.05, magnitude of gradient - 0.047348646386820646\n",
      "Step - 760, Loss - 0.2816778929892347, Learning Rate - 0.05, magnitude of gradient - 0.09926430731973532\n",
      "Step - 761, Loss - 0.2701738536503174, Learning Rate - 0.05, magnitude of gradient - 0.037198861257215006\n",
      "Step - 762, Loss - 0.334204617806177, Learning Rate - 0.05, magnitude of gradient - 0.05121934667750655\n",
      "Step - 763, Loss - 0.33427247204691646, Learning Rate - 0.05, magnitude of gradient - 0.04654329734338014\n",
      "Step - 764, Loss - 0.3514462686642025, Learning Rate - 0.05, magnitude of gradient - 0.08465244106135715\n",
      "Step - 765, Loss - 0.40706111649478316, Learning Rate - 0.05, magnitude of gradient - 0.044706255316217164\n",
      "Step - 766, Loss - 0.34361171793807016, Learning Rate - 0.05, magnitude of gradient - 0.12097485790526996\n",
      "Step - 767, Loss - 0.3321256013050998, Learning Rate - 0.05, magnitude of gradient - 0.07120464638574547\n",
      "Step - 768, Loss - 0.28341214791045705, Learning Rate - 0.05, magnitude of gradient - 0.06104781856141084\n",
      "Step - 769, Loss - 0.26915006931925156, Learning Rate - 0.05, magnitude of gradient - 0.05265490515259963\n",
      "Step - 770, Loss - 0.3161625877178985, Learning Rate - 0.05, magnitude of gradient - 0.03417863424693983\n",
      "Step - 771, Loss - 0.2819314077236569, Learning Rate - 0.05, magnitude of gradient - 0.0368032393118804\n",
      "Step - 772, Loss - 0.3315036929179144, Learning Rate - 0.05, magnitude of gradient - 0.024771637100420945\n",
      "Step - 773, Loss - 0.3395784045926262, Learning Rate - 0.05, magnitude of gradient - 0.04736550860654766\n",
      "Step - 774, Loss - 0.3120422302353602, Learning Rate - 0.05, magnitude of gradient - 0.03671866780300647\n",
      "Step - 775, Loss - 0.2888385264230403, Learning Rate - 0.05, magnitude of gradient - 0.06285812843764108\n",
      "Step - 776, Loss - 0.34111858160748926, Learning Rate - 0.05, magnitude of gradient - 0.0446904411833324\n",
      "Step - 777, Loss - 0.3164183997969947, Learning Rate - 0.05, magnitude of gradient - 0.13846116978396886\n",
      "Step - 778, Loss - 0.34929907564451795, Learning Rate - 0.05, magnitude of gradient - 0.09919300307799181\n",
      "Step - 779, Loss - 0.3023345082289392, Learning Rate - 0.05, magnitude of gradient - 0.056096278151881424\n",
      "Step - 780, Loss - 0.3325123403628744, Learning Rate - 0.05, magnitude of gradient - 0.04621068879419924\n",
      "Step - 781, Loss - 0.34142614235569807, Learning Rate - 0.05, magnitude of gradient - 0.07745890328553759\n",
      "Step - 782, Loss - 0.33393517963421515, Learning Rate - 0.05, magnitude of gradient - 0.05095635748685241\n",
      "Step - 783, Loss - 0.33082255309091363, Learning Rate - 0.05, magnitude of gradient - 0.06276234026956438\n",
      "Step - 784, Loss - 0.3224819072563006, Learning Rate - 0.05, magnitude of gradient - 0.05656228528408803\n",
      "Step - 785, Loss - 0.3597106865319164, Learning Rate - 0.05, magnitude of gradient - 0.06037166008531202\n",
      "Step - 786, Loss - 0.29842183108093806, Learning Rate - 0.05, magnitude of gradient - 0.0584898423127073\n",
      "Step - 787, Loss - 0.3168627559755831, Learning Rate - 0.05, magnitude of gradient - 0.03963414681343968\n",
      "Step - 788, Loss - 0.36253113587809416, Learning Rate - 0.05, magnitude of gradient - 0.06169976740258484\n",
      "Step - 789, Loss - 0.2971376770036125, Learning Rate - 0.05, magnitude of gradient - 0.04020944080455785\n",
      "Step - 790, Loss - 0.26342814933629954, Learning Rate - 0.05, magnitude of gradient - 0.03449029828114773\n",
      "Step - 791, Loss - 0.2857553301946337, Learning Rate - 0.05, magnitude of gradient - 0.04191976345520101\n",
      "Step - 792, Loss - 0.3686255638818763, Learning Rate - 0.05, magnitude of gradient - 0.04507287397828941\n",
      "Step - 793, Loss - 0.3391181893419689, Learning Rate - 0.05, magnitude of gradient - 0.10534283068514161\n",
      "Step - 794, Loss - 0.2374504806931435, Learning Rate - 0.05, magnitude of gradient - 0.06106512770329231\n",
      "Step - 795, Loss - 0.24115282669144333, Learning Rate - 0.05, magnitude of gradient - 0.05136191528862491\n",
      "Step - 796, Loss - 0.35516305430145895, Learning Rate - 0.05, magnitude of gradient - 0.034346491353011\n",
      "Step - 797, Loss - 0.2623216621556736, Learning Rate - 0.05, magnitude of gradient - 0.027454263750729856\n",
      "Step - 798, Loss - 0.36633695374024533, Learning Rate - 0.05, magnitude of gradient - 0.07659536494428706\n",
      "Step - 799, Loss - 0.33924889651225376, Learning Rate - 0.05, magnitude of gradient - 0.08499584110625696\n",
      "Step - 800, Loss - 0.2832345157386077, Learning Rate - 0.05, magnitude of gradient - 0.05875716528203096\n",
      "Step - 801, Loss - 0.2308741853715604, Learning Rate - 0.05, magnitude of gradient - 0.03996541734284499\n",
      "Step - 802, Loss - 0.3510953926441948, Learning Rate - 0.05, magnitude of gradient - 0.02665794657495668\n",
      "Step - 803, Loss - 0.30195472194660045, Learning Rate - 0.05, magnitude of gradient - 0.03260519255868978\n",
      "Step - 804, Loss - 0.3328247631336965, Learning Rate - 0.05, magnitude of gradient - 0.03918118393570881\n",
      "Step - 805, Loss - 0.3345102603642139, Learning Rate - 0.05, magnitude of gradient - 0.08866434363173407\n",
      "Step - 806, Loss - 0.28653827297631335, Learning Rate - 0.05, magnitude of gradient - 0.04855667411084156\n",
      "Step - 807, Loss - 0.3166702392047817, Learning Rate - 0.05, magnitude of gradient - 0.04388033710030536\n",
      "Step - 808, Loss - 0.29486672111040907, Learning Rate - 0.05, magnitude of gradient - 0.039879186443788656\n",
      "Step - 809, Loss - 0.3085672391500669, Learning Rate - 0.05, magnitude of gradient - 0.037863553471072856\n",
      "Step - 810, Loss - 0.37153096560532717, Learning Rate - 0.05, magnitude of gradient - 0.05467318818139746\n",
      "Step - 811, Loss - 0.3345302448473878, Learning Rate - 0.05, magnitude of gradient - 0.06387248044517914\n",
      "Step - 812, Loss - 0.3705353083629741, Learning Rate - 0.05, magnitude of gradient - 0.026272640168594402\n",
      "Step - 813, Loss - 0.3942066817881899, Learning Rate - 0.05, magnitude of gradient - 0.11738641606299997\n",
      "Step - 814, Loss - 0.3369036544954932, Learning Rate - 0.05, magnitude of gradient - 0.0026137222054472497\n",
      "Step - 815, Loss - 0.31236122625501855, Learning Rate - 0.05, magnitude of gradient - 0.07578266932094574\n",
      "Step - 816, Loss - 0.3589855367642466, Learning Rate - 0.05, magnitude of gradient - 0.020687670895019793\n",
      "Step - 817, Loss - 0.31459624749292514, Learning Rate - 0.05, magnitude of gradient - 0.06689986097627038\n",
      "Step - 818, Loss - 0.3216003125254845, Learning Rate - 0.05, magnitude of gradient - 0.03118921507193523\n",
      "Step - 819, Loss - 0.2803771397251269, Learning Rate - 0.05, magnitude of gradient - 0.07472860134851396\n",
      "Step - 820, Loss - 0.31625986914292015, Learning Rate - 0.05, magnitude of gradient - 0.05226008097167066\n",
      "Step - 821, Loss - 0.3061830718110744, Learning Rate - 0.05, magnitude of gradient - 0.03751523906520508\n",
      "Step - 822, Loss - 0.28597421431137654, Learning Rate - 0.05, magnitude of gradient - 0.028546261731886197\n",
      "Step - 823, Loss - 0.3758338139126113, Learning Rate - 0.05, magnitude of gradient - 0.009221942563141874\n",
      "Step - 824, Loss - 0.34794603548554454, Learning Rate - 0.05, magnitude of gradient - 0.02558487938943693\n",
      "Step - 825, Loss - 0.3343482353216405, Learning Rate - 0.05, magnitude of gradient - 0.03987661650537363\n",
      "Step - 826, Loss - 0.27523344881296635, Learning Rate - 0.05, magnitude of gradient - 0.057075101896548165\n",
      "Step - 827, Loss - 0.23448131753510493, Learning Rate - 0.05, magnitude of gradient - 0.03108970915572675\n",
      "Step - 828, Loss - 0.3096590253948295, Learning Rate - 0.05, magnitude of gradient - 0.01901308165887487\n",
      "Step - 829, Loss - 0.3190676384301641, Learning Rate - 0.05, magnitude of gradient - 0.034218377092583005\n",
      "Step - 830, Loss - 0.32475927055744036, Learning Rate - 0.05, magnitude of gradient - 0.07067280755142154\n",
      "Step - 831, Loss - 0.3755111005241599, Learning Rate - 0.05, magnitude of gradient - 0.05340283954971617\n",
      "Step - 832, Loss - 0.36982654129190295, Learning Rate - 0.05, magnitude of gradient - 0.07800187095663874\n",
      "Step - 833, Loss - 0.338010450522271, Learning Rate - 0.05, magnitude of gradient - 0.03173617842659549\n",
      "Step - 834, Loss - 0.3441529878864581, Learning Rate - 0.05, magnitude of gradient - 0.12757802304601765\n",
      "Step - 835, Loss - 0.36626114166438933, Learning Rate - 0.05, magnitude of gradient - 0.04029512184743121\n",
      "Step - 836, Loss - 0.35995741031649275, Learning Rate - 0.05, magnitude of gradient - 0.11335361907946949\n",
      "Step - 837, Loss - 0.30670699570264137, Learning Rate - 0.05, magnitude of gradient - 0.018747812834364927\n",
      "Step - 838, Loss - 0.2892492432550364, Learning Rate - 0.05, magnitude of gradient - 0.08214901153937464\n",
      "Step - 839, Loss - 0.3116035844409867, Learning Rate - 0.05, magnitude of gradient - 0.0648684750548475\n",
      "Step - 840, Loss - 0.28732517458410745, Learning Rate - 0.05, magnitude of gradient - 0.06962871470179882\n",
      "Step - 841, Loss - 0.34742913026288497, Learning Rate - 0.05, magnitude of gradient - 0.009103580250963495\n",
      "Step - 842, Loss - 0.3369602356665765, Learning Rate - 0.05, magnitude of gradient - 0.06317078473305608\n",
      "Step - 843, Loss - 0.3932429318250914, Learning Rate - 0.05, magnitude of gradient - 0.046611259341706325\n",
      "Step - 844, Loss - 0.3118534693143955, Learning Rate - 0.05, magnitude of gradient - 0.062003199555288546\n",
      "Step - 845, Loss - 0.32940530474604596, Learning Rate - 0.05, magnitude of gradient - 0.06022779587521138\n",
      "Step - 846, Loss - 0.2815964731031718, Learning Rate - 0.05, magnitude of gradient - 0.01082329214972088\n",
      "Step - 847, Loss - 0.319867960589752, Learning Rate - 0.05, magnitude of gradient - 0.10681265468613597\n",
      "Step - 848, Loss - 0.2976247750771404, Learning Rate - 0.05, magnitude of gradient - 0.04150636314757693\n",
      "Step - 849, Loss - 0.25768210550445303, Learning Rate - 0.05, magnitude of gradient - 0.03703990247496259\n",
      "Step - 850, Loss - 0.3216209462994936, Learning Rate - 0.05, magnitude of gradient - 0.09245206132749935\n",
      "Step - 851, Loss - 0.28004831456263785, Learning Rate - 0.05, magnitude of gradient - 0.039124228228315176\n",
      "Step - 852, Loss - 0.3262782158626063, Learning Rate - 0.05, magnitude of gradient - 0.046561139708691995\n",
      "Step - 853, Loss - 0.3349267425342449, Learning Rate - 0.05, magnitude of gradient - 0.094135463637027\n",
      "Step - 854, Loss - 0.355299465750359, Learning Rate - 0.05, magnitude of gradient - 0.0200926603465466\n",
      "Step - 855, Loss - 0.40057987541136647, Learning Rate - 0.05, magnitude of gradient - 0.10969904786989484\n",
      "Step - 856, Loss - 0.3694689749928317, Learning Rate - 0.05, magnitude of gradient - 0.059379086354739805\n",
      "Step - 857, Loss - 0.39381849049246964, Learning Rate - 0.05, magnitude of gradient - 0.029239493309410572\n",
      "Step - 858, Loss - 0.28068023852799806, Learning Rate - 0.05, magnitude of gradient - 0.093283627926172\n",
      "Step - 859, Loss - 0.40434103262254795, Learning Rate - 0.05, magnitude of gradient - 0.09996412173467607\n",
      "Step - 860, Loss - 0.2608930898232765, Learning Rate - 0.05, magnitude of gradient - 0.03852703816539364\n",
      "Step - 861, Loss - 0.34956063330486437, Learning Rate - 0.05, magnitude of gradient - 0.0576937510367053\n",
      "Step - 862, Loss - 0.3021674579954803, Learning Rate - 0.05, magnitude of gradient - 0.13082558919717455\n",
      "Step - 863, Loss - 0.2965617373524555, Learning Rate - 0.05, magnitude of gradient - 0.03306777383000715\n",
      "Step - 864, Loss - 0.31202666259400114, Learning Rate - 0.05, magnitude of gradient - 0.08086549279714436\n",
      "Step - 865, Loss - 0.25814546741919575, Learning Rate - 0.05, magnitude of gradient - 0.024833916076703974\n",
      "Step - 866, Loss - 0.33150610760523425, Learning Rate - 0.05, magnitude of gradient - 0.12024270921675856\n",
      "Step - 867, Loss - 0.37120069162705593, Learning Rate - 0.05, magnitude of gradient - 0.05363511410696251\n",
      "Step - 868, Loss - 0.26464566502702974, Learning Rate - 0.05, magnitude of gradient - 0.10303778146903399\n",
      "Step - 869, Loss - 0.325060102500285, Learning Rate - 0.05, magnitude of gradient - 0.032291508198532486\n",
      "Step - 870, Loss - 0.34976632481597236, Learning Rate - 0.05, magnitude of gradient - 0.02252183419378356\n",
      "Step - 871, Loss - 0.3337059463905709, Learning Rate - 0.05, magnitude of gradient - 0.0896342324059496\n",
      "Step - 872, Loss - 0.3325133249192792, Learning Rate - 0.05, magnitude of gradient - 0.08196257843440209\n",
      "Step - 873, Loss - 0.34464337931104894, Learning Rate - 0.05, magnitude of gradient - 0.09674194922354466\n",
      "Step - 874, Loss - 0.36696860633044753, Learning Rate - 0.05, magnitude of gradient - 0.15067742517745378\n",
      "Step - 875, Loss - 0.2786148639601373, Learning Rate - 0.05, magnitude of gradient - 0.12711721822005437\n",
      "Step - 876, Loss - 0.4362352554981627, Learning Rate - 0.05, magnitude of gradient - 0.09157458672341556\n",
      "Step - 877, Loss - 0.3457374085680158, Learning Rate - 0.05, magnitude of gradient - 0.022337723247770825\n",
      "Step - 878, Loss - 0.3421119946719964, Learning Rate - 0.05, magnitude of gradient - 0.05418882865623251\n",
      "Step - 879, Loss - 0.2542895937302996, Learning Rate - 0.05, magnitude of gradient - 0.022113392573870058\n",
      "Step - 880, Loss - 0.2976462813522656, Learning Rate - 0.05, magnitude of gradient - 0.08329313051683448\n",
      "Step - 881, Loss - 0.4443185770846346, Learning Rate - 0.05, magnitude of gradient - 0.0644882394469524\n",
      "Step - 882, Loss - 0.32140550492429376, Learning Rate - 0.05, magnitude of gradient - 0.06922000125785317\n",
      "Step - 883, Loss - 0.32918286096138283, Learning Rate - 0.05, magnitude of gradient - 0.10349908523891732\n",
      "Step - 884, Loss - 0.3147483513755479, Learning Rate - 0.05, magnitude of gradient - 0.04310910277581075\n",
      "Step - 885, Loss - 0.2966743645277845, Learning Rate - 0.05, magnitude of gradient - 0.02011894372195787\n",
      "Step - 886, Loss - 0.3627862881304896, Learning Rate - 0.05, magnitude of gradient - 0.060331945352409815\n",
      "Step - 887, Loss - 0.4300861310068279, Learning Rate - 0.05, magnitude of gradient - 0.04680495868351589\n",
      "Step - 888, Loss - 0.29571505239742224, Learning Rate - 0.05, magnitude of gradient - 0.03396944858422951\n",
      "Step - 889, Loss - 0.307406903694195, Learning Rate - 0.05, magnitude of gradient - 0.04917237778973814\n",
      "Step - 890, Loss - 0.35068372983954094, Learning Rate - 0.05, magnitude of gradient - 0.13122416559000818\n",
      "Step - 891, Loss - 0.2609176992812223, Learning Rate - 0.05, magnitude of gradient - 0.02370642690973389\n",
      "Step - 892, Loss - 0.3985528956663821, Learning Rate - 0.05, magnitude of gradient - 0.08385302472194221\n",
      "Step - 893, Loss - 0.26126070100998683, Learning Rate - 0.05, magnitude of gradient - 0.0817345271218891\n",
      "Step - 894, Loss - 0.35759813884013014, Learning Rate - 0.05, magnitude of gradient - 0.1061807052927073\n",
      "Step - 895, Loss - 0.3810301757376469, Learning Rate - 0.05, magnitude of gradient - 0.0384400363147471\n",
      "Step - 896, Loss - 0.29006193193188723, Learning Rate - 0.05, magnitude of gradient - 0.06939245421472369\n",
      "Step - 897, Loss - 0.40054388243276445, Learning Rate - 0.05, magnitude of gradient - 0.0578603586135429\n",
      "Step - 898, Loss - 0.3008828298226307, Learning Rate - 0.05, magnitude of gradient - 0.12036606232546845\n",
      "Step - 899, Loss - 0.3816945018706066, Learning Rate - 0.05, magnitude of gradient - 0.008037684697364397\n",
      "Step - 900, Loss - 0.35183833783254537, Learning Rate - 0.05, magnitude of gradient - 0.042263377409498155\n",
      "Step - 901, Loss - 0.35394801258646713, Learning Rate - 0.05, magnitude of gradient - 0.06878361276993071\n",
      "Step - 902, Loss - 0.35940758139015927, Learning Rate - 0.05, magnitude of gradient - 0.10576916078119916\n",
      "Step - 903, Loss - 0.2862542198123714, Learning Rate - 0.05, magnitude of gradient - 0.047571419488591914\n",
      "Step - 904, Loss - 0.31500225577680496, Learning Rate - 0.05, magnitude of gradient - 0.023279312276128727\n",
      "Step - 905, Loss - 0.2621752811868137, Learning Rate - 0.05, magnitude of gradient - 0.09307842105698541\n",
      "Step - 906, Loss - 0.3695247910312652, Learning Rate - 0.05, magnitude of gradient - 0.021535911640216447\n",
      "Step - 907, Loss - 0.2714121466925158, Learning Rate - 0.05, magnitude of gradient - 0.04413163195683825\n",
      "Step - 908, Loss - 0.3248150834204143, Learning Rate - 0.05, magnitude of gradient - 0.08301214625086266\n",
      "Step - 909, Loss - 0.2883630640402831, Learning Rate - 0.05, magnitude of gradient - 0.026663117886176806\n",
      "Step - 910, Loss - 0.2930181655909918, Learning Rate - 0.05, magnitude of gradient - 0.04637314422699575\n",
      "Step - 911, Loss - 0.2974550659841117, Learning Rate - 0.05, magnitude of gradient - 0.08111661584757751\n",
      "Step - 912, Loss - 0.37805529326550696, Learning Rate - 0.05, magnitude of gradient - 0.045228149589234906\n",
      "Step - 913, Loss - 0.3230929610486859, Learning Rate - 0.05, magnitude of gradient - 0.07385619059689291\n",
      "Step - 914, Loss - 0.3477795188582511, Learning Rate - 0.05, magnitude of gradient - 0.09191972880702949\n",
      "Step - 915, Loss - 0.24482673926908846, Learning Rate - 0.05, magnitude of gradient - 0.08241950905768361\n",
      "Step - 916, Loss - 0.39342188969399217, Learning Rate - 0.05, magnitude of gradient - 0.09305428155072168\n",
      "Step - 917, Loss - 0.3794376315205196, Learning Rate - 0.05, magnitude of gradient - 0.043894682171907104\n",
      "Step - 918, Loss - 0.308222456957919, Learning Rate - 0.05, magnitude of gradient - 0.025180340186169495\n",
      "Step - 919, Loss - 0.32245631578357464, Learning Rate - 0.05, magnitude of gradient - 0.04046462430973513\n",
      "Step - 920, Loss - 0.3749868582801328, Learning Rate - 0.05, magnitude of gradient - 0.038751107546612165\n",
      "Step - 921, Loss - 0.32129934139997296, Learning Rate - 0.05, magnitude of gradient - 0.05486185185162563\n",
      "Step - 922, Loss - 0.33869726975558956, Learning Rate - 0.05, magnitude of gradient - 0.1444981067440134\n",
      "Step - 923, Loss - 0.3141726959734753, Learning Rate - 0.05, magnitude of gradient - 0.029050847123679356\n",
      "Step - 924, Loss - 0.35574692104057204, Learning Rate - 0.05, magnitude of gradient - 0.12304750508037289\n",
      "Step - 925, Loss - 0.3398527179574209, Learning Rate - 0.05, magnitude of gradient - 0.09806384484289267\n",
      "Step - 926, Loss - 0.3557940407433738, Learning Rate - 0.05, magnitude of gradient - 0.07755329702974459\n",
      "Step - 927, Loss - 0.3122699404115767, Learning Rate - 0.05, magnitude of gradient - 0.03832258019751012\n",
      "Step - 928, Loss - 0.2990890720497902, Learning Rate - 0.05, magnitude of gradient - 0.0032929292649539724\n",
      "Step - 929, Loss - 0.2529697511845178, Learning Rate - 0.05, magnitude of gradient - 0.035735722521126966\n",
      "Step - 930, Loss - 0.31269591951635456, Learning Rate - 0.05, magnitude of gradient - 0.05343494744062763\n",
      "Step - 931, Loss - 0.3263372363312261, Learning Rate - 0.05, magnitude of gradient - 0.04321465702490137\n",
      "Step - 932, Loss - 0.30947440142470867, Learning Rate - 0.05, magnitude of gradient - 0.031947212098531005\n",
      "Step - 933, Loss - 0.32463908013942194, Learning Rate - 0.05, magnitude of gradient - 0.07130625677031764\n",
      "Step - 934, Loss - 0.31779672972559136, Learning Rate - 0.05, magnitude of gradient - 0.04614683518438629\n",
      "Step - 935, Loss - 0.3655413087883464, Learning Rate - 0.05, magnitude of gradient - 0.09024979228318748\n",
      "Step - 936, Loss - 0.2803982114431353, Learning Rate - 0.05, magnitude of gradient - 0.06256806952728013\n",
      "Step - 937, Loss - 0.36145682083201686, Learning Rate - 0.05, magnitude of gradient - 0.003950562401756857\n",
      "Step - 938, Loss - 0.357400739713991, Learning Rate - 0.05, magnitude of gradient - 0.03460674039484922\n",
      "Step - 939, Loss - 0.3180682192510603, Learning Rate - 0.05, magnitude of gradient - 0.0553981425847972\n",
      "Step - 940, Loss - 0.34734612885592225, Learning Rate - 0.05, magnitude of gradient - 0.1449901125234053\n",
      "Step - 941, Loss - 0.29561208310799214, Learning Rate - 0.05, magnitude of gradient - 0.05080299171190067\n",
      "Step - 942, Loss - 0.3032867077137723, Learning Rate - 0.05, magnitude of gradient - 0.09045328619245828\n",
      "Step - 943, Loss - 0.311935213685079, Learning Rate - 0.05, magnitude of gradient - 0.07936249054326651\n",
      "Step - 944, Loss - 0.3042728668568348, Learning Rate - 0.05, magnitude of gradient - 0.059244765613666796\n",
      "Step - 945, Loss - 0.34895554744609186, Learning Rate - 0.05, magnitude of gradient - 0.09269888184655652\n",
      "Step - 946, Loss - 0.28365740788287497, Learning Rate - 0.05, magnitude of gradient - 0.11776123775283183\n",
      "Step - 947, Loss - 0.3467970263572993, Learning Rate - 0.05, magnitude of gradient - 0.08378726723254483\n",
      "Step - 948, Loss - 0.23690074489383178, Learning Rate - 0.05, magnitude of gradient - 0.010602559623606607\n",
      "Step - 949, Loss - 0.29379338802036786, Learning Rate - 0.05, magnitude of gradient - 0.033023263344976884\n",
      "Step - 950, Loss - 0.3016990771041114, Learning Rate - 0.05, magnitude of gradient - 0.0829114391936092\n",
      "Step - 951, Loss - 0.307932075961765, Learning Rate - 0.05, magnitude of gradient - 0.05050375795416699\n",
      "Step - 952, Loss - 0.30796414396352917, Learning Rate - 0.05, magnitude of gradient - 0.02020534490391394\n",
      "Step - 953, Loss - 0.3166663671248122, Learning Rate - 0.05, magnitude of gradient - 0.06845592438683577\n",
      "Step - 954, Loss - 0.3369059933842626, Learning Rate - 0.05, magnitude of gradient - 0.07111195818381415\n",
      "Step - 955, Loss - 0.3622792805367795, Learning Rate - 0.05, magnitude of gradient - 0.0967174250293127\n",
      "Step - 956, Loss - 0.3487798270639263, Learning Rate - 0.05, magnitude of gradient - 0.058282855044307906\n",
      "Step - 957, Loss - 0.30283590149515155, Learning Rate - 0.05, magnitude of gradient - 0.05610631180476035\n",
      "Step - 958, Loss - 0.2694305688209169, Learning Rate - 0.05, magnitude of gradient - 0.10072689392074063\n",
      "Step - 959, Loss - 0.31691921796480793, Learning Rate - 0.05, magnitude of gradient - 0.1339479616468658\n",
      "Step - 960, Loss - 0.38360314460404954, Learning Rate - 0.05, magnitude of gradient - 0.04341067468136082\n",
      "Step - 961, Loss - 0.29951541513266117, Learning Rate - 0.05, magnitude of gradient - 0.05918309225170066\n",
      "Step - 962, Loss - 0.25757244978572286, Learning Rate - 0.05, magnitude of gradient - 0.036124159107766146\n",
      "Step - 963, Loss - 0.28539489395631157, Learning Rate - 0.05, magnitude of gradient - 0.0678264092159069\n",
      "Step - 964, Loss - 0.3105434174470645, Learning Rate - 0.05, magnitude of gradient - 0.07470173163374015\n",
      "Step - 965, Loss - 0.21814809598378748, Learning Rate - 0.05, magnitude of gradient - 0.08275976640330789\n",
      "Step - 966, Loss - 0.4294698493007797, Learning Rate - 0.05, magnitude of gradient - 0.12178478465289616\n",
      "Step - 967, Loss - 0.3193895759705025, Learning Rate - 0.05, magnitude of gradient - 0.022580871136113404\n",
      "Step - 968, Loss - 0.3330262444594365, Learning Rate - 0.05, magnitude of gradient - 0.048811464180789396\n",
      "Step - 969, Loss - 0.24609585562804007, Learning Rate - 0.05, magnitude of gradient - 0.0695283597787442\n",
      "Step - 970, Loss - 0.3793550159486068, Learning Rate - 0.05, magnitude of gradient - 0.04950549352856556\n",
      "Step - 971, Loss - 0.3672816095378692, Learning Rate - 0.05, magnitude of gradient - 0.037124879956022844\n",
      "Step - 972, Loss - 0.29247153117884944, Learning Rate - 0.05, magnitude of gradient - 0.06541780842055622\n",
      "Step - 973, Loss - 0.27887185983918855, Learning Rate - 0.05, magnitude of gradient - 0.01705426206260052\n",
      "Step - 974, Loss - 0.3047055665398868, Learning Rate - 0.05, magnitude of gradient - 0.07208844501984914\n",
      "Step - 975, Loss - 0.3184113400323725, Learning Rate - 0.05, magnitude of gradient - 0.05629047126610933\n",
      "Step - 976, Loss - 0.2969308974348558, Learning Rate - 0.05, magnitude of gradient - 0.058287820096581115\n",
      "Step - 977, Loss - 0.3052839124956662, Learning Rate - 0.05, magnitude of gradient - 0.09004082281307337\n",
      "Step - 978, Loss - 0.26650976307813706, Learning Rate - 0.05, magnitude of gradient - 0.10269577503569177\n",
      "Step - 979, Loss - 0.38649453338327633, Learning Rate - 0.05, magnitude of gradient - 0.06266126970200063\n",
      "Step - 980, Loss - 0.3189485659344221, Learning Rate - 0.05, magnitude of gradient - 0.06161321605201028\n",
      "Step - 981, Loss - 0.35120912234496854, Learning Rate - 0.05, magnitude of gradient - 0.10044060763149329\n",
      "Step - 982, Loss - 0.3112992164635298, Learning Rate - 0.05, magnitude of gradient - 0.061418595907326866\n",
      "Step - 983, Loss - 0.314755384748463, Learning Rate - 0.05, magnitude of gradient - 0.02320038653363205\n",
      "Step - 984, Loss - 0.22035568170124326, Learning Rate - 0.05, magnitude of gradient - 0.034350797298909955\n",
      "Step - 985, Loss - 0.3529555616982436, Learning Rate - 0.05, magnitude of gradient - 0.09968528425104947\n",
      "Step - 986, Loss - 0.2913099884752212, Learning Rate - 0.05, magnitude of gradient - 0.11679380631799177\n",
      "Step - 987, Loss - 0.30998478009597263, Learning Rate - 0.05, magnitude of gradient - 0.043509687047414376\n",
      "Step - 988, Loss - 0.37638584732692215, Learning Rate - 0.05, magnitude of gradient - 0.08416219965513906\n",
      "Step - 989, Loss - 0.24304986739099815, Learning Rate - 0.05, magnitude of gradient - 0.038866785003057534\n",
      "Step - 990, Loss - 0.3689511163989374, Learning Rate - 0.05, magnitude of gradient - 0.01553036644789447\n",
      "Step - 991, Loss - 0.29523866101339924, Learning Rate - 0.05, magnitude of gradient - 0.027795124766297113\n",
      "Step - 992, Loss - 0.295544417943124, Learning Rate - 0.05, magnitude of gradient - 0.0710528561940813\n",
      "Step - 993, Loss - 0.33690636340917457, Learning Rate - 0.05, magnitude of gradient - 0.057856899879980134\n",
      "Step - 994, Loss - 0.3495395560863117, Learning Rate - 0.05, magnitude of gradient - 0.00886829934283615\n",
      "Step - 995, Loss - 0.32313478154414166, Learning Rate - 0.05, magnitude of gradient - 0.24248311061721664\n",
      "Step - 996, Loss - 0.411705388148241, Learning Rate - 0.05, magnitude of gradient - 0.174191549484887\n",
      "Step - 997, Loss - 0.3191168546543767, Learning Rate - 0.05, magnitude of gradient - 0.08098856337000952\n",
      "Step - 998, Loss - 0.2920752540417828, Learning Rate - 0.05, magnitude of gradient - 0.0780560525962631\n",
      "Step - 999, Loss - 0.3735020211904353, Learning Rate - 0.05, magnitude of gradient - 0.013438639937679983\n",
      "Step - 1000, Loss - 0.3600623834363339, Learning Rate - 0.05, magnitude of gradient - 0.047508898656269\n",
      "Step - 1001, Loss - 0.3826065740800585, Learning Rate - 0.025, magnitude of gradient - 0.13615789517930652\n",
      "Step - 1002, Loss - 0.2639256731018031, Learning Rate - 0.025, magnitude of gradient - 0.04373448711330253\n",
      "Step - 1003, Loss - 0.4257530696064057, Learning Rate - 0.025, magnitude of gradient - 0.08548993513525671\n",
      "Step - 1004, Loss - 0.39125209526268195, Learning Rate - 0.025, magnitude of gradient - 0.014776447133000952\n",
      "Step - 1005, Loss - 0.32432866006475936, Learning Rate - 0.025, magnitude of gradient - 0.09629449407952337\n",
      "Step - 1006, Loss - 0.3214833360848548, Learning Rate - 0.025, magnitude of gradient - 0.07963951598343115\n",
      "Step - 1007, Loss - 0.3611614118319838, Learning Rate - 0.025, magnitude of gradient - 0.048996939761227244\n",
      "Step - 1008, Loss - 0.3175884366816737, Learning Rate - 0.025, magnitude of gradient - 0.0536128842832118\n",
      "Step - 1009, Loss - 0.31521664323252385, Learning Rate - 0.025, magnitude of gradient - 0.021928737130818398\n",
      "Step - 1010, Loss - 0.34840979847159564, Learning Rate - 0.025, magnitude of gradient - 0.04132621310033834\n",
      "Step - 1011, Loss - 0.34983633465696096, Learning Rate - 0.025, magnitude of gradient - 0.02713025457712235\n",
      "Step - 1012, Loss - 0.3007852723281057, Learning Rate - 0.025, magnitude of gradient - 0.0789275709264757\n",
      "Step - 1013, Loss - 0.35873926528105554, Learning Rate - 0.025, magnitude of gradient - 0.04172300607341484\n",
      "Step - 1014, Loss - 0.3757394726803754, Learning Rate - 0.025, magnitude of gradient - 0.026383151546065305\n",
      "Step - 1015, Loss - 0.3003902917360817, Learning Rate - 0.025, magnitude of gradient - 0.06757625406232909\n",
      "Step - 1016, Loss - 0.3585174807477087, Learning Rate - 0.025, magnitude of gradient - 0.06735571862929374\n",
      "Step - 1017, Loss - 0.3184987973110668, Learning Rate - 0.025, magnitude of gradient - 0.05894198047372968\n",
      "Step - 1018, Loss - 0.31930201513856776, Learning Rate - 0.025, magnitude of gradient - 0.028164415820299445\n",
      "Step - 1019, Loss - 0.3040154247936627, Learning Rate - 0.025, magnitude of gradient - 0.04340098670615451\n",
      "Step - 1020, Loss - 0.25283636982712915, Learning Rate - 0.025, magnitude of gradient - 0.022950193740674495\n",
      "Step - 1021, Loss - 0.34690699091785027, Learning Rate - 0.025, magnitude of gradient - 0.017772624031728367\n",
      "Step - 1022, Loss - 0.25613105746916165, Learning Rate - 0.025, magnitude of gradient - 0.03287300084013415\n",
      "Step - 1023, Loss - 0.3275984111764777, Learning Rate - 0.025, magnitude of gradient - 0.032586747368068575\n",
      "Step - 1024, Loss - 0.31667058207329946, Learning Rate - 0.025, magnitude of gradient - 0.07361594679086264\n",
      "Step - 1025, Loss - 0.34751481876727636, Learning Rate - 0.025, magnitude of gradient - 0.010078393526244357\n",
      "Step - 1026, Loss - 0.4545712072610667, Learning Rate - 0.025, magnitude of gradient - 0.060522376688235596\n",
      "Step - 1027, Loss - 0.35268869040865, Learning Rate - 0.025, magnitude of gradient - 0.047164991741868084\n",
      "Step - 1028, Loss - 0.3262960943404084, Learning Rate - 0.025, magnitude of gradient - 0.049545957536674096\n",
      "Step - 1029, Loss - 0.2815883771111354, Learning Rate - 0.025, magnitude of gradient - 0.11491668900823457\n",
      "Step - 1030, Loss - 0.33215923271583253, Learning Rate - 0.025, magnitude of gradient - 0.03689316431512916\n",
      "Step - 1031, Loss - 0.3493397137655752, Learning Rate - 0.025, magnitude of gradient - 0.08544227011349284\n",
      "Step - 1032, Loss - 0.3082026979136073, Learning Rate - 0.025, magnitude of gradient - 0.012622607831704786\n",
      "Step - 1033, Loss - 0.34522228935596094, Learning Rate - 0.025, magnitude of gradient - 0.09280243037434843\n",
      "Step - 1034, Loss - 0.3354427713257933, Learning Rate - 0.025, magnitude of gradient - 0.02720853385171429\n",
      "Step - 1035, Loss - 0.2868935533557292, Learning Rate - 0.025, magnitude of gradient - 0.07790699147326216\n",
      "Step - 1036, Loss - 0.33552587817090873, Learning Rate - 0.025, magnitude of gradient - 0.08372902338102554\n",
      "Step - 1037, Loss - 0.3515757044011968, Learning Rate - 0.025, magnitude of gradient - 0.06960187072642768\n",
      "Step - 1038, Loss - 0.3384569297161354, Learning Rate - 0.025, magnitude of gradient - 0.02325122345171483\n",
      "Step - 1039, Loss - 0.406608658952223, Learning Rate - 0.025, magnitude of gradient - 0.06101056780137788\n",
      "Step - 1040, Loss - 0.25342538820556737, Learning Rate - 0.025, magnitude of gradient - 0.07293894656040334\n",
      "Step - 1041, Loss - 0.2430176556729891, Learning Rate - 0.025, magnitude of gradient - 0.05468766339848713\n",
      "Step - 1042, Loss - 0.35121547216592575, Learning Rate - 0.025, magnitude of gradient - 0.043182850435209204\n",
      "Step - 1043, Loss - 0.3148249272124513, Learning Rate - 0.025, magnitude of gradient - 0.06260226215691266\n",
      "Step - 1044, Loss - 0.3935085766275756, Learning Rate - 0.025, magnitude of gradient - 0.06136449096607006\n",
      "Step - 1045, Loss - 0.36081013599491923, Learning Rate - 0.025, magnitude of gradient - 0.03483204069591509\n",
      "Step - 1046, Loss - 0.3572979859340805, Learning Rate - 0.025, magnitude of gradient - 0.03171903726497468\n",
      "Step - 1047, Loss - 0.3083177568357683, Learning Rate - 0.025, magnitude of gradient - 0.022995785746718973\n",
      "Step - 1048, Loss - 0.3597156474700439, Learning Rate - 0.025, magnitude of gradient - 0.13386870129293266\n",
      "Step - 1049, Loss - 0.3129723183273186, Learning Rate - 0.025, magnitude of gradient - 0.11833722861667187\n",
      "Step - 1050, Loss - 0.28935971445280434, Learning Rate - 0.025, magnitude of gradient - 0.04383187113469828\n",
      "Step - 1051, Loss - 0.3458003194867446, Learning Rate - 0.025, magnitude of gradient - 0.029154348620716645\n",
      "Step - 1052, Loss - 0.32214799077197087, Learning Rate - 0.025, magnitude of gradient - 0.05834664294265643\n",
      "Step - 1053, Loss - 0.34068814958666677, Learning Rate - 0.025, magnitude of gradient - 0.0551289076937742\n",
      "Step - 1054, Loss - 0.29080946439556715, Learning Rate - 0.025, magnitude of gradient - 0.09668601842055324\n",
      "Step - 1055, Loss - 0.22964075239765963, Learning Rate - 0.025, magnitude of gradient - 0.05258258112174053\n",
      "Step - 1056, Loss - 0.36693649161190073, Learning Rate - 0.025, magnitude of gradient - 0.04933336202313437\n",
      "Step - 1057, Loss - 0.3777412126875109, Learning Rate - 0.025, magnitude of gradient - 0.026970329352380872\n",
      "Step - 1058, Loss - 0.29852601980338117, Learning Rate - 0.025, magnitude of gradient - 0.05779987280845278\n",
      "Step - 1059, Loss - 0.3180140746382345, Learning Rate - 0.025, magnitude of gradient - 0.00894078502062892\n",
      "Step - 1060, Loss - 0.36738214825912463, Learning Rate - 0.025, magnitude of gradient - 0.048060300834210624\n",
      "Step - 1061, Loss - 0.34209104177746863, Learning Rate - 0.025, magnitude of gradient - 0.058463760471634714\n",
      "Step - 1062, Loss - 0.3156923949000338, Learning Rate - 0.025, magnitude of gradient - 0.06946913158259861\n",
      "Step - 1063, Loss - 0.4126716409084519, Learning Rate - 0.025, magnitude of gradient - 0.034498883522668625\n",
      "Step - 1064, Loss - 0.37268836779845027, Learning Rate - 0.025, magnitude of gradient - 0.07246607457664454\n",
      "Step - 1065, Loss - 0.2857725191555266, Learning Rate - 0.025, magnitude of gradient - 0.05591748067683173\n",
      "Step - 1066, Loss - 0.2963339784459912, Learning Rate - 0.025, magnitude of gradient - 0.06152081768801372\n",
      "Step - 1067, Loss - 0.3211870292131327, Learning Rate - 0.025, magnitude of gradient - 0.04573042248505624\n",
      "Step - 1068, Loss - 0.276006784416491, Learning Rate - 0.025, magnitude of gradient - 0.0513440908199596\n",
      "Step - 1069, Loss - 0.27041617512580074, Learning Rate - 0.025, magnitude of gradient - 0.06093995701681533\n",
      "Step - 1070, Loss - 0.28123915818045164, Learning Rate - 0.025, magnitude of gradient - 0.10078995166967332\n",
      "Step - 1071, Loss - 0.32044121897050637, Learning Rate - 0.025, magnitude of gradient - 0.03035414908137208\n",
      "Step - 1072, Loss - 0.31250202192873205, Learning Rate - 0.025, magnitude of gradient - 0.03689566183224798\n",
      "Step - 1073, Loss - 0.27779079887585323, Learning Rate - 0.025, magnitude of gradient - 0.08043102697844588\n",
      "Step - 1074, Loss - 0.27537368806333284, Learning Rate - 0.025, magnitude of gradient - 0.025151659179873536\n",
      "Step - 1075, Loss - 0.33368480481417484, Learning Rate - 0.025, magnitude of gradient - 0.03197729107762635\n",
      "Step - 1076, Loss - 0.33939377441679963, Learning Rate - 0.025, magnitude of gradient - 0.05122758914498288\n",
      "Step - 1077, Loss - 0.259696717232066, Learning Rate - 0.025, magnitude of gradient - 0.013478237554913262\n",
      "Step - 1078, Loss - 0.3083464950561583, Learning Rate - 0.025, magnitude of gradient - 0.006819673072341537\n",
      "Step - 1079, Loss - 0.2984131407562149, Learning Rate - 0.025, magnitude of gradient - 0.14502452062868218\n",
      "Step - 1080, Loss - 0.47944432787664004, Learning Rate - 0.025, magnitude of gradient - 0.022229063950721363\n",
      "Step - 1081, Loss - 0.2456283885028902, Learning Rate - 0.025, magnitude of gradient - 0.0710933105311681\n",
      "Step - 1082, Loss - 0.28313045425739203, Learning Rate - 0.025, magnitude of gradient - 0.03562312171668139\n",
      "Step - 1083, Loss - 0.3276996560950001, Learning Rate - 0.025, magnitude of gradient - 0.0295669156330264\n",
      "Step - 1084, Loss - 0.36352240002745617, Learning Rate - 0.025, magnitude of gradient - 0.05131105379152671\n",
      "Step - 1085, Loss - 0.32353932331400426, Learning Rate - 0.025, magnitude of gradient - 0.01964602638277083\n",
      "Step - 1086, Loss - 0.33093453698584274, Learning Rate - 0.025, magnitude of gradient - 0.032082294930381755\n",
      "Step - 1087, Loss - 0.29272703065291444, Learning Rate - 0.025, magnitude of gradient - 0.041963861873049194\n",
      "Step - 1088, Loss - 0.32244378056977663, Learning Rate - 0.025, magnitude of gradient - 0.059558461910124656\n",
      "Step - 1089, Loss - 0.3211169593173351, Learning Rate - 0.025, magnitude of gradient - 0.06542634091095025\n",
      "Step - 1090, Loss - 0.35449913307822406, Learning Rate - 0.025, magnitude of gradient - 0.05635626218901037\n",
      "Step - 1091, Loss - 0.3525516011104575, Learning Rate - 0.025, magnitude of gradient - 0.03118321087709733\n",
      "Step - 1092, Loss - 0.33821381436922193, Learning Rate - 0.025, magnitude of gradient - 0.010784418354539792\n",
      "Step - 1093, Loss - 0.24930836608912285, Learning Rate - 0.025, magnitude of gradient - 0.05685118847170678\n",
      "Step - 1094, Loss - 0.2813003480565498, Learning Rate - 0.025, magnitude of gradient - 0.045982424580460986\n",
      "Step - 1095, Loss - 0.3049606060451201, Learning Rate - 0.025, magnitude of gradient - 0.028518983798901002\n",
      "Step - 1096, Loss - 0.337434533227946, Learning Rate - 0.025, magnitude of gradient - 0.026422187995600532\n",
      "Step - 1097, Loss - 0.34739302363781227, Learning Rate - 0.025, magnitude of gradient - 0.08950026016488116\n",
      "Step - 1098, Loss - 0.3301703176831879, Learning Rate - 0.025, magnitude of gradient - 0.04879033275881369\n",
      "Step - 1099, Loss - 0.36531948222968197, Learning Rate - 0.025, magnitude of gradient - 0.05203881820888165\n",
      "Step - 1100, Loss - 0.4447553025106593, Learning Rate - 0.025, magnitude of gradient - 0.03585814184607391\n",
      "Step - 1101, Loss - 0.33349831103505945, Learning Rate - 0.025, magnitude of gradient - 0.03662095971167744\n",
      "Step - 1102, Loss - 0.3517301289418084, Learning Rate - 0.025, magnitude of gradient - 0.06265503055049784\n",
      "Step - 1103, Loss - 0.3169160247501348, Learning Rate - 0.025, magnitude of gradient - 0.018447545738626052\n",
      "Step - 1104, Loss - 0.25188494277889056, Learning Rate - 0.025, magnitude of gradient - 0.02697916268872083\n",
      "Step - 1105, Loss - 0.3287454699990781, Learning Rate - 0.025, magnitude of gradient - 0.07272726730608232\n",
      "Step - 1106, Loss - 0.3365939765405011, Learning Rate - 0.025, magnitude of gradient - 0.034397795027813\n",
      "Step - 1107, Loss - 0.3456763804374464, Learning Rate - 0.025, magnitude of gradient - 0.06858371265998975\n",
      "Step - 1108, Loss - 0.32046445263347767, Learning Rate - 0.025, magnitude of gradient - 0.05390782817651316\n",
      "Step - 1109, Loss - 0.3855637630886837, Learning Rate - 0.025, magnitude of gradient - 0.046762068029947625\n",
      "Step - 1110, Loss - 0.3090935496024174, Learning Rate - 0.025, magnitude of gradient - 0.07033163480322356\n",
      "Step - 1111, Loss - 0.3080812150159652, Learning Rate - 0.025, magnitude of gradient - 0.013889270648443926\n",
      "Step - 1112, Loss - 0.47112684895659473, Learning Rate - 0.025, magnitude of gradient - 0.07864416461582313\n",
      "Step - 1113, Loss - 0.2550126498428035, Learning Rate - 0.025, magnitude of gradient - 0.02785111633775453\n",
      "Step - 1114, Loss - 0.29102156242252364, Learning Rate - 0.025, magnitude of gradient - 0.03430255951827716\n",
      "Step - 1115, Loss - 0.3313247728330994, Learning Rate - 0.025, magnitude of gradient - 0.04958140719997961\n",
      "Step - 1116, Loss - 0.25015745008938733, Learning Rate - 0.025, magnitude of gradient - 0.011015102756983548\n",
      "Step - 1117, Loss - 0.30286873697093136, Learning Rate - 0.025, magnitude of gradient - 0.03696376129441648\n",
      "Step - 1118, Loss - 0.2762164054860145, Learning Rate - 0.025, magnitude of gradient - 0.021774407042098286\n",
      "Step - 1119, Loss - 0.36339781481360506, Learning Rate - 0.025, magnitude of gradient - 0.07672199750256839\n",
      "Step - 1120, Loss - 0.34004394687881495, Learning Rate - 0.025, magnitude of gradient - 0.06416143692317418\n",
      "Step - 1121, Loss - 0.30565395417359614, Learning Rate - 0.025, magnitude of gradient - 0.11549137657005851\n",
      "Step - 1122, Loss - 0.34742130951047395, Learning Rate - 0.025, magnitude of gradient - 0.022178396724967957\n",
      "Step - 1123, Loss - 0.40120298504085194, Learning Rate - 0.025, magnitude of gradient - 0.01716807126858062\n",
      "Step - 1124, Loss - 0.33499594175967684, Learning Rate - 0.025, magnitude of gradient - 0.03669832331791524\n",
      "Step - 1125, Loss - 0.3595997336440479, Learning Rate - 0.025, magnitude of gradient - 0.08998290327371568\n",
      "Step - 1126, Loss - 0.2984358189918946, Learning Rate - 0.025, magnitude of gradient - 0.051119097183664114\n",
      "Step - 1127, Loss - 0.3317828431781108, Learning Rate - 0.025, magnitude of gradient - 0.09213920857032214\n",
      "Step - 1128, Loss - 0.3100662581387562, Learning Rate - 0.025, magnitude of gradient - 0.03983950076490154\n",
      "Step - 1129, Loss - 0.3653496207093989, Learning Rate - 0.025, magnitude of gradient - 0.07963277771832607\n",
      "Step - 1130, Loss - 0.3505899423429174, Learning Rate - 0.025, magnitude of gradient - 0.03076212358522206\n",
      "Step - 1131, Loss - 0.24849294533752853, Learning Rate - 0.025, magnitude of gradient - 0.03832339359517504\n",
      "Step - 1132, Loss - 0.3042800632261533, Learning Rate - 0.025, magnitude of gradient - 0.05798728531801293\n",
      "Step - 1133, Loss - 0.3668657668729852, Learning Rate - 0.025, magnitude of gradient - 0.10412756081654144\n",
      "Step - 1134, Loss - 0.27282149326401894, Learning Rate - 0.025, magnitude of gradient - 0.055516793630566885\n",
      "Step - 1135, Loss - 0.21948672019308113, Learning Rate - 0.025, magnitude of gradient - 0.03266005356529017\n",
      "Step - 1136, Loss - 0.36913609514568835, Learning Rate - 0.025, magnitude of gradient - 0.05518211420074534\n",
      "Step - 1137, Loss - 0.388319938426597, Learning Rate - 0.025, magnitude of gradient - 0.06982060301512129\n",
      "Step - 1138, Loss - 0.25003012808228686, Learning Rate - 0.025, magnitude of gradient - 0.009051479763653286\n",
      "Step - 1139, Loss - 0.29384116842888436, Learning Rate - 0.025, magnitude of gradient - 0.034123094917238375\n",
      "Step - 1140, Loss - 0.41270429053785657, Learning Rate - 0.025, magnitude of gradient - 0.06333243185618129\n",
      "Step - 1141, Loss - 0.3573318544921281, Learning Rate - 0.025, magnitude of gradient - 0.08522274013745126\n",
      "Step - 1142, Loss - 0.2572181184614266, Learning Rate - 0.025, magnitude of gradient - 0.06547686827582579\n",
      "Step - 1143, Loss - 0.32862216487097395, Learning Rate - 0.025, magnitude of gradient - 0.02498179537646165\n",
      "Step - 1144, Loss - 0.3220643113655855, Learning Rate - 0.025, magnitude of gradient - 0.03443367691960647\n",
      "Step - 1145, Loss - 0.32424342910381915, Learning Rate - 0.025, magnitude of gradient - 0.026653742761936845\n",
      "Step - 1146, Loss - 0.37131963762340814, Learning Rate - 0.025, magnitude of gradient - 0.04701974306544654\n",
      "Step - 1147, Loss - 0.29534321627152504, Learning Rate - 0.025, magnitude of gradient - 0.022260678300398257\n",
      "Step - 1148, Loss - 0.27181160409129557, Learning Rate - 0.025, magnitude of gradient - 0.10124606365064397\n",
      "Step - 1149, Loss - 0.27097578172624204, Learning Rate - 0.025, magnitude of gradient - 0.05225846122908386\n",
      "Step - 1150, Loss - 0.2731775463577051, Learning Rate - 0.025, magnitude of gradient - 0.06219108142591524\n",
      "Step - 1151, Loss - 0.24444818215474323, Learning Rate - 0.025, magnitude of gradient - 0.05201717139533054\n",
      "Step - 1152, Loss - 0.3287870151608264, Learning Rate - 0.025, magnitude of gradient - 0.024991176930625948\n",
      "Step - 1153, Loss - 0.24655819959923106, Learning Rate - 0.025, magnitude of gradient - 0.023099381705608806\n",
      "Step - 1154, Loss - 0.24879947659099605, Learning Rate - 0.025, magnitude of gradient - 0.06001642831904709\n",
      "Step - 1155, Loss - 0.2856186151691452, Learning Rate - 0.025, magnitude of gradient - 0.08212525677165074\n",
      "Step - 1156, Loss - 0.3076166995355536, Learning Rate - 0.025, magnitude of gradient - 0.10139340205920659\n",
      "Step - 1157, Loss - 0.257074174088931, Learning Rate - 0.025, magnitude of gradient - 0.018986801738023966\n",
      "Step - 1158, Loss - 0.2748333528572634, Learning Rate - 0.025, magnitude of gradient - 0.052739368689396805\n",
      "Step - 1159, Loss - 0.35472479075061614, Learning Rate - 0.025, magnitude of gradient - 0.07680812024714039\n",
      "Step - 1160, Loss - 0.2713149975514385, Learning Rate - 0.025, magnitude of gradient - 0.06377624199192372\n",
      "Step - 1161, Loss - 0.26484633733267365, Learning Rate - 0.025, magnitude of gradient - 0.040603899804203814\n",
      "Step - 1162, Loss - 0.27888297681680296, Learning Rate - 0.025, magnitude of gradient - 0.025715945341795148\n",
      "Step - 1163, Loss - 0.29853885640021627, Learning Rate - 0.025, magnitude of gradient - 0.06702836798931369\n",
      "Step - 1164, Loss - 0.3375210003164507, Learning Rate - 0.025, magnitude of gradient - 0.044582535793629\n",
      "Step - 1165, Loss - 0.33592350999789106, Learning Rate - 0.025, magnitude of gradient - 0.11397968742576527\n",
      "Step - 1166, Loss - 0.3435882907695091, Learning Rate - 0.025, magnitude of gradient - 0.04833100419036061\n",
      "Step - 1167, Loss - 0.33647934896963927, Learning Rate - 0.025, magnitude of gradient - 0.04480504533841418\n",
      "Step - 1168, Loss - 0.3472152226587234, Learning Rate - 0.025, magnitude of gradient - 0.025940011377611132\n",
      "Step - 1169, Loss - 0.35060280988416437, Learning Rate - 0.025, magnitude of gradient - 0.05316237850791211\n",
      "Step - 1170, Loss - 0.34584861000622147, Learning Rate - 0.025, magnitude of gradient - 0.03268308276538247\n",
      "Step - 1171, Loss - 0.26683477570371084, Learning Rate - 0.025, magnitude of gradient - 0.038025771260746535\n",
      "Step - 1172, Loss - 0.3966121888000749, Learning Rate - 0.025, magnitude of gradient - 0.061533018565006224\n",
      "Step - 1173, Loss - 0.400418019705907, Learning Rate - 0.025, magnitude of gradient - 0.09654497423226054\n",
      "Step - 1174, Loss - 0.3243628726920943, Learning Rate - 0.025, magnitude of gradient - 0.025254826326503846\n",
      "Step - 1175, Loss - 0.3350050272247238, Learning Rate - 0.025, magnitude of gradient - 0.06683646111971353\n",
      "Step - 1176, Loss - 0.376407529319312, Learning Rate - 0.025, magnitude of gradient - 0.0717245631131052\n",
      "Step - 1177, Loss - 0.4069323100095254, Learning Rate - 0.025, magnitude of gradient - 0.022186648809918056\n",
      "Step - 1178, Loss - 0.2699606713647772, Learning Rate - 0.025, magnitude of gradient - 0.011059783409565961\n",
      "Step - 1179, Loss - 0.32161341794766746, Learning Rate - 0.025, magnitude of gradient - 0.13819469839511295\n",
      "Step - 1180, Loss - 0.34713105860746435, Learning Rate - 0.025, magnitude of gradient - 0.038707951843413656\n",
      "Step - 1181, Loss - 0.2967555650050931, Learning Rate - 0.025, magnitude of gradient - 0.028124210982368534\n",
      "Step - 1182, Loss - 0.2841718733417457, Learning Rate - 0.025, magnitude of gradient - 0.03440944492347002\n",
      "Step - 1183, Loss - 0.3875574895205102, Learning Rate - 0.025, magnitude of gradient - 0.07719573049784936\n",
      "Step - 1184, Loss - 0.3273512342560988, Learning Rate - 0.025, magnitude of gradient - 0.08125315513900709\n",
      "Step - 1185, Loss - 0.291335422419843, Learning Rate - 0.025, magnitude of gradient - 0.08302769610878095\n",
      "Step - 1186, Loss - 0.2897744354107423, Learning Rate - 0.025, magnitude of gradient - 0.018177834161325845\n",
      "Step - 1187, Loss - 0.2911419893073498, Learning Rate - 0.025, magnitude of gradient - 0.06359443688962314\n",
      "Step - 1188, Loss - 0.3227487821016164, Learning Rate - 0.025, magnitude of gradient - 0.11194915711318013\n",
      "Step - 1189, Loss - 0.28306632947766497, Learning Rate - 0.025, magnitude of gradient - 0.07828880950248984\n",
      "Step - 1190, Loss - 0.22824744050583606, Learning Rate - 0.025, magnitude of gradient - 0.07685291007326299\n",
      "Step - 1191, Loss - 0.3592394141030355, Learning Rate - 0.025, magnitude of gradient - 0.06247978325528668\n",
      "Step - 1192, Loss - 0.3382336133057888, Learning Rate - 0.025, magnitude of gradient - 0.07333342287841166\n",
      "Step - 1193, Loss - 0.3077803130348888, Learning Rate - 0.025, magnitude of gradient - 0.0930519251592496\n",
      "Step - 1194, Loss - 0.28278871877747663, Learning Rate - 0.025, magnitude of gradient - 0.03744278267228801\n",
      "Step - 1195, Loss - 0.4139909168123307, Learning Rate - 0.025, magnitude of gradient - 0.07499827715734556\n",
      "Step - 1196, Loss - 0.3244388909881079, Learning Rate - 0.025, magnitude of gradient - 0.019492117093016438\n",
      "Step - 1197, Loss - 0.32256215653856357, Learning Rate - 0.025, magnitude of gradient - 0.010364761608811918\n",
      "Step - 1198, Loss - 0.28547057261248054, Learning Rate - 0.025, magnitude of gradient - 0.08408341076077726\n",
      "Step - 1199, Loss - 0.26550660008828875, Learning Rate - 0.025, magnitude of gradient - 0.019127812394601287\n",
      "Step - 1200, Loss - 0.3601797780776995, Learning Rate - 0.025, magnitude of gradient - 0.0621135794728435\n",
      "Step - 1201, Loss - 0.26878629729510684, Learning Rate - 0.025, magnitude of gradient - 0.03337013504351031\n",
      "Step - 1202, Loss - 0.26461922732577847, Learning Rate - 0.025, magnitude of gradient - 0.010968385597099829\n",
      "Step - 1203, Loss - 0.3445644398511946, Learning Rate - 0.025, magnitude of gradient - 0.14356429937126694\n",
      "Step - 1204, Loss - 0.32364690512496025, Learning Rate - 0.025, magnitude of gradient - 0.0993936128425297\n",
      "Step - 1205, Loss - 0.36317751047943525, Learning Rate - 0.025, magnitude of gradient - 0.05124134214125813\n",
      "Step - 1206, Loss - 0.29227159182338297, Learning Rate - 0.025, magnitude of gradient - 0.03200937909283424\n",
      "Step - 1207, Loss - 0.33316981370316023, Learning Rate - 0.025, magnitude of gradient - 0.035527901570338576\n",
      "Step - 1208, Loss - 0.3871288861168482, Learning Rate - 0.025, magnitude of gradient - 0.031669291272681165\n",
      "Step - 1209, Loss - 0.3572312115835713, Learning Rate - 0.025, magnitude of gradient - 0.029303816502502186\n",
      "Step - 1210, Loss - 0.316325504071382, Learning Rate - 0.025, magnitude of gradient - 0.05780671379122555\n",
      "Step - 1211, Loss - 0.38055348013259793, Learning Rate - 0.025, magnitude of gradient - 0.05251222235865452\n",
      "Step - 1212, Loss - 0.2878038033137799, Learning Rate - 0.025, magnitude of gradient - 0.01753906267935945\n",
      "Step - 1213, Loss - 0.28928697447248186, Learning Rate - 0.025, magnitude of gradient - 0.021230793302249303\n",
      "Step - 1214, Loss - 0.33065698969486046, Learning Rate - 0.025, magnitude of gradient - 0.01866406627477239\n",
      "Step - 1215, Loss - 0.317320739674619, Learning Rate - 0.025, magnitude of gradient - 0.0025938291628257136\n",
      "Step - 1216, Loss - 0.2717528191217364, Learning Rate - 0.025, magnitude of gradient - 0.06537494136913148\n",
      "Step - 1217, Loss - 0.3172300901943353, Learning Rate - 0.025, magnitude of gradient - 0.012896510293196361\n",
      "Step - 1218, Loss - 0.27227335486178084, Learning Rate - 0.025, magnitude of gradient - 0.0287047073236193\n",
      "Step - 1219, Loss - 0.34547121659968694, Learning Rate - 0.025, magnitude of gradient - 0.043078232719385004\n",
      "Step - 1220, Loss - 0.3032108947795999, Learning Rate - 0.025, magnitude of gradient - 0.05569378522644704\n",
      "Step - 1221, Loss - 0.3572594665659591, Learning Rate - 0.025, magnitude of gradient - 0.09393344361049667\n",
      "Step - 1222, Loss - 0.3172190048813571, Learning Rate - 0.025, magnitude of gradient - 0.037697314603437206\n",
      "Step - 1223, Loss - 0.3015687956824916, Learning Rate - 0.025, magnitude of gradient - 0.032924793112586545\n",
      "Step - 1224, Loss - 0.31129595278021577, Learning Rate - 0.025, magnitude of gradient - 0.07985419469860384\n",
      "Step - 1225, Loss - 0.28549678694577035, Learning Rate - 0.025, magnitude of gradient - 0.018830182776830028\n",
      "Step - 1226, Loss - 0.35823354934340873, Learning Rate - 0.025, magnitude of gradient - 0.029519344144948678\n",
      "Step - 1227, Loss - 0.2584068843304803, Learning Rate - 0.025, magnitude of gradient - 0.04115638347021404\n",
      "Step - 1228, Loss - 0.2887286256500028, Learning Rate - 0.025, magnitude of gradient - 0.026078402473242952\n",
      "Step - 1229, Loss - 0.3856655032716042, Learning Rate - 0.025, magnitude of gradient - 0.044349841519579596\n",
      "Step - 1230, Loss - 0.2814379573962485, Learning Rate - 0.025, magnitude of gradient - 0.034207802116981136\n",
      "Step - 1231, Loss - 0.28161180706633804, Learning Rate - 0.025, magnitude of gradient - 0.028818598805125353\n",
      "Step - 1232, Loss - 0.28223622138143994, Learning Rate - 0.025, magnitude of gradient - 0.023248451375862685\n",
      "Step - 1233, Loss - 0.296338569203212, Learning Rate - 0.025, magnitude of gradient - 0.03590187120133648\n",
      "Step - 1234, Loss - 0.441766295646696, Learning Rate - 0.025, magnitude of gradient - 0.04506071105755633\n",
      "Step - 1235, Loss - 0.28509149161014635, Learning Rate - 0.025, magnitude of gradient - 0.05963042752642035\n",
      "Step - 1236, Loss - 0.3854016836073587, Learning Rate - 0.025, magnitude of gradient - 0.02001100456693814\n",
      "Step - 1237, Loss - 0.3179215438527118, Learning Rate - 0.025, magnitude of gradient - 0.06069998996087613\n",
      "Step - 1238, Loss - 0.36892851499416285, Learning Rate - 0.025, magnitude of gradient - 0.012400803145360764\n",
      "Step - 1239, Loss - 0.32888070951755544, Learning Rate - 0.025, magnitude of gradient - 0.02786213696417595\n",
      "Step - 1240, Loss - 0.28941869317399993, Learning Rate - 0.025, magnitude of gradient - 0.11585748347885126\n",
      "Step - 1241, Loss - 0.2897312732128167, Learning Rate - 0.025, magnitude of gradient - 0.06347358281443768\n",
      "Step - 1242, Loss - 0.34705892549468725, Learning Rate - 0.025, magnitude of gradient - 0.017010175408693852\n",
      "Step - 1243, Loss - 0.31405737440595516, Learning Rate - 0.025, magnitude of gradient - 0.01566177779780251\n",
      "Step - 1244, Loss - 0.3451640229986339, Learning Rate - 0.025, magnitude of gradient - 0.05733860092679209\n",
      "Step - 1245, Loss - 0.35869082344894926, Learning Rate - 0.025, magnitude of gradient - 0.03322062466249181\n",
      "Step - 1246, Loss - 0.34502425421897076, Learning Rate - 0.025, magnitude of gradient - 0.06546033542817133\n",
      "Step - 1247, Loss - 0.3701761906262526, Learning Rate - 0.025, magnitude of gradient - 0.019640873754996596\n",
      "Step - 1248, Loss - 0.2957043755845694, Learning Rate - 0.025, magnitude of gradient - 0.04036485798167514\n",
      "Step - 1249, Loss - 0.30554886424108374, Learning Rate - 0.025, magnitude of gradient - 0.05705797557892217\n",
      "Step - 1250, Loss - 0.2671427370643541, Learning Rate - 0.025, magnitude of gradient - 0.08986493524525252\n",
      "Step - 1251, Loss - 0.3408748623114352, Learning Rate - 0.025, magnitude of gradient - 0.06496759769055717\n",
      "Step - 1252, Loss - 0.3750086863016417, Learning Rate - 0.025, magnitude of gradient - 0.09261857145554749\n",
      "Step - 1253, Loss - 0.3570032159248451, Learning Rate - 0.025, magnitude of gradient - 0.07221986659593661\n",
      "Step - 1254, Loss - 0.3508599878646036, Learning Rate - 0.025, magnitude of gradient - 0.04383933320471964\n",
      "Step - 1255, Loss - 0.33418499882469876, Learning Rate - 0.025, magnitude of gradient - 0.0855237704380657\n",
      "Step - 1256, Loss - 0.2782662062180774, Learning Rate - 0.025, magnitude of gradient - 0.07109182155553674\n",
      "Step - 1257, Loss - 0.2983725802365903, Learning Rate - 0.025, magnitude of gradient - 0.05747571427395077\n",
      "Step - 1258, Loss - 0.3374190904229544, Learning Rate - 0.025, magnitude of gradient - 0.04564523456054043\n",
      "Step - 1259, Loss - 0.3496457413673977, Learning Rate - 0.025, magnitude of gradient - 0.10716874787053719\n",
      "Step - 1260, Loss - 0.2827442219114702, Learning Rate - 0.025, magnitude of gradient - 0.06852859392108969\n",
      "Step - 1261, Loss - 0.3847061076170797, Learning Rate - 0.025, magnitude of gradient - 0.07246792903270317\n",
      "Step - 1262, Loss - 0.29926196020934964, Learning Rate - 0.025, magnitude of gradient - 0.06379769671104257\n",
      "Step - 1263, Loss - 0.2807261135307544, Learning Rate - 0.025, magnitude of gradient - 0.09432917689887291\n",
      "Step - 1264, Loss - 0.29230895364994836, Learning Rate - 0.025, magnitude of gradient - 0.06343915573559328\n",
      "Step - 1265, Loss - 0.2904390355756893, Learning Rate - 0.025, magnitude of gradient - 0.02434254850518402\n",
      "Step - 1266, Loss - 0.39137648966263794, Learning Rate - 0.025, magnitude of gradient - 0.03332187365299302\n",
      "Step - 1267, Loss - 0.2995493111577044, Learning Rate - 0.025, magnitude of gradient - 0.06449007701625638\n",
      "Step - 1268, Loss - 0.30868636883262945, Learning Rate - 0.025, magnitude of gradient - 0.02687197209610649\n",
      "Step - 1269, Loss - 0.3746327791949508, Learning Rate - 0.025, magnitude of gradient - 0.1066821969179465\n",
      "Step - 1270, Loss - 0.3468450727635013, Learning Rate - 0.025, magnitude of gradient - 0.06886009309151951\n",
      "Step - 1271, Loss - 0.36150620538169953, Learning Rate - 0.025, magnitude of gradient - 0.07307382960931867\n",
      "Step - 1272, Loss - 0.35686940721542193, Learning Rate - 0.025, magnitude of gradient - 0.018718069585026295\n",
      "Step - 1273, Loss - 0.30872102819407743, Learning Rate - 0.025, magnitude of gradient - 0.07536450553668665\n",
      "Step - 1274, Loss - 0.2788291001580384, Learning Rate - 0.025, magnitude of gradient - 0.039874706990146905\n",
      "Step - 1275, Loss - 0.3071591174989071, Learning Rate - 0.025, magnitude of gradient - 0.03720453430506191\n",
      "Step - 1276, Loss - 0.29618541579537516, Learning Rate - 0.025, magnitude of gradient - 0.03309608993115105\n",
      "Step - 1277, Loss - 0.37305923838181987, Learning Rate - 0.025, magnitude of gradient - 0.028960232376356598\n",
      "Step - 1278, Loss - 0.33782642697362125, Learning Rate - 0.025, magnitude of gradient - 0.014274392154488781\n",
      "Step - 1279, Loss - 0.3101298186443521, Learning Rate - 0.025, magnitude of gradient - 0.057588657324623996\n",
      "Step - 1280, Loss - 0.30056323093640247, Learning Rate - 0.025, magnitude of gradient - 0.02358910077724772\n",
      "Step - 1281, Loss - 0.32376242398826677, Learning Rate - 0.025, magnitude of gradient - 0.05166891267079634\n",
      "Step - 1282, Loss - 0.2898104531436717, Learning Rate - 0.025, magnitude of gradient - 0.07193263600957642\n",
      "Step - 1283, Loss - 0.3719246278223094, Learning Rate - 0.025, magnitude of gradient - 0.052794029464162576\n",
      "Step - 1284, Loss - 0.2735366697105612, Learning Rate - 0.025, magnitude of gradient - 0.04096479809019825\n",
      "Step - 1285, Loss - 0.3107687289355857, Learning Rate - 0.025, magnitude of gradient - 0.056722515883759324\n",
      "Step - 1286, Loss - 0.29568106126947025, Learning Rate - 0.025, magnitude of gradient - 0.021737907515304047\n",
      "Step - 1287, Loss - 0.29630988789704604, Learning Rate - 0.025, magnitude of gradient - 0.04537533578557204\n",
      "Step - 1288, Loss - 0.27913075544596927, Learning Rate - 0.025, magnitude of gradient - 0.06964348661784421\n",
      "Step - 1289, Loss - 0.3223111962338353, Learning Rate - 0.025, magnitude of gradient - 0.02174705115443543\n",
      "Step - 1290, Loss - 0.3461703258032366, Learning Rate - 0.025, magnitude of gradient - 0.02756405718014729\n",
      "Step - 1291, Loss - 0.32402962369373306, Learning Rate - 0.025, magnitude of gradient - 0.06869523041778812\n",
      "Step - 1292, Loss - 0.2975406723282138, Learning Rate - 0.025, magnitude of gradient - 0.06617336898638963\n",
      "Step - 1293, Loss - 0.29262444932774284, Learning Rate - 0.025, magnitude of gradient - 0.06983218652932083\n",
      "Step - 1294, Loss - 0.2989322299863263, Learning Rate - 0.025, magnitude of gradient - 0.01572931470944706\n",
      "Step - 1295, Loss - 0.3548620888381512, Learning Rate - 0.025, magnitude of gradient - 0.07759122275556808\n",
      "Step - 1296, Loss - 0.2371662693395719, Learning Rate - 0.025, magnitude of gradient - 0.06684427818356531\n",
      "Step - 1297, Loss - 0.32814446127086067, Learning Rate - 0.025, magnitude of gradient - 0.05257825531614177\n",
      "Step - 1298, Loss - 0.27209446900850226, Learning Rate - 0.025, magnitude of gradient - 0.009625724061872098\n",
      "Step - 1299, Loss - 0.35547082067351, Learning Rate - 0.025, magnitude of gradient - 0.0419314632908157\n",
      "Step - 1300, Loss - 0.3055509411844173, Learning Rate - 0.025, magnitude of gradient - 0.03572850004705665\n",
      "Step - 1301, Loss - 0.3892866923252797, Learning Rate - 0.025, magnitude of gradient - 0.03136398436444132\n",
      "Step - 1302, Loss - 0.4013395312793187, Learning Rate - 0.025, magnitude of gradient - 0.04493567148031865\n",
      "Step - 1303, Loss - 0.37415461876104905, Learning Rate - 0.025, magnitude of gradient - 0.021438592469637516\n",
      "Step - 1304, Loss - 0.33049875877377866, Learning Rate - 0.025, magnitude of gradient - 0.050205527305731155\n",
      "Step - 1305, Loss - 0.2809725531972411, Learning Rate - 0.025, magnitude of gradient - 0.05720808726964936\n",
      "Step - 1306, Loss - 0.3456151740962007, Learning Rate - 0.025, magnitude of gradient - 0.00831820867809786\n",
      "Step - 1307, Loss - 0.2797727317304656, Learning Rate - 0.025, magnitude of gradient - 0.023902640534394105\n",
      "Step - 1308, Loss - 0.294281644504445, Learning Rate - 0.025, magnitude of gradient - 0.08368475994527812\n",
      "Step - 1309, Loss - 0.2721568470236777, Learning Rate - 0.025, magnitude of gradient - 0.031930477830473904\n",
      "Step - 1310, Loss - 0.2959377005716324, Learning Rate - 0.025, magnitude of gradient - 0.06021310084952826\n",
      "Step - 1311, Loss - 0.281177565232136, Learning Rate - 0.025, magnitude of gradient - 0.02698114352419255\n",
      "Step - 1312, Loss - 0.34895973309980466, Learning Rate - 0.025, magnitude of gradient - 0.06628826753790323\n",
      "Step - 1313, Loss - 0.32984131709230263, Learning Rate - 0.025, magnitude of gradient - 0.027785555227540556\n",
      "Step - 1314, Loss - 0.36930219963283645, Learning Rate - 0.025, magnitude of gradient - 0.1292456169059961\n",
      "Step - 1315, Loss - 0.2635701122877332, Learning Rate - 0.025, magnitude of gradient - 0.030544428829270553\n",
      "Step - 1316, Loss - 0.27828002506596994, Learning Rate - 0.025, magnitude of gradient - 0.11150964574473846\n",
      "Step - 1317, Loss - 0.3118300782857955, Learning Rate - 0.025, magnitude of gradient - 0.08011824459513636\n",
      "Step - 1318, Loss - 0.28075238588518514, Learning Rate - 0.025, magnitude of gradient - 0.1299108642311067\n",
      "Step - 1319, Loss - 0.26690753530256756, Learning Rate - 0.025, magnitude of gradient - 0.050040902886572355\n",
      "Step - 1320, Loss - 0.3115211730526839, Learning Rate - 0.025, magnitude of gradient - 0.04864076164060511\n",
      "Step - 1321, Loss - 0.2780366065173538, Learning Rate - 0.025, magnitude of gradient - 0.05280313418068563\n",
      "Step - 1322, Loss - 0.28968434393484277, Learning Rate - 0.025, magnitude of gradient - 0.011243243404314335\n",
      "Step - 1323, Loss - 0.3534632933941649, Learning Rate - 0.025, magnitude of gradient - 0.027541281717591223\n",
      "Step - 1324, Loss - 0.30090181105872366, Learning Rate - 0.025, magnitude of gradient - 0.055107877238236035\n",
      "Step - 1325, Loss - 0.30276922399671935, Learning Rate - 0.025, magnitude of gradient - 0.01632080562031437\n",
      "Step - 1326, Loss - 0.3380897922830478, Learning Rate - 0.025, magnitude of gradient - 0.017151456584466217\n",
      "Step - 1327, Loss - 0.4374945238193709, Learning Rate - 0.025, magnitude of gradient - 0.0714120467371784\n",
      "Step - 1328, Loss - 0.3433624736152358, Learning Rate - 0.025, magnitude of gradient - 0.09050468355023174\n",
      "Step - 1329, Loss - 0.34579615971142064, Learning Rate - 0.025, magnitude of gradient - 0.03973610535197748\n",
      "Step - 1330, Loss - 0.2623647299917289, Learning Rate - 0.025, magnitude of gradient - 0.09244002009080188\n",
      "Step - 1331, Loss - 0.31838658325542146, Learning Rate - 0.025, magnitude of gradient - 0.024397614363821374\n",
      "Step - 1332, Loss - 0.25352404958226193, Learning Rate - 0.025, magnitude of gradient - 0.0948174532020248\n",
      "Step - 1333, Loss - 0.26033459992327546, Learning Rate - 0.025, magnitude of gradient - 0.02106096350558439\n",
      "Step - 1334, Loss - 0.27879563133653956, Learning Rate - 0.025, magnitude of gradient - 0.014816775131281153\n",
      "Step - 1335, Loss - 0.3535616521014559, Learning Rate - 0.025, magnitude of gradient - 0.03999823812324166\n",
      "Step - 1336, Loss - 0.27953564044117685, Learning Rate - 0.025, magnitude of gradient - 0.031062477014853478\n",
      "Step - 1337, Loss - 0.3262611819971646, Learning Rate - 0.025, magnitude of gradient - 0.10036290722936878\n",
      "Step - 1338, Loss - 0.28566054174749395, Learning Rate - 0.025, magnitude of gradient - 0.04317843030503316\n",
      "Step - 1339, Loss - 0.2819574447579918, Learning Rate - 0.025, magnitude of gradient - 0.05791986844482731\n",
      "Step - 1340, Loss - 0.33074796378251675, Learning Rate - 0.025, magnitude of gradient - 0.0869317452523165\n",
      "Step - 1341, Loss - 0.29312568187000154, Learning Rate - 0.025, magnitude of gradient - 0.060244532919824886\n",
      "Step - 1342, Loss - 0.29472569214349487, Learning Rate - 0.025, magnitude of gradient - 0.021922555407429087\n",
      "Step - 1343, Loss - 0.25930813997067637, Learning Rate - 0.025, magnitude of gradient - 0.03724660841781212\n",
      "Step - 1344, Loss - 0.36106405031651445, Learning Rate - 0.025, magnitude of gradient - 0.012151174418468842\n",
      "Step - 1345, Loss - 0.35909978447752916, Learning Rate - 0.025, magnitude of gradient - 0.08766084304401121\n",
      "Step - 1346, Loss - 0.3164955693977894, Learning Rate - 0.025, magnitude of gradient - 0.10883607108613477\n",
      "Step - 1347, Loss - 0.2989288286952767, Learning Rate - 0.025, magnitude of gradient - 0.007164690759538944\n",
      "Step - 1348, Loss - 0.2982340316805397, Learning Rate - 0.025, magnitude of gradient - 0.05719377389260129\n",
      "Step - 1349, Loss - 0.31352581255284695, Learning Rate - 0.025, magnitude of gradient - 0.0679314430916438\n",
      "Step - 1350, Loss - 0.3313223196545879, Learning Rate - 0.025, magnitude of gradient - 0.04792685041622125\n",
      "Step - 1351, Loss - 0.27175794018068405, Learning Rate - 0.025, magnitude of gradient - 0.08768431991259255\n",
      "Step - 1352, Loss - 0.339679696988301, Learning Rate - 0.025, magnitude of gradient - 0.026288089225267687\n",
      "Step - 1353, Loss - 0.2723626942249518, Learning Rate - 0.025, magnitude of gradient - 0.01624423753734667\n",
      "Step - 1354, Loss - 0.264343309358246, Learning Rate - 0.025, magnitude of gradient - 0.04598635604499764\n",
      "Step - 1355, Loss - 0.29915701972477393, Learning Rate - 0.025, magnitude of gradient - 0.05664135634375537\n",
      "Step - 1356, Loss - 0.3515165999039015, Learning Rate - 0.025, magnitude of gradient - 0.056503756630441104\n",
      "Step - 1357, Loss - 0.3289512362940524, Learning Rate - 0.025, magnitude of gradient - 0.06484599286498624\n",
      "Step - 1358, Loss - 0.34456044382503165, Learning Rate - 0.025, magnitude of gradient - 0.03268602906193091\n",
      "Step - 1359, Loss - 0.2952923548239202, Learning Rate - 0.025, magnitude of gradient - 0.062203313826014156\n",
      "Step - 1360, Loss - 0.29334218805476847, Learning Rate - 0.025, magnitude of gradient - 0.06815576759139216\n",
      "Step - 1361, Loss - 0.2667134621836532, Learning Rate - 0.025, magnitude of gradient - 0.04435683727576473\n",
      "Step - 1362, Loss - 0.286404715685162, Learning Rate - 0.025, magnitude of gradient - 0.033991304001651686\n",
      "Step - 1363, Loss - 0.27547783032702083, Learning Rate - 0.025, magnitude of gradient - 0.05373761156181337\n",
      "Step - 1364, Loss - 0.2916157259693761, Learning Rate - 0.025, magnitude of gradient - 0.004731535072611962\n",
      "Step - 1365, Loss - 0.40716183656473875, Learning Rate - 0.025, magnitude of gradient - 0.0170827014658837\n",
      "Step - 1366, Loss - 0.32291658319041294, Learning Rate - 0.025, magnitude of gradient - 0.05793689308378807\n",
      "Step - 1367, Loss - 0.3618917598636535, Learning Rate - 0.025, magnitude of gradient - 0.05675937787193332\n",
      "Step - 1368, Loss - 0.31954135715087556, Learning Rate - 0.025, magnitude of gradient - 0.061861358370800985\n",
      "Step - 1369, Loss - 0.3844887389020235, Learning Rate - 0.025, magnitude of gradient - 0.030523774392361876\n",
      "Step - 1370, Loss - 0.3971511687997759, Learning Rate - 0.025, magnitude of gradient - 0.13459087574917908\n",
      "Step - 1371, Loss - 0.37849462974172854, Learning Rate - 0.025, magnitude of gradient - 0.1145127477837787\n",
      "Step - 1372, Loss - 0.2532523285751541, Learning Rate - 0.025, magnitude of gradient - 0.055321602131545126\n",
      "Step - 1373, Loss - 0.2779746223388119, Learning Rate - 0.025, magnitude of gradient - 0.04367666472972953\n",
      "Step - 1374, Loss - 0.356983117293753, Learning Rate - 0.025, magnitude of gradient - 0.1028877237854846\n",
      "Step - 1375, Loss - 0.31921221278704204, Learning Rate - 0.025, magnitude of gradient - 0.021140639153425976\n",
      "Step - 1376, Loss - 0.30078833600308363, Learning Rate - 0.025, magnitude of gradient - 0.10293018513045703\n",
      "Step - 1377, Loss - 0.33079211697106936, Learning Rate - 0.025, magnitude of gradient - 0.04572652971726664\n",
      "Step - 1378, Loss - 0.3156845196682844, Learning Rate - 0.025, magnitude of gradient - 0.044056304571713695\n",
      "Step - 1379, Loss - 0.3125861583860211, Learning Rate - 0.025, magnitude of gradient - 0.05056451486128273\n",
      "Step - 1380, Loss - 0.3469315041367729, Learning Rate - 0.025, magnitude of gradient - 0.1312843435041152\n",
      "Step - 1381, Loss - 0.24758633907560085, Learning Rate - 0.025, magnitude of gradient - 0.0963638213943294\n",
      "Step - 1382, Loss - 0.35809649586712117, Learning Rate - 0.025, magnitude of gradient - 0.10153133274461536\n",
      "Step - 1383, Loss - 0.37446683438541173, Learning Rate - 0.025, magnitude of gradient - 0.07920632519208777\n",
      "Step - 1384, Loss - 0.31395370250498006, Learning Rate - 0.025, magnitude of gradient - 0.042915576531013716\n",
      "Step - 1385, Loss - 0.30121550817818515, Learning Rate - 0.025, magnitude of gradient - 0.044139831591572795\n",
      "Step - 1386, Loss - 0.33884612544011994, Learning Rate - 0.025, magnitude of gradient - 0.08289115316368377\n",
      "Step - 1387, Loss - 0.2828247357680719, Learning Rate - 0.025, magnitude of gradient - 0.05438632946281216\n",
      "Step - 1388, Loss - 0.46341985431699473, Learning Rate - 0.025, magnitude of gradient - 0.0849066042905411\n",
      "Step - 1389, Loss - 0.22862850789895478, Learning Rate - 0.025, magnitude of gradient - 0.09180840523406404\n",
      "Step - 1390, Loss - 0.2966548569563728, Learning Rate - 0.025, magnitude of gradient - 0.03484741525353851\n",
      "Step - 1391, Loss - 0.3217480368241692, Learning Rate - 0.025, magnitude of gradient - 0.03587370964495079\n",
      "Step - 1392, Loss - 0.3335342332026564, Learning Rate - 0.025, magnitude of gradient - 0.033198264371666925\n",
      "Step - 1393, Loss - 0.27272448988972847, Learning Rate - 0.025, magnitude of gradient - 0.09524094253835325\n",
      "Step - 1394, Loss - 0.2834917521470397, Learning Rate - 0.025, magnitude of gradient - 0.04520853815819986\n",
      "Step - 1395, Loss - 0.29963301242783097, Learning Rate - 0.025, magnitude of gradient - 0.1145357306075577\n",
      "Step - 1396, Loss - 0.3595337272978313, Learning Rate - 0.025, magnitude of gradient - 0.04971140206030637\n",
      "Step - 1397, Loss - 0.3458726799460321, Learning Rate - 0.025, magnitude of gradient - 0.07114357977901577\n",
      "Step - 1398, Loss - 0.33436747155281227, Learning Rate - 0.025, magnitude of gradient - 0.12914668978020793\n",
      "Step - 1399, Loss - 0.2842079471680423, Learning Rate - 0.025, magnitude of gradient - 0.07706442249666441\n",
      "Step - 1400, Loss - 0.3750747333167367, Learning Rate - 0.025, magnitude of gradient - 0.08649306348338717\n",
      "Step - 1401, Loss - 0.3645033766646277, Learning Rate - 0.025, magnitude of gradient - 0.029209767069584038\n",
      "Step - 1402, Loss - 0.32342463520980513, Learning Rate - 0.025, magnitude of gradient - 0.037982234504566156\n",
      "Step - 1403, Loss - 0.3268662639421412, Learning Rate - 0.025, magnitude of gradient - 0.049704509375466145\n",
      "Step - 1404, Loss - 0.3048478604247782, Learning Rate - 0.025, magnitude of gradient - 0.05879150230828354\n",
      "Step - 1405, Loss - 0.31377420051173815, Learning Rate - 0.025, magnitude of gradient - 0.04917658621856863\n",
      "Step - 1406, Loss - 0.2716282842410116, Learning Rate - 0.025, magnitude of gradient - 0.03826410133183655\n",
      "Step - 1407, Loss - 0.3468391508635702, Learning Rate - 0.025, magnitude of gradient - 0.0347721558370992\n",
      "Step - 1408, Loss - 0.3290014793284864, Learning Rate - 0.025, magnitude of gradient - 0.106511688145395\n",
      "Step - 1409, Loss - 0.25464132608531354, Learning Rate - 0.025, magnitude of gradient - 0.06335095584764625\n",
      "Step - 1410, Loss - 0.2672011613261823, Learning Rate - 0.025, magnitude of gradient - 0.0756485506004137\n",
      "Step - 1411, Loss - 0.3526785899697121, Learning Rate - 0.025, magnitude of gradient - 0.04081382175992148\n",
      "Step - 1412, Loss - 0.3208331333395329, Learning Rate - 0.025, magnitude of gradient - 0.018218008038932035\n",
      "Step - 1413, Loss - 0.32979184454511073, Learning Rate - 0.025, magnitude of gradient - 0.03665484375233879\n",
      "Step - 1414, Loss - 0.33879171204663694, Learning Rate - 0.025, magnitude of gradient - 0.03885497601542211\n",
      "Step - 1415, Loss - 0.3691402355547234, Learning Rate - 0.025, magnitude of gradient - 0.08972799384158135\n",
      "Step - 1416, Loss - 0.32384042487840276, Learning Rate - 0.025, magnitude of gradient - 0.02668141028338092\n",
      "Step - 1417, Loss - 0.27662397623838064, Learning Rate - 0.025, magnitude of gradient - 0.020461981048040957\n",
      "Step - 1418, Loss - 0.32272127167139436, Learning Rate - 0.025, magnitude of gradient - 0.08667016952161466\n",
      "Step - 1419, Loss - 0.3640523726793269, Learning Rate - 0.025, magnitude of gradient - 0.040850949845850706\n",
      "Step - 1420, Loss - 0.3141161928661064, Learning Rate - 0.025, magnitude of gradient - 0.10403485558989038\n",
      "Step - 1421, Loss - 0.31255050273189844, Learning Rate - 0.025, magnitude of gradient - 0.023320823003019915\n",
      "Step - 1422, Loss - 0.37631193574953126, Learning Rate - 0.025, magnitude of gradient - 0.06023819400792305\n",
      "Step - 1423, Loss - 0.31592353692024056, Learning Rate - 0.025, magnitude of gradient - 0.03177895269750142\n",
      "Step - 1424, Loss - 0.2996396455944189, Learning Rate - 0.025, magnitude of gradient - 0.022642207166061915\n",
      "Step - 1425, Loss - 0.3813832629026071, Learning Rate - 0.025, magnitude of gradient - 0.08123011073687485\n",
      "Step - 1426, Loss - 0.33186176438851545, Learning Rate - 0.025, magnitude of gradient - 0.028097496480120346\n",
      "Step - 1427, Loss - 0.28302737920875354, Learning Rate - 0.025, magnitude of gradient - 0.053644111881334985\n",
      "Step - 1428, Loss - 0.36457666172182945, Learning Rate - 0.025, magnitude of gradient - 0.04415949752836786\n",
      "Step - 1429, Loss - 0.2721945162610769, Learning Rate - 0.025, magnitude of gradient - 0.008556083310920696\n",
      "Step - 1430, Loss - 0.3752764109968752, Learning Rate - 0.025, magnitude of gradient - 0.05102990553233965\n",
      "Step - 1431, Loss - 0.37252954483232753, Learning Rate - 0.025, magnitude of gradient - 0.09896565457483615\n",
      "Step - 1432, Loss - 0.3084232666716946, Learning Rate - 0.025, magnitude of gradient - 0.07904609023782223\n",
      "Step - 1433, Loss - 0.35816112368705616, Learning Rate - 0.025, magnitude of gradient - 0.06730833560913134\n",
      "Step - 1434, Loss - 0.3794112105282769, Learning Rate - 0.025, magnitude of gradient - 0.009674561060500117\n",
      "Step - 1435, Loss - 0.34515248232347434, Learning Rate - 0.025, magnitude of gradient - 0.03919834818288905\n",
      "Step - 1436, Loss - 0.3284046452563849, Learning Rate - 0.025, magnitude of gradient - 0.09080759510902059\n",
      "Step - 1437, Loss - 0.24463915652023488, Learning Rate - 0.025, magnitude of gradient - 0.06643966613598085\n",
      "Step - 1438, Loss - 0.30688720924044044, Learning Rate - 0.025, magnitude of gradient - 0.022946591339931342\n",
      "Step - 1439, Loss - 0.29234317983650654, Learning Rate - 0.025, magnitude of gradient - 0.07309112632616034\n",
      "Step - 1440, Loss - 0.32414570987368085, Learning Rate - 0.025, magnitude of gradient - 0.03663121823642396\n",
      "Step - 1441, Loss - 0.34329362572442823, Learning Rate - 0.025, magnitude of gradient - 0.0505318009963962\n",
      "Step - 1442, Loss - 0.30598604389588374, Learning Rate - 0.025, magnitude of gradient - 0.04838833355363784\n",
      "Step - 1443, Loss - 0.3556458481382305, Learning Rate - 0.025, magnitude of gradient - 0.05172666355056881\n",
      "Step - 1444, Loss - 0.3072651401022244, Learning Rate - 0.025, magnitude of gradient - 0.12234212719034976\n",
      "Step - 1445, Loss - 0.24913594259250174, Learning Rate - 0.025, magnitude of gradient - 0.04017578742244848\n",
      "Step - 1446, Loss - 0.3281680638020761, Learning Rate - 0.025, magnitude of gradient - 0.013023962781195872\n",
      "Step - 1447, Loss - 0.3530253816979116, Learning Rate - 0.025, magnitude of gradient - 0.022930681730621995\n",
      "Step - 1448, Loss - 0.28431450141497183, Learning Rate - 0.025, magnitude of gradient - 0.02290847121615859\n",
      "Step - 1449, Loss - 0.37841054602386615, Learning Rate - 0.025, magnitude of gradient - 0.056398374207508706\n",
      "Step - 1450, Loss - 0.3106610549161475, Learning Rate - 0.025, magnitude of gradient - 0.0513577217079987\n",
      "Step - 1451, Loss - 0.3420317129429351, Learning Rate - 0.025, magnitude of gradient - 0.058621363232608956\n",
      "Step - 1452, Loss - 0.3905154634762164, Learning Rate - 0.025, magnitude of gradient - 0.07433214911214742\n",
      "Step - 1453, Loss - 0.27506568249535834, Learning Rate - 0.025, magnitude of gradient - 0.03868930822712244\n",
      "Step - 1454, Loss - 0.30432550827898375, Learning Rate - 0.025, magnitude of gradient - 0.03743249137105332\n",
      "Step - 1455, Loss - 0.27228711292857094, Learning Rate - 0.025, magnitude of gradient - 0.08826577596718858\n",
      "Step - 1456, Loss - 0.39368206451315463, Learning Rate - 0.025, magnitude of gradient - 0.043572348582870755\n",
      "Step - 1457, Loss - 0.23121154437182756, Learning Rate - 0.025, magnitude of gradient - 0.07919428091836507\n",
      "Step - 1458, Loss - 0.2603253044181493, Learning Rate - 0.025, magnitude of gradient - 0.04981158269638498\n",
      "Step - 1459, Loss - 0.33752440487926477, Learning Rate - 0.025, magnitude of gradient - 0.07160165737562064\n",
      "Step - 1460, Loss - 0.34796546457321725, Learning Rate - 0.025, magnitude of gradient - 0.04974812349012866\n",
      "Step - 1461, Loss - 0.30181628214021344, Learning Rate - 0.025, magnitude of gradient - 0.028442098261758947\n",
      "Step - 1462, Loss - 0.34368913369559895, Learning Rate - 0.025, magnitude of gradient - 0.01878332878639511\n",
      "Step - 1463, Loss - 0.3947720694836074, Learning Rate - 0.025, magnitude of gradient - 0.03378096996164403\n",
      "Step - 1464, Loss - 0.2863360373020965, Learning Rate - 0.025, magnitude of gradient - 0.07136506918252886\n",
      "Step - 1465, Loss - 0.38881005532816815, Learning Rate - 0.025, magnitude of gradient - 0.05660289987357342\n",
      "Step - 1466, Loss - 0.28410737297537536, Learning Rate - 0.025, magnitude of gradient - 0.05509992819085053\n",
      "Step - 1467, Loss - 0.34670471814055587, Learning Rate - 0.025, magnitude of gradient - 0.05120959843137147\n",
      "Step - 1468, Loss - 0.2965524932273238, Learning Rate - 0.025, magnitude of gradient - 0.0701846409470123\n",
      "Step - 1469, Loss - 0.32638158992063804, Learning Rate - 0.025, magnitude of gradient - 0.08652272968040008\n",
      "Step - 1470, Loss - 0.39742231189344235, Learning Rate - 0.025, magnitude of gradient - 0.06171965881704728\n",
      "Step - 1471, Loss - 0.3347026740825049, Learning Rate - 0.025, magnitude of gradient - 0.042185995416719566\n",
      "Step - 1472, Loss - 0.30841870600101295, Learning Rate - 0.025, magnitude of gradient - 0.05684050109081574\n",
      "Step - 1473, Loss - 0.3097410360720501, Learning Rate - 0.025, magnitude of gradient - 0.056380805618772026\n",
      "Step - 1474, Loss - 0.3486312412324045, Learning Rate - 0.025, magnitude of gradient - 0.03996710218270311\n",
      "Step - 1475, Loss - 0.33900988528270487, Learning Rate - 0.025, magnitude of gradient - 0.08941563771466568\n",
      "Step - 1476, Loss - 0.34797912088155747, Learning Rate - 0.025, magnitude of gradient - 0.05338372878234339\n",
      "Step - 1477, Loss - 0.32925050764846253, Learning Rate - 0.025, magnitude of gradient - 0.11938656151176062\n",
      "Step - 1478, Loss - 0.2639404371462236, Learning Rate - 0.025, magnitude of gradient - 0.05040509575739628\n",
      "Step - 1479, Loss - 0.3868032446668116, Learning Rate - 0.025, magnitude of gradient - 0.046389897624074396\n",
      "Step - 1480, Loss - 0.2940552040372679, Learning Rate - 0.025, magnitude of gradient - 0.05904465083353746\n",
      "Step - 1481, Loss - 0.29121804389984784, Learning Rate - 0.025, magnitude of gradient - 0.05887770279254216\n",
      "Step - 1482, Loss - 0.38727657427263656, Learning Rate - 0.025, magnitude of gradient - 0.09541057392786256\n",
      "Step - 1483, Loss - 0.34974114003542217, Learning Rate - 0.025, magnitude of gradient - 0.02282823568841597\n",
      "Step - 1484, Loss - 0.35850035496524735, Learning Rate - 0.025, magnitude of gradient - 0.05758562049400134\n",
      "Step - 1485, Loss - 0.23781599401881368, Learning Rate - 0.025, magnitude of gradient - 0.08805710295496233\n",
      "Step - 1486, Loss - 0.3588177808860389, Learning Rate - 0.025, magnitude of gradient - 0.025253732542043376\n",
      "Step - 1487, Loss - 0.30755081881362567, Learning Rate - 0.025, magnitude of gradient - 0.0165061986768738\n",
      "Step - 1488, Loss - 0.247181930614121, Learning Rate - 0.025, magnitude of gradient - 0.02114867621296304\n",
      "Step - 1489, Loss - 0.3579769827575436, Learning Rate - 0.025, magnitude of gradient - 0.014542276776851042\n",
      "Step - 1490, Loss - 0.3763759404413674, Learning Rate - 0.025, magnitude of gradient - 0.07085113248724828\n",
      "Step - 1491, Loss - 0.40732866057211436, Learning Rate - 0.025, magnitude of gradient - 0.0877914100866754\n",
      "Step - 1492, Loss - 0.3618697605459232, Learning Rate - 0.025, magnitude of gradient - 0.04596787331766825\n",
      "Step - 1493, Loss - 0.25936712483154156, Learning Rate - 0.025, magnitude of gradient - 0.0025078995563161902\n",
      "Step - 1494, Loss - 0.3743191070248706, Learning Rate - 0.025, magnitude of gradient - 0.07834120220143884\n",
      "Step - 1495, Loss - 0.3733667801957159, Learning Rate - 0.025, magnitude of gradient - 0.021770114778165525\n",
      "Step - 1496, Loss - 0.3224115734663249, Learning Rate - 0.025, magnitude of gradient - 0.060171298728402677\n",
      "Step - 1497, Loss - 0.38404912910248723, Learning Rate - 0.025, magnitude of gradient - 0.08415498191758948\n",
      "Step - 1498, Loss - 0.2782543926225917, Learning Rate - 0.025, magnitude of gradient - 0.03768925200099658\n",
      "Step - 1499, Loss - 0.3168564190135925, Learning Rate - 0.025, magnitude of gradient - 0.020394618643193314\n",
      "Step - 1500, Loss - 0.3519241369765469, Learning Rate - 0.025, magnitude of gradient - 0.09879861086229849\n",
      "Step - 1501, Loss - 0.3821135764817805, Learning Rate - 0.025, magnitude of gradient - 0.05037670399779387\n",
      "Step - 1502, Loss - 0.34214331623353866, Learning Rate - 0.025, magnitude of gradient - 0.0395354873165836\n",
      "Step - 1503, Loss - 0.32330383471876795, Learning Rate - 0.025, magnitude of gradient - 0.043128593858754664\n",
      "Step - 1504, Loss - 0.2933208146869094, Learning Rate - 0.025, magnitude of gradient - 0.025114923336590395\n",
      "Step - 1505, Loss - 0.2252647417266665, Learning Rate - 0.025, magnitude of gradient - 0.06985185313253199\n",
      "Step - 1506, Loss - 0.30381464497694377, Learning Rate - 0.025, magnitude of gradient - 0.06754604010211349\n",
      "Step - 1507, Loss - 0.30600536120423205, Learning Rate - 0.025, magnitude of gradient - 0.03728001585524177\n",
      "Step - 1508, Loss - 0.25520976902951126, Learning Rate - 0.025, magnitude of gradient - 0.027816910703901705\n",
      "Step - 1509, Loss - 0.24254165364428265, Learning Rate - 0.025, magnitude of gradient - 0.049288071255053835\n",
      "Step - 1510, Loss - 0.3178069224378005, Learning Rate - 0.025, magnitude of gradient - 0.014460012430314546\n",
      "Step - 1511, Loss - 0.2798686675480379, Learning Rate - 0.025, magnitude of gradient - 0.08577767967856426\n",
      "Step - 1512, Loss - 0.33620918110974407, Learning Rate - 0.025, magnitude of gradient - 0.0056263583429089796\n",
      "Step - 1513, Loss - 0.34156419892318723, Learning Rate - 0.025, magnitude of gradient - 0.0719195983293412\n",
      "Step - 1514, Loss - 0.29152594489848077, Learning Rate - 0.025, magnitude of gradient - 0.07422892370573733\n",
      "Step - 1515, Loss - 0.30515768476589156, Learning Rate - 0.025, magnitude of gradient - 0.06661063459627235\n",
      "Step - 1516, Loss - 0.3225354221508432, Learning Rate - 0.025, magnitude of gradient - 0.02697251960575265\n",
      "Step - 1517, Loss - 0.3146868062213611, Learning Rate - 0.025, magnitude of gradient - 0.09608507825574204\n",
      "Step - 1518, Loss - 0.2746270137725264, Learning Rate - 0.025, magnitude of gradient - 0.03849412411061819\n",
      "Step - 1519, Loss - 0.34931009408628233, Learning Rate - 0.025, magnitude of gradient - 0.07146794193180786\n",
      "Step - 1520, Loss - 0.2745628621882573, Learning Rate - 0.025, magnitude of gradient - 0.029818661691787665\n",
      "Step - 1521, Loss - 0.34101243041823426, Learning Rate - 0.025, magnitude of gradient - 0.04087836185363225\n",
      "Step - 1522, Loss - 0.3270069151369519, Learning Rate - 0.025, magnitude of gradient - 0.05925559391669498\n",
      "Step - 1523, Loss - 0.278013487861282, Learning Rate - 0.025, magnitude of gradient - 0.03322786521091622\n",
      "Step - 1524, Loss - 0.44873279867546356, Learning Rate - 0.025, magnitude of gradient - 0.03246295723761805\n",
      "Step - 1525, Loss - 0.3338501795119211, Learning Rate - 0.025, magnitude of gradient - 0.011283933441839589\n",
      "Step - 1526, Loss - 0.32822717693245373, Learning Rate - 0.025, magnitude of gradient - 0.0030877077449741382\n",
      "Step - 1527, Loss - 0.2541996002937077, Learning Rate - 0.025, magnitude of gradient - 0.045482031973060305\n",
      "Step - 1528, Loss - 0.3784041626297539, Learning Rate - 0.025, magnitude of gradient - 0.04910261955037496\n",
      "Step - 1529, Loss - 0.29315746815277105, Learning Rate - 0.025, magnitude of gradient - 0.07360826969181912\n",
      "Step - 1530, Loss - 0.39876394030273743, Learning Rate - 0.025, magnitude of gradient - 0.06264722217253853\n",
      "Step - 1531, Loss - 0.29419547550017133, Learning Rate - 0.025, magnitude of gradient - 0.0853352417819401\n",
      "Step - 1532, Loss - 0.4033675245098586, Learning Rate - 0.025, magnitude of gradient - 0.08596167039046869\n",
      "Step - 1533, Loss - 0.3382095618840938, Learning Rate - 0.025, magnitude of gradient - 0.07597657512622373\n",
      "Step - 1534, Loss - 0.3714639524375099, Learning Rate - 0.025, magnitude of gradient - 0.023167797339701708\n",
      "Step - 1535, Loss - 0.3793181112842533, Learning Rate - 0.025, magnitude of gradient - 0.0631012885188699\n",
      "Step - 1536, Loss - 0.3298385223763654, Learning Rate - 0.025, magnitude of gradient - 0.03763919621048138\n",
      "Step - 1537, Loss - 0.3458748676896971, Learning Rate - 0.025, magnitude of gradient - 0.05721528128747858\n",
      "Step - 1538, Loss - 0.3420440396993724, Learning Rate - 0.025, magnitude of gradient - 0.07016614559006679\n",
      "Step - 1539, Loss - 0.285910603578426, Learning Rate - 0.025, magnitude of gradient - 0.027374893545610384\n",
      "Step - 1540, Loss - 0.3302750241518051, Learning Rate - 0.025, magnitude of gradient - 0.04910586379352066\n",
      "Step - 1541, Loss - 0.3237018369913234, Learning Rate - 0.025, magnitude of gradient - 0.0778573836917056\n",
      "Step - 1542, Loss - 0.3497976241390995, Learning Rate - 0.025, magnitude of gradient - 0.055955878321010545\n",
      "Step - 1543, Loss - 0.3078012042424747, Learning Rate - 0.025, magnitude of gradient - 0.03152283613726332\n",
      "Step - 1544, Loss - 0.4050080354430554, Learning Rate - 0.025, magnitude of gradient - 0.03246546782599699\n",
      "Step - 1545, Loss - 0.28233194887738716, Learning Rate - 0.025, magnitude of gradient - 0.015498111026620038\n",
      "Step - 1546, Loss - 0.26173675677020636, Learning Rate - 0.025, magnitude of gradient - 0.04911149208241959\n",
      "Step - 1547, Loss - 0.3096781028747124, Learning Rate - 0.025, magnitude of gradient - 0.04859606550129786\n",
      "Step - 1548, Loss - 0.31137464190406694, Learning Rate - 0.025, magnitude of gradient - 0.06992952222370974\n",
      "Step - 1549, Loss - 0.3002712874973763, Learning Rate - 0.025, magnitude of gradient - 0.03637515438396851\n",
      "Step - 1550, Loss - 0.35211231325629994, Learning Rate - 0.025, magnitude of gradient - 0.02057625663563669\n",
      "Step - 1551, Loss - 0.2962175718533975, Learning Rate - 0.025, magnitude of gradient - 0.08769563494260446\n",
      "Step - 1552, Loss - 0.22636372496335888, Learning Rate - 0.025, magnitude of gradient - 0.07118323332733283\n",
      "Step - 1553, Loss - 0.29028578643694897, Learning Rate - 0.025, magnitude of gradient - 0.034099755509447965\n",
      "Step - 1554, Loss - 0.26111509395523635, Learning Rate - 0.025, magnitude of gradient - 0.018296610000077797\n",
      "Step - 1555, Loss - 0.3934418133867497, Learning Rate - 0.025, magnitude of gradient - 0.09134551967884863\n",
      "Step - 1556, Loss - 0.32256447393019444, Learning Rate - 0.025, magnitude of gradient - 0.036035520008328444\n",
      "Step - 1557, Loss - 0.30322057480226255, Learning Rate - 0.025, magnitude of gradient - 0.08539844631151594\n",
      "Step - 1558, Loss - 0.34313059281063407, Learning Rate - 0.025, magnitude of gradient - 0.02346931808367989\n",
      "Step - 1559, Loss - 0.3717764769117998, Learning Rate - 0.025, magnitude of gradient - 0.0959602203821044\n",
      "Step - 1560, Loss - 0.35888267407823726, Learning Rate - 0.025, magnitude of gradient - 0.030859605784101966\n",
      "Step - 1561, Loss - 0.3017666782273105, Learning Rate - 0.025, magnitude of gradient - 0.04961949833381133\n",
      "Step - 1562, Loss - 0.344864891453351, Learning Rate - 0.025, magnitude of gradient - 0.07415062440415647\n",
      "Step - 1563, Loss - 0.3482067256931082, Learning Rate - 0.025, magnitude of gradient - 0.05875957265535534\n",
      "Step - 1564, Loss - 0.2667265138217201, Learning Rate - 0.025, magnitude of gradient - 0.10707961370445679\n",
      "Step - 1565, Loss - 0.33860205584079933, Learning Rate - 0.025, magnitude of gradient - 0.00914411042307004\n",
      "Step - 1566, Loss - 0.3887413500852052, Learning Rate - 0.025, magnitude of gradient - 0.05435609040153645\n",
      "Step - 1567, Loss - 0.3046642366242105, Learning Rate - 0.025, magnitude of gradient - 0.08198838373792795\n",
      "Step - 1568, Loss - 0.3574400829760163, Learning Rate - 0.025, magnitude of gradient - 0.09302910496509433\n",
      "Step - 1569, Loss - 0.30227555265836153, Learning Rate - 0.025, magnitude of gradient - 0.08412827522405839\n",
      "Step - 1570, Loss - 0.30271065108151085, Learning Rate - 0.025, magnitude of gradient - 0.10523078097409243\n",
      "Step - 1571, Loss - 0.39136703865702577, Learning Rate - 0.025, magnitude of gradient - 0.054289637630585315\n",
      "Step - 1572, Loss - 0.33570513792411577, Learning Rate - 0.025, magnitude of gradient - 0.04241013697286404\n",
      "Step - 1573, Loss - 0.4005320833734087, Learning Rate - 0.025, magnitude of gradient - 0.10192795084351362\n",
      "Step - 1574, Loss - 0.2847905541679193, Learning Rate - 0.025, magnitude of gradient - 0.11240455313338256\n",
      "Step - 1575, Loss - 0.2814993574871331, Learning Rate - 0.025, magnitude of gradient - 0.055174182868113035\n",
      "Step - 1576, Loss - 0.3423011127902853, Learning Rate - 0.025, magnitude of gradient - 0.07354719269295511\n",
      "Step - 1577, Loss - 0.24725774618767835, Learning Rate - 0.025, magnitude of gradient - 0.02463441493577869\n",
      "Step - 1578, Loss - 0.38203335795323523, Learning Rate - 0.025, magnitude of gradient - 0.07209362562056477\n",
      "Step - 1579, Loss - 0.33404487236430846, Learning Rate - 0.025, magnitude of gradient - 0.035504662750102285\n",
      "Step - 1580, Loss - 0.407681827775348, Learning Rate - 0.025, magnitude of gradient - 0.07299505798567833\n",
      "Step - 1581, Loss - 0.33267704641422047, Learning Rate - 0.025, magnitude of gradient - 0.059819813096938\n",
      "Step - 1582, Loss - 0.43764282966610546, Learning Rate - 0.025, magnitude of gradient - 0.04193548224854064\n",
      "Step - 1583, Loss - 0.3979900781242572, Learning Rate - 0.025, magnitude of gradient - 0.04324513872827974\n",
      "Step - 1584, Loss - 0.4151625324894365, Learning Rate - 0.025, magnitude of gradient - 0.04984784394209828\n",
      "Step - 1585, Loss - 0.28587447293730095, Learning Rate - 0.025, magnitude of gradient - 0.0885032194742132\n",
      "Step - 1586, Loss - 0.3276041094101167, Learning Rate - 0.025, magnitude of gradient - 0.05113134100033452\n",
      "Step - 1587, Loss - 0.28707099333424135, Learning Rate - 0.025, magnitude of gradient - 0.027974409447940374\n",
      "Step - 1588, Loss - 0.34425020702214326, Learning Rate - 0.025, magnitude of gradient - 0.06896264908583798\n",
      "Step - 1589, Loss - 0.3317318800079502, Learning Rate - 0.025, magnitude of gradient - 0.06603557644676221\n",
      "Step - 1590, Loss - 0.31540161051735804, Learning Rate - 0.025, magnitude of gradient - 0.04346101951495174\n",
      "Step - 1591, Loss - 0.31392448198081624, Learning Rate - 0.025, magnitude of gradient - 0.04890666483787038\n",
      "Step - 1592, Loss - 0.3380447711883668, Learning Rate - 0.025, magnitude of gradient - 0.04815123609408112\n",
      "Step - 1593, Loss - 0.29885518661609445, Learning Rate - 0.025, magnitude of gradient - 0.13450040653672268\n",
      "Step - 1594, Loss - 0.341414565378321, Learning Rate - 0.025, magnitude of gradient - 0.0525377109409716\n",
      "Step - 1595, Loss - 0.3371188867268682, Learning Rate - 0.025, magnitude of gradient - 0.01980975648848335\n",
      "Step - 1596, Loss - 0.3341005597627278, Learning Rate - 0.025, magnitude of gradient - 0.07151228280757212\n",
      "Step - 1597, Loss - 0.2521459312785599, Learning Rate - 0.025, magnitude of gradient - 0.02731989110613458\n",
      "Step - 1598, Loss - 0.36463680410792, Learning Rate - 0.025, magnitude of gradient - 0.0398470683091191\n",
      "Step - 1599, Loss - 0.3067913298357192, Learning Rate - 0.025, magnitude of gradient - 0.07331689781938219\n",
      "Step - 1600, Loss - 0.31711962225268675, Learning Rate - 0.025, magnitude of gradient - 0.023284745727915186\n",
      "Step - 1601, Loss - 0.3442538529675239, Learning Rate - 0.025, magnitude of gradient - 0.06195402006075841\n",
      "Step - 1602, Loss - 0.2846421846246632, Learning Rate - 0.025, magnitude of gradient - 0.04486429557782944\n",
      "Step - 1603, Loss - 0.2790502687376987, Learning Rate - 0.025, magnitude of gradient - 0.041977144959286006\n",
      "Step - 1604, Loss - 0.25997619369989766, Learning Rate - 0.025, magnitude of gradient - 0.010027890694470788\n",
      "Step - 1605, Loss - 0.34648744003430687, Learning Rate - 0.025, magnitude of gradient - 0.052610911130993344\n",
      "Step - 1606, Loss - 0.3037059564702478, Learning Rate - 0.025, magnitude of gradient - 0.05303758901020122\n",
      "Step - 1607, Loss - 0.3359348707938818, Learning Rate - 0.025, magnitude of gradient - 0.04746007574714589\n",
      "Step - 1608, Loss - 0.3327846633504446, Learning Rate - 0.025, magnitude of gradient - 0.16102345552311204\n",
      "Step - 1609, Loss - 0.29302287580841635, Learning Rate - 0.025, magnitude of gradient - 0.05150438556168996\n",
      "Step - 1610, Loss - 0.33092843848636067, Learning Rate - 0.025, magnitude of gradient - 0.050840899569359915\n",
      "Step - 1611, Loss - 0.2989641373847144, Learning Rate - 0.025, magnitude of gradient - 0.023189813066635895\n",
      "Step - 1612, Loss - 0.38995197147429683, Learning Rate - 0.025, magnitude of gradient - 0.027348672908956703\n",
      "Step - 1613, Loss - 0.31094048603731766, Learning Rate - 0.025, magnitude of gradient - 0.03719473674734647\n",
      "Step - 1614, Loss - 0.31477096698572954, Learning Rate - 0.025, magnitude of gradient - 0.03581465294482584\n",
      "Step - 1615, Loss - 0.25177264565664387, Learning Rate - 0.025, magnitude of gradient - 0.03397269936877909\n",
      "Step - 1616, Loss - 0.2813608680633213, Learning Rate - 0.025, magnitude of gradient - 0.11726204228396656\n",
      "Step - 1617, Loss - 0.27576585112582175, Learning Rate - 0.025, magnitude of gradient - 0.06086754447818836\n",
      "Step - 1618, Loss - 0.3297113301303161, Learning Rate - 0.025, magnitude of gradient - 0.01956311028305681\n",
      "Step - 1619, Loss - 0.37391524610683013, Learning Rate - 0.025, magnitude of gradient - 0.07918723365081536\n",
      "Step - 1620, Loss - 0.2690782321747578, Learning Rate - 0.025, magnitude of gradient - 0.013087887847065986\n",
      "Step - 1621, Loss - 0.3127525784966356, Learning Rate - 0.025, magnitude of gradient - 0.15377852954000618\n",
      "Step - 1622, Loss - 0.2695076491418261, Learning Rate - 0.025, magnitude of gradient - 0.14879012868794822\n",
      "Step - 1623, Loss - 0.39177785355492184, Learning Rate - 0.025, magnitude of gradient - 0.04001752922143161\n",
      "Step - 1624, Loss - 0.3019020107873828, Learning Rate - 0.025, magnitude of gradient - 0.06390766973479144\n",
      "Step - 1625, Loss - 0.34388661767292483, Learning Rate - 0.025, magnitude of gradient - 0.07394123432624256\n",
      "Step - 1626, Loss - 0.3466742783085201, Learning Rate - 0.025, magnitude of gradient - 0.02353986943854055\n",
      "Step - 1627, Loss - 0.26776721809109394, Learning Rate - 0.025, magnitude of gradient - 0.08472106455731102\n",
      "Step - 1628, Loss - 0.3852161340358631, Learning Rate - 0.025, magnitude of gradient - 0.09524057495610681\n",
      "Step - 1629, Loss - 0.33346301457956834, Learning Rate - 0.025, magnitude of gradient - 0.03646159538355434\n",
      "Step - 1630, Loss - 0.297860536834542, Learning Rate - 0.025, magnitude of gradient - 0.024022979702339877\n",
      "Step - 1631, Loss - 0.31012193037315516, Learning Rate - 0.025, magnitude of gradient - 0.07193913805830116\n",
      "Step - 1632, Loss - 0.35079023857654834, Learning Rate - 0.025, magnitude of gradient - 0.12459747900085254\n",
      "Step - 1633, Loss - 0.29337449147964456, Learning Rate - 0.025, magnitude of gradient - 0.06351043225819646\n",
      "Step - 1634, Loss - 0.36496924815317955, Learning Rate - 0.025, magnitude of gradient - 0.028650782544626087\n",
      "Step - 1635, Loss - 0.3501970397512794, Learning Rate - 0.025, magnitude of gradient - 0.05370688025283393\n",
      "Step - 1636, Loss - 0.2590036741323829, Learning Rate - 0.025, magnitude of gradient - 0.023678234211356407\n",
      "Step - 1637, Loss - 0.3911257909559901, Learning Rate - 0.025, magnitude of gradient - 0.05390701484589109\n",
      "Step - 1638, Loss - 0.21628986854869786, Learning Rate - 0.025, magnitude of gradient - 0.15002497702941137\n",
      "Step - 1639, Loss - 0.3304928293976052, Learning Rate - 0.025, magnitude of gradient - 0.022063339675608425\n",
      "Step - 1640, Loss - 0.3327108455147483, Learning Rate - 0.025, magnitude of gradient - 0.00830489389508029\n",
      "Step - 1641, Loss - 0.31890432657240375, Learning Rate - 0.025, magnitude of gradient - 0.061986892120636884\n",
      "Step - 1642, Loss - 0.34369425194267106, Learning Rate - 0.025, magnitude of gradient - 0.044405052101709745\n",
      "Step - 1643, Loss - 0.31141406831741963, Learning Rate - 0.025, magnitude of gradient - 0.0538018805997194\n",
      "Step - 1644, Loss - 0.3820115499105746, Learning Rate - 0.025, magnitude of gradient - 0.08129556888154613\n",
      "Step - 1645, Loss - 0.30119368334468255, Learning Rate - 0.025, magnitude of gradient - 0.03363155012759021\n",
      "Step - 1646, Loss - 0.28320738494993986, Learning Rate - 0.025, magnitude of gradient - 0.05834497235095669\n",
      "Step - 1647, Loss - 0.27570679417145294, Learning Rate - 0.025, magnitude of gradient - 0.06427493042244294\n",
      "Step - 1648, Loss - 0.34868701586680484, Learning Rate - 0.025, magnitude of gradient - 0.07536508407077988\n",
      "Step - 1649, Loss - 0.34405321220488094, Learning Rate - 0.025, magnitude of gradient - 0.10245713122792925\n",
      "Step - 1650, Loss - 0.3319259892816454, Learning Rate - 0.025, magnitude of gradient - 0.033972390661601466\n",
      "Step - 1651, Loss - 0.34176431242714633, Learning Rate - 0.025, magnitude of gradient - 0.014907504163557659\n",
      "Step - 1652, Loss - 0.38680841522042514, Learning Rate - 0.025, magnitude of gradient - 0.03107491134603527\n",
      "Step - 1653, Loss - 0.33499643770764137, Learning Rate - 0.025, magnitude of gradient - 0.02698320402670607\n",
      "Step - 1654, Loss - 0.3954742602421961, Learning Rate - 0.025, magnitude of gradient - 0.046734646743395225\n",
      "Step - 1655, Loss - 0.3419884406050553, Learning Rate - 0.025, magnitude of gradient - 0.08911678747473335\n",
      "Step - 1656, Loss - 0.3336666199177692, Learning Rate - 0.025, magnitude of gradient - 0.005090525741558232\n",
      "Step - 1657, Loss - 0.3501469082771128, Learning Rate - 0.025, magnitude of gradient - 0.03456890281578562\n",
      "Step - 1658, Loss - 0.31800176588971013, Learning Rate - 0.025, magnitude of gradient - 0.10315230645963705\n",
      "Step - 1659, Loss - 0.35392097207599443, Learning Rate - 0.025, magnitude of gradient - 0.03777368603771209\n",
      "Step - 1660, Loss - 0.2980784046641149, Learning Rate - 0.025, magnitude of gradient - 0.05027307489185164\n",
      "Step - 1661, Loss - 0.34610265629387993, Learning Rate - 0.025, magnitude of gradient - 0.0964614725069604\n",
      "Step - 1662, Loss - 0.320839930156118, Learning Rate - 0.025, magnitude of gradient - 0.0218471567939269\n",
      "Step - 1663, Loss - 0.2546839165525813, Learning Rate - 0.025, magnitude of gradient - 0.056310606754671894\n",
      "Step - 1664, Loss - 0.43013437093649287, Learning Rate - 0.025, magnitude of gradient - 0.05113801173307524\n",
      "Step - 1665, Loss - 0.3678238188022175, Learning Rate - 0.025, magnitude of gradient - 0.0812589261033572\n",
      "Step - 1666, Loss - 0.3602574995337064, Learning Rate - 0.025, magnitude of gradient - 0.08182557003736737\n",
      "Step - 1667, Loss - 0.23022842533776808, Learning Rate - 0.025, magnitude of gradient - 0.07765015355809257\n",
      "Step - 1668, Loss - 0.3070525112943594, Learning Rate - 0.025, magnitude of gradient - 0.09558278326882198\n",
      "Step - 1669, Loss - 0.3383265060938878, Learning Rate - 0.025, magnitude of gradient - 0.029587226271856597\n",
      "Step - 1670, Loss - 0.29179314154899094, Learning Rate - 0.025, magnitude of gradient - 0.07067280371979161\n",
      "Step - 1671, Loss - 0.23128145007426693, Learning Rate - 0.025, magnitude of gradient - 0.031261622379067266\n",
      "Step - 1672, Loss - 0.33533834630329606, Learning Rate - 0.025, magnitude of gradient - 0.03591757842941495\n",
      "Step - 1673, Loss - 0.3587132081580493, Learning Rate - 0.025, magnitude of gradient - 0.07456595798033615\n",
      "Step - 1674, Loss - 0.3391745916335046, Learning Rate - 0.025, magnitude of gradient - 0.04673036160014315\n",
      "Step - 1675, Loss - 0.34326707507335613, Learning Rate - 0.025, magnitude of gradient - 0.03521047390162499\n",
      "Step - 1676, Loss - 0.2818551041870253, Learning Rate - 0.025, magnitude of gradient - 0.009975974605321694\n",
      "Step - 1677, Loss - 0.31416969484811713, Learning Rate - 0.025, magnitude of gradient - 0.04562853046804435\n",
      "Step - 1678, Loss - 0.28606033605667414, Learning Rate - 0.025, magnitude of gradient - 0.02534343636319203\n",
      "Step - 1679, Loss - 0.34596253714310554, Learning Rate - 0.025, magnitude of gradient - 0.013823133924038038\n",
      "Step - 1680, Loss - 0.29572945256002214, Learning Rate - 0.025, magnitude of gradient - 0.043561806176846386\n",
      "Step - 1681, Loss - 0.2996265209561767, Learning Rate - 0.025, magnitude of gradient - 0.06617944879157292\n",
      "Step - 1682, Loss - 0.3307008220259682, Learning Rate - 0.025, magnitude of gradient - 0.07753824616372461\n",
      "Step - 1683, Loss - 0.338639711150687, Learning Rate - 0.025, magnitude of gradient - 0.13275881323244462\n",
      "Step - 1684, Loss - 0.27871523258489656, Learning Rate - 0.025, magnitude of gradient - 0.04868761301943835\n",
      "Step - 1685, Loss - 0.27988630801637004, Learning Rate - 0.025, magnitude of gradient - 0.08079980618691926\n",
      "Step - 1686, Loss - 0.2525966366210335, Learning Rate - 0.025, magnitude of gradient - 0.019654184769428953\n",
      "Step - 1687, Loss - 0.414227293300623, Learning Rate - 0.025, magnitude of gradient - 0.07909477937171482\n",
      "Step - 1688, Loss - 0.2954181687144991, Learning Rate - 0.025, magnitude of gradient - 0.005565754613413556\n",
      "Step - 1689, Loss - 0.2519649204466765, Learning Rate - 0.025, magnitude of gradient - 0.03187436072416465\n",
      "Step - 1690, Loss - 0.313643724805479, Learning Rate - 0.025, magnitude of gradient - 0.08401603495915627\n",
      "Step - 1691, Loss - 0.3042721758544718, Learning Rate - 0.025, magnitude of gradient - 0.05600415185213498\n",
      "Step - 1692, Loss - 0.3299306397721392, Learning Rate - 0.025, magnitude of gradient - 0.06009359774606831\n",
      "Step - 1693, Loss - 0.33788331183886827, Learning Rate - 0.025, magnitude of gradient - 0.08710005361134876\n",
      "Step - 1694, Loss - 0.3069249967890514, Learning Rate - 0.025, magnitude of gradient - 0.06106394890019288\n",
      "Step - 1695, Loss - 0.38760247626696215, Learning Rate - 0.025, magnitude of gradient - 0.03390560836257348\n",
      "Step - 1696, Loss - 0.29052808385714546, Learning Rate - 0.025, magnitude of gradient - 0.05961062614250645\n",
      "Step - 1697, Loss - 0.3008604921904048, Learning Rate - 0.025, magnitude of gradient - 0.039022684673526084\n",
      "Step - 1698, Loss - 0.310876524062327, Learning Rate - 0.025, magnitude of gradient - 0.06539413593537623\n",
      "Step - 1699, Loss - 0.37458263375070466, Learning Rate - 0.025, magnitude of gradient - 0.09615865547101356\n",
      "Step - 1700, Loss - 0.3726434060695023, Learning Rate - 0.025, magnitude of gradient - 0.01198310129018758\n",
      "Step - 1701, Loss - 0.33628844104083305, Learning Rate - 0.025, magnitude of gradient - 0.035260300813989454\n",
      "Step - 1702, Loss - 0.30095666115301856, Learning Rate - 0.025, magnitude of gradient - 0.07449377403762697\n",
      "Step - 1703, Loss - 0.29572242267201515, Learning Rate - 0.025, magnitude of gradient - 0.0418749726568243\n",
      "Step - 1704, Loss - 0.32567434825523356, Learning Rate - 0.025, magnitude of gradient - 0.033647707029632455\n",
      "Step - 1705, Loss - 0.27252247269614815, Learning Rate - 0.025, magnitude of gradient - 0.05703400978439885\n",
      "Step - 1706, Loss - 0.31571297091956835, Learning Rate - 0.025, magnitude of gradient - 0.01679077329319659\n",
      "Step - 1707, Loss - 0.30258387068808373, Learning Rate - 0.025, magnitude of gradient - 0.10378065346208842\n",
      "Step - 1708, Loss - 0.29735052056999367, Learning Rate - 0.025, magnitude of gradient - 0.05489574782898986\n",
      "Step - 1709, Loss - 0.3288754972283593, Learning Rate - 0.025, magnitude of gradient - 0.0903560676251196\n",
      "Step - 1710, Loss - 0.30598775182187016, Learning Rate - 0.025, magnitude of gradient - 0.06392863559609749\n",
      "Step - 1711, Loss - 0.3043021417740434, Learning Rate - 0.025, magnitude of gradient - 0.0690138162578662\n",
      "Step - 1712, Loss - 0.32602948366587453, Learning Rate - 0.025, magnitude of gradient - 0.05524632390906353\n",
      "Step - 1713, Loss - 0.28251999296659214, Learning Rate - 0.025, magnitude of gradient - 0.08937258785845147\n",
      "Step - 1714, Loss - 0.2734467413177199, Learning Rate - 0.025, magnitude of gradient - 0.08341090750867054\n",
      "Step - 1715, Loss - 0.3319667340434378, Learning Rate - 0.025, magnitude of gradient - 0.0669664293480036\n",
      "Step - 1716, Loss - 0.2544224422500123, Learning Rate - 0.025, magnitude of gradient - 0.058683425707496316\n",
      "Step - 1717, Loss - 0.33290466586310624, Learning Rate - 0.025, magnitude of gradient - 0.04213296335334624\n",
      "Step - 1718, Loss - 0.3147520718470433, Learning Rate - 0.025, magnitude of gradient - 0.08841067289662394\n",
      "Step - 1719, Loss - 0.3878272988630714, Learning Rate - 0.025, magnitude of gradient - 0.0462527781962876\n",
      "Step - 1720, Loss - 0.2999838870390716, Learning Rate - 0.025, magnitude of gradient - 0.056489727627198706\n",
      "Step - 1721, Loss - 0.3654814372167209, Learning Rate - 0.025, magnitude of gradient - 0.03850731375978998\n",
      "Step - 1722, Loss - 0.30244463199945176, Learning Rate - 0.025, magnitude of gradient - 0.06613094402373895\n",
      "Step - 1723, Loss - 0.3307732278127885, Learning Rate - 0.025, magnitude of gradient - 0.0384918622522716\n",
      "Step - 1724, Loss - 0.2656471149427964, Learning Rate - 0.025, magnitude of gradient - 0.015825133144136805\n",
      "Step - 1725, Loss - 0.3751555503435937, Learning Rate - 0.025, magnitude of gradient - 0.011078639022767582\n",
      "Step - 1726, Loss - 0.3532169089118595, Learning Rate - 0.025, magnitude of gradient - 0.030719221186419673\n",
      "Step - 1727, Loss - 0.2778173564427686, Learning Rate - 0.025, magnitude of gradient - 0.020661576852615406\n",
      "Step - 1728, Loss - 0.3192187042980531, Learning Rate - 0.025, magnitude of gradient - 0.027759198815123565\n",
      "Step - 1729, Loss - 0.2970812971216946, Learning Rate - 0.025, magnitude of gradient - 0.03905206892461956\n",
      "Step - 1730, Loss - 0.3469459895145686, Learning Rate - 0.025, magnitude of gradient - 0.10204740726794861\n",
      "Step - 1731, Loss - 0.3237191954659898, Learning Rate - 0.025, magnitude of gradient - 0.08227483644297721\n",
      "Step - 1732, Loss - 0.324500670334834, Learning Rate - 0.025, magnitude of gradient - 0.024916078558136268\n",
      "Step - 1733, Loss - 0.3868859765264341, Learning Rate - 0.025, magnitude of gradient - 0.013143738248345133\n",
      "Step - 1734, Loss - 0.2372333850637467, Learning Rate - 0.025, magnitude of gradient - 0.06771179625625252\n",
      "Step - 1735, Loss - 0.24166549936755183, Learning Rate - 0.025, magnitude of gradient - 0.0811594306767395\n",
      "Step - 1736, Loss - 0.3185158450164318, Learning Rate - 0.025, magnitude of gradient - 0.030138032278366714\n",
      "Step - 1737, Loss - 0.3196238546929632, Learning Rate - 0.025, magnitude of gradient - 0.05682615689201371\n",
      "Step - 1738, Loss - 0.3163937799281308, Learning Rate - 0.025, magnitude of gradient - 0.07699237516401584\n",
      "Step - 1739, Loss - 0.3039808748643861, Learning Rate - 0.025, magnitude of gradient - 0.006485097004289772\n",
      "Step - 1740, Loss - 0.33008143931765976, Learning Rate - 0.025, magnitude of gradient - 0.03517030812158415\n",
      "Step - 1741, Loss - 0.3588057562131797, Learning Rate - 0.025, magnitude of gradient - 0.0503097095318968\n",
      "Step - 1742, Loss - 0.3004723638211608, Learning Rate - 0.025, magnitude of gradient - 0.048993565109604\n",
      "Step - 1743, Loss - 0.27401981638628514, Learning Rate - 0.025, magnitude of gradient - 0.07337000425430305\n",
      "Step - 1744, Loss - 0.27054064348940354, Learning Rate - 0.025, magnitude of gradient - 0.046148948740827894\n",
      "Step - 1745, Loss - 0.33346706851407965, Learning Rate - 0.025, magnitude of gradient - 0.08884893922584622\n",
      "Step - 1746, Loss - 0.34290845357381783, Learning Rate - 0.025, magnitude of gradient - 0.022044185515173717\n",
      "Step - 1747, Loss - 0.38062993390070016, Learning Rate - 0.025, magnitude of gradient - 0.06726998392876293\n",
      "Step - 1748, Loss - 0.3543378472126141, Learning Rate - 0.025, magnitude of gradient - 0.037883836188221165\n",
      "Step - 1749, Loss - 0.3875688014180989, Learning Rate - 0.025, magnitude of gradient - 0.03430217529342012\n",
      "Step - 1750, Loss - 0.33996247659653767, Learning Rate - 0.025, magnitude of gradient - 0.04011469820336875\n",
      "Step - 1751, Loss - 0.2894122636518456, Learning Rate - 0.025, magnitude of gradient - 0.04687297372416741\n",
      "Step - 1752, Loss - 0.2881642839653601, Learning Rate - 0.025, magnitude of gradient - 0.06794310087911982\n",
      "Step - 1753, Loss - 0.3555480025681185, Learning Rate - 0.025, magnitude of gradient - 0.07042722031864089\n",
      "Step - 1754, Loss - 0.28034697493328764, Learning Rate - 0.025, magnitude of gradient - 0.03832024481451771\n",
      "Step - 1755, Loss - 0.2605871646415499, Learning Rate - 0.025, magnitude of gradient - 0.04108312253926574\n",
      "Step - 1756, Loss - 0.3851778070456167, Learning Rate - 0.025, magnitude of gradient - 0.08900792201148285\n",
      "Step - 1757, Loss - 0.2743739685301849, Learning Rate - 0.025, magnitude of gradient - 0.051833352307856975\n",
      "Step - 1758, Loss - 0.34288266840812864, Learning Rate - 0.025, magnitude of gradient - 0.1301286471569049\n",
      "Step - 1759, Loss - 0.2767504744913758, Learning Rate - 0.025, magnitude of gradient - 0.03312529143650003\n",
      "Step - 1760, Loss - 0.32528561296941805, Learning Rate - 0.025, magnitude of gradient - 0.04362452378194854\n",
      "Step - 1761, Loss - 0.3070486693384816, Learning Rate - 0.025, magnitude of gradient - 0.06107516373742459\n",
      "Step - 1762, Loss - 0.30959542504585913, Learning Rate - 0.025, magnitude of gradient - 0.01445103241809815\n",
      "Step - 1763, Loss - 0.3055349254689752, Learning Rate - 0.025, magnitude of gradient - 0.055013013236140856\n",
      "Step - 1764, Loss - 0.3491235306531599, Learning Rate - 0.025, magnitude of gradient - 0.01887309808757166\n",
      "Step - 1765, Loss - 0.3216806282044788, Learning Rate - 0.025, magnitude of gradient - 0.017260216489808206\n",
      "Step - 1766, Loss - 0.2894152514529328, Learning Rate - 0.025, magnitude of gradient - 0.05344867667594717\n",
      "Step - 1767, Loss - 0.25126730902095396, Learning Rate - 0.025, magnitude of gradient - 0.05030844695446013\n",
      "Step - 1768, Loss - 0.22529434951845234, Learning Rate - 0.025, magnitude of gradient - 0.0681692895243338\n",
      "Step - 1769, Loss - 0.3431725671493304, Learning Rate - 0.025, magnitude of gradient - 0.022656029896539902\n",
      "Step - 1770, Loss - 0.35706960617386685, Learning Rate - 0.025, magnitude of gradient - 0.013559892607692412\n",
      "Step - 1771, Loss - 0.3090811646692916, Learning Rate - 0.025, magnitude of gradient - 0.053991130513391127\n",
      "Step - 1772, Loss - 0.3309293888816179, Learning Rate - 0.025, magnitude of gradient - 0.03250574642029343\n",
      "Step - 1773, Loss - 0.2975999630733852, Learning Rate - 0.025, magnitude of gradient - 0.09937385726411435\n",
      "Step - 1774, Loss - 0.3592233261698558, Learning Rate - 0.025, magnitude of gradient - 0.0735398633435803\n",
      "Step - 1775, Loss - 0.29613506239381526, Learning Rate - 0.025, magnitude of gradient - 0.02011582951604986\n",
      "Step - 1776, Loss - 0.3275348968472451, Learning Rate - 0.025, magnitude of gradient - 0.026334507616551123\n",
      "Step - 1777, Loss - 0.34588570766843274, Learning Rate - 0.025, magnitude of gradient - 0.024817695491601302\n",
      "Step - 1778, Loss - 0.33037233572263547, Learning Rate - 0.025, magnitude of gradient - 0.11133653460063885\n",
      "Step - 1779, Loss - 0.31930173668048056, Learning Rate - 0.025, magnitude of gradient - 0.07376298307198607\n",
      "Step - 1780, Loss - 0.28262832541337934, Learning Rate - 0.025, magnitude of gradient - 0.09100500352154775\n",
      "Step - 1781, Loss - 0.3084721379086783, Learning Rate - 0.025, magnitude of gradient - 0.0680252189261048\n",
      "Step - 1782, Loss - 0.34306781676192327, Learning Rate - 0.025, magnitude of gradient - 0.06058792407978323\n",
      "Step - 1783, Loss - 0.350815528504124, Learning Rate - 0.025, magnitude of gradient - 0.022242268942796274\n",
      "Step - 1784, Loss - 0.3454217543896204, Learning Rate - 0.025, magnitude of gradient - 0.10572494735280832\n",
      "Step - 1785, Loss - 0.31113375641059743, Learning Rate - 0.025, magnitude of gradient - 0.028925283787289275\n",
      "Step - 1786, Loss - 0.37425787005373, Learning Rate - 0.025, magnitude of gradient - 0.09641905814428313\n",
      "Step - 1787, Loss - 0.34063303368165376, Learning Rate - 0.025, magnitude of gradient - 0.03821714681871088\n",
      "Step - 1788, Loss - 0.34709699257693, Learning Rate - 0.025, magnitude of gradient - 0.05359143665054989\n",
      "Step - 1789, Loss - 0.3162199769187635, Learning Rate - 0.025, magnitude of gradient - 0.11712984742854748\n",
      "Step - 1790, Loss - 0.35415352867201333, Learning Rate - 0.025, magnitude of gradient - 0.06151994273512835\n",
      "Step - 1791, Loss - 0.34447933335163805, Learning Rate - 0.025, magnitude of gradient - 0.05017997219488816\n",
      "Step - 1792, Loss - 0.35945840634135995, Learning Rate - 0.025, magnitude of gradient - 0.02143754453454585\n",
      "Step - 1793, Loss - 0.30300624316275643, Learning Rate - 0.025, magnitude of gradient - 0.13877949053596567\n",
      "Step - 1794, Loss - 0.3815006569494789, Learning Rate - 0.025, magnitude of gradient - 0.02996737848571743\n",
      "Step - 1795, Loss - 0.4169541054022711, Learning Rate - 0.025, magnitude of gradient - 0.04488568619626101\n",
      "Step - 1796, Loss - 0.3212721103844994, Learning Rate - 0.025, magnitude of gradient - 0.08280221600004006\n",
      "Step - 1797, Loss - 0.3160912724972439, Learning Rate - 0.025, magnitude of gradient - 0.12134725141396954\n",
      "Step - 1798, Loss - 0.3293828111750222, Learning Rate - 0.025, magnitude of gradient - 0.07559023693540466\n",
      "Step - 1799, Loss - 0.36082445864989576, Learning Rate - 0.025, magnitude of gradient - 0.011651607137864721\n",
      "Step - 1800, Loss - 0.3305701165716599, Learning Rate - 0.025, magnitude of gradient - 0.13352831102971052\n",
      "Step - 1801, Loss - 0.2784421386074514, Learning Rate - 0.025, magnitude of gradient - 0.026828496900601795\n",
      "Step - 1802, Loss - 0.41843107820248354, Learning Rate - 0.025, magnitude of gradient - 0.10356025194008832\n",
      "Step - 1803, Loss - 0.41880488578904784, Learning Rate - 0.025, magnitude of gradient - 0.03744751478356534\n",
      "Step - 1804, Loss - 0.33796042875708937, Learning Rate - 0.025, magnitude of gradient - 0.04438422881917456\n",
      "Step - 1805, Loss - 0.2631618083239595, Learning Rate - 0.025, magnitude of gradient - 0.042501444061965224\n",
      "Step - 1806, Loss - 0.37661664359177305, Learning Rate - 0.025, magnitude of gradient - 0.07523120078817386\n",
      "Step - 1807, Loss - 0.3122665711238277, Learning Rate - 0.025, magnitude of gradient - 0.030920515245350395\n",
      "Step - 1808, Loss - 0.4221724510323303, Learning Rate - 0.025, magnitude of gradient - 0.07625622513902701\n",
      "Step - 1809, Loss - 0.32835622236248474, Learning Rate - 0.025, magnitude of gradient - 0.06105067801888541\n",
      "Step - 1810, Loss - 0.36343764368844844, Learning Rate - 0.025, magnitude of gradient - 0.06341612764794768\n",
      "Step - 1811, Loss - 0.33476997774140593, Learning Rate - 0.025, magnitude of gradient - 0.09695515591167389\n",
      "Step - 1812, Loss - 0.27897843340974865, Learning Rate - 0.025, magnitude of gradient - 0.03283528872424692\n",
      "Step - 1813, Loss - 0.319849620401492, Learning Rate - 0.025, magnitude of gradient - 0.09486290562769076\n",
      "Step - 1814, Loss - 0.3566798795787659, Learning Rate - 0.025, magnitude of gradient - 0.07075377360392399\n",
      "Step - 1815, Loss - 0.42058608553674376, Learning Rate - 0.025, magnitude of gradient - 0.0335428089795366\n",
      "Step - 1816, Loss - 0.3360842134959197, Learning Rate - 0.025, magnitude of gradient - 0.08764403749643059\n",
      "Step - 1817, Loss - 0.28143136271457436, Learning Rate - 0.025, magnitude of gradient - 0.1015131814020487\n",
      "Step - 1818, Loss - 0.3006256423610374, Learning Rate - 0.025, magnitude of gradient - 0.059571752881458724\n",
      "Step - 1819, Loss - 0.40261849689641305, Learning Rate - 0.025, magnitude of gradient - 0.08329273661610417\n",
      "Step - 1820, Loss - 0.32215046194852487, Learning Rate - 0.025, magnitude of gradient - 0.027325991838566252\n",
      "Step - 1821, Loss - 0.341838536972492, Learning Rate - 0.025, magnitude of gradient - 0.07371439361237907\n",
      "Step - 1822, Loss - 0.2578481759933007, Learning Rate - 0.025, magnitude of gradient - 0.02932516494370382\n",
      "Step - 1823, Loss - 0.34658915745018243, Learning Rate - 0.025, magnitude of gradient - 0.02571756417353335\n",
      "Step - 1824, Loss - 0.2865302876873648, Learning Rate - 0.025, magnitude of gradient - 0.020676393249266452\n",
      "Step - 1825, Loss - 0.34296091334221285, Learning Rate - 0.025, magnitude of gradient - 0.058467789213711686\n",
      "Step - 1826, Loss - 0.332811754767845, Learning Rate - 0.025, magnitude of gradient - 0.0919566127322676\n",
      "Step - 1827, Loss - 0.27691165049812494, Learning Rate - 0.025, magnitude of gradient - 0.044488763123666314\n",
      "Step - 1828, Loss - 0.2482396393867602, Learning Rate - 0.025, magnitude of gradient - 0.04327636692872025\n",
      "Step - 1829, Loss - 0.27927273745486597, Learning Rate - 0.025, magnitude of gradient - 0.045042485337364935\n",
      "Step - 1830, Loss - 0.359033194262907, Learning Rate - 0.025, magnitude of gradient - 0.024477511420363734\n",
      "Step - 1831, Loss - 0.31276127792829544, Learning Rate - 0.025, magnitude of gradient - 0.0905467761551363\n",
      "Step - 1832, Loss - 0.35783110747818114, Learning Rate - 0.025, magnitude of gradient - 0.07121864058758673\n",
      "Step - 1833, Loss - 0.27989573096210973, Learning Rate - 0.025, magnitude of gradient - 0.039447461321592485\n",
      "Step - 1834, Loss - 0.3427454660760041, Learning Rate - 0.025, magnitude of gradient - 0.0436328040028331\n",
      "Step - 1835, Loss - 0.30480966946031135, Learning Rate - 0.025, magnitude of gradient - 0.051041754560123935\n",
      "Step - 1836, Loss - 0.28262220320308995, Learning Rate - 0.025, magnitude of gradient - 0.09183547105622007\n",
      "Step - 1837, Loss - 0.3186247274639624, Learning Rate - 0.025, magnitude of gradient - 0.03997623797538603\n",
      "Step - 1838, Loss - 0.31145085394900945, Learning Rate - 0.025, magnitude of gradient - 0.09027753270874118\n",
      "Step - 1839, Loss - 0.2859547507989939, Learning Rate - 0.025, magnitude of gradient - 0.08892702255722727\n",
      "Step - 1840, Loss - 0.29364867650769266, Learning Rate - 0.025, magnitude of gradient - 0.09638422913291174\n",
      "Step - 1841, Loss - 0.2680187843024491, Learning Rate - 0.025, magnitude of gradient - 0.03041211193494846\n",
      "Step - 1842, Loss - 0.3358108945559166, Learning Rate - 0.025, magnitude of gradient - 0.049220881512577155\n",
      "Step - 1843, Loss - 0.2597766770882959, Learning Rate - 0.025, magnitude of gradient - 0.03647855826715994\n",
      "Step - 1844, Loss - 0.2888925012852222, Learning Rate - 0.025, magnitude of gradient - 0.06223691710795056\n",
      "Step - 1845, Loss - 0.2539060378519316, Learning Rate - 0.025, magnitude of gradient - 0.05308402129308541\n",
      "Step - 1846, Loss - 0.3781192535291091, Learning Rate - 0.025, magnitude of gradient - 0.046550489348758016\n",
      "Step - 1847, Loss - 0.2600624378873585, Learning Rate - 0.025, magnitude of gradient - 0.04387259560123907\n",
      "Step - 1848, Loss - 0.3001364401756653, Learning Rate - 0.025, magnitude of gradient - 0.026617928443694046\n",
      "Step - 1849, Loss - 0.35647528305118187, Learning Rate - 0.025, magnitude of gradient - 0.06784133164875876\n",
      "Step - 1850, Loss - 0.2839558176286614, Learning Rate - 0.025, magnitude of gradient - 0.06494635270552038\n",
      "Step - 1851, Loss - 0.3027275846598606, Learning Rate - 0.025, magnitude of gradient - 0.04138870365010983\n",
      "Step - 1852, Loss - 0.35809767138033793, Learning Rate - 0.025, magnitude of gradient - 0.07050063660980325\n",
      "Step - 1853, Loss - 0.2873240613018384, Learning Rate - 0.025, magnitude of gradient - 0.08737016329695635\n",
      "Step - 1854, Loss - 0.4421701099982621, Learning Rate - 0.025, magnitude of gradient - 0.06289539890735336\n",
      "Step - 1855, Loss - 0.3504501331725444, Learning Rate - 0.025, magnitude of gradient - 0.07939527228039323\n",
      "Step - 1856, Loss - 0.3124741213693973, Learning Rate - 0.025, magnitude of gradient - 0.024681276229769462\n",
      "Step - 1857, Loss - 0.32525877035982853, Learning Rate - 0.025, magnitude of gradient - 0.0750428184749768\n",
      "Step - 1858, Loss - 0.2562643195996886, Learning Rate - 0.025, magnitude of gradient - 0.065934980820788\n",
      "Step - 1859, Loss - 0.24422806346550352, Learning Rate - 0.025, magnitude of gradient - 0.05414202995986963\n",
      "Step - 1860, Loss - 0.3029661730829116, Learning Rate - 0.025, magnitude of gradient - 0.051996724006421816\n",
      "Step - 1861, Loss - 0.302405977816624, Learning Rate - 0.025, magnitude of gradient - 0.015633738967944812\n",
      "Step - 1862, Loss - 0.3401851468949109, Learning Rate - 0.025, magnitude of gradient - 0.06348974725030433\n",
      "Step - 1863, Loss - 0.349607022477167, Learning Rate - 0.025, magnitude of gradient - 0.009088583568976986\n",
      "Step - 1864, Loss - 0.27801822973072277, Learning Rate - 0.025, magnitude of gradient - 0.09132835945080764\n",
      "Step - 1865, Loss - 0.3352683346468321, Learning Rate - 0.025, magnitude of gradient - 0.0940606717193236\n",
      "Step - 1866, Loss - 0.33858226591455703, Learning Rate - 0.025, magnitude of gradient - 0.044179604145619125\n",
      "Step - 1867, Loss - 0.32260398662132717, Learning Rate - 0.025, magnitude of gradient - 0.045085950183373365\n",
      "Step - 1868, Loss - 0.3473525595381144, Learning Rate - 0.025, magnitude of gradient - 0.019677223517516922\n",
      "Step - 1869, Loss - 0.3078715098328764, Learning Rate - 0.025, magnitude of gradient - 0.026848967258929487\n",
      "Step - 1870, Loss - 0.3022081157675181, Learning Rate - 0.025, magnitude of gradient - 0.03677783112109761\n",
      "Step - 1871, Loss - 0.3054697757850046, Learning Rate - 0.025, magnitude of gradient - 0.040844580024364585\n",
      "Step - 1872, Loss - 0.3519937321093319, Learning Rate - 0.025, magnitude of gradient - 0.012686653847892708\n",
      "Step - 1873, Loss - 0.2265598351478523, Learning Rate - 0.025, magnitude of gradient - 0.09805038324150224\n",
      "Step - 1874, Loss - 0.34436880059069563, Learning Rate - 0.025, magnitude of gradient - 0.07650569656652584\n",
      "Step - 1875, Loss - 0.34056377359366385, Learning Rate - 0.025, magnitude of gradient - 0.02042185713251925\n",
      "Step - 1876, Loss - 0.3182320716020959, Learning Rate - 0.025, magnitude of gradient - 0.022597164937787198\n",
      "Step - 1877, Loss - 0.30978283740582124, Learning Rate - 0.025, magnitude of gradient - 0.051166041268740046\n",
      "Step - 1878, Loss - 0.24358977628118228, Learning Rate - 0.025, magnitude of gradient - 0.07518447009540162\n",
      "Step - 1879, Loss - 0.34211047227035474, Learning Rate - 0.025, magnitude of gradient - 0.051001024373537145\n",
      "Step - 1880, Loss - 0.3602770423361579, Learning Rate - 0.025, magnitude of gradient - 0.03373679346454345\n",
      "Step - 1881, Loss - 0.2904854651157877, Learning Rate - 0.025, magnitude of gradient - 0.06431117502260997\n",
      "Step - 1882, Loss - 0.37588418650489086, Learning Rate - 0.025, magnitude of gradient - 0.01198806706538373\n",
      "Step - 1883, Loss - 0.2980044058973899, Learning Rate - 0.025, magnitude of gradient - 0.03160993091393942\n",
      "Step - 1884, Loss - 0.3189979609082256, Learning Rate - 0.025, magnitude of gradient - 0.0523449726465008\n",
      "Step - 1885, Loss - 0.336962305261883, Learning Rate - 0.025, magnitude of gradient - 0.03531408956217478\n",
      "Step - 1886, Loss - 0.3068310586041271, Learning Rate - 0.025, magnitude of gradient - 0.06229254395671831\n",
      "Step - 1887, Loss - 0.3226720963020834, Learning Rate - 0.025, magnitude of gradient - 0.04068438206241153\n",
      "Step - 1888, Loss - 0.2915388863026054, Learning Rate - 0.025, magnitude of gradient - 0.001994502425135351\n",
      "Step - 1889, Loss - 0.3924491871502036, Learning Rate - 0.025, magnitude of gradient - 0.04831209699332882\n",
      "Step - 1890, Loss - 0.31442680111978744, Learning Rate - 0.025, magnitude of gradient - 0.10514598666291887\n",
      "Step - 1891, Loss - 0.28734552243485667, Learning Rate - 0.025, magnitude of gradient - 0.03571344899776986\n",
      "Step - 1892, Loss - 0.33745064985003176, Learning Rate - 0.025, magnitude of gradient - 0.02270474171050843\n",
      "Step - 1893, Loss - 0.28051389179006836, Learning Rate - 0.025, magnitude of gradient - 0.05916613133103626\n",
      "Step - 1894, Loss - 0.3081142434821972, Learning Rate - 0.025, magnitude of gradient - 0.007141748781161121\n",
      "Step - 1895, Loss - 0.33603568061960576, Learning Rate - 0.025, magnitude of gradient - 0.012934671480454345\n",
      "Step - 1896, Loss - 0.2970702883574649, Learning Rate - 0.025, magnitude of gradient - 0.04540053236548383\n",
      "Step - 1897, Loss - 0.401543654075775, Learning Rate - 0.025, magnitude of gradient - 0.05065655117162159\n",
      "Step - 1898, Loss - 0.3384030844234398, Learning Rate - 0.025, magnitude of gradient - 0.050302359100411136\n",
      "Step - 1899, Loss - 0.30009046197846234, Learning Rate - 0.025, magnitude of gradient - 0.07724935416879965\n",
      "Step - 1900, Loss - 0.359055866730911, Learning Rate - 0.025, magnitude of gradient - 0.025129912636725395\n",
      "Step - 1901, Loss - 0.3182506231252603, Learning Rate - 0.025, magnitude of gradient - 0.041963618435244064\n",
      "Step - 1902, Loss - 0.35161405597033657, Learning Rate - 0.025, magnitude of gradient - 0.11824159381762483\n",
      "Step - 1903, Loss - 0.26222803582254917, Learning Rate - 0.025, magnitude of gradient - 0.08055996960573783\n",
      "Step - 1904, Loss - 0.33347157101195996, Learning Rate - 0.025, magnitude of gradient - 0.032205594547343015\n",
      "Step - 1905, Loss - 0.3315108854931922, Learning Rate - 0.025, magnitude of gradient - 0.024692659540854728\n",
      "Step - 1906, Loss - 0.4099768212227867, Learning Rate - 0.025, magnitude of gradient - 0.10156974190988236\n",
      "Step - 1907, Loss - 0.34036935515459016, Learning Rate - 0.025, magnitude of gradient - 0.05934968526083331\n",
      "Step - 1908, Loss - 0.309213360252439, Learning Rate - 0.025, magnitude of gradient - 0.08074038899933342\n",
      "Step - 1909, Loss - 0.30106774916852824, Learning Rate - 0.025, magnitude of gradient - 0.04825078485901292\n",
      "Step - 1910, Loss - 0.3370207184242815, Learning Rate - 0.025, magnitude of gradient - 0.0884838867764181\n",
      "Step - 1911, Loss - 0.34640025935345, Learning Rate - 0.025, magnitude of gradient - 0.03813370531883718\n",
      "Step - 1912, Loss - 0.38852124151890916, Learning Rate - 0.025, magnitude of gradient - 0.07650745623359397\n",
      "Step - 1913, Loss - 0.37914546435854535, Learning Rate - 0.025, magnitude of gradient - 0.004943658026274793\n",
      "Step - 1914, Loss - 0.33132513705066025, Learning Rate - 0.025, magnitude of gradient - 0.0814131263703538\n",
      "Step - 1915, Loss - 0.3498509211258271, Learning Rate - 0.025, magnitude of gradient - 0.02056081144330383\n",
      "Step - 1916, Loss - 0.327663558716186, Learning Rate - 0.025, magnitude of gradient - 0.06547885049287196\n",
      "Step - 1917, Loss - 0.2617420933797093, Learning Rate - 0.025, magnitude of gradient - 0.07742427127134095\n",
      "Step - 1918, Loss - 0.309146299216421, Learning Rate - 0.025, magnitude of gradient - 0.038791644244100726\n",
      "Step - 1919, Loss - 0.2535341597350327, Learning Rate - 0.025, magnitude of gradient - 0.058660066036113345\n",
      "Step - 1920, Loss - 0.2919728279607038, Learning Rate - 0.025, magnitude of gradient - 0.11511345528329217\n",
      "Step - 1921, Loss - 0.3282233553578593, Learning Rate - 0.025, magnitude of gradient - 0.053126311642839805\n",
      "Step - 1922, Loss - 0.291817099569594, Learning Rate - 0.025, magnitude of gradient - 0.04320808897131041\n",
      "Step - 1923, Loss - 0.29772730454763985, Learning Rate - 0.025, magnitude of gradient - 0.05276168528916637\n",
      "Step - 1924, Loss - 0.3205890291311777, Learning Rate - 0.025, magnitude of gradient - 0.02395488936413799\n",
      "Step - 1925, Loss - 0.3234806772795488, Learning Rate - 0.025, magnitude of gradient - 0.04976646556309774\n",
      "Step - 1926, Loss - 0.35652799367280097, Learning Rate - 0.025, magnitude of gradient - 0.06739388893422878\n",
      "Step - 1927, Loss - 0.3233392164869533, Learning Rate - 0.025, magnitude of gradient - 0.056920141675488825\n",
      "Step - 1928, Loss - 0.3161746856730348, Learning Rate - 0.025, magnitude of gradient - 0.1305976638084362\n",
      "Step - 1929, Loss - 0.29399187275989963, Learning Rate - 0.025, magnitude of gradient - 0.04912748652996752\n",
      "Step - 1930, Loss - 0.34628251597151455, Learning Rate - 0.025, magnitude of gradient - 0.0450604986257676\n",
      "Step - 1931, Loss - 0.2898621541565459, Learning Rate - 0.025, magnitude of gradient - 0.13114768672561375\n",
      "Step - 1932, Loss - 0.2688959647884664, Learning Rate - 0.025, magnitude of gradient - 0.03243471577002831\n",
      "Step - 1933, Loss - 0.3194842001810931, Learning Rate - 0.025, magnitude of gradient - 0.021893109971575644\n",
      "Step - 1934, Loss - 0.29837025918863447, Learning Rate - 0.025, magnitude of gradient - 0.11554717799571317\n",
      "Step - 1935, Loss - 0.3572475353270663, Learning Rate - 0.025, magnitude of gradient - 0.055970066341255405\n",
      "Step - 1936, Loss - 0.3216029928672598, Learning Rate - 0.025, magnitude of gradient - 0.11857322188880015\n",
      "Step - 1937, Loss - 0.30853215217346025, Learning Rate - 0.025, magnitude of gradient - 0.03245784338278229\n",
      "Step - 1938, Loss - 0.25359201789362956, Learning Rate - 0.025, magnitude of gradient - 0.010534649110023029\n",
      "Step - 1939, Loss - 0.3546058829394312, Learning Rate - 0.025, magnitude of gradient - 0.05759685048416871\n",
      "Step - 1940, Loss - 0.35541634235345915, Learning Rate - 0.025, magnitude of gradient - 0.043573545112846036\n",
      "Step - 1941, Loss - 0.30990440895355137, Learning Rate - 0.025, magnitude of gradient - 0.06206279438723643\n",
      "Step - 1942, Loss - 0.39634410376588475, Learning Rate - 0.025, magnitude of gradient - 0.06687991737655033\n",
      "Step - 1943, Loss - 0.25985889598461576, Learning Rate - 0.025, magnitude of gradient - 0.0808552575976771\n",
      "Step - 1944, Loss - 0.2940106825634066, Learning Rate - 0.025, magnitude of gradient - 0.02074734493320388\n",
      "Step - 1945, Loss - 0.3348098046250554, Learning Rate - 0.025, magnitude of gradient - 0.041431117508814785\n",
      "Step - 1946, Loss - 0.2879106301551929, Learning Rate - 0.025, magnitude of gradient - 0.03754544493132544\n",
      "Step - 1947, Loss - 0.3649767171228212, Learning Rate - 0.025, magnitude of gradient - 0.046028405396746026\n",
      "Step - 1948, Loss - 0.34293160984859533, Learning Rate - 0.025, magnitude of gradient - 0.04877431960724395\n",
      "Step - 1949, Loss - 0.35131052680233527, Learning Rate - 0.025, magnitude of gradient - 0.044279973924317435\n",
      "Step - 1950, Loss - 0.34129984273024866, Learning Rate - 0.025, magnitude of gradient - 0.023871629130709015\n",
      "Step - 1951, Loss - 0.27352147939402727, Learning Rate - 0.025, magnitude of gradient - 0.032534765116844624\n",
      "Step - 1952, Loss - 0.3873388281677356, Learning Rate - 0.025, magnitude of gradient - 0.04642025973099504\n",
      "Step - 1953, Loss - 0.2821996363192177, Learning Rate - 0.025, magnitude of gradient - 0.08049134577722299\n",
      "Step - 1954, Loss - 0.3190633155721644, Learning Rate - 0.025, magnitude of gradient - 0.03149402470613491\n",
      "Step - 1955, Loss - 0.24890964659117543, Learning Rate - 0.025, magnitude of gradient - 0.034919525796560644\n",
      "Step - 1956, Loss - 0.3534624614388425, Learning Rate - 0.025, magnitude of gradient - 0.07488038178490113\n",
      "Step - 1957, Loss - 0.25857608048825187, Learning Rate - 0.025, magnitude of gradient - 0.012265775345962055\n",
      "Step - 1958, Loss - 0.33775810817183743, Learning Rate - 0.025, magnitude of gradient - 0.05599451159057045\n",
      "Step - 1959, Loss - 0.35969395345438304, Learning Rate - 0.025, magnitude of gradient - 0.06378603718530822\n",
      "Step - 1960, Loss - 0.3091334001892328, Learning Rate - 0.025, magnitude of gradient - 0.040048345025016384\n",
      "Step - 1961, Loss - 0.2923164132613212, Learning Rate - 0.025, magnitude of gradient - 0.04726588179633802\n",
      "Step - 1962, Loss - 0.2876252780709638, Learning Rate - 0.025, magnitude of gradient - 0.05073100263475011\n",
      "Step - 1963, Loss - 0.28541898999833615, Learning Rate - 0.025, magnitude of gradient - 0.047985643628057385\n",
      "Step - 1964, Loss - 0.3972051618791294, Learning Rate - 0.025, magnitude of gradient - 0.07975589281076326\n",
      "Step - 1965, Loss - 0.2134347404530484, Learning Rate - 0.025, magnitude of gradient - 0.06827444473248225\n",
      "Step - 1966, Loss - 0.40824325516561816, Learning Rate - 0.025, magnitude of gradient - 0.043375403935299185\n",
      "Step - 1967, Loss - 0.3191251334237492, Learning Rate - 0.025, magnitude of gradient - 0.06764794968164842\n",
      "Step - 1968, Loss - 0.31220672111112147, Learning Rate - 0.025, magnitude of gradient - 0.044868585043649586\n",
      "Step - 1969, Loss - 0.2491812140524818, Learning Rate - 0.025, magnitude of gradient - 0.02220308878024759\n",
      "Step - 1970, Loss - 0.3314901035488421, Learning Rate - 0.025, magnitude of gradient - 0.03774191144966531\n",
      "Step - 1971, Loss - 0.32098065934940845, Learning Rate - 0.025, magnitude of gradient - 0.07320978828017015\n",
      "Step - 1972, Loss - 0.296321409470775, Learning Rate - 0.025, magnitude of gradient - 0.02489794628798125\n",
      "Step - 1973, Loss - 0.3912031050713132, Learning Rate - 0.025, magnitude of gradient - 0.05173871909311785\n",
      "Step - 1974, Loss - 0.27598520963466333, Learning Rate - 0.025, magnitude of gradient - 0.023169698979555866\n",
      "Step - 1975, Loss - 0.3728001148588847, Learning Rate - 0.025, magnitude of gradient - 0.035529351840444985\n",
      "Step - 1976, Loss - 0.26051092799266656, Learning Rate - 0.025, magnitude of gradient - 0.08086596889619792\n",
      "Step - 1977, Loss - 0.3395285654357048, Learning Rate - 0.025, magnitude of gradient - 0.08086536589585747\n",
      "Step - 1978, Loss - 0.31075312465186605, Learning Rate - 0.025, magnitude of gradient - 0.03900649370922486\n",
      "Step - 1979, Loss - 0.3432819197104679, Learning Rate - 0.025, magnitude of gradient - 0.12162305257874664\n",
      "Step - 1980, Loss - 0.3400001842866178, Learning Rate - 0.025, magnitude of gradient - 0.022052814017070873\n",
      "Step - 1981, Loss - 0.3302627194986695, Learning Rate - 0.025, magnitude of gradient - 0.06126681666237497\n",
      "Step - 1982, Loss - 0.4193915315292388, Learning Rate - 0.025, magnitude of gradient - 0.07974975328398107\n",
      "Step - 1983, Loss - 0.33127700512837777, Learning Rate - 0.025, magnitude of gradient - 0.010390027134428876\n",
      "Step - 1984, Loss - 0.3122292310526883, Learning Rate - 0.025, magnitude of gradient - 0.03274767481417251\n",
      "Step - 1985, Loss - 0.285903808689204, Learning Rate - 0.025, magnitude of gradient - 0.05830012623544459\n",
      "Step - 1986, Loss - 0.32150198803564617, Learning Rate - 0.025, magnitude of gradient - 0.0774151173036189\n",
      "Step - 1987, Loss - 0.34912894980562137, Learning Rate - 0.025, magnitude of gradient - 0.0441307588728462\n",
      "Step - 1988, Loss - 0.36568826792643777, Learning Rate - 0.025, magnitude of gradient - 0.04535423720182258\n",
      "Step - 1989, Loss - 0.36467729326337023, Learning Rate - 0.025, magnitude of gradient - 0.08645348520411583\n",
      "Step - 1990, Loss - 0.26172998016436394, Learning Rate - 0.025, magnitude of gradient - 0.06079015265082349\n",
      "Step - 1991, Loss - 0.31378938236610004, Learning Rate - 0.025, magnitude of gradient - 0.030729458608730924\n",
      "Step - 1992, Loss - 0.3363498605117632, Learning Rate - 0.025, magnitude of gradient - 0.03201726944308063\n",
      "Step - 1993, Loss - 0.29646472143370084, Learning Rate - 0.025, magnitude of gradient - 0.03759438014488703\n",
      "Step - 1994, Loss - 0.3249899945861456, Learning Rate - 0.025, magnitude of gradient - 0.03984063569878568\n",
      "Step - 1995, Loss - 0.27909592584956017, Learning Rate - 0.025, magnitude of gradient - 0.021432539544336597\n",
      "Step - 1996, Loss - 0.2888398944136277, Learning Rate - 0.025, magnitude of gradient - 0.06541400383907463\n",
      "Step - 1997, Loss - 0.3630993494814583, Learning Rate - 0.025, magnitude of gradient - 0.07316426116531553\n",
      "Step - 1998, Loss - 0.26510059587055707, Learning Rate - 0.025, magnitude of gradient - 0.008604773171136363\n",
      "Step - 1999, Loss - 0.32668282612683563, Learning Rate - 0.025, magnitude of gradient - 0.040321300403074786\n",
      "Step - 2000, Loss - 0.26792015098343896, Learning Rate - 0.025, magnitude of gradient - 0.02195280596266431\n",
      "Step - 2001, Loss - 0.3375014428942853, Learning Rate - 0.0125, magnitude of gradient - 0.06504270315076052\n",
      "Step - 2002, Loss - 0.29538516424722305, Learning Rate - 0.0125, magnitude of gradient - 0.046455862624465126\n",
      "Step - 2003, Loss - 0.25354157113764186, Learning Rate - 0.0125, magnitude of gradient - 0.043573338755507625\n",
      "Step - 2004, Loss - 0.28473540210624826, Learning Rate - 0.0125, magnitude of gradient - 0.06409845994989717\n",
      "Step - 2005, Loss - 0.4195218930327101, Learning Rate - 0.0125, magnitude of gradient - 0.034402645563630735\n",
      "Step - 2006, Loss - 0.28722783371907157, Learning Rate - 0.0125, magnitude of gradient - 0.019681535671555363\n",
      "Step - 2007, Loss - 0.3304575225323337, Learning Rate - 0.0125, magnitude of gradient - 0.042441577482425955\n",
      "Step - 2008, Loss - 0.2948539785237123, Learning Rate - 0.0125, magnitude of gradient - 0.015163055978636219\n",
      "Step - 2009, Loss - 0.33318731652449957, Learning Rate - 0.0125, magnitude of gradient - 0.015877399053433165\n",
      "Step - 2010, Loss - 0.24767051178783472, Learning Rate - 0.0125, magnitude of gradient - 0.057495288044410016\n",
      "Step - 2011, Loss - 0.36339220720651105, Learning Rate - 0.0125, magnitude of gradient - 0.0191761122096004\n",
      "Step - 2012, Loss - 0.3244593485478657, Learning Rate - 0.0125, magnitude of gradient - 0.04971612879239281\n",
      "Step - 2013, Loss - 0.3137227502479698, Learning Rate - 0.0125, magnitude of gradient - 0.03502175060860601\n",
      "Step - 2014, Loss - 0.28409559196701084, Learning Rate - 0.0125, magnitude of gradient - 0.040029505635482285\n",
      "Step - 2015, Loss - 0.3087758770885594, Learning Rate - 0.0125, magnitude of gradient - 0.019959531209348423\n",
      "Step - 2016, Loss - 0.3073784722976174, Learning Rate - 0.0125, magnitude of gradient - 0.09746505351184259\n",
      "Step - 2017, Loss - 0.3382268122109342, Learning Rate - 0.0125, magnitude of gradient - 0.030813342336478654\n",
      "Step - 2018, Loss - 0.27756296528232294, Learning Rate - 0.0125, magnitude of gradient - 0.012993872684900664\n",
      "Step - 2019, Loss - 0.40699593849707993, Learning Rate - 0.0125, magnitude of gradient - 0.024012436095021662\n",
      "Step - 2020, Loss - 0.3333218242713174, Learning Rate - 0.0125, magnitude of gradient - 0.05680234002880582\n",
      "Step - 2021, Loss - 0.33920865948411827, Learning Rate - 0.0125, magnitude of gradient - 0.055566805249510506\n",
      "Step - 2022, Loss - 0.22986841383037382, Learning Rate - 0.0125, magnitude of gradient - 0.0547706116306474\n",
      "Step - 2023, Loss - 0.31227935071977997, Learning Rate - 0.0125, magnitude of gradient - 0.0888678028266241\n",
      "Step - 2024, Loss - 0.41823896677413813, Learning Rate - 0.0125, magnitude of gradient - 0.0801272471394159\n",
      "Step - 2025, Loss - 0.29641993102270214, Learning Rate - 0.0125, magnitude of gradient - 0.018036825551752407\n",
      "Step - 2026, Loss - 0.32641585165278814, Learning Rate - 0.0125, magnitude of gradient - 0.09428244161816043\n",
      "Step - 2027, Loss - 0.26110015847202583, Learning Rate - 0.0125, magnitude of gradient - 0.052803251868818804\n",
      "Step - 2028, Loss - 0.3261390456747431, Learning Rate - 0.0125, magnitude of gradient - 0.058236706952929054\n",
      "Step - 2029, Loss - 0.26587532297491656, Learning Rate - 0.0125, magnitude of gradient - 0.03781525056805262\n",
      "Step - 2030, Loss - 0.3362294545304146, Learning Rate - 0.0125, magnitude of gradient - 0.09907917464435698\n",
      "Step - 2031, Loss - 0.26231464970544777, Learning Rate - 0.0125, magnitude of gradient - 0.009461857232164438\n",
      "Step - 2032, Loss - 0.3373246026065041, Learning Rate - 0.0125, magnitude of gradient - 0.13049650663205592\n",
      "Step - 2033, Loss - 0.42034129149433375, Learning Rate - 0.0125, magnitude of gradient - 0.02159898919803062\n",
      "Step - 2034, Loss - 0.34865150424770674, Learning Rate - 0.0125, magnitude of gradient - 0.06463792093677422\n",
      "Step - 2035, Loss - 0.2919287835037877, Learning Rate - 0.0125, magnitude of gradient - 0.12707618669526416\n",
      "Step - 2036, Loss - 0.3339234358401768, Learning Rate - 0.0125, magnitude of gradient - 0.03737726495244203\n",
      "Step - 2037, Loss - 0.3320603163934997, Learning Rate - 0.0125, magnitude of gradient - 0.08180199516826164\n",
      "Step - 2038, Loss - 0.3691352685854977, Learning Rate - 0.0125, magnitude of gradient - 0.014871520969353194\n",
      "Step - 2039, Loss - 0.33647413620932026, Learning Rate - 0.0125, magnitude of gradient - 0.047254269021023955\n",
      "Step - 2040, Loss - 0.36036511918583874, Learning Rate - 0.0125, magnitude of gradient - 0.06774914452740376\n",
      "Step - 2041, Loss - 0.31074170692270353, Learning Rate - 0.0125, magnitude of gradient - 0.07488900404360063\n",
      "Step - 2042, Loss - 0.3502069571289022, Learning Rate - 0.0125, magnitude of gradient - 0.04564834435282578\n",
      "Step - 2043, Loss - 0.370189172062686, Learning Rate - 0.0125, magnitude of gradient - 0.07374120676513837\n",
      "Step - 2044, Loss - 0.36167310802477215, Learning Rate - 0.0125, magnitude of gradient - 0.06883800206202591\n",
      "Step - 2045, Loss - 0.3509039121044166, Learning Rate - 0.0125, magnitude of gradient - 0.036513428469473856\n",
      "Step - 2046, Loss - 0.35861654415326844, Learning Rate - 0.0125, magnitude of gradient - 0.06452680995643278\n",
      "Step - 2047, Loss - 0.32230615266999696, Learning Rate - 0.0125, magnitude of gradient - 0.09078834410967992\n",
      "Step - 2048, Loss - 0.38721371746203254, Learning Rate - 0.0125, magnitude of gradient - 0.04347138459825767\n",
      "Step - 2049, Loss - 0.30776867442529465, Learning Rate - 0.0125, magnitude of gradient - 0.03671028327699565\n",
      "Step - 2050, Loss - 0.31925320168815646, Learning Rate - 0.0125, magnitude of gradient - 0.04254892027983635\n",
      "Step - 2051, Loss - 0.33004671309018346, Learning Rate - 0.0125, magnitude of gradient - 0.08098925023843125\n",
      "Step - 2052, Loss - 0.3203793231780241, Learning Rate - 0.0125, magnitude of gradient - 0.02234743670852469\n",
      "Step - 2053, Loss - 0.37267764867220854, Learning Rate - 0.0125, magnitude of gradient - 0.059432942036645196\n",
      "Step - 2054, Loss - 0.3563701910073829, Learning Rate - 0.0125, magnitude of gradient - 0.03534674544979684\n",
      "Step - 2055, Loss - 0.33915890294851636, Learning Rate - 0.0125, magnitude of gradient - 0.03533412350589238\n",
      "Step - 2056, Loss - 0.40114848240601375, Learning Rate - 0.0125, magnitude of gradient - 0.01569376102858886\n",
      "Step - 2057, Loss - 0.32865577771817905, Learning Rate - 0.0125, magnitude of gradient - 0.03689583510648812\n",
      "Step - 2058, Loss - 0.3420603244081584, Learning Rate - 0.0125, magnitude of gradient - 0.06535085565930487\n",
      "Step - 2059, Loss - 0.2808444534537112, Learning Rate - 0.0125, magnitude of gradient - 0.0030780551640172686\n",
      "Step - 2060, Loss - 0.27336199426184965, Learning Rate - 0.0125, magnitude of gradient - 0.06559241078060937\n",
      "Step - 2061, Loss - 0.23403955861942163, Learning Rate - 0.0125, magnitude of gradient - 0.06571357426803574\n",
      "Step - 2062, Loss - 0.3063205558289743, Learning Rate - 0.0125, magnitude of gradient - 0.06812579378995538\n",
      "Step - 2063, Loss - 0.3083139310075591, Learning Rate - 0.0125, magnitude of gradient - 0.07417600253460324\n",
      "Step - 2064, Loss - 0.3399646096561122, Learning Rate - 0.0125, magnitude of gradient - 0.10062202658290562\n",
      "Step - 2065, Loss - 0.3771460452719172, Learning Rate - 0.0125, magnitude of gradient - 0.06258684233702211\n",
      "Step - 2066, Loss - 0.2985310599333747, Learning Rate - 0.0125, magnitude of gradient - 0.03252834746395023\n",
      "Step - 2067, Loss - 0.3176727013183448, Learning Rate - 0.0125, magnitude of gradient - 0.049951808781481484\n",
      "Step - 2068, Loss - 0.3296666656266672, Learning Rate - 0.0125, magnitude of gradient - 0.06777274352043029\n",
      "Step - 2069, Loss - 0.34409668888795153, Learning Rate - 0.0125, magnitude of gradient - 0.060580833616114514\n",
      "Step - 2070, Loss - 0.28922056399344986, Learning Rate - 0.0125, magnitude of gradient - 0.08394149007664664\n",
      "Step - 2071, Loss - 0.3466223555942423, Learning Rate - 0.0125, magnitude of gradient - 0.02443371076395141\n",
      "Step - 2072, Loss - 0.27966457997910743, Learning Rate - 0.0125, magnitude of gradient - 0.049384311085368705\n",
      "Step - 2073, Loss - 0.3653258900724369, Learning Rate - 0.0125, magnitude of gradient - 0.07417782147165866\n",
      "Step - 2074, Loss - 0.27328674485625093, Learning Rate - 0.0125, magnitude of gradient - 0.024295234445736377\n",
      "Step - 2075, Loss - 0.3105603563101467, Learning Rate - 0.0125, magnitude of gradient - 0.09060548350661411\n",
      "Step - 2076, Loss - 0.29127915337723137, Learning Rate - 0.0125, magnitude of gradient - 0.02792687378736936\n",
      "Step - 2077, Loss - 0.36345612371998576, Learning Rate - 0.0125, magnitude of gradient - 0.026493416619732778\n",
      "Step - 2078, Loss - 0.36831445467552393, Learning Rate - 0.0125, magnitude of gradient - 0.048057793489828506\n",
      "Step - 2079, Loss - 0.38189928637294357, Learning Rate - 0.0125, magnitude of gradient - 0.05974673370803699\n",
      "Step - 2080, Loss - 0.3372236972929641, Learning Rate - 0.0125, magnitude of gradient - 0.11132756618342489\n",
      "Step - 2081, Loss - 0.25797982332269814, Learning Rate - 0.0125, magnitude of gradient - 0.01710049196105357\n",
      "Step - 2082, Loss - 0.23368356364441367, Learning Rate - 0.0125, magnitude of gradient - 0.0505612579944893\n",
      "Step - 2083, Loss - 0.2737495577140202, Learning Rate - 0.0125, magnitude of gradient - 0.052524360190614953\n",
      "Step - 2084, Loss - 0.3664040656472679, Learning Rate - 0.0125, magnitude of gradient - 0.051177573761385646\n",
      "Step - 2085, Loss - 0.3171738705011158, Learning Rate - 0.0125, magnitude of gradient - 0.036042465229452814\n",
      "Step - 2086, Loss - 0.26675980008858846, Learning Rate - 0.0125, magnitude of gradient - 0.06181887565775267\n",
      "Step - 2087, Loss - 0.23504505599810566, Learning Rate - 0.0125, magnitude of gradient - 0.021754877174107855\n",
      "Step - 2088, Loss - 0.3048743095867871, Learning Rate - 0.0125, magnitude of gradient - 0.05374373920837947\n",
      "Step - 2089, Loss - 0.3199736116187937, Learning Rate - 0.0125, magnitude of gradient - 0.01169281379316232\n",
      "Step - 2090, Loss - 0.3244853050998763, Learning Rate - 0.0125, magnitude of gradient - 0.026408897960973024\n",
      "Step - 2091, Loss - 0.2663308366821628, Learning Rate - 0.0125, magnitude of gradient - 0.05619115786614481\n",
      "Step - 2092, Loss - 0.30120846502323706, Learning Rate - 0.0125, magnitude of gradient - 0.03731040887782877\n",
      "Step - 2093, Loss - 0.29564725844590684, Learning Rate - 0.0125, magnitude of gradient - 0.04101925932605516\n",
      "Step - 2094, Loss - 0.3335767961589164, Learning Rate - 0.0125, magnitude of gradient - 0.048161042726917576\n",
      "Step - 2095, Loss - 0.31270217367038744, Learning Rate - 0.0125, magnitude of gradient - 0.02312582252153543\n",
      "Step - 2096, Loss - 0.3285539927035114, Learning Rate - 0.0125, magnitude of gradient - 0.03436769870827504\n",
      "Step - 2097, Loss - 0.3556722667220258, Learning Rate - 0.0125, magnitude of gradient - 0.08648595044310267\n",
      "Step - 2098, Loss - 0.3163367095084508, Learning Rate - 0.0125, magnitude of gradient - 0.07059180961251908\n",
      "Step - 2099, Loss - 0.2657621910276594, Learning Rate - 0.0125, magnitude of gradient - 0.07139015205949371\n",
      "Step - 2100, Loss - 0.3066353487310774, Learning Rate - 0.0125, magnitude of gradient - 0.02186625631757563\n",
      "Step - 2101, Loss - 0.2889882296807572, Learning Rate - 0.0125, magnitude of gradient - 0.03231907980207492\n",
      "Step - 2102, Loss - 0.3443577831137923, Learning Rate - 0.0125, magnitude of gradient - 0.007221208017875374\n",
      "Step - 2103, Loss - 0.2945484459036092, Learning Rate - 0.0125, magnitude of gradient - 0.018979813991915305\n",
      "Step - 2104, Loss - 0.27270162964039724, Learning Rate - 0.0125, magnitude of gradient - 0.0506827515468926\n",
      "Step - 2105, Loss - 0.33960478526941257, Learning Rate - 0.0125, magnitude of gradient - 0.06184774984853741\n",
      "Step - 2106, Loss - 0.32369061018529693, Learning Rate - 0.0125, magnitude of gradient - 0.004653407884523495\n",
      "Step - 2107, Loss - 0.3928961994083651, Learning Rate - 0.0125, magnitude of gradient - 0.06018488639677596\n",
      "Step - 2108, Loss - 0.4002666010332524, Learning Rate - 0.0125, magnitude of gradient - 0.04417595641302877\n",
      "Step - 2109, Loss - 0.39879854396259307, Learning Rate - 0.0125, magnitude of gradient - 0.023621288503831623\n",
      "Step - 2110, Loss - 0.29886009593507695, Learning Rate - 0.0125, magnitude of gradient - 0.09240315353402026\n",
      "Step - 2111, Loss - 0.29938195529266204, Learning Rate - 0.0125, magnitude of gradient - 0.00918754098718103\n",
      "Step - 2112, Loss - 0.2848190628519186, Learning Rate - 0.0125, magnitude of gradient - 0.06560213593160533\n",
      "Step - 2113, Loss - 0.2714110591442834, Learning Rate - 0.0125, magnitude of gradient - 0.02049431583771014\n",
      "Step - 2114, Loss - 0.34726102249463076, Learning Rate - 0.0125, magnitude of gradient - 0.048085718452795495\n",
      "Step - 2115, Loss - 0.31662504656414536, Learning Rate - 0.0125, magnitude of gradient - 0.08023141481659189\n",
      "Step - 2116, Loss - 0.3375569083112274, Learning Rate - 0.0125, magnitude of gradient - 0.025684963571667004\n",
      "Step - 2117, Loss - 0.23210474798161912, Learning Rate - 0.0125, magnitude of gradient - 0.0319916912130727\n",
      "Step - 2118, Loss - 0.3466675308875983, Learning Rate - 0.0125, magnitude of gradient - 0.07496012388002267\n",
      "Step - 2119, Loss - 0.35844427834702003, Learning Rate - 0.0125, magnitude of gradient - 0.04065068351932374\n",
      "Step - 2120, Loss - 0.41877655395567526, Learning Rate - 0.0125, magnitude of gradient - 0.08605676827440717\n",
      "Step - 2121, Loss - 0.37919673877203286, Learning Rate - 0.0125, magnitude of gradient - 0.05227240525518649\n",
      "Step - 2122, Loss - 0.28603070694382376, Learning Rate - 0.0125, magnitude of gradient - 0.034072276824198326\n",
      "Step - 2123, Loss - 0.35026722735414406, Learning Rate - 0.0125, magnitude of gradient - 0.026364739978092487\n",
      "Step - 2124, Loss - 0.24163597707058537, Learning Rate - 0.0125, magnitude of gradient - 0.03847311400814725\n",
      "Step - 2125, Loss - 0.28967356978427405, Learning Rate - 0.0125, magnitude of gradient - 0.08151259660291711\n",
      "Step - 2126, Loss - 0.4061371787082616, Learning Rate - 0.0125, magnitude of gradient - 0.07676033226685466\n",
      "Step - 2127, Loss - 0.20800575628942422, Learning Rate - 0.0125, magnitude of gradient - 0.09478251144220075\n",
      "Step - 2128, Loss - 0.2947769520604803, Learning Rate - 0.0125, magnitude of gradient - 0.03746607228933131\n",
      "Step - 2129, Loss - 0.29685129055279885, Learning Rate - 0.0125, magnitude of gradient - 0.05391474994365759\n",
      "Step - 2130, Loss - 0.2707574747353684, Learning Rate - 0.0125, magnitude of gradient - 0.0671046744839115\n",
      "Step - 2131, Loss - 0.30489418387864575, Learning Rate - 0.0125, magnitude of gradient - 0.03796506743471946\n",
      "Step - 2132, Loss - 0.32888754491583855, Learning Rate - 0.0125, magnitude of gradient - 0.033599452133336025\n",
      "Step - 2133, Loss - 0.2663782361211555, Learning Rate - 0.0125, magnitude of gradient - 0.07075290866182089\n",
      "Step - 2134, Loss - 0.3591138203601921, Learning Rate - 0.0125, magnitude of gradient - 0.054870706090777585\n",
      "Step - 2135, Loss - 0.2778923862908221, Learning Rate - 0.0125, magnitude of gradient - 0.04932225679873189\n",
      "Step - 2136, Loss - 0.3799608866768482, Learning Rate - 0.0125, magnitude of gradient - 0.06757012655217647\n",
      "Step - 2137, Loss - 0.3402952108360984, Learning Rate - 0.0125, magnitude of gradient - 0.04109509982682331\n",
      "Step - 2138, Loss - 0.3363873675472102, Learning Rate - 0.0125, magnitude of gradient - 0.012955039932980607\n",
      "Step - 2139, Loss - 0.3361199028400941, Learning Rate - 0.0125, magnitude of gradient - 0.0484070927024536\n",
      "Step - 2140, Loss - 0.3214462766776257, Learning Rate - 0.0125, magnitude of gradient - 0.09896297420946873\n",
      "Step - 2141, Loss - 0.2896580506771704, Learning Rate - 0.0125, magnitude of gradient - 0.031330907921334125\n",
      "Step - 2142, Loss - 0.31624382325579764, Learning Rate - 0.0125, magnitude of gradient - 0.08069890103293992\n",
      "Step - 2143, Loss - 0.2833586911600954, Learning Rate - 0.0125, magnitude of gradient - 0.025630202445721544\n",
      "Step - 2144, Loss - 0.37342608433085034, Learning Rate - 0.0125, magnitude of gradient - 0.019231780200444793\n",
      "Step - 2145, Loss - 0.31290049740190207, Learning Rate - 0.0125, magnitude of gradient - 0.04563706062706548\n",
      "Step - 2146, Loss - 0.2738017028296416, Learning Rate - 0.0125, magnitude of gradient - 0.07675551325620862\n",
      "Step - 2147, Loss - 0.3303976720552581, Learning Rate - 0.0125, magnitude of gradient - 0.07982113117174085\n",
      "Step - 2148, Loss - 0.2923177157174318, Learning Rate - 0.0125, magnitude of gradient - 0.026235819486734674\n",
      "Step - 2149, Loss - 0.3672130019389613, Learning Rate - 0.0125, magnitude of gradient - 0.051903960698294345\n",
      "Step - 2150, Loss - 0.2951427548185248, Learning Rate - 0.0125, magnitude of gradient - 0.04139244839964158\n",
      "Step - 2151, Loss - 0.3134185346721426, Learning Rate - 0.0125, magnitude of gradient - 0.06953083366003705\n",
      "Step - 2152, Loss - 0.40163263010622996, Learning Rate - 0.0125, magnitude of gradient - 0.07146416446012038\n",
      "Step - 2153, Loss - 0.30467542550951066, Learning Rate - 0.0125, magnitude of gradient - 0.01743344393667381\n",
      "Step - 2154, Loss - 0.34772143905021663, Learning Rate - 0.0125, magnitude of gradient - 0.04509320344920917\n",
      "Step - 2155, Loss - 0.3666859264850409, Learning Rate - 0.0125, magnitude of gradient - 0.08559870237219484\n",
      "Step - 2156, Loss - 0.31299077587264074, Learning Rate - 0.0125, magnitude of gradient - 0.050707372093814596\n",
      "Step - 2157, Loss - 0.34165372495212404, Learning Rate - 0.0125, magnitude of gradient - 0.03521821083067268\n",
      "Step - 2158, Loss - 0.34816516818105364, Learning Rate - 0.0125, magnitude of gradient - 0.025418632334600982\n",
      "Step - 2159, Loss - 0.3757794543305386, Learning Rate - 0.0125, magnitude of gradient - 0.07795928180393813\n",
      "Step - 2160, Loss - 0.3000953573112533, Learning Rate - 0.0125, magnitude of gradient - 0.028736185266121622\n",
      "Step - 2161, Loss - 0.3296487859735002, Learning Rate - 0.0125, magnitude of gradient - 0.05439765520603794\n",
      "Step - 2162, Loss - 0.3307033708671428, Learning Rate - 0.0125, magnitude of gradient - 0.07611257602378747\n",
      "Step - 2163, Loss - 0.3097576984020513, Learning Rate - 0.0125, magnitude of gradient - 0.06827560259590097\n",
      "Step - 2164, Loss - 0.31032717730720016, Learning Rate - 0.0125, magnitude of gradient - 0.0580538786396845\n",
      "Step - 2165, Loss - 0.3130133619256294, Learning Rate - 0.0125, magnitude of gradient - 0.04897911324029293\n",
      "Step - 2166, Loss - 0.3131781002732368, Learning Rate - 0.0125, magnitude of gradient - 0.05323714490388819\n",
      "Step - 2167, Loss - 0.3824478908274823, Learning Rate - 0.0125, magnitude of gradient - 0.05982446603109797\n",
      "Step - 2168, Loss - 0.3193340249993736, Learning Rate - 0.0125, magnitude of gradient - 0.03523871692521784\n",
      "Step - 2169, Loss - 0.2799290588452792, Learning Rate - 0.0125, magnitude of gradient - 0.05306688314262524\n",
      "Step - 2170, Loss - 0.26729347530765457, Learning Rate - 0.0125, magnitude of gradient - 0.08198205051895617\n",
      "Step - 2171, Loss - 0.3636255702159088, Learning Rate - 0.0125, magnitude of gradient - 0.04013164563221499\n",
      "Step - 2172, Loss - 0.3500995075285319, Learning Rate - 0.0125, magnitude of gradient - 0.03688034882649465\n",
      "Step - 2173, Loss - 0.3883882572065545, Learning Rate - 0.0125, magnitude of gradient - 0.031212041780255606\n",
      "Step - 2174, Loss - 0.31111195599042774, Learning Rate - 0.0125, magnitude of gradient - 0.014561432795010524\n",
      "Step - 2175, Loss - 0.35424373377577517, Learning Rate - 0.0125, magnitude of gradient - 0.04743933595488331\n",
      "Step - 2176, Loss - 0.2919624504902729, Learning Rate - 0.0125, magnitude of gradient - 0.050460466244758265\n",
      "Step - 2177, Loss - 0.33436925361767383, Learning Rate - 0.0125, magnitude of gradient - 0.0513617184297336\n",
      "Step - 2178, Loss - 0.3637134664610431, Learning Rate - 0.0125, magnitude of gradient - 0.1208458865410455\n",
      "Step - 2179, Loss - 0.3454031529612887, Learning Rate - 0.0125, magnitude of gradient - 0.11162899714187617\n",
      "Step - 2180, Loss - 0.31456800895174275, Learning Rate - 0.0125, magnitude of gradient - 0.04219257675870706\n",
      "Step - 2181, Loss - 0.2693845200189063, Learning Rate - 0.0125, magnitude of gradient - 0.00042932042919947654\n",
      "Step - 2182, Loss - 0.28093591966760134, Learning Rate - 0.0125, magnitude of gradient - 0.036216773432432105\n",
      "Step - 2183, Loss - 0.3679715925370418, Learning Rate - 0.0125, magnitude of gradient - 0.062132711435778636\n",
      "Step - 2184, Loss - 0.36184731536656634, Learning Rate - 0.0125, magnitude of gradient - 0.048448113515915686\n",
      "Step - 2185, Loss - 0.31126989976272335, Learning Rate - 0.0125, magnitude of gradient - 0.10917936961823481\n",
      "Step - 2186, Loss - 0.39395941128215406, Learning Rate - 0.0125, magnitude of gradient - 0.028236757735984292\n",
      "Step - 2187, Loss - 0.2902654815787867, Learning Rate - 0.0125, magnitude of gradient - 0.027706694175478934\n",
      "Step - 2188, Loss - 0.3103816293603899, Learning Rate - 0.0125, magnitude of gradient - 0.06083443169987048\n",
      "Step - 2189, Loss - 0.30905648223083754, Learning Rate - 0.0125, magnitude of gradient - 0.05531268039946268\n",
      "Step - 2190, Loss - 0.32528639606331505, Learning Rate - 0.0125, magnitude of gradient - 0.018342260478026652\n",
      "Step - 2191, Loss - 0.32020674092899687, Learning Rate - 0.0125, magnitude of gradient - 0.05232460318434875\n",
      "Step - 2192, Loss - 0.3838430721060095, Learning Rate - 0.0125, magnitude of gradient - 0.0836714112302073\n",
      "Step - 2193, Loss - 0.3223363015802585, Learning Rate - 0.0125, magnitude of gradient - 0.05256748777132238\n",
      "Step - 2194, Loss - 0.3666875045441899, Learning Rate - 0.0125, magnitude of gradient - 0.01304355829832392\n",
      "Step - 2195, Loss - 0.23154027437870697, Learning Rate - 0.0125, magnitude of gradient - 0.025312563106900113\n",
      "Step - 2196, Loss - 0.3176452483521218, Learning Rate - 0.0125, magnitude of gradient - 0.04071579335354031\n",
      "Step - 2197, Loss - 0.3871947749936916, Learning Rate - 0.0125, magnitude of gradient - 0.025866655871424626\n",
      "Step - 2198, Loss - 0.2648634282447353, Learning Rate - 0.0125, magnitude of gradient - 0.04758731341014193\n",
      "Step - 2199, Loss - 0.33074092999869337, Learning Rate - 0.0125, magnitude of gradient - 0.026739958737784484\n",
      "Step - 2200, Loss - 0.42065789953256, Learning Rate - 0.0125, magnitude of gradient - 0.03748406594496265\n",
      "Step - 2201, Loss - 0.4327931307994203, Learning Rate - 0.0125, magnitude of gradient - 0.05634833939813881\n",
      "Step - 2202, Loss - 0.34938134894136663, Learning Rate - 0.0125, magnitude of gradient - 0.03501498470137645\n",
      "Step - 2203, Loss - 0.3593125522429269, Learning Rate - 0.0125, magnitude of gradient - 0.06842265294629633\n",
      "Step - 2204, Loss - 0.30390995604745114, Learning Rate - 0.0125, magnitude of gradient - 0.01680340979239485\n",
      "Step - 2205, Loss - 0.3013643451932774, Learning Rate - 0.0125, magnitude of gradient - 0.025412131437971008\n",
      "Step - 2206, Loss - 0.30896980553591813, Learning Rate - 0.0125, magnitude of gradient - 0.04614101598435416\n",
      "Step - 2207, Loss - 0.34620638078087174, Learning Rate - 0.0125, magnitude of gradient - 0.04332427223178133\n",
      "Step - 2208, Loss - 0.3173839630272057, Learning Rate - 0.0125, magnitude of gradient - 0.004451879603157377\n",
      "Step - 2209, Loss - 0.2265564590315745, Learning Rate - 0.0125, magnitude of gradient - 0.06933985417174582\n",
      "Step - 2210, Loss - 0.3416992444364895, Learning Rate - 0.0125, magnitude of gradient - 0.029480424888258566\n",
      "Step - 2211, Loss - 0.25349772480793886, Learning Rate - 0.0125, magnitude of gradient - 0.055572279977943735\n",
      "Step - 2212, Loss - 0.28079532760133397, Learning Rate - 0.0125, magnitude of gradient - 0.042911672287862655\n",
      "Step - 2213, Loss - 0.33255154889703725, Learning Rate - 0.0125, magnitude of gradient - 0.04463105408086745\n",
      "Step - 2214, Loss - 0.3378673010986233, Learning Rate - 0.0125, magnitude of gradient - 0.03575832987160982\n",
      "Step - 2215, Loss - 0.3250987033848002, Learning Rate - 0.0125, magnitude of gradient - 0.09809883023533213\n",
      "Step - 2216, Loss - 0.23839601681238443, Learning Rate - 0.0125, magnitude of gradient - 0.08054069317013025\n",
      "Step - 2217, Loss - 0.3237117549512522, Learning Rate - 0.0125, magnitude of gradient - 0.05565001319179146\n",
      "Step - 2218, Loss - 0.3452968569445874, Learning Rate - 0.0125, magnitude of gradient - 0.04857963894856262\n",
      "Step - 2219, Loss - 0.33149093556030906, Learning Rate - 0.0125, magnitude of gradient - 0.037701843319496595\n",
      "Step - 2220, Loss - 0.35905208388109533, Learning Rate - 0.0125, magnitude of gradient - 0.041522385284007465\n",
      "Step - 2221, Loss - 0.2907254142272211, Learning Rate - 0.0125, magnitude of gradient - 0.04531871546595007\n",
      "Step - 2222, Loss - 0.2607548622393969, Learning Rate - 0.0125, magnitude of gradient - 0.061408805036726376\n",
      "Step - 2223, Loss - 0.35537283289851046, Learning Rate - 0.0125, magnitude of gradient - 0.10128531373224299\n",
      "Step - 2224, Loss - 0.37175589531468634, Learning Rate - 0.0125, magnitude of gradient - 0.06693715695858023\n",
      "Step - 2225, Loss - 0.34836941922245535, Learning Rate - 0.0125, magnitude of gradient - 0.02262148947645242\n",
      "Step - 2226, Loss - 0.33059451791006417, Learning Rate - 0.0125, magnitude of gradient - 0.05077851848320886\n",
      "Step - 2227, Loss - 0.2644641704131122, Learning Rate - 0.0125, magnitude of gradient - 0.04001804520716071\n",
      "Step - 2228, Loss - 0.3326393942923223, Learning Rate - 0.0125, magnitude of gradient - 0.022532281866313498\n",
      "Step - 2229, Loss - 0.3388251555874344, Learning Rate - 0.0125, magnitude of gradient - 0.06554142828751035\n",
      "Step - 2230, Loss - 0.2537202153959748, Learning Rate - 0.0125, magnitude of gradient - 0.02529446885719129\n",
      "Step - 2231, Loss - 0.31706366391531343, Learning Rate - 0.0125, magnitude of gradient - 0.04370760204205439\n",
      "Step - 2232, Loss - 0.4122812615011334, Learning Rate - 0.0125, magnitude of gradient - 0.06427961739358436\n",
      "Step - 2233, Loss - 0.3076594953956815, Learning Rate - 0.0125, magnitude of gradient - 0.03860240186935367\n",
      "Step - 2234, Loss - 0.27941240760601793, Learning Rate - 0.0125, magnitude of gradient - 0.0655906800550898\n",
      "Step - 2235, Loss - 0.28998716385048257, Learning Rate - 0.0125, magnitude of gradient - 0.040189742197076675\n",
      "Step - 2236, Loss - 0.2756185586134652, Learning Rate - 0.0125, magnitude of gradient - 0.1077552348035289\n",
      "Step - 2237, Loss - 0.26211279053105224, Learning Rate - 0.0125, magnitude of gradient - 0.06333565722038284\n",
      "Step - 2238, Loss - 0.3798993510751804, Learning Rate - 0.0125, magnitude of gradient - 0.07309524054511131\n",
      "Step - 2239, Loss - 0.3151672788684077, Learning Rate - 0.0125, magnitude of gradient - 0.07202684810937982\n",
      "Step - 2240, Loss - 0.33738884305838474, Learning Rate - 0.0125, magnitude of gradient - 0.03910260653300707\n",
      "Step - 2241, Loss - 0.30200269247762024, Learning Rate - 0.0125, magnitude of gradient - 0.05932417808003412\n",
      "Step - 2242, Loss - 0.290898580709967, Learning Rate - 0.0125, magnitude of gradient - 0.07916859656660322\n",
      "Step - 2243, Loss - 0.3255820476354515, Learning Rate - 0.0125, magnitude of gradient - 0.02598469383139471\n",
      "Step - 2244, Loss - 0.27537480065815834, Learning Rate - 0.0125, magnitude of gradient - 0.10875580835918304\n",
      "Step - 2245, Loss - 0.2733370166656197, Learning Rate - 0.0125, magnitude of gradient - 0.06288200323781999\n",
      "Step - 2246, Loss - 0.34795275051050145, Learning Rate - 0.0125, magnitude of gradient - 0.06492772836675811\n",
      "Step - 2247, Loss - 0.33955173520373405, Learning Rate - 0.0125, magnitude of gradient - 0.09356060430765464\n",
      "Step - 2248, Loss - 0.341098671688631, Learning Rate - 0.0125, magnitude of gradient - 0.026825423659816484\n",
      "Step - 2249, Loss - 0.3611337913515282, Learning Rate - 0.0125, magnitude of gradient - 0.08140178965647646\n",
      "Step - 2250, Loss - 0.2747047834002341, Learning Rate - 0.0125, magnitude of gradient - 0.05856079556907863\n",
      "Step - 2251, Loss - 0.368136198881294, Learning Rate - 0.0125, magnitude of gradient - 0.028221475504051532\n",
      "Step - 2252, Loss - 0.3626820875287284, Learning Rate - 0.0125, magnitude of gradient - 0.05091014418051402\n",
      "Step - 2253, Loss - 0.2698239351310846, Learning Rate - 0.0125, magnitude of gradient - 0.07614122756685737\n",
      "Step - 2254, Loss - 0.25393729473885296, Learning Rate - 0.0125, magnitude of gradient - 0.05668236127737665\n",
      "Step - 2255, Loss - 0.24951234176656795, Learning Rate - 0.0125, magnitude of gradient - 0.03734479528217139\n",
      "Step - 2256, Loss - 0.26310596412767184, Learning Rate - 0.0125, magnitude of gradient - 0.009293480784726336\n",
      "Step - 2257, Loss - 0.3737834877109326, Learning Rate - 0.0125, magnitude of gradient - 0.04295040647527856\n",
      "Step - 2258, Loss - 0.38022176230633087, Learning Rate - 0.0125, magnitude of gradient - 0.07564229631186968\n",
      "Step - 2259, Loss - 0.2754294004387047, Learning Rate - 0.0125, magnitude of gradient - 0.06571822481607542\n",
      "Step - 2260, Loss - 0.32486720395700913, Learning Rate - 0.0125, magnitude of gradient - 0.008854974656774239\n",
      "Step - 2261, Loss - 0.35311792307185996, Learning Rate - 0.0125, magnitude of gradient - 0.06851140890390828\n",
      "Step - 2262, Loss - 0.36618280015902205, Learning Rate - 0.0125, magnitude of gradient - 0.06500417820639554\n",
      "Step - 2263, Loss - 0.29752007727931906, Learning Rate - 0.0125, magnitude of gradient - 0.08491603402651704\n",
      "Step - 2264, Loss - 0.3468126342418525, Learning Rate - 0.0125, magnitude of gradient - 0.06404231597526465\n",
      "Step - 2265, Loss - 0.3609465266886114, Learning Rate - 0.0125, magnitude of gradient - 0.033147687369467445\n",
      "Step - 2266, Loss - 0.29745538669286353, Learning Rate - 0.0125, magnitude of gradient - 0.032803076531800854\n",
      "Step - 2267, Loss - 0.3068393395343293, Learning Rate - 0.0125, magnitude of gradient - 0.05515490218217605\n",
      "Step - 2268, Loss - 0.33076820545732905, Learning Rate - 0.0125, magnitude of gradient - 0.03857894461239291\n",
      "Step - 2269, Loss - 0.2850732068537623, Learning Rate - 0.0125, magnitude of gradient - 0.03160771881548291\n",
      "Step - 2270, Loss - 0.34340077828355986, Learning Rate - 0.0125, magnitude of gradient - 0.034077506434813186\n",
      "Step - 2271, Loss - 0.31648626162278537, Learning Rate - 0.0125, magnitude of gradient - 0.019628935233029395\n",
      "Step - 2272, Loss - 0.3855779177995371, Learning Rate - 0.0125, magnitude of gradient - 0.036481209343280155\n",
      "Step - 2273, Loss - 0.37056029206996866, Learning Rate - 0.0125, magnitude of gradient - 0.04647254559948723\n",
      "Step - 2274, Loss - 0.2467724728333361, Learning Rate - 0.0125, magnitude of gradient - 0.03159939269010324\n",
      "Step - 2275, Loss - 0.33811407295105167, Learning Rate - 0.0125, magnitude of gradient - 0.06580808961421472\n",
      "Step - 2276, Loss - 0.2570320299891513, Learning Rate - 0.0125, magnitude of gradient - 0.017374327657276166\n",
      "Step - 2277, Loss - 0.2933835311931969, Learning Rate - 0.0125, magnitude of gradient - 0.021459136362418203\n",
      "Step - 2278, Loss - 0.29751775962788285, Learning Rate - 0.0125, magnitude of gradient - 0.05647550905927088\n",
      "Step - 2279, Loss - 0.35154016834962, Learning Rate - 0.0125, magnitude of gradient - 0.007199043003848997\n",
      "Step - 2280, Loss - 0.41267783565807115, Learning Rate - 0.0125, magnitude of gradient - 0.0718616444974704\n",
      "Step - 2281, Loss - 0.3166618488689513, Learning Rate - 0.0125, magnitude of gradient - 0.017360625536253504\n",
      "Step - 2282, Loss - 0.3478543762671592, Learning Rate - 0.0125, magnitude of gradient - 0.03810546325195936\n",
      "Step - 2283, Loss - 0.31493418013717583, Learning Rate - 0.0125, magnitude of gradient - 0.06856044098709556\n",
      "Step - 2284, Loss - 0.25674427604436223, Learning Rate - 0.0125, magnitude of gradient - 0.025251995087939006\n",
      "Step - 2285, Loss - 0.3441130094076136, Learning Rate - 0.0125, magnitude of gradient - 0.100826637473362\n",
      "Step - 2286, Loss - 0.3175013004714662, Learning Rate - 0.0125, magnitude of gradient - 0.08415026208846377\n",
      "Step - 2287, Loss - 0.3475978741037917, Learning Rate - 0.0125, magnitude of gradient - 0.059982654540833694\n",
      "Step - 2288, Loss - 0.3063728063879101, Learning Rate - 0.0125, magnitude of gradient - 0.05559708980478854\n",
      "Step - 2289, Loss - 0.30163336173432576, Learning Rate - 0.0125, magnitude of gradient - 0.01394897714805445\n",
      "Step - 2290, Loss - 0.29685669800534586, Learning Rate - 0.0125, magnitude of gradient - 0.03379096673380335\n",
      "Step - 2291, Loss - 0.3740833557993791, Learning Rate - 0.0125, magnitude of gradient - 0.11455843370182604\n",
      "Step - 2292, Loss - 0.343285312755106, Learning Rate - 0.0125, magnitude of gradient - 0.09531307225548696\n",
      "Step - 2293, Loss - 0.3725858558031193, Learning Rate - 0.0125, magnitude of gradient - 0.07635190974039273\n",
      "Step - 2294, Loss - 0.37020179468400244, Learning Rate - 0.0125, magnitude of gradient - 0.013745098675455593\n",
      "Step - 2295, Loss - 0.3060296916342598, Learning Rate - 0.0125, magnitude of gradient - 0.030388879122933553\n",
      "Step - 2296, Loss - 0.36220543197130733, Learning Rate - 0.0125, magnitude of gradient - 0.033726575754042684\n",
      "Step - 2297, Loss - 0.35009656513433296, Learning Rate - 0.0125, magnitude of gradient - 0.06378003857499376\n",
      "Step - 2298, Loss - 0.2918306895702472, Learning Rate - 0.0125, magnitude of gradient - 0.011273138738980691\n",
      "Step - 2299, Loss - 0.2942438044425237, Learning Rate - 0.0125, magnitude of gradient - 0.033712992093899115\n",
      "Step - 2300, Loss - 0.3025289567876177, Learning Rate - 0.0125, magnitude of gradient - 0.03663416652170039\n",
      "Step - 2301, Loss - 0.35168352636746214, Learning Rate - 0.0125, magnitude of gradient - 0.06903015500632986\n",
      "Step - 2302, Loss - 0.32049310485425103, Learning Rate - 0.0125, magnitude of gradient - 0.011231329782121225\n",
      "Step - 2303, Loss - 0.35041684296870185, Learning Rate - 0.0125, magnitude of gradient - 0.11148185560603488\n",
      "Step - 2304, Loss - 0.27283771847994387, Learning Rate - 0.0125, magnitude of gradient - 0.03850265293744968\n",
      "Step - 2305, Loss - 0.2978683778520041, Learning Rate - 0.0125, magnitude of gradient - 0.0380930470902835\n",
      "Step - 2306, Loss - 0.2910421250591046, Learning Rate - 0.0125, magnitude of gradient - 0.008772625940385985\n",
      "Step - 2307, Loss - 0.393493526693759, Learning Rate - 0.0125, magnitude of gradient - 0.01804968642605328\n",
      "Step - 2308, Loss - 0.34058320776156575, Learning Rate - 0.0125, magnitude of gradient - 0.0504616362356295\n",
      "Step - 2309, Loss - 0.3318388193303623, Learning Rate - 0.0125, magnitude of gradient - 0.07230280721313638\n",
      "Step - 2310, Loss - 0.33073835196984097, Learning Rate - 0.0125, magnitude of gradient - 0.029658462356529042\n",
      "Step - 2311, Loss - 0.2979446807036783, Learning Rate - 0.0125, magnitude of gradient - 0.03940335930538761\n",
      "Step - 2312, Loss - 0.364019949476045, Learning Rate - 0.0125, magnitude of gradient - 0.07545734386809143\n",
      "Step - 2313, Loss - 0.28637299302206887, Learning Rate - 0.0125, magnitude of gradient - 0.025264987510961054\n",
      "Step - 2314, Loss - 0.3682873844484754, Learning Rate - 0.0125, magnitude of gradient - 0.0413032284906302\n",
      "Step - 2315, Loss - 0.24520080922147214, Learning Rate - 0.0125, magnitude of gradient - 0.03434552857471319\n",
      "Step - 2316, Loss - 0.32318633407094577, Learning Rate - 0.0125, magnitude of gradient - 0.05844939808803749\n",
      "Step - 2317, Loss - 0.23385301615826767, Learning Rate - 0.0125, magnitude of gradient - 0.0638396495909822\n",
      "Step - 2318, Loss - 0.27790976096171627, Learning Rate - 0.0125, magnitude of gradient - 0.06143146148632574\n",
      "Step - 2319, Loss - 0.2538333180706766, Learning Rate - 0.0125, magnitude of gradient - 0.05375216347055995\n",
      "Step - 2320, Loss - 0.2863557912693059, Learning Rate - 0.0125, magnitude of gradient - 0.04457861786881732\n",
      "Step - 2321, Loss - 0.2640613741737002, Learning Rate - 0.0125, magnitude of gradient - 0.03434918976028023\n",
      "Step - 2322, Loss - 0.31398190175081725, Learning Rate - 0.0125, magnitude of gradient - 0.0580740277537407\n",
      "Step - 2323, Loss - 0.3569962419520043, Learning Rate - 0.0125, magnitude of gradient - 0.005200687502991624\n",
      "Step - 2324, Loss - 0.32532369988683457, Learning Rate - 0.0125, magnitude of gradient - 0.014865951493362978\n",
      "Step - 2325, Loss - 0.32096330132171397, Learning Rate - 0.0125, magnitude of gradient - 0.0766401623824216\n",
      "Step - 2326, Loss - 0.4382569808693563, Learning Rate - 0.0125, magnitude of gradient - 0.023219477067537304\n",
      "Step - 2327, Loss - 0.37464379155709776, Learning Rate - 0.0125, magnitude of gradient - 0.013452610900747453\n",
      "Step - 2328, Loss - 0.2305574499361972, Learning Rate - 0.0125, magnitude of gradient - 0.022116715731381915\n",
      "Step - 2329, Loss - 0.27185208965082663, Learning Rate - 0.0125, magnitude of gradient - 0.028400113812256343\n",
      "Step - 2330, Loss - 0.3483464078451782, Learning Rate - 0.0125, magnitude of gradient - 0.05677152614010843\n",
      "Step - 2331, Loss - 0.3409801995461757, Learning Rate - 0.0125, magnitude of gradient - 0.030039283160565988\n",
      "Step - 2332, Loss - 0.32701943576642845, Learning Rate - 0.0125, magnitude of gradient - 0.03126826112915035\n",
      "Step - 2333, Loss - 0.3502820040672325, Learning Rate - 0.0125, magnitude of gradient - 0.09453276193524958\n",
      "Step - 2334, Loss - 0.3677844113007145, Learning Rate - 0.0125, magnitude of gradient - 0.05131986623887888\n",
      "Step - 2335, Loss - 0.3440249271889326, Learning Rate - 0.0125, magnitude of gradient - 0.037730505569404646\n",
      "Step - 2336, Loss - 0.34351455930177144, Learning Rate - 0.0125, magnitude of gradient - 0.07542057221423398\n",
      "Step - 2337, Loss - 0.31625597414741524, Learning Rate - 0.0125, magnitude of gradient - 0.04817297241463627\n",
      "Step - 2338, Loss - 0.31803652448090214, Learning Rate - 0.0125, magnitude of gradient - 0.04616233128595528\n",
      "Step - 2339, Loss - 0.39780653295646917, Learning Rate - 0.0125, magnitude of gradient - 0.09765355053715159\n",
      "Step - 2340, Loss - 0.33495339329970814, Learning Rate - 0.0125, magnitude of gradient - 0.038948896211347665\n",
      "Step - 2341, Loss - 0.3642240397405888, Learning Rate - 0.0125, magnitude of gradient - 0.08173161932562097\n",
      "Step - 2342, Loss - 0.26790198975346535, Learning Rate - 0.0125, magnitude of gradient - 0.04664107156559964\n",
      "Step - 2343, Loss - 0.34607508153802713, Learning Rate - 0.0125, magnitude of gradient - 0.05702942179383542\n",
      "Step - 2344, Loss - 0.3042360691548158, Learning Rate - 0.0125, magnitude of gradient - 0.04287236017802191\n",
      "Step - 2345, Loss - 0.2752299660478739, Learning Rate - 0.0125, magnitude of gradient - 0.0020356283022867324\n",
      "Step - 2346, Loss - 0.32199087647223745, Learning Rate - 0.0125, magnitude of gradient - 0.0941348304319092\n",
      "Step - 2347, Loss - 0.28059514294735505, Learning Rate - 0.0125, magnitude of gradient - 0.0162823379548552\n",
      "Step - 2348, Loss - 0.3006993881277205, Learning Rate - 0.0125, magnitude of gradient - 0.052252097709383515\n",
      "Step - 2349, Loss - 0.3315761231351864, Learning Rate - 0.0125, magnitude of gradient - 0.04185855772399263\n",
      "Step - 2350, Loss - 0.3552967012972813, Learning Rate - 0.0125, magnitude of gradient - 0.03820160294865299\n",
      "Step - 2351, Loss - 0.2501029040240163, Learning Rate - 0.0125, magnitude of gradient - 0.06858211594304317\n",
      "Step - 2352, Loss - 0.328488113937061, Learning Rate - 0.0125, magnitude of gradient - 0.03520191283814924\n",
      "Step - 2353, Loss - 0.39730718293159667, Learning Rate - 0.0125, magnitude of gradient - 0.05237163361221294\n",
      "Step - 2354, Loss - 0.2969440736962041, Learning Rate - 0.0125, magnitude of gradient - 0.04683815404881976\n",
      "Step - 2355, Loss - 0.3043061569371898, Learning Rate - 0.0125, magnitude of gradient - 0.08231444767438985\n",
      "Step - 2356, Loss - 0.3196590257085903, Learning Rate - 0.0125, magnitude of gradient - 0.07888103748681069\n",
      "Step - 2357, Loss - 0.2595371707758424, Learning Rate - 0.0125, magnitude of gradient - 0.035854524698930204\n",
      "Step - 2358, Loss - 0.3179261019358981, Learning Rate - 0.0125, magnitude of gradient - 0.05733040453491408\n",
      "Step - 2359, Loss - 0.2767622902058754, Learning Rate - 0.0125, magnitude of gradient - 0.065725440398719\n",
      "Step - 2360, Loss - 0.26497100377439076, Learning Rate - 0.0125, magnitude of gradient - 0.11886676322802309\n",
      "Step - 2361, Loss - 0.3260922419986784, Learning Rate - 0.0125, magnitude of gradient - 0.026187636285458282\n",
      "Step - 2362, Loss - 0.29422503175855913, Learning Rate - 0.0125, magnitude of gradient - 0.01309366135180065\n",
      "Step - 2363, Loss - 0.31551966457934644, Learning Rate - 0.0125, magnitude of gradient - 0.0471308371775843\n",
      "Step - 2364, Loss - 0.30084170876638816, Learning Rate - 0.0125, magnitude of gradient - 0.11298175350779982\n",
      "Step - 2365, Loss - 0.2877635418919961, Learning Rate - 0.0125, magnitude of gradient - 0.07173073630099432\n",
      "Step - 2366, Loss - 0.2911985960885103, Learning Rate - 0.0125, magnitude of gradient - 0.037831447076949236\n",
      "Step - 2367, Loss - 0.25411023144005956, Learning Rate - 0.0125, magnitude of gradient - 0.05534935829407208\n",
      "Step - 2368, Loss - 0.2550894642158569, Learning Rate - 0.0125, magnitude of gradient - 0.04309705208840028\n",
      "Step - 2369, Loss - 0.3269488718627128, Learning Rate - 0.0125, magnitude of gradient - 0.06821268212306332\n",
      "Step - 2370, Loss - 0.3532157906091031, Learning Rate - 0.0125, magnitude of gradient - 0.0500572106938222\n",
      "Step - 2371, Loss - 0.362429256075322, Learning Rate - 0.0125, magnitude of gradient - 0.03800789399256829\n",
      "Step - 2372, Loss - 0.40020344190633717, Learning Rate - 0.0125, magnitude of gradient - 0.04318517439584406\n",
      "Step - 2373, Loss - 0.3320163519523675, Learning Rate - 0.0125, magnitude of gradient - 0.04104812810959407\n",
      "Step - 2374, Loss - 0.23475071048849294, Learning Rate - 0.0125, magnitude of gradient - 0.038581152679061856\n",
      "Step - 2375, Loss - 0.26349491965190835, Learning Rate - 0.0125, magnitude of gradient - 0.02392399228544024\n",
      "Step - 2376, Loss - 0.29334603742533283, Learning Rate - 0.0125, magnitude of gradient - 0.08384530136676639\n",
      "Step - 2377, Loss - 0.36962873780190864, Learning Rate - 0.0125, magnitude of gradient - 0.05048926936342369\n",
      "Step - 2378, Loss - 0.37754243662570575, Learning Rate - 0.0125, magnitude of gradient - 0.03031789825235339\n",
      "Step - 2379, Loss - 0.32717698232100284, Learning Rate - 0.0125, magnitude of gradient - 0.04976692590932742\n",
      "Step - 2380, Loss - 0.29532831687602196, Learning Rate - 0.0125, magnitude of gradient - 0.10036628257404309\n",
      "Step - 2381, Loss - 0.4187609695633772, Learning Rate - 0.0125, magnitude of gradient - 0.05313820881050316\n",
      "Step - 2382, Loss - 0.2607731693325986, Learning Rate - 0.0125, magnitude of gradient - 0.062250950981116694\n",
      "Step - 2383, Loss - 0.3631696868238794, Learning Rate - 0.0125, magnitude of gradient - 0.01395620344072544\n",
      "Step - 2384, Loss - 0.34998835778735765, Learning Rate - 0.0125, magnitude of gradient - 0.07766470084391426\n",
      "Step - 2385, Loss - 0.261447350928933, Learning Rate - 0.0125, magnitude of gradient - 0.08671249562567246\n",
      "Step - 2386, Loss - 0.36390010491930214, Learning Rate - 0.0125, magnitude of gradient - 0.06675348606233633\n",
      "Step - 2387, Loss - 0.2613505988892789, Learning Rate - 0.0125, magnitude of gradient - 0.051305445277752576\n",
      "Step - 2388, Loss - 0.25431517064679526, Learning Rate - 0.0125, magnitude of gradient - 0.059947113347798224\n",
      "Step - 2389, Loss - 0.24014072789050694, Learning Rate - 0.0125, magnitude of gradient - 0.02175726414034267\n",
      "Step - 2390, Loss - 0.42702640273751064, Learning Rate - 0.0125, magnitude of gradient - 0.028455226429784375\n",
      "Step - 2391, Loss - 0.34286731064605097, Learning Rate - 0.0125, magnitude of gradient - 0.0873005938070187\n",
      "Step - 2392, Loss - 0.31760804126308045, Learning Rate - 0.0125, magnitude of gradient - 0.036072237131874116\n",
      "Step - 2393, Loss - 0.3320272449039985, Learning Rate - 0.0125, magnitude of gradient - 0.03927410748627702\n",
      "Step - 2394, Loss - 0.3111611877923956, Learning Rate - 0.0125, magnitude of gradient - 0.03545537513192699\n",
      "Step - 2395, Loss - 0.3066213884652518, Learning Rate - 0.0125, magnitude of gradient - 0.06950162422498635\n",
      "Step - 2396, Loss - 0.3206936850192347, Learning Rate - 0.0125, magnitude of gradient - 0.021326685548591295\n",
      "Step - 2397, Loss - 0.2682114737327116, Learning Rate - 0.0125, magnitude of gradient - 0.04752582544544932\n",
      "Step - 2398, Loss - 0.33942855918846504, Learning Rate - 0.0125, magnitude of gradient - 0.07912542425622535\n",
      "Step - 2399, Loss - 0.3961261694572259, Learning Rate - 0.0125, magnitude of gradient - 0.06790709449272644\n",
      "Step - 2400, Loss - 0.38757328431816973, Learning Rate - 0.0125, magnitude of gradient - 0.015387584317472966\n",
      "Step - 2401, Loss - 0.3068004819434594, Learning Rate - 0.0125, magnitude of gradient - 0.08184728199014378\n",
      "Step - 2402, Loss - 0.23893972406632302, Learning Rate - 0.0125, magnitude of gradient - 0.03620912423914902\n",
      "Step - 2403, Loss - 0.29983128924524605, Learning Rate - 0.0125, magnitude of gradient - 0.09183382401170438\n",
      "Step - 2404, Loss - 0.3828004095971662, Learning Rate - 0.0125, magnitude of gradient - 0.03566830203340831\n",
      "Step - 2405, Loss - 0.30215112457854576, Learning Rate - 0.0125, magnitude of gradient - 0.054668686572046585\n",
      "Step - 2406, Loss - 0.29362507614393885, Learning Rate - 0.0125, magnitude of gradient - 0.010511036391214545\n",
      "Step - 2407, Loss - 0.320449792410121, Learning Rate - 0.0125, magnitude of gradient - 0.041601495265188075\n",
      "Step - 2408, Loss - 0.3008339666318678, Learning Rate - 0.0125, magnitude of gradient - 0.05642432585726474\n",
      "Step - 2409, Loss - 0.26869521573926464, Learning Rate - 0.0125, magnitude of gradient - 0.07582117602089511\n",
      "Step - 2410, Loss - 0.4328063428053186, Learning Rate - 0.0125, magnitude of gradient - 0.0710871037348988\n",
      "Step - 2411, Loss - 0.2716954154600508, Learning Rate - 0.0125, magnitude of gradient - 0.03593375912486332\n",
      "Step - 2412, Loss - 0.31509523680268453, Learning Rate - 0.0125, magnitude of gradient - 0.05045741481216439\n",
      "Step - 2413, Loss - 0.38558495466411075, Learning Rate - 0.0125, magnitude of gradient - 0.12527157910210912\n",
      "Step - 2414, Loss - 0.34239423589259815, Learning Rate - 0.0125, magnitude of gradient - 0.07910703014328808\n",
      "Step - 2415, Loss - 0.3302325563999856, Learning Rate - 0.0125, magnitude of gradient - 0.04558585120236428\n",
      "Step - 2416, Loss - 0.32751295688995324, Learning Rate - 0.0125, magnitude of gradient - 0.08989040868150255\n",
      "Step - 2417, Loss - 0.3394323350434992, Learning Rate - 0.0125, magnitude of gradient - 0.04789662437765478\n",
      "Step - 2418, Loss - 0.2890002188520938, Learning Rate - 0.0125, magnitude of gradient - 0.06088030398610361\n",
      "Step - 2419, Loss - 0.34821397218491423, Learning Rate - 0.0125, magnitude of gradient - 0.08950804514522322\n",
      "Step - 2420, Loss - 0.33251867932021606, Learning Rate - 0.0125, magnitude of gradient - 0.05067170475961524\n",
      "Step - 2421, Loss - 0.2705263187697851, Learning Rate - 0.0125, magnitude of gradient - 0.04325617139571982\n",
      "Step - 2422, Loss - 0.2703229396461081, Learning Rate - 0.0125, magnitude of gradient - 0.0653960245078164\n",
      "Step - 2423, Loss - 0.33912847145356123, Learning Rate - 0.0125, magnitude of gradient - 0.041339994223116974\n",
      "Step - 2424, Loss - 0.26145168241355765, Learning Rate - 0.0125, magnitude of gradient - 0.07854243032967423\n",
      "Step - 2425, Loss - 0.27242853181080395, Learning Rate - 0.0125, magnitude of gradient - 0.030782293361601164\n",
      "Step - 2426, Loss - 0.30893285215693583, Learning Rate - 0.0125, magnitude of gradient - 0.040899800260908614\n",
      "Step - 2427, Loss - 0.27819590522090326, Learning Rate - 0.0125, magnitude of gradient - 0.04753385413768007\n",
      "Step - 2428, Loss - 0.38626215433132544, Learning Rate - 0.0125, magnitude of gradient - 0.05488109645674897\n",
      "Step - 2429, Loss - 0.2937362292828952, Learning Rate - 0.0125, magnitude of gradient - 0.06195443078883688\n",
      "Step - 2430, Loss - 0.2517962943410557, Learning Rate - 0.0125, magnitude of gradient - 0.04748294679913534\n",
      "Step - 2431, Loss - 0.38657623125749296, Learning Rate - 0.0125, magnitude of gradient - 0.05307370015083119\n",
      "Step - 2432, Loss - 0.34034962660901796, Learning Rate - 0.0125, magnitude of gradient - 0.04584917882775581\n",
      "Step - 2433, Loss - 0.21460440690937538, Learning Rate - 0.0125, magnitude of gradient - 0.10112213333866624\n",
      "Step - 2434, Loss - 0.3133735175742731, Learning Rate - 0.0125, magnitude of gradient - 0.02068593641568426\n",
      "Step - 2435, Loss - 0.24872967798630172, Learning Rate - 0.0125, magnitude of gradient - 0.13307166556054176\n",
      "Step - 2436, Loss - 0.39630499566299704, Learning Rate - 0.0125, magnitude of gradient - 0.0514179494222862\n",
      "Step - 2437, Loss - 0.3229542907966447, Learning Rate - 0.0125, magnitude of gradient - 0.03856888744446213\n",
      "Step - 2438, Loss - 0.2791876241663941, Learning Rate - 0.0125, magnitude of gradient - 0.11563206264713619\n",
      "Step - 2439, Loss - 0.36234651647656824, Learning Rate - 0.0125, magnitude of gradient - 0.038395654987852135\n",
      "Step - 2440, Loss - 0.32167720866894195, Learning Rate - 0.0125, magnitude of gradient - 0.06848935584252129\n",
      "Step - 2441, Loss - 0.3303630972157834, Learning Rate - 0.0125, magnitude of gradient - 0.03335547625384161\n",
      "Step - 2442, Loss - 0.3744619052020128, Learning Rate - 0.0125, magnitude of gradient - 0.08380781021898241\n",
      "Step - 2443, Loss - 0.3494614480164297, Learning Rate - 0.0125, magnitude of gradient - 0.06751351743740143\n",
      "Step - 2444, Loss - 0.3565287349224474, Learning Rate - 0.0125, magnitude of gradient - 0.05589497002599439\n",
      "Step - 2445, Loss - 0.3415021667485606, Learning Rate - 0.0125, magnitude of gradient - 0.053443140029908597\n",
      "Step - 2446, Loss - 0.3590814959913122, Learning Rate - 0.0125, magnitude of gradient - 0.05035002214551544\n",
      "Step - 2447, Loss - 0.3774999098991372, Learning Rate - 0.0125, magnitude of gradient - 0.04260913717236306\n",
      "Step - 2448, Loss - 0.4062353044917551, Learning Rate - 0.0125, magnitude of gradient - 0.029613361827164612\n",
      "Step - 2449, Loss - 0.32481506609553323, Learning Rate - 0.0125, magnitude of gradient - 0.012372202059575208\n",
      "Step - 2450, Loss - 0.3692973667585312, Learning Rate - 0.0125, magnitude of gradient - 0.03855840424327594\n",
      "Step - 2451, Loss - 0.364140656721945, Learning Rate - 0.0125, magnitude of gradient - 0.09380555515323237\n",
      "Step - 2452, Loss - 0.31805833750499257, Learning Rate - 0.0125, magnitude of gradient - 0.07078544633231804\n",
      "Step - 2453, Loss - 0.26058450564015234, Learning Rate - 0.0125, magnitude of gradient - 0.060780221655670284\n",
      "Step - 2454, Loss - 0.3718608840643958, Learning Rate - 0.0125, magnitude of gradient - 0.09183644863424914\n",
      "Step - 2455, Loss - 0.38529957477015736, Learning Rate - 0.0125, magnitude of gradient - 0.04456350531366727\n",
      "Step - 2456, Loss - 0.35794280658069355, Learning Rate - 0.0125, magnitude of gradient - 0.0318259827306205\n",
      "Step - 2457, Loss - 0.28546016459720086, Learning Rate - 0.0125, magnitude of gradient - 0.03141325880972113\n",
      "Step - 2458, Loss - 0.3111599880799112, Learning Rate - 0.0125, magnitude of gradient - 0.03150439038248774\n",
      "Step - 2459, Loss - 0.271523878222543, Learning Rate - 0.0125, magnitude of gradient - 0.11193492027270978\n",
      "Step - 2460, Loss - 0.2588217059693893, Learning Rate - 0.0125, magnitude of gradient - 0.06275025008186533\n",
      "Step - 2461, Loss - 0.29077867808419144, Learning Rate - 0.0125, magnitude of gradient - 0.01219786467714745\n",
      "Step - 2462, Loss - 0.22275203315040054, Learning Rate - 0.0125, magnitude of gradient - 0.09473658929664246\n",
      "Step - 2463, Loss - 0.35051299634254285, Learning Rate - 0.0125, magnitude of gradient - 0.07717595934421179\n",
      "Step - 2464, Loss - 0.2688493350961078, Learning Rate - 0.0125, magnitude of gradient - 0.04907194959300604\n",
      "Step - 2465, Loss - 0.2565140907605289, Learning Rate - 0.0125, magnitude of gradient - 0.04320390343207711\n",
      "Step - 2466, Loss - 0.2665467280908916, Learning Rate - 0.0125, magnitude of gradient - 0.03254189038669849\n",
      "Step - 2467, Loss - 0.3669347529609246, Learning Rate - 0.0125, magnitude of gradient - 0.031557884241335245\n",
      "Step - 2468, Loss - 0.2925255005582819, Learning Rate - 0.0125, magnitude of gradient - 0.08471625679651089\n",
      "Step - 2469, Loss - 0.31474242521148543, Learning Rate - 0.0125, magnitude of gradient - 0.022312063919061993\n",
      "Step - 2470, Loss - 0.3429352632883001, Learning Rate - 0.0125, magnitude of gradient - 0.05871417134867905\n",
      "Step - 2471, Loss - 0.3425192568635011, Learning Rate - 0.0125, magnitude of gradient - 0.05062494326700628\n",
      "Step - 2472, Loss - 0.2958518210556858, Learning Rate - 0.0125, magnitude of gradient - 0.05930204537845028\n",
      "Step - 2473, Loss - 0.35048512666333864, Learning Rate - 0.0125, magnitude of gradient - 0.06811783848601641\n",
      "Step - 2474, Loss - 0.3585970597467577, Learning Rate - 0.0125, magnitude of gradient - 0.056212640605193064\n",
      "Step - 2475, Loss - 0.3671312806556937, Learning Rate - 0.0125, magnitude of gradient - 0.06784215660793087\n",
      "Step - 2476, Loss - 0.30006010015947143, Learning Rate - 0.0125, magnitude of gradient - 0.05589907057614151\n",
      "Step - 2477, Loss - 0.2946989866438756, Learning Rate - 0.0125, magnitude of gradient - 0.07530128120719677\n",
      "Step - 2478, Loss - 0.3075134174650467, Learning Rate - 0.0125, magnitude of gradient - 0.05320182805027784\n",
      "Step - 2479, Loss - 0.3473891857319279, Learning Rate - 0.0125, magnitude of gradient - 0.06827292740189482\n",
      "Step - 2480, Loss - 0.25735483343273136, Learning Rate - 0.0125, magnitude of gradient - 0.0028887103932114843\n",
      "Step - 2481, Loss - 0.30657921248188846, Learning Rate - 0.0125, magnitude of gradient - 0.04850542617689628\n",
      "Step - 2482, Loss - 0.2991325182045427, Learning Rate - 0.0125, magnitude of gradient - 0.024658497079750413\n",
      "Step - 2483, Loss - 0.4074826757668959, Learning Rate - 0.0125, magnitude of gradient - 0.08537152997010787\n",
      "Step - 2484, Loss - 0.3243049927476581, Learning Rate - 0.0125, magnitude of gradient - 0.0776665716857713\n",
      "Step - 2485, Loss - 0.25567325447071065, Learning Rate - 0.0125, magnitude of gradient - 0.07214406943582494\n",
      "Step - 2486, Loss - 0.35337025883333, Learning Rate - 0.0125, magnitude of gradient - 0.07963180289135736\n",
      "Step - 2487, Loss - 0.24573042603347417, Learning Rate - 0.0125, magnitude of gradient - 0.04656008219196466\n",
      "Step - 2488, Loss - 0.3304238049500674, Learning Rate - 0.0125, magnitude of gradient - 0.05007627962529141\n",
      "Step - 2489, Loss - 0.2654264146339144, Learning Rate - 0.0125, magnitude of gradient - 0.01189962656957765\n",
      "Step - 2490, Loss - 0.26797403688731364, Learning Rate - 0.0125, magnitude of gradient - 0.03250226985102549\n",
      "Step - 2491, Loss - 0.301465205708698, Learning Rate - 0.0125, magnitude of gradient - 0.05287024211829011\n",
      "Step - 2492, Loss - 0.329686810035109, Learning Rate - 0.0125, magnitude of gradient - 0.04316721153711321\n",
      "Step - 2493, Loss - 0.4281991440163606, Learning Rate - 0.0125, magnitude of gradient - 0.0373190908823068\n",
      "Step - 2494, Loss - 0.431707174595839, Learning Rate - 0.0125, magnitude of gradient - 0.06745113761940878\n",
      "Step - 2495, Loss - 0.2507470974348063, Learning Rate - 0.0125, magnitude of gradient - 0.015370871462578841\n",
      "Step - 2496, Loss - 0.27087482660280926, Learning Rate - 0.0125, magnitude of gradient - 0.04857524450680181\n",
      "Step - 2497, Loss - 0.33084585010827117, Learning Rate - 0.0125, magnitude of gradient - 0.03680896963947613\n",
      "Step - 2498, Loss - 0.3019883981418111, Learning Rate - 0.0125, magnitude of gradient - 0.01988158280623987\n",
      "Step - 2499, Loss - 0.41260751128459305, Learning Rate - 0.0125, magnitude of gradient - 0.030338524461478982\n",
      "Step - 2500, Loss - 0.31986163390621913, Learning Rate - 0.0125, magnitude of gradient - 0.08224827287176407\n",
      "Step - 2501, Loss - 0.31890007983234114, Learning Rate - 0.0125, magnitude of gradient - 0.07951896010178292\n",
      "Step - 2502, Loss - 0.36030595271980087, Learning Rate - 0.0125, magnitude of gradient - 0.05066172002417912\n",
      "Step - 2503, Loss - 0.2775474453681513, Learning Rate - 0.0125, magnitude of gradient - 0.06109556162138673\n",
      "Step - 2504, Loss - 0.3233449896468763, Learning Rate - 0.0125, magnitude of gradient - 0.03057822022967279\n",
      "Step - 2505, Loss - 0.2689544072552563, Learning Rate - 0.0125, magnitude of gradient - 0.05750247584833812\n",
      "Step - 2506, Loss - 0.2615968977455782, Learning Rate - 0.0125, magnitude of gradient - 0.03946424891773259\n",
      "Step - 2507, Loss - 0.30019807046908636, Learning Rate - 0.0125, magnitude of gradient - 0.07142647197517375\n",
      "Step - 2508, Loss - 0.33386278354206017, Learning Rate - 0.0125, magnitude of gradient - 0.06809987276254945\n",
      "Step - 2509, Loss - 0.28243364426052897, Learning Rate - 0.0125, magnitude of gradient - 0.05084572578280595\n",
      "Step - 2510, Loss - 0.4073223609724081, Learning Rate - 0.0125, magnitude of gradient - 0.046554910616718986\n",
      "Step - 2511, Loss - 0.3951488717354379, Learning Rate - 0.0125, magnitude of gradient - 0.05071930366225086\n",
      "Step - 2512, Loss - 0.3263044194240755, Learning Rate - 0.0125, magnitude of gradient - 0.010889601710218619\n",
      "Step - 2513, Loss - 0.3540812343682599, Learning Rate - 0.0125, magnitude of gradient - 0.018698273106935122\n",
      "Step - 2514, Loss - 0.36250731084848964, Learning Rate - 0.0125, magnitude of gradient - 0.03420055395165636\n",
      "Step - 2515, Loss - 0.30075450943618176, Learning Rate - 0.0125, magnitude of gradient - 0.043287516615925795\n",
      "Step - 2516, Loss - 0.31460576760806563, Learning Rate - 0.0125, magnitude of gradient - 0.06679805470090283\n",
      "Step - 2517, Loss - 0.32805546396574925, Learning Rate - 0.0125, magnitude of gradient - 0.06310695790912353\n",
      "Step - 2518, Loss - 0.2609422774049677, Learning Rate - 0.0125, magnitude of gradient - 0.016266873619760567\n",
      "Step - 2519, Loss - 0.27821521287339324, Learning Rate - 0.0125, magnitude of gradient - 0.02925362482668758\n",
      "Step - 2520, Loss - 0.33368951832263977, Learning Rate - 0.0125, magnitude of gradient - 0.0929171748093721\n",
      "Step - 2521, Loss - 0.2597688958312696, Learning Rate - 0.0125, magnitude of gradient - 0.04026768476296548\n",
      "Step - 2522, Loss - 0.29601432022185614, Learning Rate - 0.0125, magnitude of gradient - 0.03287782476763797\n",
      "Step - 2523, Loss - 0.2470421302849254, Learning Rate - 0.0125, magnitude of gradient - 0.02957978602470071\n",
      "Step - 2524, Loss - 0.3119993159741238, Learning Rate - 0.0125, magnitude of gradient - 0.01698436290049515\n",
      "Step - 2525, Loss - 0.24929231891925274, Learning Rate - 0.0125, magnitude of gradient - 0.027199618513815627\n",
      "Step - 2526, Loss - 0.33965435230533864, Learning Rate - 0.0125, magnitude of gradient - 0.07975579328426999\n",
      "Step - 2527, Loss - 0.3386442562063539, Learning Rate - 0.0125, magnitude of gradient - 0.02231419689782881\n",
      "Step - 2528, Loss - 0.46368061277347106, Learning Rate - 0.0125, magnitude of gradient - 0.08276251727449226\n",
      "Step - 2529, Loss - 0.3304604932809349, Learning Rate - 0.0125, magnitude of gradient - 0.0339098332957293\n",
      "Step - 2530, Loss - 0.2724675272791208, Learning Rate - 0.0125, magnitude of gradient - 0.10926676392160733\n",
      "Step - 2531, Loss - 0.29181535330108377, Learning Rate - 0.0125, magnitude of gradient - 0.0358559417208149\n",
      "Step - 2532, Loss - 0.3590115227822409, Learning Rate - 0.0125, magnitude of gradient - 0.044168093462667826\n",
      "Step - 2533, Loss - 0.3011952801929129, Learning Rate - 0.0125, magnitude of gradient - 0.0368621934073112\n",
      "Step - 2534, Loss - 0.29686848173185065, Learning Rate - 0.0125, magnitude of gradient - 0.02533711206551789\n",
      "Step - 2535, Loss - 0.2904468803323601, Learning Rate - 0.0125, magnitude of gradient - 0.04200443387928387\n",
      "Step - 2536, Loss - 0.32209457700303784, Learning Rate - 0.0125, magnitude of gradient - 0.08182951379659285\n",
      "Step - 2537, Loss - 0.3119488635150349, Learning Rate - 0.0125, magnitude of gradient - 0.03228259994143528\n",
      "Step - 2538, Loss - 0.32781121430782106, Learning Rate - 0.0125, magnitude of gradient - 0.05627853150796585\n",
      "Step - 2539, Loss - 0.24732577579748394, Learning Rate - 0.0125, magnitude of gradient - 0.07713130464166056\n",
      "Step - 2540, Loss - 0.3202499330414977, Learning Rate - 0.0125, magnitude of gradient - 0.026592555843208434\n",
      "Step - 2541, Loss - 0.4036378821747392, Learning Rate - 0.0125, magnitude of gradient - 0.08883006406533757\n",
      "Step - 2542, Loss - 0.2663492648642035, Learning Rate - 0.0125, magnitude of gradient - 0.03496783148624399\n",
      "Step - 2543, Loss - 0.31198979374424984, Learning Rate - 0.0125, magnitude of gradient - 0.03425280831462421\n",
      "Step - 2544, Loss - 0.3061879307749772, Learning Rate - 0.0125, magnitude of gradient - 0.0622490797235608\n",
      "Step - 2545, Loss - 0.28351389246350595, Learning Rate - 0.0125, magnitude of gradient - 0.05388491975402441\n",
      "Step - 2546, Loss - 0.24783356640903326, Learning Rate - 0.0125, magnitude of gradient - 0.06966437556287074\n",
      "Step - 2547, Loss - 0.3603046463009066, Learning Rate - 0.0125, magnitude of gradient - 0.04710129949294556\n",
      "Step - 2548, Loss - 0.3443253635460295, Learning Rate - 0.0125, magnitude of gradient - 0.07398677508876775\n",
      "Step - 2549, Loss - 0.401599831159789, Learning Rate - 0.0125, magnitude of gradient - 0.037281317407871856\n",
      "Step - 2550, Loss - 0.2948185106802148, Learning Rate - 0.0125, magnitude of gradient - 0.04103205045421778\n",
      "Step - 2551, Loss - 0.34966115635870376, Learning Rate - 0.0125, magnitude of gradient - 0.059394548842443065\n",
      "Step - 2552, Loss - 0.38749302551160214, Learning Rate - 0.0125, magnitude of gradient - 0.01357078868830755\n",
      "Step - 2553, Loss - 0.3315987088014553, Learning Rate - 0.0125, magnitude of gradient - 0.11324430308103141\n",
      "Step - 2554, Loss - 0.3341131071436525, Learning Rate - 0.0125, magnitude of gradient - 0.05225895739413896\n",
      "Step - 2555, Loss - 0.27964269713506285, Learning Rate - 0.0125, magnitude of gradient - 0.06755913994783266\n",
      "Step - 2556, Loss - 0.3325651824786977, Learning Rate - 0.0125, magnitude of gradient - 0.042769824330253\n",
      "Step - 2557, Loss - 0.3444625274754305, Learning Rate - 0.0125, magnitude of gradient - 0.07546038325915688\n",
      "Step - 2558, Loss - 0.2996383250700999, Learning Rate - 0.0125, magnitude of gradient - 0.07627939929512813\n",
      "Step - 2559, Loss - 0.27450367469492687, Learning Rate - 0.0125, magnitude of gradient - 0.06428580830619315\n",
      "Step - 2560, Loss - 0.23272119428551716, Learning Rate - 0.0125, magnitude of gradient - 0.03210360863322589\n",
      "Step - 2561, Loss - 0.32577242604839773, Learning Rate - 0.0125, magnitude of gradient - 0.0703216292123243\n",
      "Step - 2562, Loss - 0.3740212732929611, Learning Rate - 0.0125, magnitude of gradient - 0.05631982641059775\n",
      "Step - 2563, Loss - 0.27387320981300717, Learning Rate - 0.0125, magnitude of gradient - 0.03592359605084347\n",
      "Step - 2564, Loss - 0.3478502155390869, Learning Rate - 0.0125, magnitude of gradient - 0.01877778612093983\n",
      "Step - 2565, Loss - 0.3248564345351477, Learning Rate - 0.0125, magnitude of gradient - 0.029019249353747962\n",
      "Step - 2566, Loss - 0.31278802082742696, Learning Rate - 0.0125, magnitude of gradient - 0.12476183705616145\n",
      "Step - 2567, Loss - 0.3738350277486214, Learning Rate - 0.0125, magnitude of gradient - 0.07688071671348658\n",
      "Step - 2568, Loss - 0.3284887125113116, Learning Rate - 0.0125, magnitude of gradient - 0.0108925986471093\n",
      "Step - 2569, Loss - 0.40598574708115753, Learning Rate - 0.0125, magnitude of gradient - 0.04842667360813507\n",
      "Step - 2570, Loss - 0.2869516132485261, Learning Rate - 0.0125, magnitude of gradient - 0.06831551616635159\n",
      "Step - 2571, Loss - 0.3832247819628678, Learning Rate - 0.0125, magnitude of gradient - 0.07578785004115175\n",
      "Step - 2572, Loss - 0.32352608245554104, Learning Rate - 0.0125, magnitude of gradient - 0.10487035053314712\n",
      "Step - 2573, Loss - 0.3512502102516517, Learning Rate - 0.0125, magnitude of gradient - 0.06104519281232903\n",
      "Step - 2574, Loss - 0.3217139195719273, Learning Rate - 0.0125, magnitude of gradient - 0.058898311491111306\n",
      "Step - 2575, Loss - 0.293776398999456, Learning Rate - 0.0125, magnitude of gradient - 0.051560229540068586\n",
      "Step - 2576, Loss - 0.33180967816839857, Learning Rate - 0.0125, magnitude of gradient - 0.03001794424964688\n",
      "Step - 2577, Loss - 0.37856157229873577, Learning Rate - 0.0125, magnitude of gradient - 0.04435880487027182\n",
      "Step - 2578, Loss - 0.30088096379853346, Learning Rate - 0.0125, magnitude of gradient - 0.023574055395884485\n",
      "Step - 2579, Loss - 0.3123803571692886, Learning Rate - 0.0125, magnitude of gradient - 0.005682810260438528\n",
      "Step - 2580, Loss - 0.2591910650145821, Learning Rate - 0.0125, magnitude of gradient - 0.07800406227138892\n",
      "Step - 2581, Loss - 0.405106444375802, Learning Rate - 0.0125, magnitude of gradient - 0.05487621419848536\n",
      "Step - 2582, Loss - 0.32275760458859426, Learning Rate - 0.0125, magnitude of gradient - 0.02965312155895913\n",
      "Step - 2583, Loss - 0.34307835471258485, Learning Rate - 0.0125, magnitude of gradient - 0.04409668809781372\n",
      "Step - 2584, Loss - 0.27601773867379154, Learning Rate - 0.0125, magnitude of gradient - 0.10298651655209354\n",
      "Step - 2585, Loss - 0.3759352792968042, Learning Rate - 0.0125, magnitude of gradient - 0.022219348974492632\n",
      "Step - 2586, Loss - 0.28870170627834857, Learning Rate - 0.0125, magnitude of gradient - 0.04120008161612693\n",
      "Step - 2587, Loss - 0.29977664534003357, Learning Rate - 0.0125, magnitude of gradient - 0.023082929415102905\n",
      "Step - 2588, Loss - 0.34779640974717835, Learning Rate - 0.0125, magnitude of gradient - 0.1023962866487201\n",
      "Step - 2589, Loss - 0.3712263526516652, Learning Rate - 0.0125, magnitude of gradient - 0.03510604260263618\n",
      "Step - 2590, Loss - 0.26912117521187573, Learning Rate - 0.0125, magnitude of gradient - 0.07083710163037772\n",
      "Step - 2591, Loss - 0.3493953429448491, Learning Rate - 0.0125, magnitude of gradient - 0.04055558962799619\n",
      "Step - 2592, Loss - 0.33168103345301414, Learning Rate - 0.0125, magnitude of gradient - 0.026308244903562285\n",
      "Step - 2593, Loss - 0.2853814021963437, Learning Rate - 0.0125, magnitude of gradient - 0.018184690551886262\n",
      "Step - 2594, Loss - 0.26235225659424416, Learning Rate - 0.0125, magnitude of gradient - 0.023024012116527662\n",
      "Step - 2595, Loss - 0.36638878158096205, Learning Rate - 0.0125, magnitude of gradient - 0.03726139404945955\n",
      "Step - 2596, Loss - 0.34790935109335686, Learning Rate - 0.0125, magnitude of gradient - 0.031606944290878417\n",
      "Step - 2597, Loss - 0.3604233460651661, Learning Rate - 0.0125, magnitude of gradient - 0.06354578966248807\n",
      "Step - 2598, Loss - 0.3380921139521128, Learning Rate - 0.0125, magnitude of gradient - 0.05388104116574239\n",
      "Step - 2599, Loss - 0.23976921429736492, Learning Rate - 0.0125, magnitude of gradient - 0.05072959044624747\n",
      "Step - 2600, Loss - 0.2837099417972526, Learning Rate - 0.0125, magnitude of gradient - 0.040381423089834494\n",
      "Step - 2601, Loss - 0.346492884019677, Learning Rate - 0.0125, magnitude of gradient - 0.03846548499009556\n",
      "Step - 2602, Loss - 0.39576820015966596, Learning Rate - 0.0125, magnitude of gradient - 0.03736482780862385\n",
      "Step - 2603, Loss - 0.3268895786073242, Learning Rate - 0.0125, magnitude of gradient - 0.058650568868557004\n",
      "Step - 2604, Loss - 0.30790088629208234, Learning Rate - 0.0125, magnitude of gradient - 0.004948902254951093\n",
      "Step - 2605, Loss - 0.307869005459357, Learning Rate - 0.0125, magnitude of gradient - 0.05742555086197005\n",
      "Step - 2606, Loss - 0.3118499580466655, Learning Rate - 0.0125, magnitude of gradient - 0.07073280367364515\n",
      "Step - 2607, Loss - 0.368837100511994, Learning Rate - 0.0125, magnitude of gradient - 0.01562770882677246\n",
      "Step - 2608, Loss - 0.29394710476905944, Learning Rate - 0.0125, magnitude of gradient - 0.04342666676030192\n",
      "Step - 2609, Loss - 0.3530851742804044, Learning Rate - 0.0125, magnitude of gradient - 0.0427050217602954\n",
      "Step - 2610, Loss - 0.3449991518765173, Learning Rate - 0.0125, magnitude of gradient - 0.03499942214837291\n",
      "Step - 2611, Loss - 0.29886188323572827, Learning Rate - 0.0125, magnitude of gradient - 0.08985212221445865\n",
      "Step - 2612, Loss - 0.3380973374627313, Learning Rate - 0.0125, magnitude of gradient - 0.056826465958489066\n",
      "Step - 2613, Loss - 0.39767143441540176, Learning Rate - 0.0125, magnitude of gradient - 0.08940759632646837\n",
      "Step - 2614, Loss - 0.2799781651989987, Learning Rate - 0.0125, magnitude of gradient - 0.07080264004388935\n",
      "Step - 2615, Loss - 0.2627415946304186, Learning Rate - 0.0125, magnitude of gradient - 0.04748418968969049\n",
      "Step - 2616, Loss - 0.3618370043694994, Learning Rate - 0.0125, magnitude of gradient - 0.09301633680839323\n",
      "Step - 2617, Loss - 0.36484906437412173, Learning Rate - 0.0125, magnitude of gradient - 0.020596251614985545\n",
      "Step - 2618, Loss - 0.29454360147420744, Learning Rate - 0.0125, magnitude of gradient - 0.07962476097743017\n",
      "Step - 2619, Loss - 0.27996190741044036, Learning Rate - 0.0125, magnitude of gradient - 0.054802644308154264\n",
      "Step - 2620, Loss - 0.2992557127232507, Learning Rate - 0.0125, magnitude of gradient - 0.030361913509377725\n",
      "Step - 2621, Loss - 0.29066771600057806, Learning Rate - 0.0125, magnitude of gradient - 0.04323526711248637\n",
      "Step - 2622, Loss - 0.3332274212339919, Learning Rate - 0.0125, magnitude of gradient - 0.07945743224281296\n",
      "Step - 2623, Loss - 0.2936359857717036, Learning Rate - 0.0125, magnitude of gradient - 0.02633167716247859\n",
      "Step - 2624, Loss - 0.3941388439230501, Learning Rate - 0.0125, magnitude of gradient - 0.03372235613827369\n",
      "Step - 2625, Loss - 0.41583589023096323, Learning Rate - 0.0125, magnitude of gradient - 0.08765362441727852\n",
      "Step - 2626, Loss - 0.3317451030924937, Learning Rate - 0.0125, magnitude of gradient - 0.04285632771307857\n",
      "Step - 2627, Loss - 0.3523947082451445, Learning Rate - 0.0125, magnitude of gradient - 0.056635915631926305\n",
      "Step - 2628, Loss - 0.22724258250945978, Learning Rate - 0.0125, magnitude of gradient - 0.044876917384153364\n",
      "Step - 2629, Loss - 0.2709180474339516, Learning Rate - 0.0125, magnitude of gradient - 0.051399354300249264\n",
      "Step - 2630, Loss - 0.3028507144832786, Learning Rate - 0.0125, magnitude of gradient - 0.053670530890197715\n",
      "Step - 2631, Loss - 0.27873090615035234, Learning Rate - 0.0125, magnitude of gradient - 0.04530223143900951\n",
      "Step - 2632, Loss - 0.31439145892629006, Learning Rate - 0.0125, magnitude of gradient - 0.020290769046123727\n",
      "Step - 2633, Loss - 0.36759742518707905, Learning Rate - 0.0125, magnitude of gradient - 0.05193776048104339\n",
      "Step - 2634, Loss - 0.3309407647939535, Learning Rate - 0.0125, magnitude of gradient - 0.05163369408127006\n",
      "Step - 2635, Loss - 0.2797829984454577, Learning Rate - 0.0125, magnitude of gradient - 0.07399511378169903\n",
      "Step - 2636, Loss - 0.32282760734125393, Learning Rate - 0.0125, magnitude of gradient - 0.022660526471479515\n",
      "Step - 2637, Loss - 0.2902216475435292, Learning Rate - 0.0125, magnitude of gradient - 0.007086615230669141\n",
      "Step - 2638, Loss - 0.42881354808378186, Learning Rate - 0.0125, magnitude of gradient - 0.0259391756057519\n",
      "Step - 2639, Loss - 0.2501099697095002, Learning Rate - 0.0125, magnitude of gradient - 0.06934169049564327\n",
      "Step - 2640, Loss - 0.3305198382213426, Learning Rate - 0.0125, magnitude of gradient - 0.08760086262762161\n",
      "Step - 2641, Loss - 0.2904945616497027, Learning Rate - 0.0125, magnitude of gradient - 0.03528107998868574\n",
      "Step - 2642, Loss - 0.3870039375308781, Learning Rate - 0.0125, magnitude of gradient - 0.03934274861865421\n",
      "Step - 2643, Loss - 0.28325408415079484, Learning Rate - 0.0125, magnitude of gradient - 0.036641211631237854\n",
      "Step - 2644, Loss - 0.2625391429809383, Learning Rate - 0.0125, magnitude of gradient - 0.0337275189842883\n",
      "Step - 2645, Loss - 0.28830837882378935, Learning Rate - 0.0125, magnitude of gradient - 0.01822860352711142\n",
      "Step - 2646, Loss - 0.2635639023722307, Learning Rate - 0.0125, magnitude of gradient - 0.07134139453449136\n",
      "Step - 2647, Loss - 0.3461806708151001, Learning Rate - 0.0125, magnitude of gradient - 0.07516222621858132\n",
      "Step - 2648, Loss - 0.2392771704658398, Learning Rate - 0.0125, magnitude of gradient - 0.096188976940552\n",
      "Step - 2649, Loss - 0.28471889843975395, Learning Rate - 0.0125, magnitude of gradient - 0.029058540441886844\n",
      "Step - 2650, Loss - 0.3610276918807297, Learning Rate - 0.0125, magnitude of gradient - 0.034449445964940255\n",
      "Step - 2651, Loss - 0.28642318662219995, Learning Rate - 0.0125, magnitude of gradient - 0.022597143460307043\n",
      "Step - 2652, Loss - 0.35312515997288974, Learning Rate - 0.0125, magnitude of gradient - 0.0535376440453888\n",
      "Step - 2653, Loss - 0.32506776251172487, Learning Rate - 0.0125, magnitude of gradient - 0.07085932159285725\n",
      "Step - 2654, Loss - 0.3234586418897416, Learning Rate - 0.0125, magnitude of gradient - 0.04303749871662135\n",
      "Step - 2655, Loss - 0.29325343432552475, Learning Rate - 0.0125, magnitude of gradient - 0.03109834286002442\n",
      "Step - 2656, Loss - 0.2927827409489936, Learning Rate - 0.0125, magnitude of gradient - 0.03529886260841662\n",
      "Step - 2657, Loss - 0.2983580371118516, Learning Rate - 0.0125, magnitude of gradient - 0.07647957058126803\n",
      "Step - 2658, Loss - 0.2950603887220419, Learning Rate - 0.0125, magnitude of gradient - 0.06005435888682195\n",
      "Step - 2659, Loss - 0.3387893049585074, Learning Rate - 0.0125, magnitude of gradient - 0.03359482794188894\n",
      "Step - 2660, Loss - 0.2957872326653299, Learning Rate - 0.0125, magnitude of gradient - 0.018811389976337467\n",
      "Step - 2661, Loss - 0.36181249039753577, Learning Rate - 0.0125, magnitude of gradient - 0.040012269121696635\n",
      "Step - 2662, Loss - 0.3132677644357539, Learning Rate - 0.0125, magnitude of gradient - 0.08124241220957044\n",
      "Step - 2663, Loss - 0.3688276049624343, Learning Rate - 0.0125, magnitude of gradient - 0.052840632476148404\n",
      "Step - 2664, Loss - 0.3188189600517773, Learning Rate - 0.0125, magnitude of gradient - 0.06689340795788243\n",
      "Step - 2665, Loss - 0.3275318205543497, Learning Rate - 0.0125, magnitude of gradient - 0.03372132691912281\n",
      "Step - 2666, Loss - 0.2873443152365253, Learning Rate - 0.0125, magnitude of gradient - 0.030795538606401256\n",
      "Step - 2667, Loss - 0.32631733107366473, Learning Rate - 0.0125, magnitude of gradient - 0.08282222139756841\n",
      "Step - 2668, Loss - 0.37763725454461033, Learning Rate - 0.0125, magnitude of gradient - 0.11065682157069073\n",
      "Step - 2669, Loss - 0.3131343075022033, Learning Rate - 0.0125, magnitude of gradient - 0.06365134172592936\n",
      "Step - 2670, Loss - 0.2555426925881974, Learning Rate - 0.0125, magnitude of gradient - 0.05244478433999346\n",
      "Step - 2671, Loss - 0.3430708649244807, Learning Rate - 0.0125, magnitude of gradient - 0.028939525181136124\n",
      "Step - 2672, Loss - 0.3839499145805496, Learning Rate - 0.0125, magnitude of gradient - 0.05084710045976761\n",
      "Step - 2673, Loss - 0.343024636291762, Learning Rate - 0.0125, magnitude of gradient - 0.027972527854989184\n",
      "Step - 2674, Loss - 0.3742522591879868, Learning Rate - 0.0125, magnitude of gradient - 0.08266727213909358\n",
      "Step - 2675, Loss - 0.29662122771132593, Learning Rate - 0.0125, magnitude of gradient - 0.050177224096473926\n",
      "Step - 2676, Loss - 0.3452756819177521, Learning Rate - 0.0125, magnitude of gradient - 0.06862664557628727\n",
      "Step - 2677, Loss - 0.3156023334830859, Learning Rate - 0.0125, magnitude of gradient - 0.051780042967505956\n",
      "Step - 2678, Loss - 0.29382858744891116, Learning Rate - 0.0125, magnitude of gradient - 0.008618999242571741\n",
      "Step - 2679, Loss - 0.2511195724748957, Learning Rate - 0.0125, magnitude of gradient - 0.08955840309038245\n",
      "Step - 2680, Loss - 0.36840235684508493, Learning Rate - 0.0125, magnitude of gradient - 0.04159640758693767\n",
      "Step - 2681, Loss - 0.34960603288037206, Learning Rate - 0.0125, magnitude of gradient - 0.019568055501423497\n",
      "Step - 2682, Loss - 0.3668185063684207, Learning Rate - 0.0125, magnitude of gradient - 0.04612865184389904\n",
      "Step - 2683, Loss - 0.25592565222270214, Learning Rate - 0.0125, magnitude of gradient - 0.032336852269972245\n",
      "Step - 2684, Loss - 0.26448139474840066, Learning Rate - 0.0125, magnitude of gradient - 0.0054117086975345\n",
      "Step - 2685, Loss - 0.21873447638638363, Learning Rate - 0.0125, magnitude of gradient - 0.03714794658888367\n",
      "Step - 2686, Loss - 0.3507550466034278, Learning Rate - 0.0125, magnitude of gradient - 0.04895851406604303\n",
      "Step - 2687, Loss - 0.3225145987510456, Learning Rate - 0.0125, magnitude of gradient - 0.007258205792840734\n",
      "Step - 2688, Loss - 0.3314026023666063, Learning Rate - 0.0125, magnitude of gradient - 0.05715366200467825\n",
      "Step - 2689, Loss - 0.2879212956899479, Learning Rate - 0.0125, magnitude of gradient - 0.01579929325899753\n",
      "Step - 2690, Loss - 0.32583649321365327, Learning Rate - 0.0125, magnitude of gradient - 0.04034154351402492\n",
      "Step - 2691, Loss - 0.3867718513547039, Learning Rate - 0.0125, magnitude of gradient - 0.04862737583257667\n",
      "Step - 2692, Loss - 0.2688812493992112, Learning Rate - 0.0125, magnitude of gradient - 0.06092101301187261\n",
      "Step - 2693, Loss - 0.3447745190874244, Learning Rate - 0.0125, magnitude of gradient - 0.04689558817681063\n",
      "Step - 2694, Loss - 0.27283103228049516, Learning Rate - 0.0125, magnitude of gradient - 0.08603337506428171\n",
      "Step - 2695, Loss - 0.4155385470491817, Learning Rate - 0.0125, magnitude of gradient - 0.05431792000972915\n",
      "Step - 2696, Loss - 0.29712611956814555, Learning Rate - 0.0125, magnitude of gradient - 0.06744904664047546\n",
      "Step - 2697, Loss - 0.3110801493690223, Learning Rate - 0.0125, magnitude of gradient - 0.06203294231735131\n",
      "Step - 2698, Loss - 0.32005322907870726, Learning Rate - 0.0125, magnitude of gradient - 0.081044006787727\n",
      "Step - 2699, Loss - 0.32458346951332157, Learning Rate - 0.0125, magnitude of gradient - 0.08573419392535786\n",
      "Step - 2700, Loss - 0.27812612805251724, Learning Rate - 0.0125, magnitude of gradient - 0.044866796964547456\n",
      "Step - 2701, Loss - 0.2769313459049183, Learning Rate - 0.0125, magnitude of gradient - 0.02801247406800187\n",
      "Step - 2702, Loss - 0.3165528548780707, Learning Rate - 0.0125, magnitude of gradient - 0.0932102588390051\n",
      "Step - 2703, Loss - 0.32404260223191217, Learning Rate - 0.0125, magnitude of gradient - 0.05115122942975427\n",
      "Step - 2704, Loss - 0.34215212265117806, Learning Rate - 0.0125, magnitude of gradient - 0.014878914699404989\n",
      "Step - 2705, Loss - 0.3012001025005744, Learning Rate - 0.0125, magnitude of gradient - 0.04592371437900392\n",
      "Step - 2706, Loss - 0.3015231289803699, Learning Rate - 0.0125, magnitude of gradient - 0.06609296305873506\n",
      "Step - 2707, Loss - 0.34299332867607546, Learning Rate - 0.0125, magnitude of gradient - 0.020998408441141918\n",
      "Step - 2708, Loss - 0.32589149878143897, Learning Rate - 0.0125, magnitude of gradient - 0.07870532045771625\n",
      "Step - 2709, Loss - 0.24205266251422985, Learning Rate - 0.0125, magnitude of gradient - 0.03829456701965331\n",
      "Step - 2710, Loss - 0.3571505257228952, Learning Rate - 0.0125, magnitude of gradient - 0.04186653667337512\n",
      "Step - 2711, Loss - 0.33956199497568584, Learning Rate - 0.0125, magnitude of gradient - 0.018510540798495935\n",
      "Step - 2712, Loss - 0.3155740099963217, Learning Rate - 0.0125, magnitude of gradient - 0.0692803363706063\n",
      "Step - 2713, Loss - 0.3942829753951326, Learning Rate - 0.0125, magnitude of gradient - 0.06109893737788319\n",
      "Step - 2714, Loss - 0.2635769451784547, Learning Rate - 0.0125, magnitude of gradient - 0.072600680381651\n",
      "Step - 2715, Loss - 0.2920919734455174, Learning Rate - 0.0125, magnitude of gradient - 0.08233961125914498\n",
      "Step - 2716, Loss - 0.3065131107292379, Learning Rate - 0.0125, magnitude of gradient - 0.059647001710135754\n",
      "Step - 2717, Loss - 0.30960563255852164, Learning Rate - 0.0125, magnitude of gradient - 0.003670122768515062\n",
      "Step - 2718, Loss - 0.30779324657971613, Learning Rate - 0.0125, magnitude of gradient - 0.014966775367423434\n",
      "Step - 2719, Loss - 0.3408810373446231, Learning Rate - 0.0125, magnitude of gradient - 0.057111888493676245\n",
      "Step - 2720, Loss - 0.2757903861515615, Learning Rate - 0.0125, magnitude of gradient - 0.04362170013798833\n",
      "Step - 2721, Loss - 0.3086343335810657, Learning Rate - 0.0125, magnitude of gradient - 0.07707030873164412\n",
      "Step - 2722, Loss - 0.2956655736831105, Learning Rate - 0.0125, magnitude of gradient - 0.044211160401194556\n",
      "Step - 2723, Loss - 0.31513239567639645, Learning Rate - 0.0125, magnitude of gradient - 0.02713927669246865\n",
      "Step - 2724, Loss - 0.37016581062541554, Learning Rate - 0.0125, magnitude of gradient - 0.05363414171655011\n",
      "Step - 2725, Loss - 0.34258914468718554, Learning Rate - 0.0125, magnitude of gradient - 0.1270613328121012\n",
      "Step - 2726, Loss - 0.3017352687124666, Learning Rate - 0.0125, magnitude of gradient - 0.027240731117754286\n",
      "Step - 2727, Loss - 0.3654687672736008, Learning Rate - 0.0125, magnitude of gradient - 0.05952360591879239\n",
      "Step - 2728, Loss - 0.3424217442434223, Learning Rate - 0.0125, magnitude of gradient - 0.05830701400257363\n",
      "Step - 2729, Loss - 0.28069851863276835, Learning Rate - 0.0125, magnitude of gradient - 0.10651553255514348\n",
      "Step - 2730, Loss - 0.4262363064343759, Learning Rate - 0.0125, magnitude of gradient - 0.05816994149442796\n",
      "Step - 2731, Loss - 0.2989463721354088, Learning Rate - 0.0125, magnitude of gradient - 0.025643680798630303\n",
      "Step - 2732, Loss - 0.3315994352735151, Learning Rate - 0.0125, magnitude of gradient - 0.042999545936938915\n",
      "Step - 2733, Loss - 0.32133517066374145, Learning Rate - 0.0125, magnitude of gradient - 0.029671636916773995\n",
      "Step - 2734, Loss - 0.33957166834441843, Learning Rate - 0.0125, magnitude of gradient - 0.058075472545989694\n",
      "Step - 2735, Loss - 0.3367798636774798, Learning Rate - 0.0125, magnitude of gradient - 0.014104081114890032\n",
      "Step - 2736, Loss - 0.2539893820983605, Learning Rate - 0.0125, magnitude of gradient - 0.03921303774967189\n",
      "Step - 2737, Loss - 0.2920148226957334, Learning Rate - 0.0125, magnitude of gradient - 0.06813382853398336\n",
      "Step - 2738, Loss - 0.34705790462511954, Learning Rate - 0.0125, magnitude of gradient - 0.044067834131775833\n",
      "Step - 2739, Loss - 0.4246743065554771, Learning Rate - 0.0125, magnitude of gradient - 0.09943820502374433\n",
      "Step - 2740, Loss - 0.3289760043398151, Learning Rate - 0.0125, magnitude of gradient - 0.03708434390827568\n",
      "Step - 2741, Loss - 0.3636807820267909, Learning Rate - 0.0125, magnitude of gradient - 0.043010259218568596\n",
      "Step - 2742, Loss - 0.29383765870955114, Learning Rate - 0.0125, magnitude of gradient - 0.00951968632869423\n",
      "Step - 2743, Loss - 0.33772747752445714, Learning Rate - 0.0125, magnitude of gradient - 0.03008709043159032\n",
      "Step - 2744, Loss - 0.33138602690950625, Learning Rate - 0.0125, magnitude of gradient - 0.05286382869315659\n",
      "Step - 2745, Loss - 0.3490177642157972, Learning Rate - 0.0125, magnitude of gradient - 0.006727069865855296\n",
      "Step - 2746, Loss - 0.34474207484734276, Learning Rate - 0.0125, magnitude of gradient - 0.06973254510833982\n",
      "Step - 2747, Loss - 0.38296119633967896, Learning Rate - 0.0125, magnitude of gradient - 0.010069464334473017\n",
      "Step - 2748, Loss - 0.3260434797861518, Learning Rate - 0.0125, magnitude of gradient - 0.08425713684754324\n",
      "Step - 2749, Loss - 0.32165046871129555, Learning Rate - 0.0125, magnitude of gradient - 0.01754686325764363\n",
      "Step - 2750, Loss - 0.3386147376770423, Learning Rate - 0.0125, magnitude of gradient - 0.06824241609211949\n",
      "Step - 2751, Loss - 0.2704963669861762, Learning Rate - 0.0125, magnitude of gradient - 0.027853162996258812\n",
      "Step - 2752, Loss - 0.2783407062775136, Learning Rate - 0.0125, magnitude of gradient - 0.019929943459130777\n",
      "Step - 2753, Loss - 0.3234569321843724, Learning Rate - 0.0125, magnitude of gradient - 0.0579123041766605\n",
      "Step - 2754, Loss - 0.3808822305958881, Learning Rate - 0.0125, magnitude of gradient - 0.0964769709351189\n",
      "Step - 2755, Loss - 0.32615081686485387, Learning Rate - 0.0125, magnitude of gradient - 0.062059087606575365\n",
      "Step - 2756, Loss - 0.43020318316030065, Learning Rate - 0.0125, magnitude of gradient - 0.037346370001795345\n",
      "Step - 2757, Loss - 0.32912925689979633, Learning Rate - 0.0125, magnitude of gradient - 0.03594170314041552\n",
      "Step - 2758, Loss - 0.33502871587178834, Learning Rate - 0.0125, magnitude of gradient - 0.09835271966880763\n",
      "Step - 2759, Loss - 0.3311246800862893, Learning Rate - 0.0125, magnitude of gradient - 0.0423177530793907\n",
      "Step - 2760, Loss - 0.3459556575260472, Learning Rate - 0.0125, magnitude of gradient - 0.05218445702056655\n",
      "Step - 2761, Loss - 0.3340498194189643, Learning Rate - 0.0125, magnitude of gradient - 0.03398764148145753\n",
      "Step - 2762, Loss - 0.37370761072093456, Learning Rate - 0.0125, magnitude of gradient - 0.057695368307749\n",
      "Step - 2763, Loss - 0.3367085376059405, Learning Rate - 0.0125, magnitude of gradient - 0.04158518332599639\n",
      "Step - 2764, Loss - 0.32347051187568543, Learning Rate - 0.0125, magnitude of gradient - 0.08270548328369194\n",
      "Step - 2765, Loss - 0.3018583750703484, Learning Rate - 0.0125, magnitude of gradient - 0.057075375144032374\n",
      "Step - 2766, Loss - 0.36287069693540164, Learning Rate - 0.0125, magnitude of gradient - 0.023111696754435283\n",
      "Step - 2767, Loss - 0.3121976804176021, Learning Rate - 0.0125, magnitude of gradient - 0.07030229192598901\n",
      "Step - 2768, Loss - 0.31875895994509895, Learning Rate - 0.0125, magnitude of gradient - 0.11431737793551168\n",
      "Step - 2769, Loss - 0.32122714733512525, Learning Rate - 0.0125, magnitude of gradient - 0.03585485674971309\n",
      "Step - 2770, Loss - 0.355699330615821, Learning Rate - 0.0125, magnitude of gradient - 0.04198809170930408\n",
      "Step - 2771, Loss - 0.31508166269673454, Learning Rate - 0.0125, magnitude of gradient - 0.018544883863482017\n",
      "Step - 2772, Loss - 0.35092641692462867, Learning Rate - 0.0125, magnitude of gradient - 0.07872863931325577\n",
      "Step - 2773, Loss - 0.22163694660996322, Learning Rate - 0.0125, magnitude of gradient - 0.06753778675456666\n",
      "Step - 2774, Loss - 0.34996082613989193, Learning Rate - 0.0125, magnitude of gradient - 0.014523534933969797\n",
      "Step - 2775, Loss - 0.3634765001699164, Learning Rate - 0.0125, magnitude of gradient - 0.0497206150412103\n",
      "Step - 2776, Loss - 0.27266653444221667, Learning Rate - 0.0125, magnitude of gradient - 0.0392605787102256\n",
      "Step - 2777, Loss - 0.3597503861381786, Learning Rate - 0.0125, magnitude of gradient - 0.05801715475815694\n",
      "Step - 2778, Loss - 0.33674777737756034, Learning Rate - 0.0125, magnitude of gradient - 0.10342432953613971\n",
      "Step - 2779, Loss - 0.3161492424428545, Learning Rate - 0.0125, magnitude of gradient - 0.025280218930141696\n",
      "Step - 2780, Loss - 0.22059885917416747, Learning Rate - 0.0125, magnitude of gradient - 0.021498193850902653\n",
      "Step - 2781, Loss - 0.2871411342049478, Learning Rate - 0.0125, magnitude of gradient - 0.01527376021099783\n",
      "Step - 2782, Loss - 0.3534204432066126, Learning Rate - 0.0125, magnitude of gradient - 0.035657395821315725\n",
      "Step - 2783, Loss - 0.32465516386909005, Learning Rate - 0.0125, magnitude of gradient - 0.06552171771807\n",
      "Step - 2784, Loss - 0.3175669429154984, Learning Rate - 0.0125, magnitude of gradient - 0.11091349699665044\n",
      "Step - 2785, Loss - 0.29486098973122343, Learning Rate - 0.0125, magnitude of gradient - 0.009176439685965302\n",
      "Step - 2786, Loss - 0.2429430968752112, Learning Rate - 0.0125, magnitude of gradient - 0.051191011994346725\n",
      "Step - 2787, Loss - 0.3614682525692801, Learning Rate - 0.0125, magnitude of gradient - 0.07424691401699568\n",
      "Step - 2788, Loss - 0.3827520372970602, Learning Rate - 0.0125, magnitude of gradient - 0.01672670255906895\n",
      "Step - 2789, Loss - 0.34189499402927587, Learning Rate - 0.0125, magnitude of gradient - 0.11933857159269556\n",
      "Step - 2790, Loss - 0.26990220177631546, Learning Rate - 0.0125, magnitude of gradient - 0.09533116118215887\n",
      "Step - 2791, Loss - 0.4118480240509186, Learning Rate - 0.0125, magnitude of gradient - 0.07608495114823116\n",
      "Step - 2792, Loss - 0.334371553649667, Learning Rate - 0.0125, magnitude of gradient - 0.04085388073001555\n",
      "Step - 2793, Loss - 0.33404038880831055, Learning Rate - 0.0125, magnitude of gradient - 0.03500199179871759\n",
      "Step - 2794, Loss - 0.25312667581298703, Learning Rate - 0.0125, magnitude of gradient - 0.04165307241944049\n",
      "Step - 2795, Loss - 0.3384235942655046, Learning Rate - 0.0125, magnitude of gradient - 0.06033517468334599\n",
      "Step - 2796, Loss - 0.31995785028763113, Learning Rate - 0.0125, magnitude of gradient - 0.023561430989185526\n",
      "Step - 2797, Loss - 0.3270067290403064, Learning Rate - 0.0125, magnitude of gradient - 0.057354942323815036\n",
      "Step - 2798, Loss - 0.28954790902306743, Learning Rate - 0.0125, magnitude of gradient - 0.08555662177365415\n",
      "Step - 2799, Loss - 0.34302335970805353, Learning Rate - 0.0125, magnitude of gradient - 0.08999724145868936\n",
      "Step - 2800, Loss - 0.29235364802619146, Learning Rate - 0.0125, magnitude of gradient - 0.025220252108846066\n",
      "Step - 2801, Loss - 0.2964971935352141, Learning Rate - 0.0125, magnitude of gradient - 0.041247872694644666\n",
      "Step - 2802, Loss - 0.26963741007041386, Learning Rate - 0.0125, magnitude of gradient - 0.060870657585274025\n",
      "Step - 2803, Loss - 0.3210073275479578, Learning Rate - 0.0125, magnitude of gradient - 0.03071513738480246\n",
      "Step - 2804, Loss - 0.29568912110160117, Learning Rate - 0.0125, magnitude of gradient - 0.12571779822741305\n",
      "Step - 2805, Loss - 0.4579856244395127, Learning Rate - 0.0125, magnitude of gradient - 0.02699206392805629\n",
      "Step - 2806, Loss - 0.3269001905245078, Learning Rate - 0.0125, magnitude of gradient - 0.0384645369105058\n",
      "Step - 2807, Loss - 0.23313929368261915, Learning Rate - 0.0125, magnitude of gradient - 0.04217606745610633\n",
      "Step - 2808, Loss - 0.3544916065519107, Learning Rate - 0.0125, magnitude of gradient - 0.04172161094668575\n",
      "Step - 2809, Loss - 0.27058869297607346, Learning Rate - 0.0125, magnitude of gradient - 0.04446462642351086\n",
      "Step - 2810, Loss - 0.3152150329136879, Learning Rate - 0.0125, magnitude of gradient - 0.06813884453532186\n",
      "Step - 2811, Loss - 0.3515016030085989, Learning Rate - 0.0125, magnitude of gradient - 0.03419182649336067\n",
      "Step - 2812, Loss - 0.29485081525510193, Learning Rate - 0.0125, magnitude of gradient - 0.05430020698380126\n",
      "Step - 2813, Loss - 0.33426988088220827, Learning Rate - 0.0125, magnitude of gradient - 0.024820125675538478\n",
      "Step - 2814, Loss - 0.3562880506692355, Learning Rate - 0.0125, magnitude of gradient - 0.05581661106887203\n",
      "Step - 2815, Loss - 0.27343840531370345, Learning Rate - 0.0125, magnitude of gradient - 0.038296307774325675\n",
      "Step - 2816, Loss - 0.30272428162299864, Learning Rate - 0.0125, magnitude of gradient - 0.07724142968025624\n",
      "Step - 2817, Loss - 0.2415968292023844, Learning Rate - 0.0125, magnitude of gradient - 0.06881972106670692\n",
      "Step - 2818, Loss - 0.34450618864074106, Learning Rate - 0.0125, magnitude of gradient - 0.12296945645650673\n",
      "Step - 2819, Loss - 0.2648809204614692, Learning Rate - 0.0125, magnitude of gradient - 0.06148557007466767\n",
      "Step - 2820, Loss - 0.23126533117989057, Learning Rate - 0.0125, magnitude of gradient - 0.044051648597938114\n",
      "Step - 2821, Loss - 0.27093767459710927, Learning Rate - 0.0125, magnitude of gradient - 0.05545826391306153\n",
      "Step - 2822, Loss - 0.375732678308244, Learning Rate - 0.0125, magnitude of gradient - 0.041581512330438865\n",
      "Step - 2823, Loss - 0.3486008896529153, Learning Rate - 0.0125, magnitude of gradient - 0.054463984412156184\n",
      "Step - 2824, Loss - 0.28991549886452456, Learning Rate - 0.0125, magnitude of gradient - 0.054285677659278934\n",
      "Step - 2825, Loss - 0.3788028978668975, Learning Rate - 0.0125, magnitude of gradient - 0.01187887756244154\n",
      "Step - 2826, Loss - 0.32391033007011677, Learning Rate - 0.0125, magnitude of gradient - 0.08950335567530904\n",
      "Step - 2827, Loss - 0.2964519829752617, Learning Rate - 0.0125, magnitude of gradient - 0.04322323764170513\n",
      "Step - 2828, Loss - 0.25700099411424643, Learning Rate - 0.0125, magnitude of gradient - 0.0704261511113328\n",
      "Step - 2829, Loss - 0.3009415952980573, Learning Rate - 0.0125, magnitude of gradient - 0.05074421973469069\n",
      "Step - 2830, Loss - 0.3261312618121319, Learning Rate - 0.0125, magnitude of gradient - 0.07792007773933597\n",
      "Step - 2831, Loss - 0.2963078543945017, Learning Rate - 0.0125, magnitude of gradient - 0.03488734882900008\n",
      "Step - 2832, Loss - 0.3269323085039638, Learning Rate - 0.0125, magnitude of gradient - 0.020208125695249542\n",
      "Step - 2833, Loss - 0.33370403119069963, Learning Rate - 0.0125, magnitude of gradient - 0.03932173644705191\n",
      "Step - 2834, Loss - 0.2804466796061559, Learning Rate - 0.0125, magnitude of gradient - 0.021168320415229244\n",
      "Step - 2835, Loss - 0.33478310470029404, Learning Rate - 0.0125, magnitude of gradient - 0.00255062396810793\n",
      "Step - 2836, Loss - 0.27518549262110015, Learning Rate - 0.0125, magnitude of gradient - 0.0695889954029807\n",
      "Step - 2837, Loss - 0.30360174657768113, Learning Rate - 0.0125, magnitude of gradient - 0.04652604103494914\n",
      "Step - 2838, Loss - 0.34027497693425174, Learning Rate - 0.0125, magnitude of gradient - 0.0599815215739036\n",
      "Step - 2839, Loss - 0.292204116728213, Learning Rate - 0.0125, magnitude of gradient - 0.022535803973367376\n",
      "Step - 2840, Loss - 0.2610829162476149, Learning Rate - 0.0125, magnitude of gradient - 0.08678455358421164\n",
      "Step - 2841, Loss - 0.2882993165258349, Learning Rate - 0.0125, magnitude of gradient - 0.009576973299438426\n",
      "Step - 2842, Loss - 0.34565227273741517, Learning Rate - 0.0125, magnitude of gradient - 0.06728471788930585\n",
      "Step - 2843, Loss - 0.4177058569504308, Learning Rate - 0.0125, magnitude of gradient - 0.09071103586849982\n",
      "Step - 2844, Loss - 0.3225669124758688, Learning Rate - 0.0125, magnitude of gradient - 0.02635882133038473\n",
      "Step - 2845, Loss - 0.2648278010792713, Learning Rate - 0.0125, magnitude of gradient - 0.07629187188311788\n",
      "Step - 2846, Loss - 0.420710648369659, Learning Rate - 0.0125, magnitude of gradient - 0.03813956101683079\n",
      "Step - 2847, Loss - 0.35632317068457775, Learning Rate - 0.0125, magnitude of gradient - 0.07306874138952944\n",
      "Step - 2848, Loss - 0.3272936925080011, Learning Rate - 0.0125, magnitude of gradient - 0.05784089089370569\n",
      "Step - 2849, Loss - 0.31251041367928706, Learning Rate - 0.0125, magnitude of gradient - 0.050800698527762235\n",
      "Step - 2850, Loss - 0.2952038436183535, Learning Rate - 0.0125, magnitude of gradient - 0.03752753500830751\n",
      "Step - 2851, Loss - 0.3587814049389634, Learning Rate - 0.0125, magnitude of gradient - 0.10645498065080762\n",
      "Step - 2852, Loss - 0.30030810276865133, Learning Rate - 0.0125, magnitude of gradient - 0.06338966071638961\n",
      "Step - 2853, Loss - 0.2892860108575974, Learning Rate - 0.0125, magnitude of gradient - 0.03763399185602943\n",
      "Step - 2854, Loss - 0.36073843064045896, Learning Rate - 0.0125, magnitude of gradient - 0.0158503520033144\n",
      "Step - 2855, Loss - 0.3363270513691805, Learning Rate - 0.0125, magnitude of gradient - 0.0464214965973011\n",
      "Step - 2856, Loss - 0.2934414078837131, Learning Rate - 0.0125, magnitude of gradient - 0.06525648269309157\n",
      "Step - 2857, Loss - 0.3212658931593749, Learning Rate - 0.0125, magnitude of gradient - 0.04507744117100661\n",
      "Step - 2858, Loss - 0.3159947071747168, Learning Rate - 0.0125, magnitude of gradient - 0.03377903734743167\n",
      "Step - 2859, Loss - 0.3015740755935706, Learning Rate - 0.0125, magnitude of gradient - 0.0419903668209486\n",
      "Step - 2860, Loss - 0.34565015857123904, Learning Rate - 0.0125, magnitude of gradient - 0.05980434260950414\n",
      "Step - 2861, Loss - 0.32426909452110325, Learning Rate - 0.0125, magnitude of gradient - 0.04015533097400483\n",
      "Step - 2862, Loss - 0.30587584224927167, Learning Rate - 0.0125, magnitude of gradient - 0.058157998955666174\n",
      "Step - 2863, Loss - 0.30300641555710434, Learning Rate - 0.0125, magnitude of gradient - 0.03962743799800915\n",
      "Step - 2864, Loss - 0.2899935537158587, Learning Rate - 0.0125, magnitude of gradient - 0.035120087899348486\n",
      "Step - 2865, Loss - 0.3144847719405911, Learning Rate - 0.0125, magnitude of gradient - 0.0211600208271456\n",
      "Step - 2866, Loss - 0.22828217291381075, Learning Rate - 0.0125, magnitude of gradient - 0.021504394765679877\n",
      "Step - 2867, Loss - 0.30157688212041295, Learning Rate - 0.0125, magnitude of gradient - 0.07308939128669074\n",
      "Step - 2868, Loss - 0.3984857590329426, Learning Rate - 0.0125, magnitude of gradient - 0.059830029332506626\n",
      "Step - 2869, Loss - 0.3309377679265373, Learning Rate - 0.0125, magnitude of gradient - 0.06177299023746867\n",
      "Step - 2870, Loss - 0.352010398200021, Learning Rate - 0.0125, magnitude of gradient - 0.08465568235938477\n",
      "Step - 2871, Loss - 0.27434942552958075, Learning Rate - 0.0125, magnitude of gradient - 0.02810926554049314\n",
      "Step - 2872, Loss - 0.3349911059947257, Learning Rate - 0.0125, magnitude of gradient - 0.05313612356087094\n",
      "Step - 2873, Loss - 0.3310498411396249, Learning Rate - 0.0125, magnitude of gradient - 0.02879908845898924\n",
      "Step - 2874, Loss - 0.3431537584769936, Learning Rate - 0.0125, magnitude of gradient - 0.04005200006035948\n",
      "Step - 2875, Loss - 0.41127640613225336, Learning Rate - 0.0125, magnitude of gradient - 0.016246685753744737\n",
      "Step - 2876, Loss - 0.3007034294288534, Learning Rate - 0.0125, magnitude of gradient - 0.062316536643342184\n",
      "Step - 2877, Loss - 0.3081488731240605, Learning Rate - 0.0125, magnitude of gradient - 0.10233403278128797\n",
      "Step - 2878, Loss - 0.35176192891063285, Learning Rate - 0.0125, magnitude of gradient - 0.04789909454042054\n",
      "Step - 2879, Loss - 0.4178377399375174, Learning Rate - 0.0125, magnitude of gradient - 0.03537118824971156\n",
      "Step - 2880, Loss - 0.29351429883571983, Learning Rate - 0.0125, magnitude of gradient - 0.03264471748725336\n",
      "Step - 2881, Loss - 0.36319809322389485, Learning Rate - 0.0125, magnitude of gradient - 0.04039532575592975\n",
      "Step - 2882, Loss - 0.31297722290641355, Learning Rate - 0.0125, magnitude of gradient - 0.020694589195540684\n",
      "Step - 2883, Loss - 0.3078831654080551, Learning Rate - 0.0125, magnitude of gradient - 0.030052302167170784\n",
      "Step - 2884, Loss - 0.378459755914523, Learning Rate - 0.0125, magnitude of gradient - 0.05030077962714028\n",
      "Step - 2885, Loss - 0.36972578043563864, Learning Rate - 0.0125, magnitude of gradient - 0.06425377942462471\n",
      "Step - 2886, Loss - 0.3752306892781284, Learning Rate - 0.0125, magnitude of gradient - 0.028927662243770063\n",
      "Step - 2887, Loss - 0.3235724027237433, Learning Rate - 0.0125, magnitude of gradient - 0.09136366361188647\n",
      "Step - 2888, Loss - 0.2811010809971229, Learning Rate - 0.0125, magnitude of gradient - 0.046207340759728356\n",
      "Step - 2889, Loss - 0.3153426982471761, Learning Rate - 0.0125, magnitude of gradient - 0.05717312936951536\n",
      "Step - 2890, Loss - 0.37896735701683226, Learning Rate - 0.0125, magnitude of gradient - 0.03954826756287546\n",
      "Step - 2891, Loss - 0.27995154936897587, Learning Rate - 0.0125, magnitude of gradient - 0.013298819878342274\n",
      "Step - 2892, Loss - 0.32920677015833405, Learning Rate - 0.0125, magnitude of gradient - 0.02032916553513385\n",
      "Step - 2893, Loss - 0.38952261758767454, Learning Rate - 0.0125, magnitude of gradient - 0.0894924897563359\n",
      "Step - 2894, Loss - 0.3628491291759286, Learning Rate - 0.0125, magnitude of gradient - 0.032982385590695155\n",
      "Step - 2895, Loss - 0.28425015690273947, Learning Rate - 0.0125, magnitude of gradient - 0.036583536603055127\n",
      "Step - 2896, Loss - 0.27314798829904047, Learning Rate - 0.0125, magnitude of gradient - 0.06026462199790612\n",
      "Step - 2897, Loss - 0.33356333861931103, Learning Rate - 0.0125, magnitude of gradient - 0.11414373403938653\n",
      "Step - 2898, Loss - 0.3035571688438284, Learning Rate - 0.0125, magnitude of gradient - 0.10110427272311265\n",
      "Step - 2899, Loss - 0.3211274014924576, Learning Rate - 0.0125, magnitude of gradient - 0.05427495908693466\n",
      "Step - 2900, Loss - 0.30121457096870474, Learning Rate - 0.0125, magnitude of gradient - 0.047574332307930296\n",
      "Step - 2901, Loss - 0.42356694659127464, Learning Rate - 0.0125, magnitude of gradient - 0.05846379895848049\n",
      "Step - 2902, Loss - 0.25636263577040236, Learning Rate - 0.0125, magnitude of gradient - 0.008374691850721327\n",
      "Step - 2903, Loss - 0.3284430831264281, Learning Rate - 0.0125, magnitude of gradient - 0.06993471184303071\n",
      "Step - 2904, Loss - 0.30181020035545136, Learning Rate - 0.0125, magnitude of gradient - 0.06467292267996469\n",
      "Step - 2905, Loss - 0.3594320841869537, Learning Rate - 0.0125, magnitude of gradient - 0.034691143713081365\n",
      "Step - 2906, Loss - 0.3699002635469704, Learning Rate - 0.0125, magnitude of gradient - 0.056956601763770855\n",
      "Step - 2907, Loss - 0.33945745467258215, Learning Rate - 0.0125, magnitude of gradient - 0.033933056492035193\n",
      "Step - 2908, Loss - 0.26473540888007874, Learning Rate - 0.0125, magnitude of gradient - 0.015975400222955193\n",
      "Step - 2909, Loss - 0.28863432913289894, Learning Rate - 0.0125, magnitude of gradient - 0.13483876032395295\n",
      "Step - 2910, Loss - 0.3220595636212709, Learning Rate - 0.0125, magnitude of gradient - 0.023317176764365646\n",
      "Step - 2911, Loss - 0.3195406559626114, Learning Rate - 0.0125, magnitude of gradient - 0.017725270179950712\n",
      "Step - 2912, Loss - 0.32754026023057115, Learning Rate - 0.0125, magnitude of gradient - 0.07039318295269142\n",
      "Step - 2913, Loss - 0.2810867755252562, Learning Rate - 0.0125, magnitude of gradient - 0.056205107596772344\n",
      "Step - 2914, Loss - 0.30996944577918384, Learning Rate - 0.0125, magnitude of gradient - 0.04827649674806855\n",
      "Step - 2915, Loss - 0.2536256197770851, Learning Rate - 0.0125, magnitude of gradient - 0.02729851721929097\n",
      "Step - 2916, Loss - 0.31594887943812855, Learning Rate - 0.0125, magnitude of gradient - 0.041061310436520206\n",
      "Step - 2917, Loss - 0.36403027234032276, Learning Rate - 0.0125, magnitude of gradient - 0.005968666649176402\n",
      "Step - 2918, Loss - 0.29635641429969306, Learning Rate - 0.0125, magnitude of gradient - 0.01521357804520499\n",
      "Step - 2919, Loss - 0.2723556506554973, Learning Rate - 0.0125, magnitude of gradient - 0.007899755948953101\n",
      "Step - 2920, Loss - 0.3037225611901142, Learning Rate - 0.0125, magnitude of gradient - 0.03265140846410224\n",
      "Step - 2921, Loss - 0.3354081278606369, Learning Rate - 0.0125, magnitude of gradient - 0.04003422131084988\n",
      "Step - 2922, Loss - 0.3872385323900097, Learning Rate - 0.0125, magnitude of gradient - 0.0836108672810118\n",
      "Step - 2923, Loss - 0.2960756176141579, Learning Rate - 0.0125, magnitude of gradient - 0.05771002156936654\n",
      "Step - 2924, Loss - 0.3043649044309104, Learning Rate - 0.0125, magnitude of gradient - 0.033136136489679786\n",
      "Step - 2925, Loss - 0.31243647123783025, Learning Rate - 0.0125, magnitude of gradient - 0.056580960317857816\n",
      "Step - 2926, Loss - 0.3298365938044866, Learning Rate - 0.0125, magnitude of gradient - 0.01882784261899515\n",
      "Step - 2927, Loss - 0.2797381489671476, Learning Rate - 0.0125, magnitude of gradient - 0.0629637707996526\n",
      "Step - 2928, Loss - 0.3756133070557245, Learning Rate - 0.0125, magnitude of gradient - 0.06447329082077904\n",
      "Step - 2929, Loss - 0.2920549422370637, Learning Rate - 0.0125, magnitude of gradient - 0.08499819731429553\n",
      "Step - 2930, Loss - 0.38225263990352804, Learning Rate - 0.0125, magnitude of gradient - 0.09451930318139007\n",
      "Step - 2931, Loss - 0.2943935818224176, Learning Rate - 0.0125, magnitude of gradient - 0.042713388013816946\n",
      "Step - 2932, Loss - 0.3394669128570744, Learning Rate - 0.0125, magnitude of gradient - 0.09510949020588925\n",
      "Step - 2933, Loss - 0.3628624558489745, Learning Rate - 0.0125, magnitude of gradient - 0.037665547487281466\n",
      "Step - 2934, Loss - 0.26249828527298735, Learning Rate - 0.0125, magnitude of gradient - 0.029622743987905473\n",
      "Step - 2935, Loss - 0.332495000489897, Learning Rate - 0.0125, magnitude of gradient - 0.005788821801881951\n",
      "Step - 2936, Loss - 0.31698876754698296, Learning Rate - 0.0125, magnitude of gradient - 0.02896484047060928\n",
      "Step - 2937, Loss - 0.32308986932219225, Learning Rate - 0.0125, magnitude of gradient - 0.019064582148913902\n",
      "Step - 2938, Loss - 0.33773498510231387, Learning Rate - 0.0125, magnitude of gradient - 0.06020121900629652\n",
      "Step - 2939, Loss - 0.3634130518114083, Learning Rate - 0.0125, magnitude of gradient - 0.049028394705707024\n",
      "Step - 2940, Loss - 0.3946071920029144, Learning Rate - 0.0125, magnitude of gradient - 0.030134237525262366\n",
      "Step - 2941, Loss - 0.28918669892276017, Learning Rate - 0.0125, magnitude of gradient - 0.04101119258974739\n",
      "Step - 2942, Loss - 0.2921812800621176, Learning Rate - 0.0125, magnitude of gradient - 0.02685135772631902\n",
      "Step - 2943, Loss - 0.3631802575821809, Learning Rate - 0.0125, magnitude of gradient - 0.05734065229768055\n",
      "Step - 2944, Loss - 0.33156799489953787, Learning Rate - 0.0125, magnitude of gradient - 0.07542643202499859\n",
      "Step - 2945, Loss - 0.33716111473034865, Learning Rate - 0.0125, magnitude of gradient - 0.08767104743398538\n",
      "Step - 2946, Loss - 0.2659461407090635, Learning Rate - 0.0125, magnitude of gradient - 0.04946180649906498\n",
      "Step - 2947, Loss - 0.33469789326095645, Learning Rate - 0.0125, magnitude of gradient - 0.06335341307419393\n",
      "Step - 2948, Loss - 0.2795801010807279, Learning Rate - 0.0125, magnitude of gradient - 0.0032127226874763297\n",
      "Step - 2949, Loss - 0.32047662043964187, Learning Rate - 0.0125, magnitude of gradient - 0.05584413488404761\n",
      "Step - 2950, Loss - 0.3480734929580789, Learning Rate - 0.0125, magnitude of gradient - 0.05626877361054287\n",
      "Step - 2951, Loss - 0.36376866268765873, Learning Rate - 0.0125, magnitude of gradient - 0.04044138004274233\n",
      "Step - 2952, Loss - 0.31923505911379424, Learning Rate - 0.0125, magnitude of gradient - 0.09914659397541359\n",
      "Step - 2953, Loss - 0.34451153345898566, Learning Rate - 0.0125, magnitude of gradient - 0.04271631002573187\n",
      "Step - 2954, Loss - 0.27233785856115367, Learning Rate - 0.0125, magnitude of gradient - 0.038794474907196896\n",
      "Step - 2955, Loss - 0.2911010313604539, Learning Rate - 0.0125, magnitude of gradient - 0.030855307273412284\n",
      "Step - 2956, Loss - 0.33213803480636084, Learning Rate - 0.0125, magnitude of gradient - 0.0035348659031699727\n",
      "Step - 2957, Loss - 0.3528498282945807, Learning Rate - 0.0125, magnitude of gradient - 0.04118882001731354\n",
      "Step - 2958, Loss - 0.29508858745755373, Learning Rate - 0.0125, magnitude of gradient - 0.04244333344539926\n",
      "Step - 2959, Loss - 0.3720260508689081, Learning Rate - 0.0125, magnitude of gradient - 0.09542489279602777\n",
      "Step - 2960, Loss - 0.34680577692979414, Learning Rate - 0.0125, magnitude of gradient - 0.05567428048213269\n",
      "Step - 2961, Loss - 0.26829200634559913, Learning Rate - 0.0125, magnitude of gradient - 0.061143647997230585\n",
      "Step - 2962, Loss - 0.4001683066314744, Learning Rate - 0.0125, magnitude of gradient - 0.017015931829814358\n",
      "Step - 2963, Loss - 0.37102388542465364, Learning Rate - 0.0125, magnitude of gradient - 0.09385235710370247\n",
      "Step - 2964, Loss - 0.32531315373123876, Learning Rate - 0.0125, magnitude of gradient - 0.07096155820668873\n",
      "Step - 2965, Loss - 0.3491426862123986, Learning Rate - 0.0125, magnitude of gradient - 0.07244432048963939\n",
      "Step - 2966, Loss - 0.2942627081171627, Learning Rate - 0.0125, magnitude of gradient - 0.014562292671178792\n",
      "Step - 2967, Loss - 0.3301314549062155, Learning Rate - 0.0125, magnitude of gradient - 0.05140349065021488\n",
      "Step - 2968, Loss - 0.3557603616811901, Learning Rate - 0.0125, magnitude of gradient - 0.07374352520953309\n",
      "Step - 2969, Loss - 0.2944764331314641, Learning Rate - 0.0125, magnitude of gradient - 0.045906819304204344\n",
      "Step - 2970, Loss - 0.3261905048785247, Learning Rate - 0.0125, magnitude of gradient - 0.024682936570726073\n",
      "Step - 2971, Loss - 0.3480204963539828, Learning Rate - 0.0125, magnitude of gradient - 0.06710451030177716\n",
      "Step - 2972, Loss - 0.3343651993654059, Learning Rate - 0.0125, magnitude of gradient - 0.04932934008648207\n",
      "Step - 2973, Loss - 0.3687255049146281, Learning Rate - 0.0125, magnitude of gradient - 0.04938874017407799\n",
      "Step - 2974, Loss - 0.29534725922632177, Learning Rate - 0.0125, magnitude of gradient - 0.049915660659699906\n",
      "Step - 2975, Loss - 0.24175076854328015, Learning Rate - 0.0125, magnitude of gradient - 0.048507771455449565\n",
      "Step - 2976, Loss - 0.2859488179634595, Learning Rate - 0.0125, magnitude of gradient - 0.0534540606792971\n",
      "Step - 2977, Loss - 0.3018973061319547, Learning Rate - 0.0125, magnitude of gradient - 0.01824832379973689\n",
      "Step - 2978, Loss - 0.3364749278502489, Learning Rate - 0.0125, magnitude of gradient - 0.01701921433303039\n",
      "Step - 2979, Loss - 0.2965848458857207, Learning Rate - 0.0125, magnitude of gradient - 0.08809552017372291\n",
      "Step - 2980, Loss - 0.3446796746751975, Learning Rate - 0.0125, magnitude of gradient - 0.06212359060341381\n",
      "Step - 2981, Loss - 0.2965842621647897, Learning Rate - 0.0125, magnitude of gradient - 0.043902856339254476\n",
      "Step - 2982, Loss - 0.3803044254672673, Learning Rate - 0.0125, magnitude of gradient - 0.02227144107926684\n",
      "Step - 2983, Loss - 0.32778590480810843, Learning Rate - 0.0125, magnitude of gradient - 0.016624861425455748\n",
      "Step - 2984, Loss - 0.38222522637790113, Learning Rate - 0.0125, magnitude of gradient - 0.05174306273744552\n",
      "Step - 2985, Loss - 0.30380497564941267, Learning Rate - 0.0125, magnitude of gradient - 0.024696297489411104\n",
      "Step - 2986, Loss - 0.33125743304856403, Learning Rate - 0.0125, magnitude of gradient - 0.025481392962500005\n",
      "Step - 2987, Loss - 0.33368873488060147, Learning Rate - 0.0125, magnitude of gradient - 0.07818111896591572\n",
      "Step - 2988, Loss - 0.3440870205289649, Learning Rate - 0.0125, magnitude of gradient - 0.11362729664633205\n",
      "Step - 2989, Loss - 0.24241631264931035, Learning Rate - 0.0125, magnitude of gradient - 0.024054913505361968\n",
      "Step - 2990, Loss - 0.27569351989518714, Learning Rate - 0.0125, magnitude of gradient - 0.0894327681189155\n",
      "Step - 2991, Loss - 0.28731094471192503, Learning Rate - 0.0125, magnitude of gradient - 0.08004116155639858\n",
      "Step - 2992, Loss - 0.36264653113734463, Learning Rate - 0.0125, magnitude of gradient - 0.06313520085165877\n",
      "Step - 2993, Loss - 0.3914656454480798, Learning Rate - 0.0125, magnitude of gradient - 0.11962355265179736\n",
      "Step - 2994, Loss - 0.30966052636546765, Learning Rate - 0.0125, magnitude of gradient - 0.0688022586883189\n",
      "Step - 2995, Loss - 0.3082536221158044, Learning Rate - 0.0125, magnitude of gradient - 0.07281241296656975\n",
      "Step - 2996, Loss - 0.357919537217006, Learning Rate - 0.0125, magnitude of gradient - 0.07741637455508145\n",
      "Step - 2997, Loss - 0.3373290704891748, Learning Rate - 0.0125, magnitude of gradient - 0.06971399035739324\n",
      "Step - 2998, Loss - 0.3166532343629437, Learning Rate - 0.0125, magnitude of gradient - 0.09964884568582244\n",
      "Step - 2999, Loss - 0.2869397132597232, Learning Rate - 0.0125, magnitude of gradient - 0.0892476454669471\n",
      "Step - 3000, Loss - 0.2694876822925818, Learning Rate - 0.0125, magnitude of gradient - 0.038086667505042124\n",
      "Step - 3001, Loss - 0.28562579923108977, Learning Rate - 0.00625, magnitude of gradient - 0.03187593037122454\n",
      "Step - 3002, Loss - 0.37333716904748127, Learning Rate - 0.00625, magnitude of gradient - 0.06352247783537168\n",
      "Step - 3003, Loss - 0.3417736273225728, Learning Rate - 0.00625, magnitude of gradient - 0.05056908531994855\n",
      "Step - 3004, Loss - 0.3012222738437316, Learning Rate - 0.00625, magnitude of gradient - 0.03849297100916613\n",
      "Step - 3005, Loss - 0.2728158789908284, Learning Rate - 0.00625, magnitude of gradient - 0.0335671363802468\n",
      "Step - 3006, Loss - 0.32945289269577055, Learning Rate - 0.00625, magnitude of gradient - 0.06630391810171725\n",
      "Step - 3007, Loss - 0.3165646526535766, Learning Rate - 0.00625, magnitude of gradient - 0.06679212962204777\n",
      "Step - 3008, Loss - 0.33357783040635824, Learning Rate - 0.00625, magnitude of gradient - 0.04816263901997822\n",
      "Step - 3009, Loss - 0.3456755992828137, Learning Rate - 0.00625, magnitude of gradient - 0.051782694113844945\n",
      "Step - 3010, Loss - 0.39057547727000086, Learning Rate - 0.00625, magnitude of gradient - 0.10600380049770189\n",
      "Step - 3011, Loss - 0.34126542242807706, Learning Rate - 0.00625, magnitude of gradient - 0.004437369379680482\n",
      "Step - 3012, Loss - 0.32560825021034473, Learning Rate - 0.00625, magnitude of gradient - 0.0666359549492121\n",
      "Step - 3013, Loss - 0.30270668717911775, Learning Rate - 0.00625, magnitude of gradient - 0.040997460649856945\n",
      "Step - 3014, Loss - 0.27279436571536175, Learning Rate - 0.00625, magnitude of gradient - 0.0850062331535906\n",
      "Step - 3015, Loss - 0.318073953029633, Learning Rate - 0.00625, magnitude of gradient - 0.07279187505287582\n",
      "Step - 3016, Loss - 0.3646581327133703, Learning Rate - 0.00625, magnitude of gradient - 0.04849019996086311\n",
      "Step - 3017, Loss - 0.2940235037658754, Learning Rate - 0.00625, magnitude of gradient - 0.06340655807403459\n",
      "Step - 3018, Loss - 0.3816759662152958, Learning Rate - 0.00625, magnitude of gradient - 0.0663817621625511\n",
      "Step - 3019, Loss - 0.267232833313543, Learning Rate - 0.00625, magnitude of gradient - 0.01038633454524206\n",
      "Step - 3020, Loss - 0.345596704665627, Learning Rate - 0.00625, magnitude of gradient - 0.09672080798161378\n",
      "Step - 3021, Loss - 0.3510771655771664, Learning Rate - 0.00625, magnitude of gradient - 0.12263183430774306\n",
      "Step - 3022, Loss - 0.28683646552377023, Learning Rate - 0.00625, magnitude of gradient - 0.08151902519109618\n",
      "Step - 3023, Loss - 0.3261310035226702, Learning Rate - 0.00625, magnitude of gradient - 0.0769357064908795\n",
      "Step - 3024, Loss - 0.24596863336496183, Learning Rate - 0.00625, magnitude of gradient - 0.016220290661428854\n",
      "Step - 3025, Loss - 0.26945305669096264, Learning Rate - 0.00625, magnitude of gradient - 0.049142077352991594\n",
      "Step - 3026, Loss - 0.40432583085479223, Learning Rate - 0.00625, magnitude of gradient - 0.10113411133180379\n",
      "Step - 3027, Loss - 0.2854462258519165, Learning Rate - 0.00625, magnitude of gradient - 0.05604127870674027\n",
      "Step - 3028, Loss - 0.2880877058874266, Learning Rate - 0.00625, magnitude of gradient - 0.0463729299458988\n",
      "Step - 3029, Loss - 0.36482898238568384, Learning Rate - 0.00625, magnitude of gradient - 0.02645049509252321\n",
      "Step - 3030, Loss - 0.3145787353419952, Learning Rate - 0.00625, magnitude of gradient - 0.03375209878542834\n",
      "Step - 3031, Loss - 0.35094215397485023, Learning Rate - 0.00625, magnitude of gradient - 0.07282779634984399\n",
      "Step - 3032, Loss - 0.3488151244012996, Learning Rate - 0.00625, magnitude of gradient - 0.06694466564229883\n",
      "Step - 3033, Loss - 0.2909927049414062, Learning Rate - 0.00625, magnitude of gradient - 0.0021307224841274047\n",
      "Step - 3034, Loss - 0.34272228203475513, Learning Rate - 0.00625, magnitude of gradient - 0.029005786623111318\n",
      "Step - 3035, Loss - 0.3247731698985586, Learning Rate - 0.00625, magnitude of gradient - 0.06391525207533356\n",
      "Step - 3036, Loss - 0.28223910311656303, Learning Rate - 0.00625, magnitude of gradient - 0.03692193476188069\n",
      "Step - 3037, Loss - 0.30841623709950616, Learning Rate - 0.00625, magnitude of gradient - 0.04201597660909002\n",
      "Step - 3038, Loss - 0.34430808386178535, Learning Rate - 0.00625, magnitude of gradient - 0.12856953960064427\n",
      "Step - 3039, Loss - 0.28506791530344994, Learning Rate - 0.00625, magnitude of gradient - 0.12684511394925627\n",
      "Step - 3040, Loss - 0.33850335763340944, Learning Rate - 0.00625, magnitude of gradient - 0.05628263583472258\n",
      "Step - 3041, Loss - 0.3685370690400512, Learning Rate - 0.00625, magnitude of gradient - 0.09249987263042976\n",
      "Step - 3042, Loss - 0.44015721275037556, Learning Rate - 0.00625, magnitude of gradient - 0.02718957843096239\n",
      "Step - 3043, Loss - 0.37634859526991415, Learning Rate - 0.00625, magnitude of gradient - 0.053255848015069766\n",
      "Step - 3044, Loss - 0.3921879428027488, Learning Rate - 0.00625, magnitude of gradient - 0.0799568491956643\n",
      "Step - 3045, Loss - 0.24974471360936384, Learning Rate - 0.00625, magnitude of gradient - 0.06062440345853146\n",
      "Step - 3046, Loss - 0.3018792048989436, Learning Rate - 0.00625, magnitude of gradient - 0.04514225245907884\n",
      "Step - 3047, Loss - 0.2951763482138175, Learning Rate - 0.00625, magnitude of gradient - 0.07063016915673052\n",
      "Step - 3048, Loss - 0.2851904439733115, Learning Rate - 0.00625, magnitude of gradient - 0.01398858791580075\n",
      "Step - 3049, Loss - 0.3480230286297672, Learning Rate - 0.00625, magnitude of gradient - 0.04261885125997988\n",
      "Step - 3050, Loss - 0.2883142047331494, Learning Rate - 0.00625, magnitude of gradient - 0.032810752441247336\n",
      "Step - 3051, Loss - 0.2807492209870258, Learning Rate - 0.00625, magnitude of gradient - 0.012412205297313161\n",
      "Step - 3052, Loss - 0.3184166031501507, Learning Rate - 0.00625, magnitude of gradient - 0.043522677062040674\n",
      "Step - 3053, Loss - 0.30573376752500203, Learning Rate - 0.00625, magnitude of gradient - 0.05096230150320042\n",
      "Step - 3054, Loss - 0.23659966607734223, Learning Rate - 0.00625, magnitude of gradient - 0.02220089628161659\n",
      "Step - 3055, Loss - 0.3012557618881785, Learning Rate - 0.00625, magnitude of gradient - 0.030760500759980162\n",
      "Step - 3056, Loss - 0.29944237407438934, Learning Rate - 0.00625, magnitude of gradient - 0.04517656419098885\n",
      "Step - 3057, Loss - 0.2902391061707472, Learning Rate - 0.00625, magnitude of gradient - 0.08256139097908605\n",
      "Step - 3058, Loss - 0.4075083070644165, Learning Rate - 0.00625, magnitude of gradient - 0.012534104015554227\n",
      "Step - 3059, Loss - 0.3624934383261259, Learning Rate - 0.00625, magnitude of gradient - 0.026374518070082637\n",
      "Step - 3060, Loss - 0.3287666196403276, Learning Rate - 0.00625, magnitude of gradient - 0.07186989874793298\n",
      "Step - 3061, Loss - 0.3155312377227182, Learning Rate - 0.00625, magnitude of gradient - 0.0749156368460425\n",
      "Step - 3062, Loss - 0.3109693203573985, Learning Rate - 0.00625, magnitude of gradient - 0.06714222939530755\n",
      "Step - 3063, Loss - 0.28188263057389423, Learning Rate - 0.00625, magnitude of gradient - 0.0357220588647873\n",
      "Step - 3064, Loss - 0.3322483956857766, Learning Rate - 0.00625, magnitude of gradient - 0.04826785557962789\n",
      "Step - 3065, Loss - 0.24264239104861599, Learning Rate - 0.00625, magnitude of gradient - 0.015053158938922175\n",
      "Step - 3066, Loss - 0.46689590635256806, Learning Rate - 0.00625, magnitude of gradient - 0.06786165468615686\n",
      "Step - 3067, Loss - 0.29892293238418344, Learning Rate - 0.00625, magnitude of gradient - 0.062402137841435126\n",
      "Step - 3068, Loss - 0.3523309150776174, Learning Rate - 0.00625, magnitude of gradient - 0.09439450785019673\n",
      "Step - 3069, Loss - 0.318068505867153, Learning Rate - 0.00625, magnitude of gradient - 0.021964440235124373\n",
      "Step - 3070, Loss - 0.3689592473453907, Learning Rate - 0.00625, magnitude of gradient - 0.12275969155388948\n",
      "Step - 3071, Loss - 0.36784476556593276, Learning Rate - 0.00625, magnitude of gradient - 0.031041984347311966\n",
      "Step - 3072, Loss - 0.2913484629848774, Learning Rate - 0.00625, magnitude of gradient - 0.029580743084315873\n",
      "Step - 3073, Loss - 0.3328637854340249, Learning Rate - 0.00625, magnitude of gradient - 0.05867531889018524\n",
      "Step - 3074, Loss - 0.30200018813795704, Learning Rate - 0.00625, magnitude of gradient - 0.11400402935601123\n",
      "Step - 3075, Loss - 0.28127646856393834, Learning Rate - 0.00625, magnitude of gradient - 0.08262008369243024\n",
      "Step - 3076, Loss - 0.3170534990612556, Learning Rate - 0.00625, magnitude of gradient - 0.03745989011662973\n",
      "Step - 3077, Loss - 0.28492314791644113, Learning Rate - 0.00625, magnitude of gradient - 0.04042941498489348\n",
      "Step - 3078, Loss - 0.31177701451600953, Learning Rate - 0.00625, magnitude of gradient - 0.05316077122374581\n",
      "Step - 3079, Loss - 0.3729567283645616, Learning Rate - 0.00625, magnitude of gradient - 0.044369795888219876\n",
      "Step - 3080, Loss - 0.35477911377550914, Learning Rate - 0.00625, magnitude of gradient - 0.06801329715664078\n",
      "Step - 3081, Loss - 0.2673433850288108, Learning Rate - 0.00625, magnitude of gradient - 0.019896710549419612\n",
      "Step - 3082, Loss - 0.36873476188939286, Learning Rate - 0.00625, magnitude of gradient - 0.04850462993721209\n",
      "Step - 3083, Loss - 0.3875549826378854, Learning Rate - 0.00625, magnitude of gradient - 0.05353060275448119\n",
      "Step - 3084, Loss - 0.3338578254234177, Learning Rate - 0.00625, magnitude of gradient - 0.01231297222658068\n",
      "Step - 3085, Loss - 0.2826066651938749, Learning Rate - 0.00625, magnitude of gradient - 0.08146441722584398\n",
      "Step - 3086, Loss - 0.30423489345449056, Learning Rate - 0.00625, magnitude of gradient - 0.08428452934459431\n",
      "Step - 3087, Loss - 0.26098895700000746, Learning Rate - 0.00625, magnitude of gradient - 0.07627722470473519\n",
      "Step - 3088, Loss - 0.3445755610734358, Learning Rate - 0.00625, magnitude of gradient - 0.04016332116303212\n",
      "Step - 3089, Loss - 0.3302836473948293, Learning Rate - 0.00625, magnitude of gradient - 0.07979908299487215\n",
      "Step - 3090, Loss - 0.35330030252566497, Learning Rate - 0.00625, magnitude of gradient - 0.04800629673201858\n",
      "Step - 3091, Loss - 0.3343548917907011, Learning Rate - 0.00625, magnitude of gradient - 0.05291581813606403\n",
      "Step - 3092, Loss - 0.34078636935016665, Learning Rate - 0.00625, magnitude of gradient - 0.056562682168282585\n",
      "Step - 3093, Loss - 0.3168730578316423, Learning Rate - 0.00625, magnitude of gradient - 0.05558895630211872\n",
      "Step - 3094, Loss - 0.3403345647230416, Learning Rate - 0.00625, magnitude of gradient - 0.02743363668449646\n",
      "Step - 3095, Loss - 0.3267276042924216, Learning Rate - 0.00625, magnitude of gradient - 0.01915511014567316\n",
      "Step - 3096, Loss - 0.3075307593007115, Learning Rate - 0.00625, magnitude of gradient - 0.02001467806378025\n",
      "Step - 3097, Loss - 0.34536398958492753, Learning Rate - 0.00625, magnitude of gradient - 0.06176903544831934\n",
      "Step - 3098, Loss - 0.3894853775973391, Learning Rate - 0.00625, magnitude of gradient - 0.0628539585124761\n",
      "Step - 3099, Loss - 0.3912706801117233, Learning Rate - 0.00625, magnitude of gradient - 0.09008924839210705\n",
      "Step - 3100, Loss - 0.3286207001122388, Learning Rate - 0.00625, magnitude of gradient - 0.025373869569262342\n",
      "Step - 3101, Loss - 0.34565066281733087, Learning Rate - 0.00625, magnitude of gradient - 0.07944589857787604\n",
      "Step - 3102, Loss - 0.35150959031755136, Learning Rate - 0.00625, magnitude of gradient - 0.03161024162644141\n",
      "Step - 3103, Loss - 0.3811599188732168, Learning Rate - 0.00625, magnitude of gradient - 0.024861533189321564\n",
      "Step - 3104, Loss - 0.29508405642775926, Learning Rate - 0.00625, magnitude of gradient - 0.03145840173091477\n",
      "Step - 3105, Loss - 0.38194157933914885, Learning Rate - 0.00625, magnitude of gradient - 0.06101752631215576\n",
      "Step - 3106, Loss - 0.3691979702002069, Learning Rate - 0.00625, magnitude of gradient - 0.1108634406838582\n",
      "Step - 3107, Loss - 0.38017298975524233, Learning Rate - 0.00625, magnitude of gradient - 0.024424958174648265\n",
      "Step - 3108, Loss - 0.29668625438300195, Learning Rate - 0.00625, magnitude of gradient - 0.02610058679799193\n",
      "Step - 3109, Loss - 0.386877790931815, Learning Rate - 0.00625, magnitude of gradient - 0.05115419560768866\n",
      "Step - 3110, Loss - 0.3067045202335711, Learning Rate - 0.00625, magnitude of gradient - 0.049682473823761196\n",
      "Step - 3111, Loss - 0.37353809941774574, Learning Rate - 0.00625, magnitude of gradient - 0.05910483873077934\n",
      "Step - 3112, Loss - 0.31081381288648224, Learning Rate - 0.00625, magnitude of gradient - 0.05487016971407884\n",
      "Step - 3113, Loss - 0.3337036797160136, Learning Rate - 0.00625, magnitude of gradient - 0.07661938344776757\n",
      "Step - 3114, Loss - 0.33544989371878, Learning Rate - 0.00625, magnitude of gradient - 0.08947066260330759\n",
      "Step - 3115, Loss - 0.34663428671676366, Learning Rate - 0.00625, magnitude of gradient - 0.05030341359469587\n",
      "Step - 3116, Loss - 0.3230604713022009, Learning Rate - 0.00625, magnitude of gradient - 0.09002581616266524\n",
      "Step - 3117, Loss - 0.3351666529973579, Learning Rate - 0.00625, magnitude of gradient - 0.027288074652011534\n",
      "Step - 3118, Loss - 0.26829952665650925, Learning Rate - 0.00625, magnitude of gradient - 0.05216077997115478\n",
      "Step - 3119, Loss - 0.3391636395508677, Learning Rate - 0.00625, magnitude of gradient - 0.052776933724212605\n",
      "Step - 3120, Loss - 0.3020117303255244, Learning Rate - 0.00625, magnitude of gradient - 0.0510255285811962\n",
      "Step - 3121, Loss - 0.30833409529991457, Learning Rate - 0.00625, magnitude of gradient - 0.0044553496689979715\n",
      "Step - 3122, Loss - 0.24101736458132522, Learning Rate - 0.00625, magnitude of gradient - 0.022479187506317378\n",
      "Step - 3123, Loss - 0.2725908289106642, Learning Rate - 0.00625, magnitude of gradient - 0.027449688961523624\n",
      "Step - 3124, Loss - 0.30565836491996684, Learning Rate - 0.00625, magnitude of gradient - 0.040412782408929745\n",
      "Step - 3125, Loss - 0.2498431293423173, Learning Rate - 0.00625, magnitude of gradient - 0.031829486584472745\n",
      "Step - 3126, Loss - 0.3098150547666338, Learning Rate - 0.00625, magnitude of gradient - 0.04358020971456875\n",
      "Step - 3127, Loss - 0.2986036828679944, Learning Rate - 0.00625, magnitude of gradient - 0.06838066203953959\n",
      "Step - 3128, Loss - 0.29546891751453264, Learning Rate - 0.00625, magnitude of gradient - 0.0668185886880501\n",
      "Step - 3129, Loss - 0.31802494213215693, Learning Rate - 0.00625, magnitude of gradient - 0.10196320562296345\n",
      "Step - 3130, Loss - 0.3204373180840482, Learning Rate - 0.00625, magnitude of gradient - 0.028786857310761207\n",
      "Step - 3131, Loss - 0.373091584762405, Learning Rate - 0.00625, magnitude of gradient - 0.06118732354174398\n",
      "Step - 3132, Loss - 0.3164131978512922, Learning Rate - 0.00625, magnitude of gradient - 0.06911097012377243\n",
      "Step - 3133, Loss - 0.31603414088080406, Learning Rate - 0.00625, magnitude of gradient - 0.0668571225319463\n",
      "Step - 3134, Loss - 0.32700048064894105, Learning Rate - 0.00625, magnitude of gradient - 0.05378975538360423\n",
      "Step - 3135, Loss - 0.3014872592422636, Learning Rate - 0.00625, magnitude of gradient - 0.035762241230099676\n",
      "Step - 3136, Loss - 0.25765724236290355, Learning Rate - 0.00625, magnitude of gradient - 0.05464899265823208\n",
      "Step - 3137, Loss - 0.3844387725698391, Learning Rate - 0.00625, magnitude of gradient - 0.0026602594518629623\n",
      "Step - 3138, Loss - 0.2699976680782037, Learning Rate - 0.00625, magnitude of gradient - 0.037799364895919736\n",
      "Step - 3139, Loss - 0.25504694740074435, Learning Rate - 0.00625, magnitude of gradient - 0.06232032842564476\n",
      "Step - 3140, Loss - 0.30220762052014355, Learning Rate - 0.00625, magnitude of gradient - 0.01694147392777342\n",
      "Step - 3141, Loss - 0.3447119664885857, Learning Rate - 0.00625, magnitude of gradient - 0.08943847019189687\n",
      "Step - 3142, Loss - 0.27555024441241865, Learning Rate - 0.00625, magnitude of gradient - 0.05429967713245476\n",
      "Step - 3143, Loss - 0.34280537078549606, Learning Rate - 0.00625, magnitude of gradient - 0.05373823271201907\n",
      "Step - 3144, Loss - 0.29060458917126325, Learning Rate - 0.00625, magnitude of gradient - 0.008641730700236053\n",
      "Step - 3145, Loss - 0.32540597484357553, Learning Rate - 0.00625, magnitude of gradient - 0.011822264697764831\n",
      "Step - 3146, Loss - 0.3527452171196933, Learning Rate - 0.00625, magnitude of gradient - 0.05289401413924937\n",
      "Step - 3147, Loss - 0.3426641894838567, Learning Rate - 0.00625, magnitude of gradient - 0.06022669776283797\n",
      "Step - 3148, Loss - 0.3139202660945496, Learning Rate - 0.00625, magnitude of gradient - 0.01727692794913197\n",
      "Step - 3149, Loss - 0.3863666155040089, Learning Rate - 0.00625, magnitude of gradient - 0.04903886568187628\n",
      "Step - 3150, Loss - 0.2936017464801022, Learning Rate - 0.00625, magnitude of gradient - 0.053487197558246505\n",
      "Step - 3151, Loss - 0.29536752079616546, Learning Rate - 0.00625, magnitude of gradient - 0.03207258950051325\n",
      "Step - 3152, Loss - 0.3388445751978412, Learning Rate - 0.00625, magnitude of gradient - 0.09906592068164566\n",
      "Step - 3153, Loss - 0.3137281176957224, Learning Rate - 0.00625, magnitude of gradient - 0.06699685206470221\n",
      "Step - 3154, Loss - 0.4067033089669351, Learning Rate - 0.00625, magnitude of gradient - 0.043286451442350785\n",
      "Step - 3155, Loss - 0.34496240438211667, Learning Rate - 0.00625, magnitude of gradient - 0.020594691027670396\n",
      "Step - 3156, Loss - 0.38460162350721316, Learning Rate - 0.00625, magnitude of gradient - 0.031637920914657\n",
      "Step - 3157, Loss - 0.28498814288733754, Learning Rate - 0.00625, magnitude of gradient - 0.055326383865440074\n",
      "Step - 3158, Loss - 0.2667403642903593, Learning Rate - 0.00625, magnitude of gradient - 0.03301189049585356\n",
      "Step - 3159, Loss - 0.32514686314155333, Learning Rate - 0.00625, magnitude of gradient - 0.05126564004990924\n",
      "Step - 3160, Loss - 0.2974012463166999, Learning Rate - 0.00625, magnitude of gradient - 0.11287920299768593\n",
      "Step - 3161, Loss - 0.3406272242579327, Learning Rate - 0.00625, magnitude of gradient - 0.04542023931918433\n",
      "Step - 3162, Loss - 0.23888997092815908, Learning Rate - 0.00625, magnitude of gradient - 0.04231342544763559\n",
      "Step - 3163, Loss - 0.3595052115559422, Learning Rate - 0.00625, magnitude of gradient - 0.08450596471802198\n",
      "Step - 3164, Loss - 0.28156436743580054, Learning Rate - 0.00625, magnitude of gradient - 0.032186670172482244\n",
      "Step - 3165, Loss - 0.38008099393779693, Learning Rate - 0.00625, magnitude of gradient - 0.03690364398584179\n",
      "Step - 3166, Loss - 0.2930854661858185, Learning Rate - 0.00625, magnitude of gradient - 0.04369584520870269\n",
      "Step - 3167, Loss - 0.31315079006919366, Learning Rate - 0.00625, magnitude of gradient - 0.04187987010733984\n",
      "Step - 3168, Loss - 0.32638577937978486, Learning Rate - 0.00625, magnitude of gradient - 0.061801682692518484\n",
      "Step - 3169, Loss - 0.2802041837362538, Learning Rate - 0.00625, magnitude of gradient - 0.025546071809430533\n",
      "Step - 3170, Loss - 0.3054221087236446, Learning Rate - 0.00625, magnitude of gradient - 0.0332051190806068\n",
      "Step - 3171, Loss - 0.33322530913770193, Learning Rate - 0.00625, magnitude of gradient - 0.01236400770935934\n",
      "Step - 3172, Loss - 0.3594126736939547, Learning Rate - 0.00625, magnitude of gradient - 0.09605981408494838\n",
      "Step - 3173, Loss - 0.29134427377238953, Learning Rate - 0.00625, magnitude of gradient - 0.07753539556549048\n",
      "Step - 3174, Loss - 0.31356241430491694, Learning Rate - 0.00625, magnitude of gradient - 0.03716603097159\n",
      "Step - 3175, Loss - 0.39820082684315455, Learning Rate - 0.00625, magnitude of gradient - 0.04511988368133207\n",
      "Step - 3176, Loss - 0.3397544624629786, Learning Rate - 0.00625, magnitude of gradient - 0.07077717419953095\n",
      "Step - 3177, Loss - 0.30833724152838, Learning Rate - 0.00625, magnitude of gradient - 0.05633706844394709\n",
      "Step - 3178, Loss - 0.35244111288351176, Learning Rate - 0.00625, magnitude of gradient - 0.05406254408488873\n",
      "Step - 3179, Loss - 0.26376869048497253, Learning Rate - 0.00625, magnitude of gradient - 0.0637118715269931\n",
      "Step - 3180, Loss - 0.28041647230425326, Learning Rate - 0.00625, magnitude of gradient - 0.049648413955696635\n",
      "Step - 3181, Loss - 0.3429599027618503, Learning Rate - 0.00625, magnitude of gradient - 0.0132120468522104\n",
      "Step - 3182, Loss - 0.3760330443045702, Learning Rate - 0.00625, magnitude of gradient - 0.03556152823369454\n",
      "Step - 3183, Loss - 0.3735973782045996, Learning Rate - 0.00625, magnitude of gradient - 0.04116671132329459\n",
      "Step - 3184, Loss - 0.3577377245864769, Learning Rate - 0.00625, magnitude of gradient - 0.056180471062518396\n",
      "Step - 3185, Loss - 0.30600241984519433, Learning Rate - 0.00625, magnitude of gradient - 0.07636707947108355\n",
      "Step - 3186, Loss - 0.2356283510329124, Learning Rate - 0.00625, magnitude of gradient - 0.06991064661359479\n",
      "Step - 3187, Loss - 0.2853481344493601, Learning Rate - 0.00625, magnitude of gradient - 0.03986114658123236\n",
      "Step - 3188, Loss - 0.33402184290093245, Learning Rate - 0.00625, magnitude of gradient - 0.05828274197521099\n",
      "Step - 3189, Loss - 0.3323236964927464, Learning Rate - 0.00625, magnitude of gradient - 0.024729037764366305\n",
      "Step - 3190, Loss - 0.2814034607543379, Learning Rate - 0.00625, magnitude of gradient - 0.04690686480255914\n",
      "Step - 3191, Loss - 0.2995973382575271, Learning Rate - 0.00625, magnitude of gradient - 0.05559135520320167\n",
      "Step - 3192, Loss - 0.3667890058174446, Learning Rate - 0.00625, magnitude of gradient - 0.06395195706848908\n",
      "Step - 3193, Loss - 0.41371085630141435, Learning Rate - 0.00625, magnitude of gradient - 0.03394504292085047\n",
      "Step - 3194, Loss - 0.34169132383184136, Learning Rate - 0.00625, magnitude of gradient - 0.015822211087678235\n",
      "Step - 3195, Loss - 0.33819549290732287, Learning Rate - 0.00625, magnitude of gradient - 0.0299057165180196\n",
      "Step - 3196, Loss - 0.38040788895287253, Learning Rate - 0.00625, magnitude of gradient - 0.034632202583371345\n",
      "Step - 3197, Loss - 0.282509799860665, Learning Rate - 0.00625, magnitude of gradient - 0.09308539406620549\n",
      "Step - 3198, Loss - 0.23557070012000547, Learning Rate - 0.00625, magnitude of gradient - 0.09454457454449827\n",
      "Step - 3199, Loss - 0.3120113042719207, Learning Rate - 0.00625, magnitude of gradient - 0.08332516661819579\n",
      "Step - 3200, Loss - 0.3215362163431885, Learning Rate - 0.00625, magnitude of gradient - 0.07748207473458762\n",
      "Step - 3201, Loss - 0.3585301778963169, Learning Rate - 0.00625, magnitude of gradient - 0.10957942895019053\n",
      "Step - 3202, Loss - 0.37830255567285664, Learning Rate - 0.00625, magnitude of gradient - 0.010051196283025685\n",
      "Step - 3203, Loss - 0.36704414425484844, Learning Rate - 0.00625, magnitude of gradient - 0.10575923353699587\n",
      "Step - 3204, Loss - 0.3486183527718616, Learning Rate - 0.00625, magnitude of gradient - 0.020074766971415796\n",
      "Step - 3205, Loss - 0.31933977869404584, Learning Rate - 0.00625, magnitude of gradient - 0.1046305501351391\n",
      "Step - 3206, Loss - 0.3868746415427329, Learning Rate - 0.00625, magnitude of gradient - 0.0423680283597168\n",
      "Step - 3207, Loss - 0.34108354691568, Learning Rate - 0.00625, magnitude of gradient - 0.06255046069283114\n",
      "Step - 3208, Loss - 0.29442041976837074, Learning Rate - 0.00625, magnitude of gradient - 0.04356305271329751\n",
      "Step - 3209, Loss - 0.2220792247846383, Learning Rate - 0.00625, magnitude of gradient - 0.12490841737127269\n",
      "Step - 3210, Loss - 0.25655956225340093, Learning Rate - 0.00625, magnitude of gradient - 0.07909840345553236\n",
      "Step - 3211, Loss - 0.3278178980029187, Learning Rate - 0.00625, magnitude of gradient - 0.0672308760351019\n",
      "Step - 3212, Loss - 0.29357740387777126, Learning Rate - 0.00625, magnitude of gradient - 0.03736658079706313\n",
      "Step - 3213, Loss - 0.26984647003230694, Learning Rate - 0.00625, magnitude of gradient - 0.02722980731202179\n",
      "Step - 3214, Loss - 0.21525203236546991, Learning Rate - 0.00625, magnitude of gradient - 0.04451091962230191\n",
      "Step - 3215, Loss - 0.37514594844210125, Learning Rate - 0.00625, magnitude of gradient - 0.07579250975286217\n",
      "Step - 3216, Loss - 0.2808746728896216, Learning Rate - 0.00625, magnitude of gradient - 0.054071521175428\n",
      "Step - 3217, Loss - 0.3739193625803241, Learning Rate - 0.00625, magnitude of gradient - 0.041573900600353327\n",
      "Step - 3218, Loss - 0.32572423243688764, Learning Rate - 0.00625, magnitude of gradient - 0.03149849083651768\n",
      "Step - 3219, Loss - 0.27761870698565494, Learning Rate - 0.00625, magnitude of gradient - 0.05410566868276136\n",
      "Step - 3220, Loss - 0.39131850006673236, Learning Rate - 0.00625, magnitude of gradient - 0.05922561055918731\n",
      "Step - 3221, Loss - 0.33010266976571784, Learning Rate - 0.00625, magnitude of gradient - 0.06689061720584678\n",
      "Step - 3222, Loss - 0.3322564385763369, Learning Rate - 0.00625, magnitude of gradient - 0.04198150767421666\n",
      "Step - 3223, Loss - 0.34561419078189887, Learning Rate - 0.00625, magnitude of gradient - 0.08168723800572719\n",
      "Step - 3224, Loss - 0.37337129276392256, Learning Rate - 0.00625, magnitude of gradient - 0.049249359361753814\n",
      "Step - 3225, Loss - 0.31200084001749856, Learning Rate - 0.00625, magnitude of gradient - 0.06367055854945036\n",
      "Step - 3226, Loss - 0.3903242761758071, Learning Rate - 0.00625, magnitude of gradient - 0.04641322873715715\n",
      "Step - 3227, Loss - 0.31956024817116824, Learning Rate - 0.00625, magnitude of gradient - 0.10311373533920981\n",
      "Step - 3228, Loss - 0.36944425766867295, Learning Rate - 0.00625, magnitude of gradient - 0.011511702396687211\n",
      "Step - 3229, Loss - 0.28761346387359804, Learning Rate - 0.00625, magnitude of gradient - 0.05731057721217084\n",
      "Step - 3230, Loss - 0.3627917820865324, Learning Rate - 0.00625, magnitude of gradient - 0.045049913251860634\n",
      "Step - 3231, Loss - 0.2776380915127892, Learning Rate - 0.00625, magnitude of gradient - 0.07037433381763165\n",
      "Step - 3232, Loss - 0.2664038829581427, Learning Rate - 0.00625, magnitude of gradient - 0.017289616064808104\n",
      "Step - 3233, Loss - 0.2684667094342151, Learning Rate - 0.00625, magnitude of gradient - 0.02768992396163776\n",
      "Step - 3234, Loss - 0.2678560655108282, Learning Rate - 0.00625, magnitude of gradient - 0.06615105088135861\n",
      "Step - 3235, Loss - 0.3430604194657807, Learning Rate - 0.00625, magnitude of gradient - 0.06571381791600928\n",
      "Step - 3236, Loss - 0.24334687778540753, Learning Rate - 0.00625, magnitude of gradient - 0.0459531280620416\n",
      "Step - 3237, Loss - 0.29607361623280976, Learning Rate - 0.00625, magnitude of gradient - 0.04487094255056953\n",
      "Step - 3238, Loss - 0.34214786030876254, Learning Rate - 0.00625, magnitude of gradient - 0.023092089969935437\n",
      "Step - 3239, Loss - 0.35251363284815873, Learning Rate - 0.00625, magnitude of gradient - 0.03916927454024365\n",
      "Step - 3240, Loss - 0.3387973751402897, Learning Rate - 0.00625, magnitude of gradient - 0.07749783122634346\n",
      "Step - 3241, Loss - 0.36819378414829174, Learning Rate - 0.00625, magnitude of gradient - 0.05084936524357172\n",
      "Step - 3242, Loss - 0.28416346792539243, Learning Rate - 0.00625, magnitude of gradient - 0.02216077849891028\n",
      "Step - 3243, Loss - 0.36560424134537034, Learning Rate - 0.00625, magnitude of gradient - 0.05115047264479764\n",
      "Step - 3244, Loss - 0.3753642709418096, Learning Rate - 0.00625, magnitude of gradient - 0.04116178324214282\n",
      "Step - 3245, Loss - 0.23618595102350792, Learning Rate - 0.00625, magnitude of gradient - 0.03418430167486808\n",
      "Step - 3246, Loss - 0.2988422338617057, Learning Rate - 0.00625, magnitude of gradient - 0.021912341994118786\n",
      "Step - 3247, Loss - 0.3149828811138085, Learning Rate - 0.00625, magnitude of gradient - 0.04897055678511663\n",
      "Step - 3248, Loss - 0.32164672406833855, Learning Rate - 0.00625, magnitude of gradient - 0.054195108767239576\n",
      "Step - 3249, Loss - 0.3233045440966122, Learning Rate - 0.00625, magnitude of gradient - 0.02052790144769647\n",
      "Step - 3250, Loss - 0.37016021046791214, Learning Rate - 0.00625, magnitude of gradient - 0.028062644771224207\n",
      "Step - 3251, Loss - 0.2722609115191502, Learning Rate - 0.00625, magnitude of gradient - 0.020166366930681627\n",
      "Step - 3252, Loss - 0.41708009116451056, Learning Rate - 0.00625, magnitude of gradient - 0.06820528712178928\n",
      "Step - 3253, Loss - 0.28832065593195, Learning Rate - 0.00625, magnitude of gradient - 0.058774231609078284\n",
      "Step - 3254, Loss - 0.28697464732712585, Learning Rate - 0.00625, magnitude of gradient - 0.09602123862086352\n",
      "Step - 3255, Loss - 0.3331984518287734, Learning Rate - 0.00625, magnitude of gradient - 0.026899330869637694\n",
      "Step - 3256, Loss - 0.39504470884299825, Learning Rate - 0.00625, magnitude of gradient - 0.05397517774534563\n",
      "Step - 3257, Loss - 0.285407147058523, Learning Rate - 0.00625, magnitude of gradient - 0.10018776886114934\n",
      "Step - 3258, Loss - 0.33182957386262435, Learning Rate - 0.00625, magnitude of gradient - 0.02752534616648936\n",
      "Step - 3259, Loss - 0.2576995936245711, Learning Rate - 0.00625, magnitude of gradient - 0.02028911401142647\n",
      "Step - 3260, Loss - 0.33748804345207, Learning Rate - 0.00625, magnitude of gradient - 0.06950628613005014\n",
      "Step - 3261, Loss - 0.36126723795373455, Learning Rate - 0.00625, magnitude of gradient - 0.04431971889494107\n",
      "Step - 3262, Loss - 0.3697304323816679, Learning Rate - 0.00625, magnitude of gradient - 0.03646244870020937\n",
      "Step - 3263, Loss - 0.3370542932196704, Learning Rate - 0.00625, magnitude of gradient - 0.05540306838370112\n",
      "Step - 3264, Loss - 0.28039264918851203, Learning Rate - 0.00625, magnitude of gradient - 0.04449825135561687\n",
      "Step - 3265, Loss - 0.33456638592621457, Learning Rate - 0.00625, magnitude of gradient - 0.05495524814113762\n",
      "Step - 3266, Loss - 0.30624869284086464, Learning Rate - 0.00625, magnitude of gradient - 0.051717982435298196\n",
      "Step - 3267, Loss - 0.2966520701577223, Learning Rate - 0.00625, magnitude of gradient - 0.033213140297243024\n",
      "Step - 3268, Loss - 0.3882067063595516, Learning Rate - 0.00625, magnitude of gradient - 0.06399431897337507\n",
      "Step - 3269, Loss - 0.2930970023054199, Learning Rate - 0.00625, magnitude of gradient - 0.022098018847503065\n",
      "Step - 3270, Loss - 0.26874187992590826, Learning Rate - 0.00625, magnitude of gradient - 0.06688492242040968\n",
      "Step - 3271, Loss - 0.3540479061669126, Learning Rate - 0.00625, magnitude of gradient - 0.0751484387306103\n",
      "Step - 3272, Loss - 0.3711046155123986, Learning Rate - 0.00625, magnitude of gradient - 0.021397321735531435\n",
      "Step - 3273, Loss - 0.3433487688619526, Learning Rate - 0.00625, magnitude of gradient - 0.04455295447005307\n",
      "Step - 3274, Loss - 0.2781380248613806, Learning Rate - 0.00625, magnitude of gradient - 0.017712512970826377\n",
      "Step - 3275, Loss - 0.4280229942197166, Learning Rate - 0.00625, magnitude of gradient - 0.04802585845849318\n",
      "Step - 3276, Loss - 0.3841356350479862, Learning Rate - 0.00625, magnitude of gradient - 0.02828090051733719\n",
      "Step - 3277, Loss - 0.2781219915123752, Learning Rate - 0.00625, magnitude of gradient - 0.05319197437399152\n",
      "Step - 3278, Loss - 0.35625504295762056, Learning Rate - 0.00625, magnitude of gradient - 0.10446314157623204\n",
      "Step - 3279, Loss - 0.36734965231648875, Learning Rate - 0.00625, magnitude of gradient - 0.060231875398002656\n",
      "Step - 3280, Loss - 0.2906479144497598, Learning Rate - 0.00625, magnitude of gradient - 0.05066458429143664\n",
      "Step - 3281, Loss - 0.24703877099798496, Learning Rate - 0.00625, magnitude of gradient - 0.0802443393203022\n",
      "Step - 3282, Loss - 0.33297887326827597, Learning Rate - 0.00625, magnitude of gradient - 0.06337957979242714\n",
      "Step - 3283, Loss - 0.31881518201307857, Learning Rate - 0.00625, magnitude of gradient - 0.029497416070784262\n",
      "Step - 3284, Loss - 0.32224608216810124, Learning Rate - 0.00625, magnitude of gradient - 0.03378016892557291\n",
      "Step - 3285, Loss - 0.2816442005985439, Learning Rate - 0.00625, magnitude of gradient - 0.08288843768907646\n",
      "Step - 3286, Loss - 0.24738381184730573, Learning Rate - 0.00625, magnitude of gradient - 0.04798463935477221\n",
      "Step - 3287, Loss - 0.3244465506864491, Learning Rate - 0.00625, magnitude of gradient - 0.05639636259528662\n",
      "Step - 3288, Loss - 0.31209241391016584, Learning Rate - 0.00625, magnitude of gradient - 0.03976821019648662\n",
      "Step - 3289, Loss - 0.3080298630786223, Learning Rate - 0.00625, magnitude of gradient - 0.03158841563475508\n",
      "Step - 3290, Loss - 0.3716497352247032, Learning Rate - 0.00625, magnitude of gradient - 0.06680222641503512\n",
      "Step - 3291, Loss - 0.3840134088632824, Learning Rate - 0.00625, magnitude of gradient - 0.05606900734032457\n",
      "Step - 3292, Loss - 0.29804673383885416, Learning Rate - 0.00625, magnitude of gradient - 0.052916894986368035\n",
      "Step - 3293, Loss - 0.2626104740310512, Learning Rate - 0.00625, magnitude of gradient - 0.059340879621492426\n",
      "Step - 3294, Loss - 0.33393774301168444, Learning Rate - 0.00625, magnitude of gradient - 0.06937976812169303\n",
      "Step - 3295, Loss - 0.28165828570654994, Learning Rate - 0.00625, magnitude of gradient - 0.011642534988464522\n",
      "Step - 3296, Loss - 0.2840937328728551, Learning Rate - 0.00625, magnitude of gradient - 0.06906214457800104\n",
      "Step - 3297, Loss - 0.29630854049140276, Learning Rate - 0.00625, magnitude of gradient - 0.035846306771128794\n",
      "Step - 3298, Loss - 0.4080074851370735, Learning Rate - 0.00625, magnitude of gradient - 0.05123194854256495\n",
      "Step - 3299, Loss - 0.2712219757813277, Learning Rate - 0.00625, magnitude of gradient - 0.08071514143374948\n",
      "Step - 3300, Loss - 0.23732921149801994, Learning Rate - 0.00625, magnitude of gradient - 0.04575134987570823\n",
      "Step - 3301, Loss - 0.3092650592632301, Learning Rate - 0.00625, magnitude of gradient - 0.13726183335956096\n",
      "Step - 3302, Loss - 0.34486893966922455, Learning Rate - 0.00625, magnitude of gradient - 0.08439322729330115\n",
      "Step - 3303, Loss - 0.4194802316743801, Learning Rate - 0.00625, magnitude of gradient - 0.07999804894920456\n",
      "Step - 3304, Loss - 0.36533520426687816, Learning Rate - 0.00625, magnitude of gradient - 0.06958076416703549\n",
      "Step - 3305, Loss - 0.3149529706647257, Learning Rate - 0.00625, magnitude of gradient - 0.041821065441901194\n",
      "Step - 3306, Loss - 0.29773803116711417, Learning Rate - 0.00625, magnitude of gradient - 0.06389307404859564\n",
      "Step - 3307, Loss - 0.3584171416142367, Learning Rate - 0.00625, magnitude of gradient - 0.05006571862508459\n",
      "Step - 3308, Loss - 0.327844813356752, Learning Rate - 0.00625, magnitude of gradient - 0.050676116494662415\n",
      "Step - 3309, Loss - 0.3414962968301671, Learning Rate - 0.00625, magnitude of gradient - 0.06155886069137842\n",
      "Step - 3310, Loss - 0.3049089058532054, Learning Rate - 0.00625, magnitude of gradient - 0.1033969781617667\n",
      "Step - 3311, Loss - 0.293921152139614, Learning Rate - 0.00625, magnitude of gradient - 0.03714909675512222\n",
      "Step - 3312, Loss - 0.3279049774830063, Learning Rate - 0.00625, magnitude of gradient - 0.07370972045828425\n",
      "Step - 3313, Loss - 0.29112355149421804, Learning Rate - 0.00625, magnitude of gradient - 0.06049941515465166\n",
      "Step - 3314, Loss - 0.34015954989657887, Learning Rate - 0.00625, magnitude of gradient - 0.07681738786164498\n",
      "Step - 3315, Loss - 0.2985176205268833, Learning Rate - 0.00625, magnitude of gradient - 0.029310345639883276\n",
      "Step - 3316, Loss - 0.3816344949447928, Learning Rate - 0.00625, magnitude of gradient - 0.014201349094234927\n",
      "Step - 3317, Loss - 0.32705592519741467, Learning Rate - 0.00625, magnitude of gradient - 0.09828652552125357\n",
      "Step - 3318, Loss - 0.277506123393311, Learning Rate - 0.00625, magnitude of gradient - 0.01490326631696845\n",
      "Step - 3319, Loss - 0.33599782326116534, Learning Rate - 0.00625, magnitude of gradient - 0.02119014638153817\n",
      "Step - 3320, Loss - 0.3219485663934527, Learning Rate - 0.00625, magnitude of gradient - 0.03166482361899914\n",
      "Step - 3321, Loss - 0.3236256855617135, Learning Rate - 0.00625, magnitude of gradient - 0.030120581979433698\n",
      "Step - 3322, Loss - 0.301899178384262, Learning Rate - 0.00625, magnitude of gradient - 0.055117383358012535\n",
      "Step - 3323, Loss - 0.32266400393911354, Learning Rate - 0.00625, magnitude of gradient - 0.030760524274708777\n",
      "Step - 3324, Loss - 0.27459331865566416, Learning Rate - 0.00625, magnitude of gradient - 0.01672712002788939\n",
      "Step - 3325, Loss - 0.26616397139255976, Learning Rate - 0.00625, magnitude of gradient - 0.03225494555742722\n",
      "Step - 3326, Loss - 0.3266579805275883, Learning Rate - 0.00625, magnitude of gradient - 0.06316410948020224\n",
      "Step - 3327, Loss - 0.3956958018372557, Learning Rate - 0.00625, magnitude of gradient - 0.054493964913641495\n",
      "Step - 3328, Loss - 0.377284932468804, Learning Rate - 0.00625, magnitude of gradient - 0.0553101273780339\n",
      "Step - 3329, Loss - 0.3217738403843923, Learning Rate - 0.00625, magnitude of gradient - 0.04722109056423796\n",
      "Step - 3330, Loss - 0.2880938329296773, Learning Rate - 0.00625, magnitude of gradient - 0.031769161125211554\n",
      "Step - 3331, Loss - 0.26920985668436337, Learning Rate - 0.00625, magnitude of gradient - 0.08011537406451934\n",
      "Step - 3332, Loss - 0.3700616169196452, Learning Rate - 0.00625, magnitude of gradient - 0.10277744467089428\n",
      "Step - 3333, Loss - 0.24998891941251955, Learning Rate - 0.00625, magnitude of gradient - 0.044605852169652856\n",
      "Step - 3334, Loss - 0.2667954864452112, Learning Rate - 0.00625, magnitude of gradient - 0.042667862846754175\n",
      "Step - 3335, Loss - 0.23404212577326106, Learning Rate - 0.00625, magnitude of gradient - 0.055064137289634336\n",
      "Step - 3336, Loss - 0.33351180358394, Learning Rate - 0.00625, magnitude of gradient - 0.017651718197568105\n",
      "Step - 3337, Loss - 0.32856407276546495, Learning Rate - 0.00625, magnitude of gradient - 0.06770148408943846\n",
      "Step - 3338, Loss - 0.41460553041217046, Learning Rate - 0.00625, magnitude of gradient - 0.022039389361192024\n",
      "Step - 3339, Loss - 0.3084283179014933, Learning Rate - 0.00625, magnitude of gradient - 0.05407366801919193\n",
      "Step - 3340, Loss - 0.35100221644280494, Learning Rate - 0.00625, magnitude of gradient - 0.029972223592619515\n",
      "Step - 3341, Loss - 0.3486982383079542, Learning Rate - 0.00625, magnitude of gradient - 0.05065275598898403\n",
      "Step - 3342, Loss - 0.3137948028994299, Learning Rate - 0.00625, magnitude of gradient - 0.030236400310608962\n",
      "Step - 3343, Loss - 0.3148397471770088, Learning Rate - 0.00625, magnitude of gradient - 0.0158039855522985\n",
      "Step - 3344, Loss - 0.3838384445096601, Learning Rate - 0.00625, magnitude of gradient - 0.0022154838878951904\n",
      "Step - 3345, Loss - 0.27170061820130187, Learning Rate - 0.00625, magnitude of gradient - 0.028024253561299953\n",
      "Step - 3346, Loss - 0.3206238210588616, Learning Rate - 0.00625, magnitude of gradient - 0.05274938197026757\n",
      "Step - 3347, Loss - 0.3912432375922238, Learning Rate - 0.00625, magnitude of gradient - 0.04945517206077957\n",
      "Step - 3348, Loss - 0.317079182641942, Learning Rate - 0.00625, magnitude of gradient - 0.05069420692487173\n",
      "Step - 3349, Loss - 0.3650659859252708, Learning Rate - 0.00625, magnitude of gradient - 0.07528871919755954\n",
      "Step - 3350, Loss - 0.2594492295969184, Learning Rate - 0.00625, magnitude of gradient - 0.07312980862631056\n",
      "Step - 3351, Loss - 0.29212117057960907, Learning Rate - 0.00625, magnitude of gradient - 0.030413050989376737\n",
      "Step - 3352, Loss - 0.2796668065229599, Learning Rate - 0.00625, magnitude of gradient - 0.05435439609098126\n",
      "Step - 3353, Loss - 0.4163456581284891, Learning Rate - 0.00625, magnitude of gradient - 0.047586904026384706\n",
      "Step - 3354, Loss - 0.2621361369140028, Learning Rate - 0.00625, magnitude of gradient - 0.00963424854079479\n",
      "Step - 3355, Loss - 0.34111556441254803, Learning Rate - 0.00625, magnitude of gradient - 0.04661342191693451\n",
      "Step - 3356, Loss - 0.3971987513148496, Learning Rate - 0.00625, magnitude of gradient - 0.05854732288716427\n",
      "Step - 3357, Loss - 0.3929782855535324, Learning Rate - 0.00625, magnitude of gradient - 0.024164312723426385\n",
      "Step - 3358, Loss - 0.3170959874663196, Learning Rate - 0.00625, magnitude of gradient - 0.036133238609211825\n",
      "Step - 3359, Loss - 0.3625964119341385, Learning Rate - 0.00625, magnitude of gradient - 0.03271812500902973\n",
      "Step - 3360, Loss - 0.25229414478296536, Learning Rate - 0.00625, magnitude of gradient - 0.011047286162128982\n",
      "Step - 3361, Loss - 0.24299580226299455, Learning Rate - 0.00625, magnitude of gradient - 0.07181018839026525\n",
      "Step - 3362, Loss - 0.2089956226334082, Learning Rate - 0.00625, magnitude of gradient - 0.10974143723881437\n",
      "Step - 3363, Loss - 0.34633658159812514, Learning Rate - 0.00625, magnitude of gradient - 0.0655365564285663\n",
      "Step - 3364, Loss - 0.2899871073468704, Learning Rate - 0.00625, magnitude of gradient - 0.07145514530895626\n",
      "Step - 3365, Loss - 0.2861414382086689, Learning Rate - 0.00625, magnitude of gradient - 0.09875580151920936\n",
      "Step - 3366, Loss - 0.2933873347862389, Learning Rate - 0.00625, magnitude of gradient - 0.1130908977172399\n",
      "Step - 3367, Loss - 0.34633415739150697, Learning Rate - 0.00625, magnitude of gradient - 0.06522441809958211\n",
      "Step - 3368, Loss - 0.27561743477008227, Learning Rate - 0.00625, magnitude of gradient - 0.06167764196677097\n",
      "Step - 3369, Loss - 0.40784494860352255, Learning Rate - 0.00625, magnitude of gradient - 0.023925616319377785\n",
      "Step - 3370, Loss - 0.28643895237731165, Learning Rate - 0.00625, magnitude of gradient - 0.04480426362259362\n",
      "Step - 3371, Loss - 0.32858773136155467, Learning Rate - 0.00625, magnitude of gradient - 0.030510498186998674\n",
      "Step - 3372, Loss - 0.32816016788529445, Learning Rate - 0.00625, magnitude of gradient - 0.04723138147700765\n",
      "Step - 3373, Loss - 0.3102842863239334, Learning Rate - 0.00625, magnitude of gradient - 0.05316203280254483\n",
      "Step - 3374, Loss - 0.3477841790595122, Learning Rate - 0.00625, magnitude of gradient - 0.038930419291384306\n",
      "Step - 3375, Loss - 0.3233073515286959, Learning Rate - 0.00625, magnitude of gradient - 0.03847080728126077\n",
      "Step - 3376, Loss - 0.2820379568133683, Learning Rate - 0.00625, magnitude of gradient - 0.018663634843751054\n",
      "Step - 3377, Loss - 0.32291722849234983, Learning Rate - 0.00625, magnitude of gradient - 0.08201626114080768\n",
      "Step - 3378, Loss - 0.3881733700680012, Learning Rate - 0.00625, magnitude of gradient - 0.058144426465317416\n",
      "Step - 3379, Loss - 0.31639581073681533, Learning Rate - 0.00625, magnitude of gradient - 0.03325568021095815\n",
      "Step - 3380, Loss - 0.33592836081254673, Learning Rate - 0.00625, magnitude of gradient - 0.06324736065476017\n",
      "Step - 3381, Loss - 0.3698843319071487, Learning Rate - 0.00625, magnitude of gradient - 0.03648542975740115\n",
      "Step - 3382, Loss - 0.3777572878139879, Learning Rate - 0.00625, magnitude of gradient - 0.021081426958347037\n",
      "Step - 3383, Loss - 0.30479143181989643, Learning Rate - 0.00625, magnitude of gradient - 0.033152816269196285\n",
      "Step - 3384, Loss - 0.3288198532267094, Learning Rate - 0.00625, magnitude of gradient - 0.08977092737548684\n",
      "Step - 3385, Loss - 0.2760104078130362, Learning Rate - 0.00625, magnitude of gradient - 0.11911508789836743\n",
      "Step - 3386, Loss - 0.2595738124010636, Learning Rate - 0.00625, magnitude of gradient - 0.06118545018828022\n",
      "Step - 3387, Loss - 0.35244745798747235, Learning Rate - 0.00625, magnitude of gradient - 0.03564019913111864\n",
      "Step - 3388, Loss - 0.37214867418117564, Learning Rate - 0.00625, magnitude of gradient - 0.03868956298404936\n",
      "Step - 3389, Loss - 0.24829789335182234, Learning Rate - 0.00625, magnitude of gradient - 0.034223441458077083\n",
      "Step - 3390, Loss - 0.3266289936896349, Learning Rate - 0.00625, magnitude of gradient - 0.1036432597704205\n",
      "Step - 3391, Loss - 0.3069090671002697, Learning Rate - 0.00625, magnitude of gradient - 0.015556288650411181\n",
      "Step - 3392, Loss - 0.4167103198233148, Learning Rate - 0.00625, magnitude of gradient - 0.045423021258841983\n",
      "Step - 3393, Loss - 0.31817049625148935, Learning Rate - 0.00625, magnitude of gradient - 0.001279819756068993\n",
      "Step - 3394, Loss - 0.3357161920012489, Learning Rate - 0.00625, magnitude of gradient - 0.05735568185315227\n",
      "Step - 3395, Loss - 0.396426966486194, Learning Rate - 0.00625, magnitude of gradient - 0.11456739487062575\n",
      "Step - 3396, Loss - 0.3381489604675519, Learning Rate - 0.00625, magnitude of gradient - 0.0717798382577454\n",
      "Step - 3397, Loss - 0.3039304711525924, Learning Rate - 0.00625, magnitude of gradient - 0.040918677521363\n",
      "Step - 3398, Loss - 0.3462977976650468, Learning Rate - 0.00625, magnitude of gradient - 0.037348483723700025\n",
      "Step - 3399, Loss - 0.31211508188263914, Learning Rate - 0.00625, magnitude of gradient - 0.049482216422372764\n",
      "Step - 3400, Loss - 0.3350595255947282, Learning Rate - 0.00625, magnitude of gradient - 0.068216554741621\n",
      "Step - 3401, Loss - 0.31059836573237093, Learning Rate - 0.00625, magnitude of gradient - 0.03684065664008018\n",
      "Step - 3402, Loss - 0.3884504220740944, Learning Rate - 0.00625, magnitude of gradient - 0.04200119587810922\n",
      "Step - 3403, Loss - 0.3620140175120316, Learning Rate - 0.00625, magnitude of gradient - 0.05129595929851163\n",
      "Step - 3404, Loss - 0.2980691259845824, Learning Rate - 0.00625, magnitude of gradient - 0.10523047976794866\n",
      "Step - 3405, Loss - 0.4839026312049081, Learning Rate - 0.00625, magnitude of gradient - 0.06823530345513018\n",
      "Step - 3406, Loss - 0.32372738439623705, Learning Rate - 0.00625, magnitude of gradient - 0.05925052722768975\n",
      "Step - 3407, Loss - 0.30530830869975, Learning Rate - 0.00625, magnitude of gradient - 0.05060587308316484\n",
      "Step - 3408, Loss - 0.3239819522096005, Learning Rate - 0.00625, magnitude of gradient - 0.06637984726552926\n",
      "Step - 3409, Loss - 0.35033404578178307, Learning Rate - 0.00625, magnitude of gradient - 0.05196998039000419\n",
      "Step - 3410, Loss - 0.358224900579959, Learning Rate - 0.00625, magnitude of gradient - 0.0882562678189266\n",
      "Step - 3411, Loss - 0.3372824685573707, Learning Rate - 0.00625, magnitude of gradient - 0.05144295952179624\n",
      "Step - 3412, Loss - 0.3346930809043915, Learning Rate - 0.00625, magnitude of gradient - 0.06339760265234064\n",
      "Step - 3413, Loss - 0.37035050273145576, Learning Rate - 0.00625, magnitude of gradient - 0.033003444734159906\n",
      "Step - 3414, Loss - 0.32441439536730865, Learning Rate - 0.00625, magnitude of gradient - 0.036001663158453316\n",
      "Step - 3415, Loss - 0.25920716341367156, Learning Rate - 0.00625, magnitude of gradient - 0.03774699424932356\n",
      "Step - 3416, Loss - 0.3337993007358614, Learning Rate - 0.00625, magnitude of gradient - 0.07307301886152799\n",
      "Step - 3417, Loss - 0.3154638340194816, Learning Rate - 0.00625, magnitude of gradient - 0.036632184278899464\n",
      "Step - 3418, Loss - 0.3265546852932596, Learning Rate - 0.00625, magnitude of gradient - 0.02237375968068626\n",
      "Step - 3419, Loss - 0.32309777360755976, Learning Rate - 0.00625, magnitude of gradient - 0.022816782825929846\n",
      "Step - 3420, Loss - 0.24852949485795173, Learning Rate - 0.00625, magnitude of gradient - 0.05315027368961542\n",
      "Step - 3421, Loss - 0.3373360647716928, Learning Rate - 0.00625, magnitude of gradient - 0.07881032360764058\n",
      "Step - 3422, Loss - 0.3292084205549168, Learning Rate - 0.00625, magnitude of gradient - 0.11097370019787017\n",
      "Step - 3423, Loss - 0.3079938642027092, Learning Rate - 0.00625, magnitude of gradient - 0.07697645997079283\n",
      "Step - 3424, Loss - 0.3031962022039962, Learning Rate - 0.00625, magnitude of gradient - 0.0336112516204539\n",
      "Step - 3425, Loss - 0.3342482882080138, Learning Rate - 0.00625, magnitude of gradient - 0.026713029405173148\n",
      "Step - 3426, Loss - 0.35095409041617587, Learning Rate - 0.00625, magnitude of gradient - 0.04345547201906823\n",
      "Step - 3427, Loss - 0.37499485141326777, Learning Rate - 0.00625, magnitude of gradient - 0.05056808305444865\n",
      "Step - 3428, Loss - 0.34155034893830133, Learning Rate - 0.00625, magnitude of gradient - 0.12515476398487957\n",
      "Step - 3429, Loss - 0.3444297419455007, Learning Rate - 0.00625, magnitude of gradient - 0.06065023145934543\n",
      "Step - 3430, Loss - 0.37526230444771275, Learning Rate - 0.00625, magnitude of gradient - 0.03758388795188437\n",
      "Step - 3431, Loss - 0.36115930432897914, Learning Rate - 0.00625, magnitude of gradient - 0.09523840764675362\n",
      "Step - 3432, Loss - 0.30006414255045566, Learning Rate - 0.00625, magnitude of gradient - 0.014562406180782451\n",
      "Step - 3433, Loss - 0.2826333541464527, Learning Rate - 0.00625, magnitude of gradient - 0.049420382797066856\n",
      "Step - 3434, Loss - 0.30250521957938714, Learning Rate - 0.00625, magnitude of gradient - 0.007775490889733497\n",
      "Step - 3435, Loss - 0.3575745096200336, Learning Rate - 0.00625, magnitude of gradient - 0.1234333432390188\n",
      "Step - 3436, Loss - 0.31018375587081864, Learning Rate - 0.00625, magnitude of gradient - 0.07243700061626795\n",
      "Step - 3437, Loss - 0.32603485305687024, Learning Rate - 0.00625, magnitude of gradient - 0.08640041507464509\n",
      "Step - 3438, Loss - 0.3476308526127977, Learning Rate - 0.00625, magnitude of gradient - 0.011478031327232236\n",
      "Step - 3439, Loss - 0.38851762070313745, Learning Rate - 0.00625, magnitude of gradient - 0.01988014783099479\n",
      "Step - 3440, Loss - 0.384330087049271, Learning Rate - 0.00625, magnitude of gradient - 0.04101632132296044\n",
      "Step - 3441, Loss - 0.27438322884523264, Learning Rate - 0.00625, magnitude of gradient - 0.05570796641084991\n",
      "Step - 3442, Loss - 0.30940480810277393, Learning Rate - 0.00625, magnitude of gradient - 0.04325649876205436\n",
      "Step - 3443, Loss - 0.3476675020450281, Learning Rate - 0.00625, magnitude of gradient - 0.03953029658699648\n",
      "Step - 3444, Loss - 0.2706400624210049, Learning Rate - 0.00625, magnitude of gradient - 0.02343789248941771\n",
      "Step - 3445, Loss - 0.315208758347841, Learning Rate - 0.00625, magnitude of gradient - 0.011834566515193693\n",
      "Step - 3446, Loss - 0.3791186664305731, Learning Rate - 0.00625, magnitude of gradient - 0.03804529781877799\n",
      "Step - 3447, Loss - 0.35248959152388065, Learning Rate - 0.00625, magnitude of gradient - 0.06593324755118793\n",
      "Step - 3448, Loss - 0.30152248846033225, Learning Rate - 0.00625, magnitude of gradient - 0.046179880854257505\n",
      "Step - 3449, Loss - 0.354052732281043, Learning Rate - 0.00625, magnitude of gradient - 0.02821560899458552\n",
      "Step - 3450, Loss - 0.3481407173608658, Learning Rate - 0.00625, magnitude of gradient - 0.04845174983763127\n",
      "Step - 3451, Loss - 0.3076382957602198, Learning Rate - 0.00625, magnitude of gradient - 0.023095893220985337\n",
      "Step - 3452, Loss - 0.2995072746255642, Learning Rate - 0.00625, magnitude of gradient - 0.037244658572359264\n",
      "Step - 3453, Loss - 0.27879194099341936, Learning Rate - 0.00625, magnitude of gradient - 0.02198623640969486\n",
      "Step - 3454, Loss - 0.36775846631888365, Learning Rate - 0.00625, magnitude of gradient - 0.029198253750247236\n",
      "Step - 3455, Loss - 0.3629001425556713, Learning Rate - 0.00625, magnitude of gradient - 0.06497372296807762\n",
      "Step - 3456, Loss - 0.3162757409812823, Learning Rate - 0.00625, magnitude of gradient - 0.0970722324805139\n",
      "Step - 3457, Loss - 0.4006232232435827, Learning Rate - 0.00625, magnitude of gradient - 0.04237957081345685\n",
      "Step - 3458, Loss - 0.3418098354880895, Learning Rate - 0.00625, magnitude of gradient - 0.10341931070095915\n",
      "Step - 3459, Loss - 0.3904118449679229, Learning Rate - 0.00625, magnitude of gradient - 0.08618203109155381\n",
      "Step - 3460, Loss - 0.30618757507129624, Learning Rate - 0.00625, magnitude of gradient - 0.0972898234195281\n",
      "Step - 3461, Loss - 0.31444365366192906, Learning Rate - 0.00625, magnitude of gradient - 0.049218350228428666\n",
      "Step - 3462, Loss - 0.27753446404137205, Learning Rate - 0.00625, magnitude of gradient - 0.06336655947203042\n",
      "Step - 3463, Loss - 0.3591767498072795, Learning Rate - 0.00625, magnitude of gradient - 0.058893007324822846\n",
      "Step - 3464, Loss - 0.35864378854027007, Learning Rate - 0.00625, magnitude of gradient - 0.0590475296496509\n",
      "Step - 3465, Loss - 0.3008984379703602, Learning Rate - 0.00625, magnitude of gradient - 0.08337793751006743\n",
      "Step - 3466, Loss - 0.30028888241292584, Learning Rate - 0.00625, magnitude of gradient - 0.04117381851741624\n",
      "Step - 3467, Loss - 0.29708194981875646, Learning Rate - 0.00625, magnitude of gradient - 0.04067445696063211\n",
      "Step - 3468, Loss - 0.29283291860212085, Learning Rate - 0.00625, magnitude of gradient - 0.03764640918308046\n",
      "Step - 3469, Loss - 0.318705177905382, Learning Rate - 0.00625, magnitude of gradient - 0.014774272556670059\n",
      "Step - 3470, Loss - 0.2445990949409126, Learning Rate - 0.00625, magnitude of gradient - 0.06036222533789296\n",
      "Step - 3471, Loss - 0.27842077082584465, Learning Rate - 0.00625, magnitude of gradient - 0.036698368037924665\n",
      "Step - 3472, Loss - 0.3574923720444377, Learning Rate - 0.00625, magnitude of gradient - 0.056681178794361436\n",
      "Step - 3473, Loss - 0.38981265862170517, Learning Rate - 0.00625, magnitude of gradient - 0.11975851528064534\n",
      "Step - 3474, Loss - 0.2926074368196444, Learning Rate - 0.00625, magnitude of gradient - 0.021070394044133214\n",
      "Step - 3475, Loss - 0.352952734452807, Learning Rate - 0.00625, magnitude of gradient - 0.07727780771060631\n",
      "Step - 3476, Loss - 0.30026828541071465, Learning Rate - 0.00625, magnitude of gradient - 0.04770154829860502\n",
      "Step - 3477, Loss - 0.30058872036013123, Learning Rate - 0.00625, magnitude of gradient - 0.04495934608850957\n",
      "Step - 3478, Loss - 0.30265679172123233, Learning Rate - 0.00625, magnitude of gradient - 0.06902811856464694\n",
      "Step - 3479, Loss - 0.3483402396095103, Learning Rate - 0.00625, magnitude of gradient - 0.018772196361040022\n",
      "Step - 3480, Loss - 0.33895701090413605, Learning Rate - 0.00625, magnitude of gradient - 0.07032357349703432\n",
      "Step - 3481, Loss - 0.30792452290653144, Learning Rate - 0.00625, magnitude of gradient - 0.054321954286084446\n",
      "Step - 3482, Loss - 0.4197447409050317, Learning Rate - 0.00625, magnitude of gradient - 0.05453086269005883\n",
      "Step - 3483, Loss - 0.35426630574872664, Learning Rate - 0.00625, magnitude of gradient - 0.09593346134457946\n",
      "Step - 3484, Loss - 0.3185164747288903, Learning Rate - 0.00625, magnitude of gradient - 0.014106266239150733\n",
      "Step - 3485, Loss - 0.24817350102528743, Learning Rate - 0.00625, magnitude of gradient - 0.04487100024203375\n",
      "Step - 3486, Loss - 0.282108315732087, Learning Rate - 0.00625, magnitude of gradient - 0.028093363691154432\n",
      "Step - 3487, Loss - 0.3041096024235854, Learning Rate - 0.00625, magnitude of gradient - 0.029623849711842835\n",
      "Step - 3488, Loss - 0.309300866725744, Learning Rate - 0.00625, magnitude of gradient - 0.036639835200111597\n",
      "Step - 3489, Loss - 0.2784024905262066, Learning Rate - 0.00625, magnitude of gradient - 0.0694707773427869\n",
      "Step - 3490, Loss - 0.22726577474288812, Learning Rate - 0.00625, magnitude of gradient - 0.050535535321513114\n",
      "Step - 3491, Loss - 0.2613638141433704, Learning Rate - 0.00625, magnitude of gradient - 0.046849644284293626\n",
      "Step - 3492, Loss - 0.309669306055381, Learning Rate - 0.00625, magnitude of gradient - 0.02354222069834805\n",
      "Step - 3493, Loss - 0.3176871849299075, Learning Rate - 0.00625, magnitude of gradient - 0.003729595015718532\n",
      "Step - 3494, Loss - 0.24202145786356133, Learning Rate - 0.00625, magnitude of gradient - 0.07894900826508149\n",
      "Step - 3495, Loss - 0.35478520321154905, Learning Rate - 0.00625, magnitude of gradient - 0.04518212691668684\n",
      "Step - 3496, Loss - 0.29667765324704093, Learning Rate - 0.00625, magnitude of gradient - 0.07674593446593349\n",
      "Step - 3497, Loss - 0.3166529560023818, Learning Rate - 0.00625, magnitude of gradient - 0.023724600439095394\n",
      "Step - 3498, Loss - 0.3335946928530141, Learning Rate - 0.00625, magnitude of gradient - 0.018095662472474277\n",
      "Step - 3499, Loss - 0.2393681849769909, Learning Rate - 0.00625, magnitude of gradient - 0.05564452986418481\n",
      "Step - 3500, Loss - 0.3277040218037164, Learning Rate - 0.00625, magnitude of gradient - 0.003930519004938905\n",
      "Step - 3501, Loss - 0.38213686928726076, Learning Rate - 0.00625, magnitude of gradient - 0.08823720908647519\n",
      "Step - 3502, Loss - 0.2655840312518287, Learning Rate - 0.00625, magnitude of gradient - 0.08123646873941609\n",
      "Step - 3503, Loss - 0.29384964943380865, Learning Rate - 0.00625, magnitude of gradient - 0.05676535837454791\n",
      "Step - 3504, Loss - 0.30224025037563024, Learning Rate - 0.00625, magnitude of gradient - 0.038168835319346214\n",
      "Step - 3505, Loss - 0.33861395174266673, Learning Rate - 0.00625, magnitude of gradient - 0.03263586397360142\n",
      "Step - 3506, Loss - 0.3155216426692891, Learning Rate - 0.00625, magnitude of gradient - 0.039552955348136715\n",
      "Step - 3507, Loss - 0.3094727150821041, Learning Rate - 0.00625, magnitude of gradient - 0.08235283013427873\n",
      "Step - 3508, Loss - 0.2644252020140704, Learning Rate - 0.00625, magnitude of gradient - 0.07780738169291185\n",
      "Step - 3509, Loss - 0.22518922599975255, Learning Rate - 0.00625, magnitude of gradient - 0.06117974799864659\n",
      "Step - 3510, Loss - 0.3978082294300671, Learning Rate - 0.00625, magnitude of gradient - 0.020332615672503128\n",
      "Step - 3511, Loss - 0.34436217029578753, Learning Rate - 0.00625, magnitude of gradient - 0.009938578533241507\n",
      "Step - 3512, Loss - 0.3515248090200004, Learning Rate - 0.00625, magnitude of gradient - 0.053838485176720796\n",
      "Step - 3513, Loss - 0.27947535685644886, Learning Rate - 0.00625, magnitude of gradient - 0.030795326324906124\n",
      "Step - 3514, Loss - 0.31146258909492686, Learning Rate - 0.00625, magnitude of gradient - 0.07469065199704059\n",
      "Step - 3515, Loss - 0.3391973823266897, Learning Rate - 0.00625, magnitude of gradient - 0.06852338498186915\n",
      "Step - 3516, Loss - 0.40694096567601223, Learning Rate - 0.00625, magnitude of gradient - 0.06861783468699179\n",
      "Step - 3517, Loss - 0.29204133890490086, Learning Rate - 0.00625, magnitude of gradient - 0.023743037457348955\n",
      "Step - 3518, Loss - 0.3835565735912669, Learning Rate - 0.00625, magnitude of gradient - 0.015422151264380596\n",
      "Step - 3519, Loss - 0.3407915081176357, Learning Rate - 0.00625, magnitude of gradient - 0.04622788520452034\n",
      "Step - 3520, Loss - 0.3426766745784023, Learning Rate - 0.00625, magnitude of gradient - 0.07456159098359647\n",
      "Step - 3521, Loss - 0.2687736721927155, Learning Rate - 0.00625, magnitude of gradient - 0.04773103092399172\n",
      "Step - 3522, Loss - 0.3370990207469671, Learning Rate - 0.00625, magnitude of gradient - 0.009944577029540672\n",
      "Step - 3523, Loss - 0.31652116574682077, Learning Rate - 0.00625, magnitude of gradient - 0.025788033955520758\n",
      "Step - 3524, Loss - 0.2636620276023343, Learning Rate - 0.00625, magnitude of gradient - 0.08599728236144309\n",
      "Step - 3525, Loss - 0.3801406295346256, Learning Rate - 0.00625, magnitude of gradient - 0.03406119334971701\n",
      "Step - 3526, Loss - 0.2672822583248345, Learning Rate - 0.00625, magnitude of gradient - 0.09420232735009253\n",
      "Step - 3527, Loss - 0.3140042721933247, Learning Rate - 0.00625, magnitude of gradient - 0.008874555867413932\n",
      "Step - 3528, Loss - 0.3186766773299469, Learning Rate - 0.00625, magnitude of gradient - 0.10462302975487507\n",
      "Step - 3529, Loss - 0.4474138356697662, Learning Rate - 0.00625, magnitude of gradient - 0.051664673696314986\n",
      "Step - 3530, Loss - 0.28956593534616554, Learning Rate - 0.00625, magnitude of gradient - 0.0752076898989429\n",
      "Step - 3531, Loss - 0.30454824477556786, Learning Rate - 0.00625, magnitude of gradient - 0.017304483662319032\n",
      "Step - 3532, Loss - 0.35264903656777313, Learning Rate - 0.00625, magnitude of gradient - 0.06076390731798405\n",
      "Step - 3533, Loss - 0.2570938478353559, Learning Rate - 0.00625, magnitude of gradient - 0.051884439101328714\n",
      "Step - 3534, Loss - 0.28708659037949114, Learning Rate - 0.00625, magnitude of gradient - 0.018523611094896017\n",
      "Step - 3535, Loss - 0.2943306646147706, Learning Rate - 0.00625, magnitude of gradient - 0.0543775809775509\n",
      "Step - 3536, Loss - 0.3395786827787772, Learning Rate - 0.00625, magnitude of gradient - 0.0944858016903511\n",
      "Step - 3537, Loss - 0.24907400436002075, Learning Rate - 0.00625, magnitude of gradient - 0.07672605870846345\n",
      "Step - 3538, Loss - 0.31264354853825693, Learning Rate - 0.00625, magnitude of gradient - 0.013474354124211054\n",
      "Step - 3539, Loss - 0.3510854486740372, Learning Rate - 0.00625, magnitude of gradient - 0.05956094849073006\n",
      "Step - 3540, Loss - 0.3707984261085893, Learning Rate - 0.00625, magnitude of gradient - 0.014600390090565636\n",
      "Step - 3541, Loss - 0.32038372107131063, Learning Rate - 0.00625, magnitude of gradient - 0.05954857263993201\n",
      "Step - 3542, Loss - 0.37824188907950196, Learning Rate - 0.00625, magnitude of gradient - 0.11393484213550954\n",
      "Step - 3543, Loss - 0.29332240531358833, Learning Rate - 0.00625, magnitude of gradient - 0.05907786757796044\n",
      "Step - 3544, Loss - 0.2911991144835751, Learning Rate - 0.00625, magnitude of gradient - 0.04298855476338775\n",
      "Step - 3545, Loss - 0.3014723288857234, Learning Rate - 0.00625, magnitude of gradient - 0.093019486234231\n",
      "Step - 3546, Loss - 0.27244801710718447, Learning Rate - 0.00625, magnitude of gradient - 0.04637318712860381\n",
      "Step - 3547, Loss - 0.3194831080651336, Learning Rate - 0.00625, magnitude of gradient - 0.07268417267582525\n",
      "Step - 3548, Loss - 0.3062885273501193, Learning Rate - 0.00625, magnitude of gradient - 0.057103718172813185\n",
      "Step - 3549, Loss - 0.3235063221524572, Learning Rate - 0.00625, magnitude of gradient - 0.04999100856284803\n",
      "Step - 3550, Loss - 0.3577369751372188, Learning Rate - 0.00625, magnitude of gradient - 0.07458500201043236\n",
      "Step - 3551, Loss - 0.3738124266723695, Learning Rate - 0.00625, magnitude of gradient - 0.011651222223908585\n",
      "Step - 3552, Loss - 0.32486089195868073, Learning Rate - 0.00625, magnitude of gradient - 0.04854529153584939\n",
      "Step - 3553, Loss - 0.33248604491443745, Learning Rate - 0.00625, magnitude of gradient - 0.017020022856401706\n",
      "Step - 3554, Loss - 0.3401527158782779, Learning Rate - 0.00625, magnitude of gradient - 0.040522436185469385\n",
      "Step - 3555, Loss - 0.28567303695374374, Learning Rate - 0.00625, magnitude of gradient - 0.04392722167262258\n",
      "Step - 3556, Loss - 0.3748852609707323, Learning Rate - 0.00625, magnitude of gradient - 0.11022305953271094\n",
      "Step - 3557, Loss - 0.3583030294065428, Learning Rate - 0.00625, magnitude of gradient - 0.09817215599691204\n",
      "Step - 3558, Loss - 0.28314020248909627, Learning Rate - 0.00625, magnitude of gradient - 0.057362570817918394\n",
      "Step - 3559, Loss - 0.3012069017039652, Learning Rate - 0.00625, magnitude of gradient - 0.05865164067806636\n",
      "Step - 3560, Loss - 0.32120722757746717, Learning Rate - 0.00625, magnitude of gradient - 0.08099810820277387\n",
      "Step - 3561, Loss - 0.36235470828260996, Learning Rate - 0.00625, magnitude of gradient - 0.04564425587630978\n",
      "Step - 3562, Loss - 0.31550964921184166, Learning Rate - 0.00625, magnitude of gradient - 0.05186570619333312\n",
      "Step - 3563, Loss - 0.3070585870504223, Learning Rate - 0.00625, magnitude of gradient - 0.042768345279430325\n",
      "Step - 3564, Loss - 0.2838715905611376, Learning Rate - 0.00625, magnitude of gradient - 0.004206087366572566\n",
      "Step - 3565, Loss - 0.4115384059694923, Learning Rate - 0.00625, magnitude of gradient - 0.03741461689845371\n",
      "Step - 3566, Loss - 0.27360424467771416, Learning Rate - 0.00625, magnitude of gradient - 0.04724081219134307\n",
      "Step - 3567, Loss - 0.4388964301599868, Learning Rate - 0.00625, magnitude of gradient - 0.08301443249257406\n",
      "Step - 3568, Loss - 0.28826831869299535, Learning Rate - 0.00625, magnitude of gradient - 0.08199904537728844\n",
      "Step - 3569, Loss - 0.35153390911570426, Learning Rate - 0.00625, magnitude of gradient - 0.03369910483498587\n",
      "Step - 3570, Loss - 0.29398757044010126, Learning Rate - 0.00625, magnitude of gradient - 0.0590543047606665\n",
      "Step - 3571, Loss - 0.30468338237845183, Learning Rate - 0.00625, magnitude of gradient - 0.02152757403235025\n",
      "Step - 3572, Loss - 0.3240339268410221, Learning Rate - 0.00625, magnitude of gradient - 0.05506784254908863\n",
      "Step - 3573, Loss - 0.31711347697447945, Learning Rate - 0.00625, magnitude of gradient - 0.08979953816223495\n",
      "Step - 3574, Loss - 0.35514895433279464, Learning Rate - 0.00625, magnitude of gradient - 0.0860079920696259\n",
      "Step - 3575, Loss - 0.2949532214438184, Learning Rate - 0.00625, magnitude of gradient - 0.029119470683212193\n",
      "Step - 3576, Loss - 0.40078560814332914, Learning Rate - 0.00625, magnitude of gradient - 0.0921472549523929\n",
      "Step - 3577, Loss - 0.3034929158291073, Learning Rate - 0.00625, magnitude of gradient - 0.043788270090309916\n",
      "Step - 3578, Loss - 0.26497512458875405, Learning Rate - 0.00625, magnitude of gradient - 0.0687733241118517\n",
      "Step - 3579, Loss - 0.3529610106836294, Learning Rate - 0.00625, magnitude of gradient - 0.06699160180275976\n",
      "Step - 3580, Loss - 0.27316085865075035, Learning Rate - 0.00625, magnitude of gradient - 0.017009784656973444\n",
      "Step - 3581, Loss - 0.3170127655982342, Learning Rate - 0.00625, magnitude of gradient - 0.12288820424010381\n",
      "Step - 3582, Loss - 0.27908261280318236, Learning Rate - 0.00625, magnitude of gradient - 0.08889162848635417\n",
      "Step - 3583, Loss - 0.38244725157223136, Learning Rate - 0.00625, magnitude of gradient - 0.0571943408436747\n",
      "Step - 3584, Loss - 0.30595691952164106, Learning Rate - 0.00625, magnitude of gradient - 0.01704917554363425\n",
      "Step - 3585, Loss - 0.3022209417371099, Learning Rate - 0.00625, magnitude of gradient - 0.02220337489765779\n",
      "Step - 3586, Loss - 0.3419181830856192, Learning Rate - 0.00625, magnitude of gradient - 0.01800949598901728\n",
      "Step - 3587, Loss - 0.3563684739990244, Learning Rate - 0.00625, magnitude of gradient - 0.03511922590686087\n",
      "Step - 3588, Loss - 0.37488607130570883, Learning Rate - 0.00625, magnitude of gradient - 0.05358854513081045\n",
      "Step - 3589, Loss - 0.3074782631934811, Learning Rate - 0.00625, magnitude of gradient - 0.03693509811293069\n",
      "Step - 3590, Loss - 0.3563348765745097, Learning Rate - 0.00625, magnitude of gradient - 0.034740641079295136\n",
      "Step - 3591, Loss - 0.3700711438575323, Learning Rate - 0.00625, magnitude of gradient - 0.029577392727547988\n",
      "Step - 3592, Loss - 0.35181465305306353, Learning Rate - 0.00625, magnitude of gradient - 0.04320037218900402\n",
      "Step - 3593, Loss - 0.3350741075039861, Learning Rate - 0.00625, magnitude of gradient - 0.054898897113544405\n",
      "Step - 3594, Loss - 0.34416721718071763, Learning Rate - 0.00625, magnitude of gradient - 0.06522825560939872\n",
      "Step - 3595, Loss - 0.33713181681708143, Learning Rate - 0.00625, magnitude of gradient - 0.03714685000418613\n",
      "Step - 3596, Loss - 0.36717396338265545, Learning Rate - 0.00625, magnitude of gradient - 0.03679414307666151\n",
      "Step - 3597, Loss - 0.3008142915961047, Learning Rate - 0.00625, magnitude of gradient - 0.03862107804264457\n",
      "Step - 3598, Loss - 0.27502088597467084, Learning Rate - 0.00625, magnitude of gradient - 0.0667211927836521\n",
      "Step - 3599, Loss - 0.32638534377666356, Learning Rate - 0.00625, magnitude of gradient - 0.04328941890739664\n",
      "Step - 3600, Loss - 0.29591337105707427, Learning Rate - 0.00625, magnitude of gradient - 0.04694591419309475\n",
      "Step - 3601, Loss - 0.302040208197124, Learning Rate - 0.00625, magnitude of gradient - 0.1036916173700505\n",
      "Step - 3602, Loss - 0.24232795129526746, Learning Rate - 0.00625, magnitude of gradient - 0.008290933160478615\n",
      "Step - 3603, Loss - 0.35067283501481494, Learning Rate - 0.00625, magnitude of gradient - 0.10242939128762844\n",
      "Step - 3604, Loss - 0.3383415183715286, Learning Rate - 0.00625, magnitude of gradient - 0.0589317517652913\n",
      "Step - 3605, Loss - 0.3447355718012827, Learning Rate - 0.00625, magnitude of gradient - 0.04677643565023558\n",
      "Step - 3606, Loss - 0.2826458663043788, Learning Rate - 0.00625, magnitude of gradient - 0.03758735719375515\n",
      "Step - 3607, Loss - 0.27638304432699445, Learning Rate - 0.00625, magnitude of gradient - 0.08202368783028084\n",
      "Step - 3608, Loss - 0.317972782420845, Learning Rate - 0.00625, magnitude of gradient - 0.022790891078297442\n",
      "Step - 3609, Loss - 0.3608292976654525, Learning Rate - 0.00625, magnitude of gradient - 0.07600309972596439\n",
      "Step - 3610, Loss - 0.4025922038085106, Learning Rate - 0.00625, magnitude of gradient - 0.040965657738795604\n",
      "Step - 3611, Loss - 0.36780908577096416, Learning Rate - 0.00625, magnitude of gradient - 0.0606911658216006\n",
      "Step - 3612, Loss - 0.30253274179013134, Learning Rate - 0.00625, magnitude of gradient - 0.06262056747931274\n",
      "Step - 3613, Loss - 0.2890485382662068, Learning Rate - 0.00625, magnitude of gradient - 0.034147556970237476\n",
      "Step - 3614, Loss - 0.3523973080044168, Learning Rate - 0.00625, magnitude of gradient - 0.07297063185796633\n",
      "Step - 3615, Loss - 0.3810065797748231, Learning Rate - 0.00625, magnitude of gradient - 0.058936681847927275\n",
      "Step - 3616, Loss - 0.3413487937870761, Learning Rate - 0.00625, magnitude of gradient - 0.029122939280260763\n",
      "Step - 3617, Loss - 0.3534661533952588, Learning Rate - 0.00625, magnitude of gradient - 0.05645745662259475\n",
      "Step - 3618, Loss - 0.3433291342503503, Learning Rate - 0.00625, magnitude of gradient - 0.05945353602863527\n",
      "Step - 3619, Loss - 0.28553041298376003, Learning Rate - 0.00625, magnitude of gradient - 0.030086464274701737\n",
      "Step - 3620, Loss - 0.26622946192718167, Learning Rate - 0.00625, magnitude of gradient - 0.039471562022563605\n",
      "Step - 3621, Loss - 0.32046111642754366, Learning Rate - 0.00625, magnitude of gradient - 0.07276362652419223\n",
      "Step - 3622, Loss - 0.2995369546199952, Learning Rate - 0.00625, magnitude of gradient - 0.010097908523843398\n",
      "Step - 3623, Loss - 0.34630279617958376, Learning Rate - 0.00625, magnitude of gradient - 0.03187336841965689\n",
      "Step - 3624, Loss - 0.30836206243693354, Learning Rate - 0.00625, magnitude of gradient - 0.011391367988729943\n",
      "Step - 3625, Loss - 0.3143238954962836, Learning Rate - 0.00625, magnitude of gradient - 0.08470064167076548\n",
      "Step - 3626, Loss - 0.28177000504652816, Learning Rate - 0.00625, magnitude of gradient - 0.058751418986166044\n",
      "Step - 3627, Loss - 0.32523686528723933, Learning Rate - 0.00625, magnitude of gradient - 0.04394443010199297\n",
      "Step - 3628, Loss - 0.336035793679603, Learning Rate - 0.00625, magnitude of gradient - 0.0023486527514150852\n",
      "Step - 3629, Loss - 0.24763799759873825, Learning Rate - 0.00625, magnitude of gradient - 0.08082244702986892\n",
      "Step - 3630, Loss - 0.349733920087237, Learning Rate - 0.00625, magnitude of gradient - 0.06647427530565092\n",
      "Step - 3631, Loss - 0.27458097886971206, Learning Rate - 0.00625, magnitude of gradient - 0.059743563972926204\n",
      "Step - 3632, Loss - 0.31898789633263613, Learning Rate - 0.00625, magnitude of gradient - 0.03103697885040876\n",
      "Step - 3633, Loss - 0.250970047158456, Learning Rate - 0.00625, magnitude of gradient - 0.02888425361802575\n",
      "Step - 3634, Loss - 0.23135941138774924, Learning Rate - 0.00625, magnitude of gradient - 0.09104172311269841\n",
      "Step - 3635, Loss - 0.2287978548267606, Learning Rate - 0.00625, magnitude of gradient - 0.0482817888841415\n",
      "Step - 3636, Loss - 0.26251854075028924, Learning Rate - 0.00625, magnitude of gradient - 0.01743852023114218\n",
      "Step - 3637, Loss - 0.2916072440181573, Learning Rate - 0.00625, magnitude of gradient - 0.005727271078813898\n",
      "Step - 3638, Loss - 0.25019450606454796, Learning Rate - 0.00625, magnitude of gradient - 0.06356089017788366\n",
      "Step - 3639, Loss - 0.28219028095464005, Learning Rate - 0.00625, magnitude of gradient - 0.015138265090757122\n",
      "Step - 3640, Loss - 0.30114995328297756, Learning Rate - 0.00625, magnitude of gradient - 0.07279993080349181\n",
      "Step - 3641, Loss - 0.26698005704394123, Learning Rate - 0.00625, magnitude of gradient - 0.024821075632763312\n",
      "Step - 3642, Loss - 0.42024734388899065, Learning Rate - 0.00625, magnitude of gradient - 0.1283987644044168\n",
      "Step - 3643, Loss - 0.3445318351946554, Learning Rate - 0.00625, magnitude of gradient - 0.018707666871609206\n",
      "Step - 3644, Loss - 0.375006120683442, Learning Rate - 0.00625, magnitude of gradient - 0.04437741629168293\n",
      "Step - 3645, Loss - 0.35532518980507133, Learning Rate - 0.00625, magnitude of gradient - 0.02189285790481822\n",
      "Step - 3646, Loss - 0.36537321994440464, Learning Rate - 0.00625, magnitude of gradient - 0.02799621398536128\n",
      "Step - 3647, Loss - 0.34028352210623575, Learning Rate - 0.00625, magnitude of gradient - 0.0999046844069181\n",
      "Step - 3648, Loss - 0.35298554725477427, Learning Rate - 0.00625, magnitude of gradient - 0.01222537347378015\n",
      "Step - 3649, Loss - 0.2655793730798599, Learning Rate - 0.00625, magnitude of gradient - 0.0592355244539421\n",
      "Step - 3650, Loss - 0.384030848755593, Learning Rate - 0.00625, magnitude of gradient - 0.08961971234602532\n",
      "Step - 3651, Loss - 0.2982905599135929, Learning Rate - 0.00625, magnitude of gradient - 0.03608807040238113\n",
      "Step - 3652, Loss - 0.3385647445466641, Learning Rate - 0.00625, magnitude of gradient - 0.017329174976961976\n",
      "Step - 3653, Loss - 0.32194484095606984, Learning Rate - 0.00625, magnitude of gradient - 0.005803499231854828\n",
      "Step - 3654, Loss - 0.3172184602587175, Learning Rate - 0.00625, magnitude of gradient - 0.031602199741184875\n",
      "Step - 3655, Loss - 0.3183239038908766, Learning Rate - 0.00625, magnitude of gradient - 0.011808749186617649\n",
      "Step - 3656, Loss - 0.3584679223175358, Learning Rate - 0.00625, magnitude of gradient - 0.06903889617596319\n",
      "Step - 3657, Loss - 0.37061508075207467, Learning Rate - 0.00625, magnitude of gradient - 0.03009489793624862\n",
      "Step - 3658, Loss - 0.3776337061576097, Learning Rate - 0.00625, magnitude of gradient - 0.06316446730989289\n",
      "Step - 3659, Loss - 0.3163914494475372, Learning Rate - 0.00625, magnitude of gradient - 0.08981611902419642\n",
      "Step - 3660, Loss - 0.3236397841307755, Learning Rate - 0.00625, magnitude of gradient - 0.00918458467005583\n",
      "Step - 3661, Loss - 0.27676201108953946, Learning Rate - 0.00625, magnitude of gradient - 0.014941403409420122\n",
      "Step - 3662, Loss - 0.29700416580588573, Learning Rate - 0.00625, magnitude of gradient - 0.0877035465358178\n",
      "Step - 3663, Loss - 0.3439918678952958, Learning Rate - 0.00625, magnitude of gradient - 0.01871619154684625\n",
      "Step - 3664, Loss - 0.3354869288572663, Learning Rate - 0.00625, magnitude of gradient - 0.06392932764641608\n",
      "Step - 3665, Loss - 0.3505352051473705, Learning Rate - 0.00625, magnitude of gradient - 0.052223503783839054\n",
      "Step - 3666, Loss - 0.34097800426906266, Learning Rate - 0.00625, magnitude of gradient - 0.051583956080473836\n",
      "Step - 3667, Loss - 0.3424068075939622, Learning Rate - 0.00625, magnitude of gradient - 0.12135509331771847\n",
      "Step - 3668, Loss - 0.2799592626807311, Learning Rate - 0.00625, magnitude of gradient - 0.05469469507407197\n",
      "Step - 3669, Loss - 0.32333215326596315, Learning Rate - 0.00625, magnitude of gradient - 0.05503903711834378\n",
      "Step - 3670, Loss - 0.3433009973557944, Learning Rate - 0.00625, magnitude of gradient - 0.06774684447450463\n",
      "Step - 3671, Loss - 0.3700889870617221, Learning Rate - 0.00625, magnitude of gradient - 0.03146909718678841\n",
      "Step - 3672, Loss - 0.31184016611808624, Learning Rate - 0.00625, magnitude of gradient - 0.09331877616236942\n",
      "Step - 3673, Loss - 0.3424253346905043, Learning Rate - 0.00625, magnitude of gradient - 0.03201063283788834\n",
      "Step - 3674, Loss - 0.312877406637185, Learning Rate - 0.00625, magnitude of gradient - 0.011661012307732392\n",
      "Step - 3675, Loss - 0.38819505909180385, Learning Rate - 0.00625, magnitude of gradient - 0.09303917973539562\n",
      "Step - 3676, Loss - 0.30905025125663577, Learning Rate - 0.00625, magnitude of gradient - 0.10126987927878374\n",
      "Step - 3677, Loss - 0.31881879248679124, Learning Rate - 0.00625, magnitude of gradient - 0.04246663670325636\n",
      "Step - 3678, Loss - 0.2982446485872215, Learning Rate - 0.00625, magnitude of gradient - 0.023190529088790737\n",
      "Step - 3679, Loss - 0.35473459266471713, Learning Rate - 0.00625, magnitude of gradient - 0.05212075728125813\n",
      "Step - 3680, Loss - 0.3593957201851267, Learning Rate - 0.00625, magnitude of gradient - 0.08517904614943617\n",
      "Step - 3681, Loss - 0.33946123756956226, Learning Rate - 0.00625, magnitude of gradient - 0.011909277103145361\n",
      "Step - 3682, Loss - 0.32099980385993493, Learning Rate - 0.00625, magnitude of gradient - 0.01253411909911991\n",
      "Step - 3683, Loss - 0.3232942870230195, Learning Rate - 0.00625, magnitude of gradient - 0.08931969511034171\n",
      "Step - 3684, Loss - 0.3366691692298021, Learning Rate - 0.00625, magnitude of gradient - 0.07306183594542197\n",
      "Step - 3685, Loss - 0.32865473380519755, Learning Rate - 0.00625, magnitude of gradient - 0.07118823249903598\n",
      "Step - 3686, Loss - 0.28410210838766503, Learning Rate - 0.00625, magnitude of gradient - 0.0162920992544445\n",
      "Step - 3687, Loss - 0.38631267499808525, Learning Rate - 0.00625, magnitude of gradient - 0.024269164618612785\n",
      "Step - 3688, Loss - 0.3614975230631923, Learning Rate - 0.00625, magnitude of gradient - 0.022641338984990107\n",
      "Step - 3689, Loss - 0.33555133792805825, Learning Rate - 0.00625, magnitude of gradient - 0.11877368940683694\n",
      "Step - 3690, Loss - 0.33692738617588336, Learning Rate - 0.00625, magnitude of gradient - 0.0651912213167959\n",
      "Step - 3691, Loss - 0.3728695032282201, Learning Rate - 0.00625, magnitude of gradient - 0.06793799519554049\n",
      "Step - 3692, Loss - 0.3188280803284587, Learning Rate - 0.00625, magnitude of gradient - 0.02697285686188501\n",
      "Step - 3693, Loss - 0.3211185170864598, Learning Rate - 0.00625, magnitude of gradient - 0.08667527867903134\n",
      "Step - 3694, Loss - 0.2811732757633511, Learning Rate - 0.00625, magnitude of gradient - 0.0469284399406099\n",
      "Step - 3695, Loss - 0.2960356120951175, Learning Rate - 0.00625, magnitude of gradient - 0.034702956220903976\n",
      "Step - 3696, Loss - 0.32524677349787323, Learning Rate - 0.00625, magnitude of gradient - 0.0006575841378385822\n",
      "Step - 3697, Loss - 0.2301031121635707, Learning Rate - 0.00625, magnitude of gradient - 0.055622983501613266\n",
      "Step - 3698, Loss - 0.26991259747664575, Learning Rate - 0.00625, magnitude of gradient - 0.02813059855934856\n",
      "Step - 3699, Loss - 0.31903835310024475, Learning Rate - 0.00625, magnitude of gradient - 0.028328117827130975\n",
      "Step - 3700, Loss - 0.3082294427992737, Learning Rate - 0.00625, magnitude of gradient - 0.04206956985317652\n",
      "Step - 3701, Loss - 0.28816455754427106, Learning Rate - 0.00625, magnitude of gradient - 0.0352055036611374\n",
      "Step - 3702, Loss - 0.3896228998530749, Learning Rate - 0.00625, magnitude of gradient - 0.05548119848134096\n",
      "Step - 3703, Loss - 0.30512491424023214, Learning Rate - 0.00625, magnitude of gradient - 0.07375720770139382\n",
      "Step - 3704, Loss - 0.3022154943804334, Learning Rate - 0.00625, magnitude of gradient - 0.049421512208493744\n",
      "Step - 3705, Loss - 0.2738830715444047, Learning Rate - 0.00625, magnitude of gradient - 0.0414680049484134\n",
      "Step - 3706, Loss - 0.31072157986005916, Learning Rate - 0.00625, magnitude of gradient - 0.05371223753232064\n",
      "Step - 3707, Loss - 0.31299473369034914, Learning Rate - 0.00625, magnitude of gradient - 0.06766148933625195\n",
      "Step - 3708, Loss - 0.2712702726549391, Learning Rate - 0.00625, magnitude of gradient - 0.025597322471123684\n",
      "Step - 3709, Loss - 0.35551078836473293, Learning Rate - 0.00625, magnitude of gradient - 0.05501077301536215\n",
      "Step - 3710, Loss - 0.3180022714218563, Learning Rate - 0.00625, magnitude of gradient - 0.039994169363641044\n",
      "Step - 3711, Loss - 0.33585035927528717, Learning Rate - 0.00625, magnitude of gradient - 0.10209409111163034\n",
      "Step - 3712, Loss - 0.3509311234510204, Learning Rate - 0.00625, magnitude of gradient - 0.0301836447685413\n",
      "Step - 3713, Loss - 0.31862969739511904, Learning Rate - 0.00625, magnitude of gradient - 0.016204687268357394\n",
      "Step - 3714, Loss - 0.3002606372567449, Learning Rate - 0.00625, magnitude of gradient - 0.0724038895490786\n",
      "Step - 3715, Loss - 0.3601765392681397, Learning Rate - 0.00625, magnitude of gradient - 0.026981999392194818\n",
      "Step - 3716, Loss - 0.24174404046444944, Learning Rate - 0.00625, magnitude of gradient - 0.049570629942748023\n",
      "Step - 3717, Loss - 0.2981242370906611, Learning Rate - 0.00625, magnitude of gradient - 0.045855529329262915\n",
      "Step - 3718, Loss - 0.21637922269279794, Learning Rate - 0.00625, magnitude of gradient - 0.0254971994716622\n",
      "Step - 3719, Loss - 0.37474534167569507, Learning Rate - 0.00625, magnitude of gradient - 0.0710313606356169\n",
      "Step - 3720, Loss - 0.3432086233503049, Learning Rate - 0.00625, magnitude of gradient - 0.07063505002064069\n",
      "Step - 3721, Loss - 0.3937945471365115, Learning Rate - 0.00625, magnitude of gradient - 0.0516606387800724\n",
      "Step - 3722, Loss - 0.3502122549673014, Learning Rate - 0.00625, magnitude of gradient - 0.04286960680840144\n",
      "Step - 3723, Loss - 0.4264937720223283, Learning Rate - 0.00625, magnitude of gradient - 0.10113357552944567\n",
      "Step - 3724, Loss - 0.3450366323257824, Learning Rate - 0.00625, magnitude of gradient - 0.06411087837738226\n",
      "Step - 3725, Loss - 0.3373290312618505, Learning Rate - 0.00625, magnitude of gradient - 0.008480258407213956\n",
      "Step - 3726, Loss - 0.23565233140864972, Learning Rate - 0.00625, magnitude of gradient - 0.06703460544077462\n",
      "Step - 3727, Loss - 0.3455033572421159, Learning Rate - 0.00625, magnitude of gradient - 0.05118988967405202\n",
      "Step - 3728, Loss - 0.2839032263491187, Learning Rate - 0.00625, magnitude of gradient - 0.05255519045649002\n",
      "Step - 3729, Loss - 0.2985124301195501, Learning Rate - 0.00625, magnitude of gradient - 0.02613944370100782\n",
      "Step - 3730, Loss - 0.39017061565135247, Learning Rate - 0.00625, magnitude of gradient - 0.04886172308965771\n",
      "Step - 3731, Loss - 0.34351574514471717, Learning Rate - 0.00625, magnitude of gradient - 0.08021369201201825\n",
      "Step - 3732, Loss - 0.353030099748727, Learning Rate - 0.00625, magnitude of gradient - 0.02178947223186012\n",
      "Step - 3733, Loss - 0.35217527696262957, Learning Rate - 0.00625, magnitude of gradient - 0.11342330272288163\n",
      "Step - 3734, Loss - 0.4097565893339433, Learning Rate - 0.00625, magnitude of gradient - 0.03386703782080696\n",
      "Step - 3735, Loss - 0.3387949405126257, Learning Rate - 0.00625, magnitude of gradient - 0.11703704344449187\n",
      "Step - 3736, Loss - 0.30944513570097776, Learning Rate - 0.00625, magnitude of gradient - 0.028663124108086364\n",
      "Step - 3737, Loss - 0.3083789579341787, Learning Rate - 0.00625, magnitude of gradient - 0.08906441947278015\n",
      "Step - 3738, Loss - 0.2907976779958814, Learning Rate - 0.00625, magnitude of gradient - 0.03689486531624062\n",
      "Step - 3739, Loss - 0.40196152945349073, Learning Rate - 0.00625, magnitude of gradient - 0.1039366816233684\n",
      "Step - 3740, Loss - 0.2980054106159545, Learning Rate - 0.00625, magnitude of gradient - 0.07311545394569059\n",
      "Step - 3741, Loss - 0.3028419814117581, Learning Rate - 0.00625, magnitude of gradient - 0.054839070429428864\n",
      "Step - 3742, Loss - 0.3376783027824116, Learning Rate - 0.00625, magnitude of gradient - 0.04972088343320269\n",
      "Step - 3743, Loss - 0.26310865769671044, Learning Rate - 0.00625, magnitude of gradient - 0.05405705867045135\n",
      "Step - 3744, Loss - 0.4024382870198215, Learning Rate - 0.00625, magnitude of gradient - 0.020507937183133564\n",
      "Step - 3745, Loss - 0.2435674297418636, Learning Rate - 0.00625, magnitude of gradient - 0.026713854725880403\n",
      "Step - 3746, Loss - 0.3503526892010805, Learning Rate - 0.00625, magnitude of gradient - 0.022452532745714387\n",
      "Step - 3747, Loss - 0.34840643039910935, Learning Rate - 0.00625, magnitude of gradient - 0.02802482643343859\n",
      "Step - 3748, Loss - 0.3274968591029661, Learning Rate - 0.00625, magnitude of gradient - 0.03238997084564213\n",
      "Step - 3749, Loss - 0.32727862569712973, Learning Rate - 0.00625, magnitude of gradient - 0.03681231696302186\n",
      "Step - 3750, Loss - 0.31300157382572547, Learning Rate - 0.00625, magnitude of gradient - 0.03450521358912944\n",
      "Step - 3751, Loss - 0.29898598545195243, Learning Rate - 0.00625, magnitude of gradient - 0.019406969112602288\n",
      "Step - 3752, Loss - 0.37629686345613955, Learning Rate - 0.00625, magnitude of gradient - 0.040024844154052046\n",
      "Step - 3753, Loss - 0.239975266763892, Learning Rate - 0.00625, magnitude of gradient - 0.08864345951277898\n",
      "Step - 3754, Loss - 0.2848461711737487, Learning Rate - 0.00625, magnitude of gradient - 0.07011309103788893\n",
      "Step - 3755, Loss - 0.2936112221765539, Learning Rate - 0.00625, magnitude of gradient - 0.053994893057703275\n",
      "Step - 3756, Loss - 0.31167049194041674, Learning Rate - 0.00625, magnitude of gradient - 0.054017928440124506\n",
      "Step - 3757, Loss - 0.3082828727776255, Learning Rate - 0.00625, magnitude of gradient - 0.02562663997311305\n",
      "Step - 3758, Loss - 0.31580489653888916, Learning Rate - 0.00625, magnitude of gradient - 0.06347358165638149\n",
      "Step - 3759, Loss - 0.26935391729524294, Learning Rate - 0.00625, magnitude of gradient - 0.022200146154223287\n",
      "Step - 3760, Loss - 0.29642536381320383, Learning Rate - 0.00625, magnitude of gradient - 0.01666784072338734\n",
      "Step - 3761, Loss - 0.3194990863635852, Learning Rate - 0.00625, magnitude of gradient - 0.09619168899046532\n",
      "Step - 3762, Loss - 0.29937617147632734, Learning Rate - 0.00625, magnitude of gradient - 0.05534192516952519\n",
      "Step - 3763, Loss - 0.3565871112121987, Learning Rate - 0.00625, magnitude of gradient - 0.011949099045495523\n",
      "Step - 3764, Loss - 0.32446700261819184, Learning Rate - 0.00625, magnitude of gradient - 0.03923249079060668\n",
      "Step - 3765, Loss - 0.34833983538540586, Learning Rate - 0.00625, magnitude of gradient - 0.042897830924858535\n",
      "Step - 3766, Loss - 0.2588568628916701, Learning Rate - 0.00625, magnitude of gradient - 0.0453894695690771\n",
      "Step - 3767, Loss - 0.30648041983406266, Learning Rate - 0.00625, magnitude of gradient - 0.10335665983885824\n",
      "Step - 3768, Loss - 0.35860770854868207, Learning Rate - 0.00625, magnitude of gradient - 0.023343730074344704\n",
      "Step - 3769, Loss - 0.33830148888625367, Learning Rate - 0.00625, magnitude of gradient - 0.07443413179967229\n",
      "Step - 3770, Loss - 0.32633879379553854, Learning Rate - 0.00625, magnitude of gradient - 0.03173691473582916\n",
      "Step - 3771, Loss - 0.33764325787442295, Learning Rate - 0.00625, magnitude of gradient - 0.035783561608183356\n",
      "Step - 3772, Loss - 0.3180969047043108, Learning Rate - 0.00625, magnitude of gradient - 0.07958167330243593\n",
      "Step - 3773, Loss - 0.31942084848083685, Learning Rate - 0.00625, magnitude of gradient - 0.04594598710034413\n",
      "Step - 3774, Loss - 0.3473913054901394, Learning Rate - 0.00625, magnitude of gradient - 0.018937648145229392\n",
      "Step - 3775, Loss - 0.3113992173357695, Learning Rate - 0.00625, magnitude of gradient - 0.04543150088679445\n",
      "Step - 3776, Loss - 0.3215272048152261, Learning Rate - 0.00625, magnitude of gradient - 0.028248801740200256\n",
      "Step - 3777, Loss - 0.2673647727301894, Learning Rate - 0.00625, magnitude of gradient - 0.05879157460967532\n",
      "Step - 3778, Loss - 0.3182423393919624, Learning Rate - 0.00625, magnitude of gradient - 0.026616294236647596\n",
      "Step - 3779, Loss - 0.3107647033015808, Learning Rate - 0.00625, magnitude of gradient - 0.020275597214623364\n",
      "Step - 3780, Loss - 0.2827656784394043, Learning Rate - 0.00625, magnitude of gradient - 0.057760425326190294\n",
      "Step - 3781, Loss - 0.2991062175597822, Learning Rate - 0.00625, magnitude of gradient - 0.058122136020867354\n",
      "Step - 3782, Loss - 0.32452107466756785, Learning Rate - 0.00625, magnitude of gradient - 0.08927156588385167\n",
      "Step - 3783, Loss - 0.2791235010396239, Learning Rate - 0.00625, magnitude of gradient - 0.04374125645365956\n",
      "Step - 3784, Loss - 0.24080912502133373, Learning Rate - 0.00625, magnitude of gradient - 0.10283727715052811\n",
      "Step - 3785, Loss - 0.31771002764557144, Learning Rate - 0.00625, magnitude of gradient - 0.05969253686992306\n",
      "Step - 3786, Loss - 0.260606588067403, Learning Rate - 0.00625, magnitude of gradient - 0.056027046413515136\n",
      "Step - 3787, Loss - 0.34237452922472866, Learning Rate - 0.00625, magnitude of gradient - 0.09381306840905958\n",
      "Step - 3788, Loss - 0.3832428247053755, Learning Rate - 0.00625, magnitude of gradient - 0.01872426878753632\n",
      "Step - 3789, Loss - 0.34789002674600406, Learning Rate - 0.00625, magnitude of gradient - 0.03570767571894307\n",
      "Step - 3790, Loss - 0.25051241871419694, Learning Rate - 0.00625, magnitude of gradient - 0.034342468813181995\n",
      "Step - 3791, Loss - 0.3078443619604648, Learning Rate - 0.00625, magnitude of gradient - 0.07162269795441734\n",
      "Step - 3792, Loss - 0.32501271905627716, Learning Rate - 0.00625, magnitude of gradient - 0.035414502098991954\n",
      "Step - 3793, Loss - 0.27687234680904904, Learning Rate - 0.00625, magnitude of gradient - 0.04903431973679999\n",
      "Step - 3794, Loss - 0.31632415342426196, Learning Rate - 0.00625, magnitude of gradient - 0.05202070510008116\n",
      "Step - 3795, Loss - 0.35178048493871034, Learning Rate - 0.00625, magnitude of gradient - 0.016926271054557792\n",
      "Step - 3796, Loss - 0.3756824505333391, Learning Rate - 0.00625, magnitude of gradient - 0.06762349473094159\n",
      "Step - 3797, Loss - 0.27014062911878284, Learning Rate - 0.00625, magnitude of gradient - 0.05839931894290377\n",
      "Step - 3798, Loss - 0.3588595835436895, Learning Rate - 0.00625, magnitude of gradient - 0.017258486857609934\n",
      "Step - 3799, Loss - 0.3441991616667378, Learning Rate - 0.00625, magnitude of gradient - 0.08936712067767472\n",
      "Step - 3800, Loss - 0.28690456381908475, Learning Rate - 0.00625, magnitude of gradient - 0.006084773645869065\n",
      "Step - 3801, Loss - 0.33555791353717146, Learning Rate - 0.00625, magnitude of gradient - 0.059084029698022705\n",
      "Step - 3802, Loss - 0.3141196074937971, Learning Rate - 0.00625, magnitude of gradient - 0.06417450369349706\n",
      "Step - 3803, Loss - 0.2682085062190207, Learning Rate - 0.00625, magnitude of gradient - 0.05192736358700344\n",
      "Step - 3804, Loss - 0.2974111227976095, Learning Rate - 0.00625, magnitude of gradient - 0.09091272937195959\n",
      "Step - 3805, Loss - 0.37980449578951236, Learning Rate - 0.00625, magnitude of gradient - 0.07070530107535607\n",
      "Step - 3806, Loss - 0.2590646087597698, Learning Rate - 0.00625, magnitude of gradient - 0.041910562825052615\n",
      "Step - 3807, Loss - 0.24411095957721024, Learning Rate - 0.00625, magnitude of gradient - 0.1526132256977183\n",
      "Step - 3808, Loss - 0.3127190269186579, Learning Rate - 0.00625, magnitude of gradient - 0.0936303230363829\n",
      "Step - 3809, Loss - 0.3046131082101245, Learning Rate - 0.00625, magnitude of gradient - 0.01910604350116296\n",
      "Step - 3810, Loss - 0.3919745167986902, Learning Rate - 0.00625, magnitude of gradient - 0.0531549317267564\n",
      "Step - 3811, Loss - 0.34127747046806045, Learning Rate - 0.00625, magnitude of gradient - 0.06482514380017844\n",
      "Step - 3812, Loss - 0.38590978381250196, Learning Rate - 0.00625, magnitude of gradient - 0.028237042988373418\n",
      "Step - 3813, Loss - 0.33810856406194517, Learning Rate - 0.00625, magnitude of gradient - 0.06243687712560912\n",
      "Step - 3814, Loss - 0.2778303786666652, Learning Rate - 0.00625, magnitude of gradient - 0.0015025076358933952\n",
      "Step - 3815, Loss - 0.3944143478087415, Learning Rate - 0.00625, magnitude of gradient - 0.018780016953676905\n",
      "Step - 3816, Loss - 0.34330206745302183, Learning Rate - 0.00625, magnitude of gradient - 0.07870862343233874\n",
      "Step - 3817, Loss - 0.3672925543359047, Learning Rate - 0.00625, magnitude of gradient - 0.10317037812184768\n",
      "Step - 3818, Loss - 0.33198544359808657, Learning Rate - 0.00625, magnitude of gradient - 0.02599742222420976\n",
      "Step - 3819, Loss - 0.3283905379951535, Learning Rate - 0.00625, magnitude of gradient - 0.08035298352504076\n",
      "Step - 3820, Loss - 0.2981536997485804, Learning Rate - 0.00625, magnitude of gradient - 0.056927069395415836\n",
      "Step - 3821, Loss - 0.3078756834369782, Learning Rate - 0.00625, magnitude of gradient - 0.011154941841505008\n",
      "Step - 3822, Loss - 0.35815067488736646, Learning Rate - 0.00625, magnitude of gradient - 0.043931673998584664\n",
      "Step - 3823, Loss - 0.3500704054271916, Learning Rate - 0.00625, magnitude of gradient - 0.06151269409067732\n",
      "Step - 3824, Loss - 0.299389895634023, Learning Rate - 0.00625, magnitude of gradient - 0.0897934025910754\n",
      "Step - 3825, Loss - 0.31191639213096123, Learning Rate - 0.00625, magnitude of gradient - 0.039036620615699236\n",
      "Step - 3826, Loss - 0.29628589367623215, Learning Rate - 0.00625, magnitude of gradient - 0.027011733006090916\n",
      "Step - 3827, Loss - 0.32678799818450976, Learning Rate - 0.00625, magnitude of gradient - 0.008019182930165559\n",
      "Step - 3828, Loss - 0.2806066305815028, Learning Rate - 0.00625, magnitude of gradient - 0.08491842095924887\n",
      "Step - 3829, Loss - 0.36940150593881194, Learning Rate - 0.00625, magnitude of gradient - 0.0674265283459711\n",
      "Step - 3830, Loss - 0.2442132255455686, Learning Rate - 0.00625, magnitude of gradient - 0.04916474598164749\n",
      "Step - 3831, Loss - 0.33571415702168106, Learning Rate - 0.00625, magnitude of gradient - 0.04748440816098663\n",
      "Step - 3832, Loss - 0.3171257152203667, Learning Rate - 0.00625, magnitude of gradient - 0.016409769887810493\n",
      "Step - 3833, Loss - 0.34504364482280364, Learning Rate - 0.00625, magnitude of gradient - 0.0404574445533062\n",
      "Step - 3834, Loss - 0.2613301202482334, Learning Rate - 0.00625, magnitude of gradient - 0.09090725280017777\n",
      "Step - 3835, Loss - 0.3351419802165755, Learning Rate - 0.00625, magnitude of gradient - 0.10829911532844694\n",
      "Step - 3836, Loss - 0.343893733518212, Learning Rate - 0.00625, magnitude of gradient - 0.018733776406013012\n",
      "Step - 3837, Loss - 0.3600040635669981, Learning Rate - 0.00625, magnitude of gradient - 0.04985617676625346\n",
      "Step - 3838, Loss - 0.34865844560920445, Learning Rate - 0.00625, magnitude of gradient - 0.05822612013206626\n",
      "Step - 3839, Loss - 0.39348889931308595, Learning Rate - 0.00625, magnitude of gradient - 0.04331496154082234\n",
      "Step - 3840, Loss - 0.347775058075191, Learning Rate - 0.00625, magnitude of gradient - 0.03779368300722084\n",
      "Step - 3841, Loss - 0.24450351890610134, Learning Rate - 0.00625, magnitude of gradient - 0.09333770506446078\n",
      "Step - 3842, Loss - 0.3361114886590994, Learning Rate - 0.00625, magnitude of gradient - 0.02420987248955056\n",
      "Step - 3843, Loss - 0.3644626911768673, Learning Rate - 0.00625, magnitude of gradient - 0.045381120289235785\n",
      "Step - 3844, Loss - 0.28410935046800584, Learning Rate - 0.00625, magnitude of gradient - 0.03923725957288155\n",
      "Step - 3845, Loss - 0.30752462112497825, Learning Rate - 0.00625, magnitude of gradient - 0.09359445764707826\n",
      "Step - 3846, Loss - 0.37249541763082955, Learning Rate - 0.00625, magnitude of gradient - 0.01825068001404151\n",
      "Step - 3847, Loss - 0.3028986560016239, Learning Rate - 0.00625, magnitude of gradient - 0.04856095233077796\n",
      "Step - 3848, Loss - 0.32283518330080485, Learning Rate - 0.00625, magnitude of gradient - 0.06966140033295355\n",
      "Step - 3849, Loss - 0.287928422744239, Learning Rate - 0.00625, magnitude of gradient - 0.024423928322219696\n",
      "Step - 3850, Loss - 0.3526016031048512, Learning Rate - 0.00625, magnitude of gradient - 0.07822630051974788\n",
      "Step - 3851, Loss - 0.29756206395477586, Learning Rate - 0.00625, magnitude of gradient - 0.006081610589598647\n",
      "Step - 3852, Loss - 0.2994932349365937, Learning Rate - 0.00625, magnitude of gradient - 0.06830271427886836\n",
      "Step - 3853, Loss - 0.34581133266901076, Learning Rate - 0.00625, magnitude of gradient - 0.08044952041813412\n",
      "Step - 3854, Loss - 0.3271759706388978, Learning Rate - 0.00625, magnitude of gradient - 0.07578036610669033\n",
      "Step - 3855, Loss - 0.26632587568081434, Learning Rate - 0.00625, magnitude of gradient - 0.06034160369931133\n",
      "Step - 3856, Loss - 0.3086921259322087, Learning Rate - 0.00625, magnitude of gradient - 0.07876989081844789\n",
      "Step - 3857, Loss - 0.2783780027371309, Learning Rate - 0.00625, magnitude of gradient - 0.04698901516482193\n",
      "Step - 3858, Loss - 0.28172612567040706, Learning Rate - 0.00625, magnitude of gradient - 0.06735598996585743\n",
      "Step - 3859, Loss - 0.29910203283617576, Learning Rate - 0.00625, magnitude of gradient - 0.09665615502498427\n",
      "Step - 3860, Loss - 0.34593351284710766, Learning Rate - 0.00625, magnitude of gradient - 0.10339068226715213\n",
      "Step - 3861, Loss - 0.3233603894456858, Learning Rate - 0.00625, magnitude of gradient - 0.053721133122827815\n",
      "Step - 3862, Loss - 0.34389867115751566, Learning Rate - 0.00625, magnitude of gradient - 0.05881443194207156\n",
      "Step - 3863, Loss - 0.26978320352967045, Learning Rate - 0.00625, magnitude of gradient - 0.08195685484517785\n",
      "Step - 3864, Loss - 0.32878849128609194, Learning Rate - 0.00625, magnitude of gradient - 0.02092706069765474\n",
      "Step - 3865, Loss - 0.3768456929297865, Learning Rate - 0.00625, magnitude of gradient - 0.010669427870991887\n",
      "Step - 3866, Loss - 0.32104510158323774, Learning Rate - 0.00625, magnitude of gradient - 0.017160615006443156\n",
      "Step - 3867, Loss - 0.3037648217068055, Learning Rate - 0.00625, magnitude of gradient - 0.04327211225882837\n",
      "Step - 3868, Loss - 0.34788964773926156, Learning Rate - 0.00625, magnitude of gradient - 0.12232132158002669\n",
      "Step - 3869, Loss - 0.28348123744474696, Learning Rate - 0.00625, magnitude of gradient - 0.09818018396000111\n",
      "Step - 3870, Loss - 0.32937260348282815, Learning Rate - 0.00625, magnitude of gradient - 0.03445160983535327\n",
      "Step - 3871, Loss - 0.36842701314591214, Learning Rate - 0.00625, magnitude of gradient - 0.09587437208229124\n",
      "Step - 3872, Loss - 0.29145398663460903, Learning Rate - 0.00625, magnitude of gradient - 0.048081530466450174\n",
      "Step - 3873, Loss - 0.32732607640284056, Learning Rate - 0.00625, magnitude of gradient - 0.06715204157465295\n",
      "Step - 3874, Loss - 0.296255957126006, Learning Rate - 0.00625, magnitude of gradient - 0.033159095813796134\n",
      "Step - 3875, Loss - 0.3461638955012955, Learning Rate - 0.00625, magnitude of gradient - 0.02116822129097214\n",
      "Step - 3876, Loss - 0.3321910662403465, Learning Rate - 0.00625, magnitude of gradient - 0.09475082542655494\n",
      "Step - 3877, Loss - 0.3332220059396264, Learning Rate - 0.00625, magnitude of gradient - 0.028019779022904517\n",
      "Step - 3878, Loss - 0.32948829629414417, Learning Rate - 0.00625, magnitude of gradient - 0.05207992542185873\n",
      "Step - 3879, Loss - 0.30140592800248056, Learning Rate - 0.00625, magnitude of gradient - 0.018143868454730927\n",
      "Step - 3880, Loss - 0.30148313210163397, Learning Rate - 0.00625, magnitude of gradient - 0.019082031376357635\n",
      "Step - 3881, Loss - 0.2831892330194699, Learning Rate - 0.00625, magnitude of gradient - 0.06111200412342082\n",
      "Step - 3882, Loss - 0.3099357119844313, Learning Rate - 0.00625, magnitude of gradient - 0.011440524677211312\n",
      "Step - 3883, Loss - 0.2815231752232798, Learning Rate - 0.00625, magnitude of gradient - 0.039485537511087763\n",
      "Step - 3884, Loss - 0.3207173369340278, Learning Rate - 0.00625, magnitude of gradient - 0.0810137735152286\n",
      "Step - 3885, Loss - 0.3900984757261545, Learning Rate - 0.00625, magnitude of gradient - 0.06636379228412327\n",
      "Step - 3886, Loss - 0.2956667380007515, Learning Rate - 0.00625, magnitude of gradient - 0.051159121418181984\n",
      "Step - 3887, Loss - 0.3287055298083248, Learning Rate - 0.00625, magnitude of gradient - 0.04792458524890445\n",
      "Step - 3888, Loss - 0.31637303041873194, Learning Rate - 0.00625, magnitude of gradient - 0.0717255616811791\n",
      "Step - 3889, Loss - 0.35869185829240546, Learning Rate - 0.00625, magnitude of gradient - 0.05140955688179278\n",
      "Step - 3890, Loss - 0.3255351530137836, Learning Rate - 0.00625, magnitude of gradient - 0.07515679996506602\n",
      "Step - 3891, Loss - 0.30136809597365466, Learning Rate - 0.00625, magnitude of gradient - 0.05314980580440972\n",
      "Step - 3892, Loss - 0.32626327322809123, Learning Rate - 0.00625, magnitude of gradient - 0.06299896441066263\n",
      "Step - 3893, Loss - 0.3542126653223596, Learning Rate - 0.00625, magnitude of gradient - 0.08351447149404399\n",
      "Step - 3894, Loss - 0.3857126097309441, Learning Rate - 0.00625, magnitude of gradient - 0.08418632669245435\n",
      "Step - 3895, Loss - 0.30562488829736195, Learning Rate - 0.00625, magnitude of gradient - 0.09104750251665297\n",
      "Step - 3896, Loss - 0.40530072501464987, Learning Rate - 0.00625, magnitude of gradient - 0.06838160925327788\n",
      "Step - 3897, Loss - 0.2854487377721888, Learning Rate - 0.00625, magnitude of gradient - 0.05425058928919985\n",
      "Step - 3898, Loss - 0.3100817432236595, Learning Rate - 0.00625, magnitude of gradient - 0.008402804142606711\n",
      "Step - 3899, Loss - 0.29556984064128006, Learning Rate - 0.00625, magnitude of gradient - 0.049251950474903626\n",
      "Step - 3900, Loss - 0.3287852843749377, Learning Rate - 0.00625, magnitude of gradient - 0.03212086922131284\n",
      "Step - 3901, Loss - 0.38659999881187035, Learning Rate - 0.00625, magnitude of gradient - 0.042830712722359186\n",
      "Step - 3902, Loss - 0.3568815510013413, Learning Rate - 0.00625, magnitude of gradient - 0.029818071501470995\n",
      "Step - 3903, Loss - 0.27023095431981725, Learning Rate - 0.00625, magnitude of gradient - 0.056227920754601056\n",
      "Step - 3904, Loss - 0.28481277663410565, Learning Rate - 0.00625, magnitude of gradient - 0.003493089171480033\n",
      "Step - 3905, Loss - 0.2547763690651157, Learning Rate - 0.00625, magnitude of gradient - 0.040483022000435236\n",
      "Step - 3906, Loss - 0.3703870753532816, Learning Rate - 0.00625, magnitude of gradient - 0.042975372867536075\n",
      "Step - 3907, Loss - 0.3727568863839124, Learning Rate - 0.00625, magnitude of gradient - 0.12067081071317341\n",
      "Step - 3908, Loss - 0.266048020409812, Learning Rate - 0.00625, magnitude of gradient - 0.06225565022079491\n",
      "Step - 3909, Loss - 0.313319751756569, Learning Rate - 0.00625, magnitude of gradient - 0.06522626457705907\n",
      "Step - 3910, Loss - 0.30151800074702567, Learning Rate - 0.00625, magnitude of gradient - 0.05930102841581435\n",
      "Step - 3911, Loss - 0.316154048121118, Learning Rate - 0.00625, magnitude of gradient - 0.08098672619381642\n",
      "Step - 3912, Loss - 0.37561153687696686, Learning Rate - 0.00625, magnitude of gradient - 0.09984159629104099\n",
      "Step - 3913, Loss - 0.33169335942523287, Learning Rate - 0.00625, magnitude of gradient - 0.02449800419587565\n",
      "Step - 3914, Loss - 0.3369949750646566, Learning Rate - 0.00625, magnitude of gradient - 0.04085269545791568\n",
      "Step - 3915, Loss - 0.2960544575140926, Learning Rate - 0.00625, magnitude of gradient - 0.053792346825355435\n",
      "Step - 3916, Loss - 0.38550567468547636, Learning Rate - 0.00625, magnitude of gradient - 0.03538579462263318\n",
      "Step - 3917, Loss - 0.2705986586855688, Learning Rate - 0.00625, magnitude of gradient - 0.08297851393077652\n",
      "Step - 3918, Loss - 0.3081355407442969, Learning Rate - 0.00625, magnitude of gradient - 0.02210892205968751\n",
      "Step - 3919, Loss - 0.32502003820144393, Learning Rate - 0.00625, magnitude of gradient - 0.059679198906231704\n",
      "Step - 3920, Loss - 0.2712318455927943, Learning Rate - 0.00625, magnitude of gradient - 0.010757591821787163\n",
      "Step - 3921, Loss - 0.21098838034207573, Learning Rate - 0.00625, magnitude of gradient - 0.03649452132935303\n",
      "Step - 3922, Loss - 0.2862792748677113, Learning Rate - 0.00625, magnitude of gradient - 0.0791273868301211\n",
      "Step - 3923, Loss - 0.2732900100114827, Learning Rate - 0.00625, magnitude of gradient - 0.022547966757499753\n",
      "Step - 3924, Loss - 0.2888879828116915, Learning Rate - 0.00625, magnitude of gradient - 0.02751147946691843\n",
      "Step - 3925, Loss - 0.3412998480999345, Learning Rate - 0.00625, magnitude of gradient - 0.05382123800604922\n",
      "Step - 3926, Loss - 0.32145671599697534, Learning Rate - 0.00625, magnitude of gradient - 0.048305896694051075\n",
      "Step - 3927, Loss - 0.4260610323016919, Learning Rate - 0.00625, magnitude of gradient - 0.03086727753256064\n",
      "Step - 3928, Loss - 0.3164075898991604, Learning Rate - 0.00625, magnitude of gradient - 0.045879821786902676\n",
      "Step - 3929, Loss - 0.2594693562996899, Learning Rate - 0.00625, magnitude of gradient - 0.05854572878393975\n",
      "Step - 3930, Loss - 0.26044670413016513, Learning Rate - 0.00625, magnitude of gradient - 0.044314247002315295\n",
      "Step - 3931, Loss - 0.2989682918768579, Learning Rate - 0.00625, magnitude of gradient - 0.06614483144422816\n",
      "Step - 3932, Loss - 0.28396522466825275, Learning Rate - 0.00625, magnitude of gradient - 0.07980227231509486\n",
      "Step - 3933, Loss - 0.2829318031960481, Learning Rate - 0.00625, magnitude of gradient - 0.018135964739043567\n",
      "Step - 3934, Loss - 0.3119557713624386, Learning Rate - 0.00625, magnitude of gradient - 0.04987633657513193\n",
      "Step - 3935, Loss - 0.37852634287044246, Learning Rate - 0.00625, magnitude of gradient - 0.06640973632482668\n",
      "Step - 3936, Loss - 0.30475678720646776, Learning Rate - 0.00625, magnitude of gradient - 0.015737805381191627\n",
      "Step - 3937, Loss - 0.27327051767384597, Learning Rate - 0.00625, magnitude of gradient - 0.03827103157177919\n",
      "Step - 3938, Loss - 0.3032567234952619, Learning Rate - 0.00625, magnitude of gradient - 0.029503500281127984\n",
      "Step - 3939, Loss - 0.24295204055374922, Learning Rate - 0.00625, magnitude of gradient - 0.05077028666960563\n",
      "Step - 3940, Loss - 0.3502455301275258, Learning Rate - 0.00625, magnitude of gradient - 0.051415958038967466\n",
      "Step - 3941, Loss - 0.3752504283315916, Learning Rate - 0.00625, magnitude of gradient - 0.043351997121416276\n",
      "Step - 3942, Loss - 0.24411571604632634, Learning Rate - 0.00625, magnitude of gradient - 0.03672588592211971\n",
      "Step - 3943, Loss - 0.2962076255199389, Learning Rate - 0.00625, magnitude of gradient - 0.047096229076289665\n",
      "Step - 3944, Loss - 0.32909377144567, Learning Rate - 0.00625, magnitude of gradient - 0.0365686741950734\n",
      "Step - 3945, Loss - 0.3622218463435246, Learning Rate - 0.00625, magnitude of gradient - 0.1260943189320747\n",
      "Step - 3946, Loss - 0.35495331546269304, Learning Rate - 0.00625, magnitude of gradient - 0.008230227009375611\n",
      "Step - 3947, Loss - 0.26625331098864047, Learning Rate - 0.00625, magnitude of gradient - 0.05085643594133181\n",
      "Step - 3948, Loss - 0.3568248308534228, Learning Rate - 0.00625, magnitude of gradient - 0.09203721214026887\n",
      "Step - 3949, Loss - 0.29261974585841954, Learning Rate - 0.00625, magnitude of gradient - 0.05859022024914557\n",
      "Step - 3950, Loss - 0.30364745681224237, Learning Rate - 0.00625, magnitude of gradient - 0.021674166726403276\n",
      "Step - 3951, Loss - 0.3571574382408727, Learning Rate - 0.00625, magnitude of gradient - 0.06362940009681403\n",
      "Step - 3952, Loss - 0.26574078870339807, Learning Rate - 0.00625, magnitude of gradient - 0.03998193386601821\n",
      "Step - 3953, Loss - 0.3195548134967937, Learning Rate - 0.00625, magnitude of gradient - 0.056012650304215206\n",
      "Step - 3954, Loss - 0.3513277993840836, Learning Rate - 0.00625, magnitude of gradient - 0.06284743601833195\n",
      "Step - 3955, Loss - 0.3645774127709945, Learning Rate - 0.00625, magnitude of gradient - 0.022843861634946224\n",
      "Step - 3956, Loss - 0.33234543042880654, Learning Rate - 0.00625, magnitude of gradient - 0.08260893921507644\n",
      "Step - 3957, Loss - 0.39317148509246846, Learning Rate - 0.00625, magnitude of gradient - 0.019938834204655552\n",
      "Step - 3958, Loss - 0.33869540129365716, Learning Rate - 0.00625, magnitude of gradient - 0.033759983412649526\n",
      "Step - 3959, Loss - 0.3009607984689905, Learning Rate - 0.00625, magnitude of gradient - 0.033267441040742536\n",
      "Step - 3960, Loss - 0.3365894505053049, Learning Rate - 0.00625, magnitude of gradient - 0.10851286057841379\n",
      "Step - 3961, Loss - 0.3294910173587997, Learning Rate - 0.00625, magnitude of gradient - 0.04812378369522208\n",
      "Step - 3962, Loss - 0.2780203785789059, Learning Rate - 0.00625, magnitude of gradient - 0.04018052692545451\n",
      "Step - 3963, Loss - 0.34021342966506546, Learning Rate - 0.00625, magnitude of gradient - 0.049038835414031874\n",
      "Step - 3964, Loss - 0.28762876530212456, Learning Rate - 0.00625, magnitude of gradient - 0.032799260399059435\n",
      "Step - 3965, Loss - 0.3312711972638347, Learning Rate - 0.00625, magnitude of gradient - 0.09008112242833208\n",
      "Step - 3966, Loss - 0.290253108662569, Learning Rate - 0.00625, magnitude of gradient - 0.0462056628935649\n",
      "Step - 3967, Loss - 0.3171801278975302, Learning Rate - 0.00625, magnitude of gradient - 0.045402394171624276\n",
      "Step - 3968, Loss - 0.3119013054257595, Learning Rate - 0.00625, magnitude of gradient - 0.022153254376390413\n",
      "Step - 3969, Loss - 0.3253837714386656, Learning Rate - 0.00625, magnitude of gradient - 0.017336989619050922\n",
      "Step - 3970, Loss - 0.3905170184322564, Learning Rate - 0.00625, magnitude of gradient - 0.0362028997580903\n",
      "Step - 3971, Loss - 0.27280819154876035, Learning Rate - 0.00625, magnitude of gradient - 0.01501776450036762\n",
      "Step - 3972, Loss - 0.36115760708168293, Learning Rate - 0.00625, magnitude of gradient - 0.033187918121119986\n",
      "Step - 3973, Loss - 0.2656056879096093, Learning Rate - 0.00625, magnitude of gradient - 0.06577900223486903\n",
      "Step - 3974, Loss - 0.34952023806341986, Learning Rate - 0.00625, magnitude of gradient - 0.024317945772633506\n",
      "Step - 3975, Loss - 0.33956951739724855, Learning Rate - 0.00625, magnitude of gradient - 0.043933608679948444\n",
      "Step - 3976, Loss - 0.34817376704487474, Learning Rate - 0.00625, magnitude of gradient - 0.011340615518309293\n",
      "Step - 3977, Loss - 0.38564641774760633, Learning Rate - 0.00625, magnitude of gradient - 0.04748434670123074\n",
      "Step - 3978, Loss - 0.29557024690968053, Learning Rate - 0.00625, magnitude of gradient - 0.052623160392233324\n",
      "Step - 3979, Loss - 0.30478951562789247, Learning Rate - 0.00625, magnitude of gradient - 0.022259761117376365\n",
      "Step - 3980, Loss - 0.36243875931150915, Learning Rate - 0.00625, magnitude of gradient - 0.08746296716152853\n",
      "Step - 3981, Loss - 0.35766714853404125, Learning Rate - 0.00625, magnitude of gradient - 0.0523982072738476\n",
      "Step - 3982, Loss - 0.28494897037896394, Learning Rate - 0.00625, magnitude of gradient - 0.0634901048396796\n",
      "Step - 3983, Loss - 0.3026061620567296, Learning Rate - 0.00625, magnitude of gradient - 0.06344686049515788\n",
      "Step - 3984, Loss - 0.21530593367827966, Learning Rate - 0.00625, magnitude of gradient - 0.10721065476388479\n",
      "Step - 3985, Loss - 0.3447912724956569, Learning Rate - 0.00625, magnitude of gradient - 0.039228810770402474\n",
      "Step - 3986, Loss - 0.3844059155676498, Learning Rate - 0.00625, magnitude of gradient - 0.027454823456944943\n",
      "Step - 3987, Loss - 0.2822465685211314, Learning Rate - 0.00625, magnitude of gradient - 0.024257954464719744\n",
      "Step - 3988, Loss - 0.3109944878520823, Learning Rate - 0.00625, magnitude of gradient - 0.04592301076820702\n",
      "Step - 3989, Loss - 0.3234814155269531, Learning Rate - 0.00625, magnitude of gradient - 0.05501662675275262\n",
      "Step - 3990, Loss - 0.3615356514520647, Learning Rate - 0.00625, magnitude of gradient - 0.03771534009304976\n",
      "Step - 3991, Loss - 0.29499945611272727, Learning Rate - 0.00625, magnitude of gradient - 0.06902209999843535\n",
      "Step - 3992, Loss - 0.3422647347942919, Learning Rate - 0.00625, magnitude of gradient - 0.01620509581469364\n",
      "Step - 3993, Loss - 0.3212974932748303, Learning Rate - 0.00625, magnitude of gradient - 0.07572058627780845\n",
      "Step - 3994, Loss - 0.38898899742712423, Learning Rate - 0.00625, magnitude of gradient - 0.09099486324271193\n",
      "Step - 3995, Loss - 0.32334067115774806, Learning Rate - 0.00625, magnitude of gradient - 0.02899864497598379\n",
      "Step - 3996, Loss - 0.22964933418518862, Learning Rate - 0.00625, magnitude of gradient - 0.05725606886284277\n",
      "Step - 3997, Loss - 0.28092663635045445, Learning Rate - 0.00625, magnitude of gradient - 0.07914324158259808\n",
      "Step - 3998, Loss - 0.3029459380177657, Learning Rate - 0.00625, magnitude of gradient - 0.03471357724545203\n",
      "Step - 3999, Loss - 0.25907943409130424, Learning Rate - 0.00625, magnitude of gradient - 0.07620139174418884\n",
      "Step - 4000, Loss - 0.30740068533965315, Learning Rate - 0.00625, magnitude of gradient - 0.010156374238518765\n",
      "Step - 4001, Loss - 0.2597504103184938, Learning Rate - 0.003125, magnitude of gradient - 0.02034041851694726\n",
      "Step - 4002, Loss - 0.345993964428922, Learning Rate - 0.003125, magnitude of gradient - 0.047079858853439564\n",
      "Step - 4003, Loss - 0.32204537837462277, Learning Rate - 0.003125, magnitude of gradient - 0.044333212337851056\n",
      "Step - 4004, Loss - 0.24373173360980677, Learning Rate - 0.003125, magnitude of gradient - 0.035564816533669316\n",
      "Step - 4005, Loss - 0.35227172252310734, Learning Rate - 0.003125, magnitude of gradient - 0.02309280294591393\n",
      "Step - 4006, Loss - 0.31800423710068626, Learning Rate - 0.003125, magnitude of gradient - 0.02112107140958792\n",
      "Step - 4007, Loss - 0.41513663106651333, Learning Rate - 0.003125, magnitude of gradient - 0.06411161998159198\n",
      "Step - 4008, Loss - 0.3364176288553688, Learning Rate - 0.003125, magnitude of gradient - 0.08493654147262453\n",
      "Step - 4009, Loss - 0.291884738728648, Learning Rate - 0.003125, magnitude of gradient - 0.06623554906543504\n",
      "Step - 4010, Loss - 0.40190533979052906, Learning Rate - 0.003125, magnitude of gradient - 0.11511928187708013\n",
      "Step - 4011, Loss - 0.2795260349237957, Learning Rate - 0.003125, magnitude of gradient - 0.09304174374088782\n",
      "Step - 4012, Loss - 0.20544847040627107, Learning Rate - 0.003125, magnitude of gradient - 0.07105444743453114\n",
      "Step - 4013, Loss - 0.3768539247703581, Learning Rate - 0.003125, magnitude of gradient - 0.022418228421893387\n",
      "Step - 4014, Loss - 0.3595760569984835, Learning Rate - 0.003125, magnitude of gradient - 0.12012464136920284\n",
      "Step - 4015, Loss - 0.3535506157938398, Learning Rate - 0.003125, magnitude of gradient - 0.08431277175062019\n",
      "Step - 4016, Loss - 0.36182261361957513, Learning Rate - 0.003125, magnitude of gradient - 0.06339272881607108\n",
      "Step - 4017, Loss - 0.3384013265192367, Learning Rate - 0.003125, magnitude of gradient - 0.08233953277535665\n",
      "Step - 4018, Loss - 0.2786730181153605, Learning Rate - 0.003125, magnitude of gradient - 0.05421898224657758\n",
      "Step - 4019, Loss - 0.33337458432329087, Learning Rate - 0.003125, magnitude of gradient - 0.014102678623144748\n",
      "Step - 4020, Loss - 0.29320355553152433, Learning Rate - 0.003125, magnitude of gradient - 0.04451811605888312\n",
      "Step - 4021, Loss - 0.3213729829783605, Learning Rate - 0.003125, magnitude of gradient - 0.08456840254569016\n",
      "Step - 4022, Loss - 0.2901476087868382, Learning Rate - 0.003125, magnitude of gradient - 0.014881815480253953\n",
      "Step - 4023, Loss - 0.3809653048062528, Learning Rate - 0.003125, magnitude of gradient - 0.05532088127980916\n",
      "Step - 4024, Loss - 0.2928389278029565, Learning Rate - 0.003125, magnitude of gradient - 0.02084569537141825\n",
      "Step - 4025, Loss - 0.33017886854014256, Learning Rate - 0.003125, magnitude of gradient - 0.06964116947163905\n",
      "Step - 4026, Loss - 0.37352992691191206, Learning Rate - 0.003125, magnitude of gradient - 0.07237400010479404\n",
      "Step - 4027, Loss - 0.3575277531354613, Learning Rate - 0.003125, magnitude of gradient - 0.0749264568441725\n",
      "Step - 4028, Loss - 0.30859466050338097, Learning Rate - 0.003125, magnitude of gradient - 0.05970969994459556\n",
      "Step - 4029, Loss - 0.2897490723187618, Learning Rate - 0.003125, magnitude of gradient - 0.05145109439017313\n",
      "Step - 4030, Loss - 0.30838228553266317, Learning Rate - 0.003125, magnitude of gradient - 0.012734956837135675\n",
      "Step - 4031, Loss - 0.3240249134687484, Learning Rate - 0.003125, magnitude of gradient - 0.08395865243971944\n",
      "Step - 4032, Loss - 0.3316484261240014, Learning Rate - 0.003125, magnitude of gradient - 0.04774556437152421\n",
      "Step - 4033, Loss - 0.29402655804929617, Learning Rate - 0.003125, magnitude of gradient - 0.06309151768137504\n",
      "Step - 4034, Loss - 0.33139102926394925, Learning Rate - 0.003125, magnitude of gradient - 0.16648059281400943\n",
      "Step - 4035, Loss - 0.3037604537425507, Learning Rate - 0.003125, magnitude of gradient - 0.08231891777010072\n",
      "Step - 4036, Loss - 0.2960201079173316, Learning Rate - 0.003125, magnitude of gradient - 0.018515430657383505\n",
      "Step - 4037, Loss - 0.2945763429217763, Learning Rate - 0.003125, magnitude of gradient - 0.014353709963186282\n",
      "Step - 4038, Loss - 0.2658132400134152, Learning Rate - 0.003125, magnitude of gradient - 0.02851669225893948\n",
      "Step - 4039, Loss - 0.33850956911749025, Learning Rate - 0.003125, magnitude of gradient - 0.08186932083800759\n",
      "Step - 4040, Loss - 0.3379633421199693, Learning Rate - 0.003125, magnitude of gradient - 0.038791088481813604\n",
      "Step - 4041, Loss - 0.37687915963215546, Learning Rate - 0.003125, magnitude of gradient - 0.05096759627804098\n",
      "Step - 4042, Loss - 0.3067848198948266, Learning Rate - 0.003125, magnitude of gradient - 0.05359028689963\n",
      "Step - 4043, Loss - 0.36190051930332195, Learning Rate - 0.003125, magnitude of gradient - 0.059628695038565015\n",
      "Step - 4044, Loss - 0.3094418745292282, Learning Rate - 0.003125, magnitude of gradient - 0.03343312876932377\n",
      "Step - 4045, Loss - 0.2760518181831283, Learning Rate - 0.003125, magnitude of gradient - 0.10870144216331078\n",
      "Step - 4046, Loss - 0.26986368671061867, Learning Rate - 0.003125, magnitude of gradient - 0.04457691433057263\n",
      "Step - 4047, Loss - 0.36837631438776397, Learning Rate - 0.003125, magnitude of gradient - 0.054619879303019155\n",
      "Step - 4048, Loss - 0.2780194281386865, Learning Rate - 0.003125, magnitude of gradient - 0.019709443887576318\n",
      "Step - 4049, Loss - 0.3253999114707654, Learning Rate - 0.003125, magnitude of gradient - 0.0746009221931573\n",
      "Step - 4050, Loss - 0.41240853368881397, Learning Rate - 0.003125, magnitude of gradient - 0.09146151284108976\n",
      "Step - 4051, Loss - 0.38269416052486804, Learning Rate - 0.003125, magnitude of gradient - 0.024123728502453697\n",
      "Step - 4052, Loss - 0.32605839548911814, Learning Rate - 0.003125, magnitude of gradient - 0.07306652326233691\n",
      "Step - 4053, Loss - 0.3414815462637301, Learning Rate - 0.003125, magnitude of gradient - 0.03617455958201943\n",
      "Step - 4054, Loss - 0.31555207005424785, Learning Rate - 0.003125, magnitude of gradient - 0.05285416490457696\n",
      "Step - 4055, Loss - 0.2798826096346816, Learning Rate - 0.003125, magnitude of gradient - 0.035532602464839025\n",
      "Step - 4056, Loss - 0.25884903665376763, Learning Rate - 0.003125, magnitude of gradient - 0.077209619600831\n",
      "Step - 4057, Loss - 0.3132207131156439, Learning Rate - 0.003125, magnitude of gradient - 0.02299696070480097\n",
      "Step - 4058, Loss - 0.2990876474235519, Learning Rate - 0.003125, magnitude of gradient - 0.06465946972435616\n",
      "Step - 4059, Loss - 0.31823651703195255, Learning Rate - 0.003125, magnitude of gradient - 0.061630302379721835\n",
      "Step - 4060, Loss - 0.29338769784452806, Learning Rate - 0.003125, magnitude of gradient - 0.012182764505406622\n",
      "Step - 4061, Loss - 0.28884016006511976, Learning Rate - 0.003125, magnitude of gradient - 0.02145502700859302\n",
      "Step - 4062, Loss - 0.3518586955476519, Learning Rate - 0.003125, magnitude of gradient - 0.03937955972372835\n",
      "Step - 4063, Loss - 0.374392136351104, Learning Rate - 0.003125, magnitude of gradient - 0.04591552899501699\n",
      "Step - 4064, Loss - 0.332908673924338, Learning Rate - 0.003125, magnitude of gradient - 0.0903464012127411\n",
      "Step - 4065, Loss - 0.3004929854885585, Learning Rate - 0.003125, magnitude of gradient - 0.03835675970842303\n",
      "Step - 4066, Loss - 0.274092628706812, Learning Rate - 0.003125, magnitude of gradient - 0.04654591155497992\n",
      "Step - 4067, Loss - 0.2874988086789843, Learning Rate - 0.003125, magnitude of gradient - 0.04203058641172657\n",
      "Step - 4068, Loss - 0.31326944302335474, Learning Rate - 0.003125, magnitude of gradient - 0.0957565645865366\n",
      "Step - 4069, Loss - 0.23788309461716656, Learning Rate - 0.003125, magnitude of gradient - 0.042030648044236125\n",
      "Step - 4070, Loss - 0.2959934525656037, Learning Rate - 0.003125, magnitude of gradient - 0.023181422453966448\n",
      "Step - 4071, Loss - 0.38807377013678346, Learning Rate - 0.003125, magnitude of gradient - 0.035710478386296235\n",
      "Step - 4072, Loss - 0.3453884354367638, Learning Rate - 0.003125, magnitude of gradient - 0.01109348411843889\n",
      "Step - 4073, Loss - 0.34197601864536814, Learning Rate - 0.003125, magnitude of gradient - 0.06400262706406148\n",
      "Step - 4074, Loss - 0.3094688346529704, Learning Rate - 0.003125, magnitude of gradient - 0.03978004547828627\n",
      "Step - 4075, Loss - 0.38710687883727213, Learning Rate - 0.003125, magnitude of gradient - 0.0063785289192003465\n",
      "Step - 4076, Loss - 0.34925760936446726, Learning Rate - 0.003125, magnitude of gradient - 0.08814732197559429\n",
      "Step - 4077, Loss - 0.3474314799986456, Learning Rate - 0.003125, magnitude of gradient - 0.0926378775324064\n",
      "Step - 4078, Loss - 0.35712710098235223, Learning Rate - 0.003125, magnitude of gradient - 0.029813376119486115\n",
      "Step - 4079, Loss - 0.32826662628207626, Learning Rate - 0.003125, magnitude of gradient - 0.03214693297160999\n",
      "Step - 4080, Loss - 0.2712366029748642, Learning Rate - 0.003125, magnitude of gradient - 0.048375174607674674\n",
      "Step - 4081, Loss - 0.28645486357227024, Learning Rate - 0.003125, magnitude of gradient - 0.04284997178768958\n",
      "Step - 4082, Loss - 0.26667558502997885, Learning Rate - 0.003125, magnitude of gradient - 0.054083154949016825\n",
      "Step - 4083, Loss - 0.26957534287806156, Learning Rate - 0.003125, magnitude of gradient - 0.037194340765819574\n",
      "Step - 4084, Loss - 0.3223332828197422, Learning Rate - 0.003125, magnitude of gradient - 0.0928357293734668\n",
      "Step - 4085, Loss - 0.3242250192635712, Learning Rate - 0.003125, magnitude of gradient - 0.019262192653939844\n",
      "Step - 4086, Loss - 0.3173152982416778, Learning Rate - 0.003125, magnitude of gradient - 0.0461688220200934\n",
      "Step - 4087, Loss - 0.27461222848404465, Learning Rate - 0.003125, magnitude of gradient - 0.05378396438444766\n",
      "Step - 4088, Loss - 0.3084401221385294, Learning Rate - 0.003125, magnitude of gradient - 0.03211332166270724\n",
      "Step - 4089, Loss - 0.35506466599345443, Learning Rate - 0.003125, magnitude of gradient - 0.018035057631320243\n",
      "Step - 4090, Loss - 0.29937905096475287, Learning Rate - 0.003125, magnitude of gradient - 0.06580388891947912\n",
      "Step - 4091, Loss - 0.30724275762397335, Learning Rate - 0.003125, magnitude of gradient - 0.03319954060562047\n",
      "Step - 4092, Loss - 0.34593109208833484, Learning Rate - 0.003125, magnitude of gradient - 0.056297666415357585\n",
      "Step - 4093, Loss - 0.32054622904474417, Learning Rate - 0.003125, magnitude of gradient - 0.04653412548355009\n",
      "Step - 4094, Loss - 0.37026007147839823, Learning Rate - 0.003125, magnitude of gradient - 0.055883354916048136\n",
      "Step - 4095, Loss - 0.3523660790378413, Learning Rate - 0.003125, magnitude of gradient - 0.1008552085577203\n",
      "Step - 4096, Loss - 0.2977191355452047, Learning Rate - 0.003125, magnitude of gradient - 0.08538940529435081\n",
      "Step - 4097, Loss - 0.3226106269614826, Learning Rate - 0.003125, magnitude of gradient - 0.10151129515292164\n",
      "Step - 4098, Loss - 0.32033741558576384, Learning Rate - 0.003125, magnitude of gradient - 0.07365782185488852\n",
      "Step - 4099, Loss - 0.30473422476191747, Learning Rate - 0.003125, magnitude of gradient - 0.03251721895002459\n",
      "Step - 4100, Loss - 0.40861467663542594, Learning Rate - 0.003125, magnitude of gradient - 0.05936566310609118\n",
      "Step - 4101, Loss - 0.2683428047216444, Learning Rate - 0.003125, magnitude of gradient - 0.07362822169775199\n",
      "Step - 4102, Loss - 0.37656403817327655, Learning Rate - 0.003125, magnitude of gradient - 0.03723627460510281\n",
      "Step - 4103, Loss - 0.2887129441734727, Learning Rate - 0.003125, magnitude of gradient - 0.05007440813111013\n",
      "Step - 4104, Loss - 0.3581231708960671, Learning Rate - 0.003125, magnitude of gradient - 0.045044563562889585\n",
      "Step - 4105, Loss - 0.30921029236228137, Learning Rate - 0.003125, magnitude of gradient - 0.13870424808185888\n",
      "Step - 4106, Loss - 0.38831444542550897, Learning Rate - 0.003125, magnitude of gradient - 0.04278007934524242\n",
      "Step - 4107, Loss - 0.28497593624768186, Learning Rate - 0.003125, magnitude of gradient - 0.03542479471003346\n",
      "Step - 4108, Loss - 0.2881766183504787, Learning Rate - 0.003125, magnitude of gradient - 0.03896846928269492\n",
      "Step - 4109, Loss - 0.33425984582889934, Learning Rate - 0.003125, magnitude of gradient - 0.023806007255917066\n",
      "Step - 4110, Loss - 0.2639962259906854, Learning Rate - 0.003125, magnitude of gradient - 0.030792711899990687\n",
      "Step - 4111, Loss - 0.2799194227261594, Learning Rate - 0.003125, magnitude of gradient - 0.05222257355151616\n",
      "Step - 4112, Loss - 0.30506534807522045, Learning Rate - 0.003125, magnitude of gradient - 0.09628820944648353\n",
      "Step - 4113, Loss - 0.3402412326776305, Learning Rate - 0.003125, magnitude of gradient - 0.0155883286517113\n",
      "Step - 4114, Loss - 0.29351717131486893, Learning Rate - 0.003125, magnitude of gradient - 0.0894797481065477\n",
      "Step - 4115, Loss - 0.36826622812783827, Learning Rate - 0.003125, magnitude of gradient - 0.0716420277479372\n",
      "Step - 4116, Loss - 0.25680447503856785, Learning Rate - 0.003125, magnitude of gradient - 0.059936559206006074\n",
      "Step - 4117, Loss - 0.35608944031141776, Learning Rate - 0.003125, magnitude of gradient - 0.13560634944599864\n",
      "Step - 4118, Loss - 0.36209246479052815, Learning Rate - 0.003125, magnitude of gradient - 0.05647068475204782\n",
      "Step - 4119, Loss - 0.2971344375010876, Learning Rate - 0.003125, magnitude of gradient - 0.026144514670661666\n",
      "Step - 4120, Loss - 0.23560101586125323, Learning Rate - 0.003125, magnitude of gradient - 0.08493548294778977\n",
      "Step - 4121, Loss - 0.3884519467538587, Learning Rate - 0.003125, magnitude of gradient - 0.02212108658402199\n",
      "Step - 4122, Loss - 0.2919570530017958, Learning Rate - 0.003125, magnitude of gradient - 0.0810881376962735\n",
      "Step - 4123, Loss - 0.32851678314045446, Learning Rate - 0.003125, magnitude of gradient - 0.07703713958636842\n",
      "Step - 4124, Loss - 0.31347363322157634, Learning Rate - 0.003125, magnitude of gradient - 0.0632648897256899\n",
      "Step - 4125, Loss - 0.2967173498985502, Learning Rate - 0.003125, magnitude of gradient - 0.068981813854473\n",
      "Step - 4126, Loss - 0.3176960818521748, Learning Rate - 0.003125, magnitude of gradient - 0.06272384775442144\n",
      "Step - 4127, Loss - 0.2965607459527435, Learning Rate - 0.003125, magnitude of gradient - 0.08851829231896252\n",
      "Step - 4128, Loss - 0.2797019917527404, Learning Rate - 0.003125, magnitude of gradient - 0.02643262555525853\n",
      "Step - 4129, Loss - 0.4042522258201675, Learning Rate - 0.003125, magnitude of gradient - 0.048005923674686056\n",
      "Step - 4130, Loss - 0.2676991762895041, Learning Rate - 0.003125, magnitude of gradient - 0.008567482497488465\n",
      "Step - 4131, Loss - 0.3235642721297536, Learning Rate - 0.003125, magnitude of gradient - 0.12447743505257836\n",
      "Step - 4132, Loss - 0.378443139425721, Learning Rate - 0.003125, magnitude of gradient - 0.04418147412655208\n",
      "Step - 4133, Loss - 0.32194811295046943, Learning Rate - 0.003125, magnitude of gradient - 0.07300267442211332\n",
      "Step - 4134, Loss - 0.412966400433032, Learning Rate - 0.003125, magnitude of gradient - 0.139198494413178\n",
      "Step - 4135, Loss - 0.43460675727216486, Learning Rate - 0.003125, magnitude of gradient - 0.044022785292825405\n",
      "Step - 4136, Loss - 0.311678525483086, Learning Rate - 0.003125, magnitude of gradient - 0.019128264324856478\n",
      "Step - 4137, Loss - 0.40416713431932394, Learning Rate - 0.003125, magnitude of gradient - 0.07073593594241703\n",
      "Step - 4138, Loss - 0.42245187378799826, Learning Rate - 0.003125, magnitude of gradient - 0.027788538462147797\n",
      "Step - 4139, Loss - 0.3016523256980452, Learning Rate - 0.003125, magnitude of gradient - 0.03153490824937649\n",
      "Step - 4140, Loss - 0.3795948291918001, Learning Rate - 0.003125, magnitude of gradient - 0.046545525340046995\n",
      "Step - 4141, Loss - 0.24658451970483986, Learning Rate - 0.003125, magnitude of gradient - 0.048031216720597\n",
      "Step - 4142, Loss - 0.3400893014595564, Learning Rate - 0.003125, magnitude of gradient - 0.0534249013566814\n",
      "Step - 4143, Loss - 0.3444909286012076, Learning Rate - 0.003125, magnitude of gradient - 0.02869220790829323\n",
      "Step - 4144, Loss - 0.3467167491246538, Learning Rate - 0.003125, magnitude of gradient - 0.01565663202867937\n",
      "Step - 4145, Loss - 0.3181084111077056, Learning Rate - 0.003125, magnitude of gradient - 0.03710548835345609\n",
      "Step - 4146, Loss - 0.25208831982621943, Learning Rate - 0.003125, magnitude of gradient - 0.0623824443575069\n",
      "Step - 4147, Loss - 0.29629499909725715, Learning Rate - 0.003125, magnitude of gradient - 0.1082523153244198\n",
      "Step - 4148, Loss - 0.3480402525584384, Learning Rate - 0.003125, magnitude of gradient - 0.047576454575738045\n",
      "Step - 4149, Loss - 0.3958535138521603, Learning Rate - 0.003125, magnitude of gradient - 0.013616976065215898\n",
      "Step - 4150, Loss - 0.2867329360540954, Learning Rate - 0.003125, magnitude of gradient - 0.05338629231144783\n",
      "Step - 4151, Loss - 0.34996491148814746, Learning Rate - 0.003125, magnitude of gradient - 0.019994902980369614\n",
      "Step - 4152, Loss - 0.34855531493483966, Learning Rate - 0.003125, magnitude of gradient - 0.07068906951764693\n",
      "Step - 4153, Loss - 0.3586099031970253, Learning Rate - 0.003125, magnitude of gradient - 0.09104443107963416\n",
      "Step - 4154, Loss - 0.28749973330614426, Learning Rate - 0.003125, magnitude of gradient - 0.03644757789475734\n",
      "Step - 4155, Loss - 0.29568152359327543, Learning Rate - 0.003125, magnitude of gradient - 0.09379453755901025\n",
      "Step - 4156, Loss - 0.30301633339057144, Learning Rate - 0.003125, magnitude of gradient - 0.047919320110574114\n",
      "Step - 4157, Loss - 0.33792502289359916, Learning Rate - 0.003125, magnitude of gradient - 0.061865293679180644\n",
      "Step - 4158, Loss - 0.3892980240713224, Learning Rate - 0.003125, magnitude of gradient - 0.03865700392823952\n",
      "Step - 4159, Loss - 0.28057464328513415, Learning Rate - 0.003125, magnitude of gradient - 0.05657743240463578\n",
      "Step - 4160, Loss - 0.32846300936748407, Learning Rate - 0.003125, magnitude of gradient - 0.05940085111658585\n",
      "Step - 4161, Loss - 0.3302833703585063, Learning Rate - 0.003125, magnitude of gradient - 0.07464876601933189\n",
      "Step - 4162, Loss - 0.3586936138790076, Learning Rate - 0.003125, magnitude of gradient - 0.054301635018330977\n",
      "Step - 4163, Loss - 0.2861578483022389, Learning Rate - 0.003125, magnitude of gradient - 0.055984453646650834\n",
      "Step - 4164, Loss - 0.3075947264382636, Learning Rate - 0.003125, magnitude of gradient - 0.03503646837108021\n",
      "Step - 4165, Loss - 0.34381254829731034, Learning Rate - 0.003125, magnitude of gradient - 0.07074634511956204\n",
      "Step - 4166, Loss - 0.32980258466152795, Learning Rate - 0.003125, magnitude of gradient - 0.06832519510334094\n",
      "Step - 4167, Loss - 0.3498672372960529, Learning Rate - 0.003125, magnitude of gradient - 0.10412472873807806\n",
      "Step - 4168, Loss - 0.3086736003089747, Learning Rate - 0.003125, magnitude of gradient - 0.0557050277951044\n",
      "Step - 4169, Loss - 0.29669629329951763, Learning Rate - 0.003125, magnitude of gradient - 0.07363993969025598\n",
      "Step - 4170, Loss - 0.4464173102734447, Learning Rate - 0.003125, magnitude of gradient - 0.056674151753221026\n",
      "Step - 4171, Loss - 0.3151504568484195, Learning Rate - 0.003125, magnitude of gradient - 0.0817647077822204\n",
      "Step - 4172, Loss - 0.3215861589564342, Learning Rate - 0.003125, magnitude of gradient - 0.014145374533382108\n",
      "Step - 4173, Loss - 0.3280330397592779, Learning Rate - 0.003125, magnitude of gradient - 0.05996152907077396\n",
      "Step - 4174, Loss - 0.297341145329014, Learning Rate - 0.003125, magnitude of gradient - 0.05787554580407432\n",
      "Step - 4175, Loss - 0.30006252619633716, Learning Rate - 0.003125, magnitude of gradient - 0.034754077504536185\n",
      "Step - 4176, Loss - 0.25331981027195893, Learning Rate - 0.003125, magnitude of gradient - 0.0288295961870219\n",
      "Step - 4177, Loss - 0.3363676518286581, Learning Rate - 0.003125, magnitude of gradient - 0.03977985282952484\n",
      "Step - 4178, Loss - 0.4467981382492177, Learning Rate - 0.003125, magnitude of gradient - 0.07217624732301933\n",
      "Step - 4179, Loss - 0.25475193411965164, Learning Rate - 0.003125, magnitude of gradient - 0.06096723139710256\n",
      "Step - 4180, Loss - 0.36794832593187843, Learning Rate - 0.003125, magnitude of gradient - 0.0089731041387035\n",
      "Step - 4181, Loss - 0.30346416520652314, Learning Rate - 0.003125, magnitude of gradient - 0.06739789149958139\n",
      "Step - 4182, Loss - 0.3243353365003635, Learning Rate - 0.003125, magnitude of gradient - 0.02736482232990207\n",
      "Step - 4183, Loss - 0.30483543066424523, Learning Rate - 0.003125, magnitude of gradient - 0.03963305983804257\n",
      "Step - 4184, Loss - 0.3955041331160616, Learning Rate - 0.003125, magnitude of gradient - 0.06443199404979787\n",
      "Step - 4185, Loss - 0.35723790761968355, Learning Rate - 0.003125, magnitude of gradient - 0.054984673056108185\n",
      "Step - 4186, Loss - 0.2843996424772602, Learning Rate - 0.003125, magnitude of gradient - 0.03250900765213148\n",
      "Step - 4187, Loss - 0.30456882672320507, Learning Rate - 0.003125, magnitude of gradient - 0.013993009679601504\n",
      "Step - 4188, Loss - 0.3594438652344333, Learning Rate - 0.003125, magnitude of gradient - 0.16630596780771476\n",
      "Step - 4189, Loss - 0.35277490913643983, Learning Rate - 0.003125, magnitude of gradient - 0.031146544847240606\n",
      "Step - 4190, Loss - 0.3106394521581936, Learning Rate - 0.003125, magnitude of gradient - 0.08553792178133876\n",
      "Step - 4191, Loss - 0.2637543616863147, Learning Rate - 0.003125, magnitude of gradient - 0.042723930303878985\n",
      "Step - 4192, Loss - 0.2562153879666938, Learning Rate - 0.003125, magnitude of gradient - 0.06074110528310884\n",
      "Step - 4193, Loss - 0.38405236381027136, Learning Rate - 0.003125, magnitude of gradient - 0.018630329611140507\n",
      "Step - 4194, Loss - 0.32808708400581404, Learning Rate - 0.003125, magnitude of gradient - 0.06927461458121395\n",
      "Step - 4195, Loss - 0.30072662963848457, Learning Rate - 0.003125, magnitude of gradient - 0.07828806620893791\n",
      "Step - 4196, Loss - 0.3407880399461095, Learning Rate - 0.003125, magnitude of gradient - 0.009451288465722114\n",
      "Step - 4197, Loss - 0.31492520587559586, Learning Rate - 0.003125, magnitude of gradient - 0.04939986571554054\n",
      "Step - 4198, Loss - 0.2546805031978496, Learning Rate - 0.003125, magnitude of gradient - 0.055995385222389414\n",
      "Step - 4199, Loss - 0.27459873344638674, Learning Rate - 0.003125, magnitude of gradient - 0.05412576371586341\n",
      "Step - 4200, Loss - 0.31534969114670985, Learning Rate - 0.003125, magnitude of gradient - 0.015429105923506322\n",
      "Step - 4201, Loss - 0.349550535128378, Learning Rate - 0.003125, magnitude of gradient - 0.028822043422076823\n",
      "Step - 4202, Loss - 0.37960673993416066, Learning Rate - 0.003125, magnitude of gradient - 0.07507812946861225\n",
      "Step - 4203, Loss - 0.3542218712900265, Learning Rate - 0.003125, magnitude of gradient - 0.08109597126653686\n",
      "Step - 4204, Loss - 0.34682174729539594, Learning Rate - 0.003125, magnitude of gradient - 0.08418780035044308\n",
      "Step - 4205, Loss - 0.33782470172949397, Learning Rate - 0.003125, magnitude of gradient - 0.054170755687403614\n",
      "Step - 4206, Loss - 0.2834862276722655, Learning Rate - 0.003125, magnitude of gradient - 0.0235581902594403\n",
      "Step - 4207, Loss - 0.32896009985524466, Learning Rate - 0.003125, magnitude of gradient - 0.045811177711131516\n",
      "Step - 4208, Loss - 0.3186475566625564, Learning Rate - 0.003125, magnitude of gradient - 0.09479724598655592\n",
      "Step - 4209, Loss - 0.35922300918161065, Learning Rate - 0.003125, magnitude of gradient - 0.06827477546915799\n",
      "Step - 4210, Loss - 0.2382493315987449, Learning Rate - 0.003125, magnitude of gradient - 0.026215731619917946\n",
      "Step - 4211, Loss - 0.3567043433683197, Learning Rate - 0.003125, magnitude of gradient - 0.042661373263652\n",
      "Step - 4212, Loss - 0.23319273582618377, Learning Rate - 0.003125, magnitude of gradient - 0.1312827446996322\n",
      "Step - 4213, Loss - 0.21313866548165328, Learning Rate - 0.003125, magnitude of gradient - 0.017030343953996475\n",
      "Step - 4214, Loss - 0.3332979429862314, Learning Rate - 0.003125, magnitude of gradient - 0.04499343779568032\n",
      "Step - 4215, Loss - 0.32628886107028476, Learning Rate - 0.003125, magnitude of gradient - 0.009098922982460853\n",
      "Step - 4216, Loss - 0.26645074785241785, Learning Rate - 0.003125, magnitude of gradient - 0.035707427202032606\n",
      "Step - 4217, Loss - 0.28271226263780236, Learning Rate - 0.003125, magnitude of gradient - 0.04504040568358955\n",
      "Step - 4218, Loss - 0.3160519322307992, Learning Rate - 0.003125, magnitude of gradient - 0.0638519596393412\n",
      "Step - 4219, Loss - 0.34468336688764656, Learning Rate - 0.003125, magnitude of gradient - 0.00819536517808298\n",
      "Step - 4220, Loss - 0.2851224231180159, Learning Rate - 0.003125, magnitude of gradient - 0.0618122822800604\n",
      "Step - 4221, Loss - 0.28399881183862924, Learning Rate - 0.003125, magnitude of gradient - 0.022766805788838884\n",
      "Step - 4222, Loss - 0.33468035230567317, Learning Rate - 0.003125, magnitude of gradient - 0.058698848003869715\n",
      "Step - 4223, Loss - 0.3233440911073819, Learning Rate - 0.003125, magnitude of gradient - 0.05841855539274481\n",
      "Step - 4224, Loss - 0.3673511301713871, Learning Rate - 0.003125, magnitude of gradient - 0.031903713730034\n",
      "Step - 4225, Loss - 0.3274839661719212, Learning Rate - 0.003125, magnitude of gradient - 0.02603108528643519\n",
      "Step - 4226, Loss - 0.29653842043191714, Learning Rate - 0.003125, magnitude of gradient - 0.02575848636546048\n",
      "Step - 4227, Loss - 0.3264210203933632, Learning Rate - 0.003125, magnitude of gradient - 0.09222619951880136\n",
      "Step - 4228, Loss - 0.30869384201030126, Learning Rate - 0.003125, magnitude of gradient - 0.05921882458712854\n",
      "Step - 4229, Loss - 0.3226111038890601, Learning Rate - 0.003125, magnitude of gradient - 0.04238534428245637\n",
      "Step - 4230, Loss - 0.31538943975519473, Learning Rate - 0.003125, magnitude of gradient - 0.08869623446328227\n",
      "Step - 4231, Loss - 0.29135164643868616, Learning Rate - 0.003125, magnitude of gradient - 0.01094209344744402\n",
      "Step - 4232, Loss - 0.3612752462623065, Learning Rate - 0.003125, magnitude of gradient - 0.026069647574207495\n",
      "Step - 4233, Loss - 0.35519799477826974, Learning Rate - 0.003125, magnitude of gradient - 0.031820041517546215\n",
      "Step - 4234, Loss - 0.314078684545269, Learning Rate - 0.003125, magnitude of gradient - 0.029719381491964236\n",
      "Step - 4235, Loss - 0.2709037958960788, Learning Rate - 0.003125, magnitude of gradient - 0.04559069050650577\n",
      "Step - 4236, Loss - 0.36475223984002436, Learning Rate - 0.003125, magnitude of gradient - 0.03948334220453357\n",
      "Step - 4237, Loss - 0.29915058336254524, Learning Rate - 0.003125, magnitude of gradient - 0.02674837398667977\n",
      "Step - 4238, Loss - 0.3386139106844579, Learning Rate - 0.003125, magnitude of gradient - 0.07343510544874096\n",
      "Step - 4239, Loss - 0.3596004138372702, Learning Rate - 0.003125, magnitude of gradient - 0.04503431101917534\n",
      "Step - 4240, Loss - 0.37220746701688057, Learning Rate - 0.003125, magnitude of gradient - 0.045951404582509625\n",
      "Step - 4241, Loss - 0.31802779706493933, Learning Rate - 0.003125, magnitude of gradient - 0.05933899907829018\n",
      "Step - 4242, Loss - 0.32528627040120145, Learning Rate - 0.003125, magnitude of gradient - 0.03787364409855882\n",
      "Step - 4243, Loss - 0.2932715490667922, Learning Rate - 0.003125, magnitude of gradient - 0.046246710098886985\n",
      "Step - 4244, Loss - 0.298898465029441, Learning Rate - 0.003125, magnitude of gradient - 0.030289755722806126\n",
      "Step - 4245, Loss - 0.357002204462883, Learning Rate - 0.003125, magnitude of gradient - 0.041726769179941225\n",
      "Step - 4246, Loss - 0.29998751246924743, Learning Rate - 0.003125, magnitude of gradient - 0.09864885944449542\n",
      "Step - 4247, Loss - 0.3597697778488634, Learning Rate - 0.003125, magnitude of gradient - 0.04974825840162678\n",
      "Step - 4248, Loss - 0.35782162589507766, Learning Rate - 0.003125, magnitude of gradient - 0.011987834529128412\n",
      "Step - 4249, Loss - 0.3642496443214063, Learning Rate - 0.003125, magnitude of gradient - 0.036253667055639094\n",
      "Step - 4250, Loss - 0.26634350208457824, Learning Rate - 0.003125, magnitude of gradient - 0.016825686913193784\n",
      "Step - 4251, Loss - 0.33170545036354937, Learning Rate - 0.003125, magnitude of gradient - 0.06262167645342533\n",
      "Step - 4252, Loss - 0.36724286355373453, Learning Rate - 0.003125, magnitude of gradient - 0.046110748004800466\n",
      "Step - 4253, Loss - 0.31227085102993307, Learning Rate - 0.003125, magnitude of gradient - 0.0410905610872099\n",
      "Step - 4254, Loss - 0.3000941508286343, Learning Rate - 0.003125, magnitude of gradient - 0.04766980045488987\n",
      "Step - 4255, Loss - 0.3352335529222689, Learning Rate - 0.003125, magnitude of gradient - 0.05319160194539268\n",
      "Step - 4256, Loss - 0.3542008414464143, Learning Rate - 0.003125, magnitude of gradient - 0.029123748942321805\n",
      "Step - 4257, Loss - 0.41650875038867896, Learning Rate - 0.003125, magnitude of gradient - 0.10294166164600549\n",
      "Step - 4258, Loss - 0.27809334189093676, Learning Rate - 0.003125, magnitude of gradient - 0.03651641508715002\n",
      "Step - 4259, Loss - 0.33796666034044254, Learning Rate - 0.003125, magnitude of gradient - 0.024241728427298145\n",
      "Step - 4260, Loss - 0.3636190834273667, Learning Rate - 0.003125, magnitude of gradient - 0.06333354187417288\n",
      "Step - 4261, Loss - 0.267043027486107, Learning Rate - 0.003125, magnitude of gradient - 0.03017741295119907\n",
      "Step - 4262, Loss - 0.31151276828001556, Learning Rate - 0.003125, magnitude of gradient - 0.015672971977714247\n",
      "Step - 4263, Loss - 0.35261033333834285, Learning Rate - 0.003125, magnitude of gradient - 0.027595914192797964\n",
      "Step - 4264, Loss - 0.3080373929349307, Learning Rate - 0.003125, magnitude of gradient - 0.057351189751329315\n",
      "Step - 4265, Loss - 0.35883298104804584, Learning Rate - 0.003125, magnitude of gradient - 0.07511846626103896\n",
      "Step - 4266, Loss - 0.3020910746984298, Learning Rate - 0.003125, magnitude of gradient - 0.10119432479233906\n",
      "Step - 4267, Loss - 0.3730038501428386, Learning Rate - 0.003125, magnitude of gradient - 0.0735371641547656\n",
      "Step - 4268, Loss - 0.32524744002302647, Learning Rate - 0.003125, magnitude of gradient - 0.018810193927358602\n",
      "Step - 4269, Loss - 0.32741156807998684, Learning Rate - 0.003125, magnitude of gradient - 0.06165999942354354\n",
      "Step - 4270, Loss - 0.3250592223932023, Learning Rate - 0.003125, magnitude of gradient - 0.04998127033192343\n",
      "Step - 4271, Loss - 0.3127485097521147, Learning Rate - 0.003125, magnitude of gradient - 0.03563033505659585\n",
      "Step - 4272, Loss - 0.2570621409233681, Learning Rate - 0.003125, magnitude of gradient - 0.013789947817660688\n",
      "Step - 4273, Loss - 0.3673516211981415, Learning Rate - 0.003125, magnitude of gradient - 0.047236611233479595\n",
      "Step - 4274, Loss - 0.29910045986545, Learning Rate - 0.003125, magnitude of gradient - 0.09865158382089187\n",
      "Step - 4275, Loss - 0.3366810866788099, Learning Rate - 0.003125, magnitude of gradient - 0.09393604704539611\n",
      "Step - 4276, Loss - 0.3221144333207329, Learning Rate - 0.003125, magnitude of gradient - 0.057835613391029266\n",
      "Step - 4277, Loss - 0.33478558301420874, Learning Rate - 0.003125, magnitude of gradient - 0.02409331223665336\n",
      "Step - 4278, Loss - 0.3638514764689449, Learning Rate - 0.003125, magnitude of gradient - 0.10601640339375352\n",
      "Step - 4279, Loss - 0.27820066978766267, Learning Rate - 0.003125, magnitude of gradient - 0.028928611671084074\n",
      "Step - 4280, Loss - 0.2683056567738812, Learning Rate - 0.003125, magnitude of gradient - 0.037945597199678785\n",
      "Step - 4281, Loss - 0.30223162791515956, Learning Rate - 0.003125, magnitude of gradient - 0.10828073660339638\n",
      "Step - 4282, Loss - 0.33632673619892006, Learning Rate - 0.003125, magnitude of gradient - 0.04325159013618858\n",
      "Step - 4283, Loss - 0.33496346355117146, Learning Rate - 0.003125, magnitude of gradient - 0.02492544936330941\n",
      "Step - 4284, Loss - 0.30512568439366805, Learning Rate - 0.003125, magnitude of gradient - 0.06669637354486325\n",
      "Step - 4285, Loss - 0.30481283822116156, Learning Rate - 0.003125, magnitude of gradient - 0.0845338605465387\n",
      "Step - 4286, Loss - 0.3633514460269047, Learning Rate - 0.003125, magnitude of gradient - 0.019480735196109135\n",
      "Step - 4287, Loss - 0.3627095219504323, Learning Rate - 0.003125, magnitude of gradient - 0.09372314540361015\n",
      "Step - 4288, Loss - 0.31576049580540183, Learning Rate - 0.003125, magnitude of gradient - 0.10604011665170324\n",
      "Step - 4289, Loss - 0.38643972416214734, Learning Rate - 0.003125, magnitude of gradient - 0.049482326937412564\n",
      "Step - 4290, Loss - 0.31950135428561516, Learning Rate - 0.003125, magnitude of gradient - 0.02184331786075523\n",
      "Step - 4291, Loss - 0.33858074324936693, Learning Rate - 0.003125, magnitude of gradient - 0.06373057268159443\n",
      "Step - 4292, Loss - 0.2748599792906352, Learning Rate - 0.003125, magnitude of gradient - 0.03559980066927716\n",
      "Step - 4293, Loss - 0.39011266007829815, Learning Rate - 0.003125, magnitude of gradient - 0.08515678958626005\n",
      "Step - 4294, Loss - 0.3157502507334828, Learning Rate - 0.003125, magnitude of gradient - 0.05228026008646267\n",
      "Step - 4295, Loss - 0.3813124612476481, Learning Rate - 0.003125, magnitude of gradient - 0.08545765905257738\n",
      "Step - 4296, Loss - 0.32191922178046184, Learning Rate - 0.003125, magnitude of gradient - 0.04742052979852358\n",
      "Step - 4297, Loss - 0.3061568058507557, Learning Rate - 0.003125, magnitude of gradient - 0.04215269382350425\n",
      "Step - 4298, Loss - 0.3024183519026306, Learning Rate - 0.003125, magnitude of gradient - 0.013904570465879172\n",
      "Step - 4299, Loss - 0.2874496572797915, Learning Rate - 0.003125, magnitude of gradient - 0.00994028258931704\n",
      "Step - 4300, Loss - 0.33215576017801735, Learning Rate - 0.003125, magnitude of gradient - 0.05833042659309351\n",
      "Step - 4301, Loss - 0.37694245465930987, Learning Rate - 0.003125, magnitude of gradient - 0.041256939961647726\n",
      "Step - 4302, Loss - 0.34553618696317373, Learning Rate - 0.003125, magnitude of gradient - 0.057221402591361874\n",
      "Step - 4303, Loss - 0.32539993507707676, Learning Rate - 0.003125, magnitude of gradient - 0.03582874755403018\n",
      "Step - 4304, Loss - 0.3435629757018634, Learning Rate - 0.003125, magnitude of gradient - 0.04053618240710817\n",
      "Step - 4305, Loss - 0.25766424195451454, Learning Rate - 0.003125, magnitude of gradient - 0.01642570187687789\n",
      "Step - 4306, Loss - 0.29456583721194507, Learning Rate - 0.003125, magnitude of gradient - 0.042509439657355916\n",
      "Step - 4307, Loss - 0.3355624556438675, Learning Rate - 0.003125, magnitude of gradient - 0.07825619991516419\n",
      "Step - 4308, Loss - 0.2942760277538926, Learning Rate - 0.003125, magnitude of gradient - 0.024592111905690228\n",
      "Step - 4309, Loss - 0.3074986905881092, Learning Rate - 0.003125, magnitude of gradient - 0.025884985956016418\n",
      "Step - 4310, Loss - 0.3150544600227223, Learning Rate - 0.003125, magnitude of gradient - 0.05541753486873109\n",
      "Step - 4311, Loss - 0.34788707751688697, Learning Rate - 0.003125, magnitude of gradient - 0.033046184454090105\n",
      "Step - 4312, Loss - 0.30947263016820437, Learning Rate - 0.003125, magnitude of gradient - 0.034382572097586786\n",
      "Step - 4313, Loss - 0.36891589554018145, Learning Rate - 0.003125, magnitude of gradient - 0.03936145317237106\n",
      "Step - 4314, Loss - 0.3322777121122379, Learning Rate - 0.003125, magnitude of gradient - 0.03255430520479505\n",
      "Step - 4315, Loss - 0.29891855965560105, Learning Rate - 0.003125, magnitude of gradient - 0.02887452549980104\n",
      "Step - 4316, Loss - 0.2990725774578105, Learning Rate - 0.003125, magnitude of gradient - 0.024340205468252645\n",
      "Step - 4317, Loss - 0.3224222280675734, Learning Rate - 0.003125, magnitude of gradient - 0.029320910889701096\n",
      "Step - 4318, Loss - 0.2553414553204743, Learning Rate - 0.003125, magnitude of gradient - 0.019324350169114022\n",
      "Step - 4319, Loss - 0.3091006191865221, Learning Rate - 0.003125, magnitude of gradient - 0.03685394703508547\n",
      "Step - 4320, Loss - 0.2679761776213947, Learning Rate - 0.003125, magnitude of gradient - 0.051958998668128226\n",
      "Step - 4321, Loss - 0.34248555634403816, Learning Rate - 0.003125, magnitude of gradient - 0.05928012616085292\n",
      "Step - 4322, Loss - 0.3291212804186172, Learning Rate - 0.003125, magnitude of gradient - 0.02600218737248688\n",
      "Step - 4323, Loss - 0.22820159862426903, Learning Rate - 0.003125, magnitude of gradient - 0.07666704824684457\n",
      "Step - 4324, Loss - 0.3085232969604189, Learning Rate - 0.003125, magnitude of gradient - 0.0338188913012917\n",
      "Step - 4325, Loss - 0.3471255433918001, Learning Rate - 0.003125, magnitude of gradient - 0.01444261884820348\n",
      "Step - 4326, Loss - 0.2918685783297792, Learning Rate - 0.003125, magnitude of gradient - 0.09009424680094591\n",
      "Step - 4327, Loss - 0.28301404668706176, Learning Rate - 0.003125, magnitude of gradient - 0.06192420015081568\n",
      "Step - 4328, Loss - 0.3779539853200097, Learning Rate - 0.003125, magnitude of gradient - 0.044652568804765834\n",
      "Step - 4329, Loss - 0.24318900502944707, Learning Rate - 0.003125, magnitude of gradient - 0.052996340844123115\n",
      "Step - 4330, Loss - 0.3536839017145352, Learning Rate - 0.003125, magnitude of gradient - 0.05735575971355153\n",
      "Step - 4331, Loss - 0.3255442570251454, Learning Rate - 0.003125, magnitude of gradient - 0.013032709265260699\n",
      "Step - 4332, Loss - 0.3655709943963378, Learning Rate - 0.003125, magnitude of gradient - 0.04058103075919946\n",
      "Step - 4333, Loss - 0.31676506580300795, Learning Rate - 0.003125, magnitude of gradient - 0.07733175514325372\n",
      "Step - 4334, Loss - 0.3055426854416543, Learning Rate - 0.003125, magnitude of gradient - 0.03540680731394301\n",
      "Step - 4335, Loss - 0.2424308642101456, Learning Rate - 0.003125, magnitude of gradient - 0.04751227974345837\n",
      "Step - 4336, Loss - 0.2958193061522451, Learning Rate - 0.003125, magnitude of gradient - 0.020314665104278234\n",
      "Step - 4337, Loss - 0.329748363160102, Learning Rate - 0.003125, magnitude of gradient - 0.01484305424860077\n",
      "Step - 4338, Loss - 0.2920762137064069, Learning Rate - 0.003125, magnitude of gradient - 0.04104461689137505\n",
      "Step - 4339, Loss - 0.29605291750306467, Learning Rate - 0.003125, magnitude of gradient - 0.11599231327344664\n",
      "Step - 4340, Loss - 0.36100600287332285, Learning Rate - 0.003125, magnitude of gradient - 0.05469811034994411\n",
      "Step - 4341, Loss - 0.31665513982793586, Learning Rate - 0.003125, magnitude of gradient - 0.03936271239586104\n",
      "Step - 4342, Loss - 0.2995627607313402, Learning Rate - 0.003125, magnitude of gradient - 0.04438926124449232\n",
      "Step - 4343, Loss - 0.32696968215259176, Learning Rate - 0.003125, magnitude of gradient - 0.05675705536010472\n",
      "Step - 4344, Loss - 0.3221905758006104, Learning Rate - 0.003125, magnitude of gradient - 0.06454357876280871\n",
      "Step - 4345, Loss - 0.40908719648593855, Learning Rate - 0.003125, magnitude of gradient - 0.0663171668780624\n",
      "Step - 4346, Loss - 0.3063758435409569, Learning Rate - 0.003125, magnitude of gradient - 0.04387106775825382\n",
      "Step - 4347, Loss - 0.3211013376322525, Learning Rate - 0.003125, magnitude of gradient - 0.043429706628197014\n",
      "Step - 4348, Loss - 0.33804593155153295, Learning Rate - 0.003125, magnitude of gradient - 0.06535414048782663\n",
      "Step - 4349, Loss - 0.35421738546087395, Learning Rate - 0.003125, magnitude of gradient - 0.030813509342628227\n",
      "Step - 4350, Loss - 0.2855549498842693, Learning Rate - 0.003125, magnitude of gradient - 0.05153605168108273\n",
      "Step - 4351, Loss - 0.27193908631286406, Learning Rate - 0.003125, magnitude of gradient - 0.028762262739483395\n",
      "Step - 4352, Loss - 0.32637816333339886, Learning Rate - 0.003125, magnitude of gradient - 0.0747725100015966\n",
      "Step - 4353, Loss - 0.31880191534402336, Learning Rate - 0.003125, magnitude of gradient - 0.07622663940786166\n",
      "Step - 4354, Loss - 0.4448600534228022, Learning Rate - 0.003125, magnitude of gradient - 0.05687270795547502\n",
      "Step - 4355, Loss - 0.2769753224806254, Learning Rate - 0.003125, magnitude of gradient - 0.025170294578881886\n",
      "Step - 4356, Loss - 0.28739830317461923, Learning Rate - 0.003125, magnitude of gradient - 0.04279091437507888\n",
      "Step - 4357, Loss - 0.34091679052245505, Learning Rate - 0.003125, magnitude of gradient - 0.07032082247596223\n",
      "Step - 4358, Loss - 0.3665353385592997, Learning Rate - 0.003125, magnitude of gradient - 0.08283722963258593\n",
      "Step - 4359, Loss - 0.33767155177188396, Learning Rate - 0.003125, magnitude of gradient - 0.051847547769901345\n",
      "Step - 4360, Loss - 0.2645572419601563, Learning Rate - 0.003125, magnitude of gradient - 0.02348338043508075\n",
      "Step - 4361, Loss - 0.35904804276879876, Learning Rate - 0.003125, magnitude of gradient - 0.03826684392960559\n",
      "Step - 4362, Loss - 0.3166528031046061, Learning Rate - 0.003125, magnitude of gradient - 0.04201871172755259\n",
      "Step - 4363, Loss - 0.34192948571541815, Learning Rate - 0.003125, magnitude of gradient - 0.01731774752042607\n",
      "Step - 4364, Loss - 0.284344458671662, Learning Rate - 0.003125, magnitude of gradient - 0.07298171238949429\n",
      "Step - 4365, Loss - 0.40107070095034664, Learning Rate - 0.003125, magnitude of gradient - 0.06878559489043491\n",
      "Step - 4366, Loss - 0.3002589784309746, Learning Rate - 0.003125, magnitude of gradient - 0.029033346213405375\n",
      "Step - 4367, Loss - 0.2587084349923633, Learning Rate - 0.003125, magnitude of gradient - 0.10627398811478915\n",
      "Step - 4368, Loss - 0.31848366642674003, Learning Rate - 0.003125, magnitude of gradient - 0.05330641168472202\n",
      "Step - 4369, Loss - 0.3553552317650668, Learning Rate - 0.003125, magnitude of gradient - 0.1095561713104373\n",
      "Step - 4370, Loss - 0.31023054141855333, Learning Rate - 0.003125, magnitude of gradient - 0.021092738529251177\n",
      "Step - 4371, Loss - 0.3969159345932383, Learning Rate - 0.003125, magnitude of gradient - 0.05498742028976088\n",
      "Step - 4372, Loss - 0.2924414381490981, Learning Rate - 0.003125, magnitude of gradient - 0.05332957598893913\n",
      "Step - 4373, Loss - 0.3033865299082889, Learning Rate - 0.003125, magnitude of gradient - 0.038206102669320445\n",
      "Step - 4374, Loss - 0.37681680284820823, Learning Rate - 0.003125, magnitude of gradient - 0.02942359205244669\n",
      "Step - 4375, Loss - 0.29011705598872656, Learning Rate - 0.003125, magnitude of gradient - 0.07229981244719455\n",
      "Step - 4376, Loss - 0.382529182943743, Learning Rate - 0.003125, magnitude of gradient - 0.024068275867137008\n",
      "Step - 4377, Loss - 0.328148549862433, Learning Rate - 0.003125, magnitude of gradient - 0.042011883834271964\n",
      "Step - 4378, Loss - 0.35998209962842526, Learning Rate - 0.003125, magnitude of gradient - 0.018088816228502967\n",
      "Step - 4379, Loss - 0.2766424763691134, Learning Rate - 0.003125, magnitude of gradient - 0.04666615156248785\n",
      "Step - 4380, Loss - 0.349970506731202, Learning Rate - 0.003125, magnitude of gradient - 0.04824684022397389\n",
      "Step - 4381, Loss - 0.33569688308486695, Learning Rate - 0.003125, magnitude of gradient - 0.04927592725692607\n",
      "Step - 4382, Loss - 0.276444350165021, Learning Rate - 0.003125, magnitude of gradient - 0.07419957110879839\n",
      "Step - 4383, Loss - 0.3752250457951677, Learning Rate - 0.003125, magnitude of gradient - 0.06951750671060096\n",
      "Step - 4384, Loss - 0.38144924246103, Learning Rate - 0.003125, magnitude of gradient - 0.02040556287600385\n",
      "Step - 4385, Loss - 0.23627819034827025, Learning Rate - 0.003125, magnitude of gradient - 0.03368604825362078\n",
      "Step - 4386, Loss - 0.2925829945648136, Learning Rate - 0.003125, magnitude of gradient - 0.03778019009152131\n",
      "Step - 4387, Loss - 0.3631386627107484, Learning Rate - 0.003125, magnitude of gradient - 0.05249216906956249\n",
      "Step - 4388, Loss - 0.2738306425079076, Learning Rate - 0.003125, magnitude of gradient - 0.06007415790261862\n",
      "Step - 4389, Loss - 0.27521669989768194, Learning Rate - 0.003125, magnitude of gradient - 0.029133351639242494\n",
      "Step - 4390, Loss - 0.2796258455815265, Learning Rate - 0.003125, magnitude of gradient - 0.029700385360375784\n",
      "Step - 4391, Loss - 0.2810562164881353, Learning Rate - 0.003125, magnitude of gradient - 0.05636479632994241\n",
      "Step - 4392, Loss - 0.3293399681180106, Learning Rate - 0.003125, magnitude of gradient - 0.052147655026374605\n",
      "Step - 4393, Loss - 0.4102351652342084, Learning Rate - 0.003125, magnitude of gradient - 0.044144119109003344\n",
      "Step - 4394, Loss - 0.29090199160089625, Learning Rate - 0.003125, magnitude of gradient - 0.043686920143468394\n",
      "Step - 4395, Loss - 0.2773015935424307, Learning Rate - 0.003125, magnitude of gradient - 0.05276717391292246\n",
      "Step - 4396, Loss - 0.36843447655291434, Learning Rate - 0.003125, magnitude of gradient - 0.027335229802144013\n",
      "Step - 4397, Loss - 0.3199330124194085, Learning Rate - 0.003125, magnitude of gradient - 0.05520154341899739\n",
      "Step - 4398, Loss - 0.2963719573946126, Learning Rate - 0.003125, magnitude of gradient - 0.0529046236045295\n",
      "Step - 4399, Loss - 0.32018253940540653, Learning Rate - 0.003125, magnitude of gradient - 0.10213573975745893\n",
      "Step - 4400, Loss - 0.314318031024585, Learning Rate - 0.003125, magnitude of gradient - 0.059818489044995884\n",
      "Step - 4401, Loss - 0.30144786175398725, Learning Rate - 0.003125, magnitude of gradient - 0.023619765498162366\n",
      "Step - 4402, Loss - 0.2569060737614733, Learning Rate - 0.003125, magnitude of gradient - 0.024895106579632186\n",
      "Step - 4403, Loss - 0.35433372113866635, Learning Rate - 0.003125, magnitude of gradient - 0.11166523048124256\n",
      "Step - 4404, Loss - 0.35427531623408925, Learning Rate - 0.003125, magnitude of gradient - 0.03146994540676137\n",
      "Step - 4405, Loss - 0.31235498056009725, Learning Rate - 0.003125, magnitude of gradient - 0.08108382128124614\n",
      "Step - 4406, Loss - 0.31232983494795585, Learning Rate - 0.003125, magnitude of gradient - 0.06455891120937723\n",
      "Step - 4407, Loss - 0.297012535460349, Learning Rate - 0.003125, magnitude of gradient - 0.05584341516995472\n",
      "Step - 4408, Loss - 0.29578126407856686, Learning Rate - 0.003125, magnitude of gradient - 0.017679423028402922\n",
      "Step - 4409, Loss - 0.29864780083445375, Learning Rate - 0.003125, magnitude of gradient - 0.07311631027623096\n",
      "Step - 4410, Loss - 0.3591329389167278, Learning Rate - 0.003125, magnitude of gradient - 0.03266906488730218\n",
      "Step - 4411, Loss - 0.3259296575777505, Learning Rate - 0.003125, magnitude of gradient - 0.06559853405940845\n",
      "Step - 4412, Loss - 0.34377441176244405, Learning Rate - 0.003125, magnitude of gradient - 0.046165437532324265\n",
      "Step - 4413, Loss - 0.29658404573765884, Learning Rate - 0.003125, magnitude of gradient - 0.055600317963276746\n",
      "Step - 4414, Loss - 0.22670369412382352, Learning Rate - 0.003125, magnitude of gradient - 0.08291454154560368\n",
      "Step - 4415, Loss - 0.3057667201209159, Learning Rate - 0.003125, magnitude of gradient - 0.046654288370708584\n",
      "Step - 4416, Loss - 0.3020436845211675, Learning Rate - 0.003125, magnitude of gradient - 0.027899577923463356\n",
      "Step - 4417, Loss - 0.31103927903855444, Learning Rate - 0.003125, magnitude of gradient - 0.09688178186224856\n",
      "Step - 4418, Loss - 0.3407237299160946, Learning Rate - 0.003125, magnitude of gradient - 0.03342355227060175\n",
      "Step - 4419, Loss - 0.3014892176112379, Learning Rate - 0.003125, magnitude of gradient - 0.009328552557647104\n",
      "Step - 4420, Loss - 0.3221554529665937, Learning Rate - 0.003125, magnitude of gradient - 0.04610589017394828\n",
      "Step - 4421, Loss - 0.27568487760013866, Learning Rate - 0.003125, magnitude of gradient - 0.04256058533517294\n",
      "Step - 4422, Loss - 0.30206272829268954, Learning Rate - 0.003125, magnitude of gradient - 0.042143973998267484\n",
      "Step - 4423, Loss - 0.2714040385972271, Learning Rate - 0.003125, magnitude of gradient - 0.053660864281703066\n",
      "Step - 4424, Loss - 0.35264931512807796, Learning Rate - 0.003125, magnitude of gradient - 0.0637950167668288\n",
      "Step - 4425, Loss - 0.36806343391144936, Learning Rate - 0.003125, magnitude of gradient - 0.07779859781950661\n",
      "Step - 4426, Loss - 0.3072225878871583, Learning Rate - 0.003125, magnitude of gradient - 0.03688137159470332\n",
      "Step - 4427, Loss - 0.3396226714323216, Learning Rate - 0.003125, magnitude of gradient - 0.021710695746119772\n",
      "Step - 4428, Loss - 0.3708671745287289, Learning Rate - 0.003125, magnitude of gradient - 0.0554226594563045\n",
      "Step - 4429, Loss - 0.3389388585177883, Learning Rate - 0.003125, magnitude of gradient - 0.04643870374214318\n",
      "Step - 4430, Loss - 0.32072072237196747, Learning Rate - 0.003125, magnitude of gradient - 0.06637288693771852\n",
      "Step - 4431, Loss - 0.3286725016442333, Learning Rate - 0.003125, magnitude of gradient - 0.017157937229200396\n",
      "Step - 4432, Loss - 0.3558060558913378, Learning Rate - 0.003125, magnitude of gradient - 0.07086105300652996\n",
      "Step - 4433, Loss - 0.2551810296628924, Learning Rate - 0.003125, magnitude of gradient - 0.052709879044532434\n",
      "Step - 4434, Loss - 0.2570801752227882, Learning Rate - 0.003125, magnitude of gradient - 0.01378134552896389\n",
      "Step - 4435, Loss - 0.2830389671151692, Learning Rate - 0.003125, magnitude of gradient - 0.04811337078096882\n",
      "Step - 4436, Loss - 0.3087950595181741, Learning Rate - 0.003125, magnitude of gradient - 0.04956176772929592\n",
      "Step - 4437, Loss - 0.34604746630118777, Learning Rate - 0.003125, magnitude of gradient - 0.052798502446987965\n",
      "Step - 4438, Loss - 0.2597535173976076, Learning Rate - 0.003125, magnitude of gradient - 0.07372759960279793\n",
      "Step - 4439, Loss - 0.31065939854379065, Learning Rate - 0.003125, magnitude of gradient - 0.016978430806275743\n",
      "Step - 4440, Loss - 0.24901705833096544, Learning Rate - 0.003125, magnitude of gradient - 0.05380156103814147\n",
      "Step - 4441, Loss - 0.38412701354421774, Learning Rate - 0.003125, magnitude of gradient - 0.09923086154628823\n",
      "Step - 4442, Loss - 0.2960507314249202, Learning Rate - 0.003125, magnitude of gradient - 0.06180102275533961\n",
      "Step - 4443, Loss - 0.27023830078210154, Learning Rate - 0.003125, magnitude of gradient - 0.0014512552317076806\n",
      "Step - 4444, Loss - 0.2577911570965075, Learning Rate - 0.003125, magnitude of gradient - 0.06960269526629187\n",
      "Step - 4445, Loss - 0.359523668085989, Learning Rate - 0.003125, magnitude of gradient - 0.06910477855776587\n",
      "Step - 4446, Loss - 0.24486817672008343, Learning Rate - 0.003125, magnitude of gradient - 0.037862945369049605\n",
      "Step - 4447, Loss - 0.3278485456917488, Learning Rate - 0.003125, magnitude of gradient - 0.04234251572639416\n",
      "Step - 4448, Loss - 0.31001079101415985, Learning Rate - 0.003125, magnitude of gradient - 0.048662258312657906\n",
      "Step - 4449, Loss - 0.32116133606242175, Learning Rate - 0.003125, magnitude of gradient - 0.04882414464434365\n",
      "Step - 4450, Loss - 0.37134001328362837, Learning Rate - 0.003125, magnitude of gradient - 0.06593306678314848\n",
      "Step - 4451, Loss - 0.24459458261765052, Learning Rate - 0.003125, magnitude of gradient - 0.05961733451651549\n",
      "Step - 4452, Loss - 0.3776844294901696, Learning Rate - 0.003125, magnitude of gradient - 0.018188078541702256\n",
      "Step - 4453, Loss - 0.34243849030149137, Learning Rate - 0.003125, magnitude of gradient - 0.08349935262617224\n",
      "Step - 4454, Loss - 0.27747122874561764, Learning Rate - 0.003125, magnitude of gradient - 0.10630514810301807\n",
      "Step - 4455, Loss - 0.3342347564147103, Learning Rate - 0.003125, magnitude of gradient - 0.01804535396700876\n",
      "Step - 4456, Loss - 0.3818686434638354, Learning Rate - 0.003125, magnitude of gradient - 0.043127852854445314\n",
      "Step - 4457, Loss - 0.34927942669436907, Learning Rate - 0.003125, magnitude of gradient - 0.013755169166875475\n",
      "Step - 4458, Loss - 0.28974366451326605, Learning Rate - 0.003125, magnitude of gradient - 0.07332037365124802\n",
      "Step - 4459, Loss - 0.3222536668367288, Learning Rate - 0.003125, magnitude of gradient - 0.014630289726348236\n",
      "Step - 4460, Loss - 0.2358853787662959, Learning Rate - 0.003125, magnitude of gradient - 0.041927969649387345\n",
      "Step - 4461, Loss - 0.30181990220094546, Learning Rate - 0.003125, magnitude of gradient - 0.07732929586190589\n",
      "Step - 4462, Loss - 0.3341388780862133, Learning Rate - 0.003125, magnitude of gradient - 0.019992819303936912\n",
      "Step - 4463, Loss - 0.3141801606121479, Learning Rate - 0.003125, magnitude of gradient - 0.07469137982745665\n",
      "Step - 4464, Loss - 0.314805635518089, Learning Rate - 0.003125, magnitude of gradient - 0.061013665947088895\n",
      "Step - 4465, Loss - 0.33776215168102197, Learning Rate - 0.003125, magnitude of gradient - 0.0630620610481771\n",
      "Step - 4466, Loss - 0.40761869138630646, Learning Rate - 0.003125, magnitude of gradient - 0.05563045377016673\n",
      "Step - 4467, Loss - 0.3040668505361943, Learning Rate - 0.003125, magnitude of gradient - 0.010061920597464634\n",
      "Step - 4468, Loss - 0.3001475437349169, Learning Rate - 0.003125, magnitude of gradient - 0.07845061650858756\n",
      "Step - 4469, Loss - 0.42713994981083725, Learning Rate - 0.003125, magnitude of gradient - 0.10738147612078083\n",
      "Step - 4470, Loss - 0.339544633021416, Learning Rate - 0.003125, magnitude of gradient - 0.03750591026547655\n",
      "Step - 4471, Loss - 0.2725887621181651, Learning Rate - 0.003125, magnitude of gradient - 0.043913145856189634\n",
      "Step - 4472, Loss - 0.3160324624356623, Learning Rate - 0.003125, magnitude of gradient - 0.09177738906639733\n",
      "Step - 4473, Loss - 0.30664805194090367, Learning Rate - 0.003125, magnitude of gradient - 0.056359592260618784\n",
      "Step - 4474, Loss - 0.3459250279845101, Learning Rate - 0.003125, magnitude of gradient - 0.07448879503362385\n",
      "Step - 4475, Loss - 0.3906772784937736, Learning Rate - 0.003125, magnitude of gradient - 0.026559846371831227\n",
      "Step - 4476, Loss - 0.3210791694123677, Learning Rate - 0.003125, magnitude of gradient - 0.08916805869221331\n",
      "Step - 4477, Loss - 0.39281342443716794, Learning Rate - 0.003125, magnitude of gradient - 0.051428630318902074\n",
      "Step - 4478, Loss - 0.3026638391581676, Learning Rate - 0.003125, magnitude of gradient - 0.04099317910298222\n",
      "Step - 4479, Loss - 0.3837934387451323, Learning Rate - 0.003125, magnitude of gradient - 0.025773325371144903\n",
      "Step - 4480, Loss - 0.38421208728747275, Learning Rate - 0.003125, magnitude of gradient - 0.055606043598534244\n",
      "Step - 4481, Loss - 0.3618094676745768, Learning Rate - 0.003125, magnitude of gradient - 0.04589889454111567\n",
      "Step - 4482, Loss - 0.3145415360388037, Learning Rate - 0.003125, magnitude of gradient - 0.07063881021700792\n",
      "Step - 4483, Loss - 0.2629057387093401, Learning Rate - 0.003125, magnitude of gradient - 0.03151507365654311\n",
      "Step - 4484, Loss - 0.30458898322197836, Learning Rate - 0.003125, magnitude of gradient - 0.056181469008955\n",
      "Step - 4485, Loss - 0.2984625867740447, Learning Rate - 0.003125, magnitude of gradient - 0.04553242621402828\n",
      "Step - 4486, Loss - 0.32140524797038006, Learning Rate - 0.003125, magnitude of gradient - 0.06522556182337304\n",
      "Step - 4487, Loss - 0.3226060139581696, Learning Rate - 0.003125, magnitude of gradient - 0.043980376958260514\n",
      "Step - 4488, Loss - 0.30320939643475386, Learning Rate - 0.003125, magnitude of gradient - 0.050282729306597315\n",
      "Step - 4489, Loss - 0.3636785949207462, Learning Rate - 0.003125, magnitude of gradient - 0.013047953018395349\n",
      "Step - 4490, Loss - 0.42133097926810786, Learning Rate - 0.003125, magnitude of gradient - 0.036718044841554665\n",
      "Step - 4491, Loss - 0.27632544231215084, Learning Rate - 0.003125, magnitude of gradient - 0.04056488212321841\n",
      "Step - 4492, Loss - 0.3415961999605875, Learning Rate - 0.003125, magnitude of gradient - 0.03659331040796827\n",
      "Step - 4493, Loss - 0.31187419139410444, Learning Rate - 0.003125, magnitude of gradient - 0.08466260299614198\n",
      "Step - 4494, Loss - 0.3561124458193761, Learning Rate - 0.003125, magnitude of gradient - 0.06652251961230006\n",
      "Step - 4495, Loss - 0.3143519983921439, Learning Rate - 0.003125, magnitude of gradient - 0.039007335773940596\n",
      "Step - 4496, Loss - 0.3460936413100757, Learning Rate - 0.003125, magnitude of gradient - 0.042619658070081945\n",
      "Step - 4497, Loss - 0.34153108606379223, Learning Rate - 0.003125, magnitude of gradient - 0.05177055369318737\n",
      "Step - 4498, Loss - 0.3609339164417781, Learning Rate - 0.003125, magnitude of gradient - 0.08494519426460959\n",
      "Step - 4499, Loss - 0.2837122201783282, Learning Rate - 0.003125, magnitude of gradient - 0.03157557287639414\n",
      "Step - 4500, Loss - 0.33929821356965434, Learning Rate - 0.003125, magnitude of gradient - 0.009836354039339362\n",
      "Step - 4501, Loss - 0.3200920999994418, Learning Rate - 0.003125, magnitude of gradient - 0.03970236188148311\n",
      "Step - 4502, Loss - 0.3147396468920679, Learning Rate - 0.003125, magnitude of gradient - 0.10894328590303909\n",
      "Step - 4503, Loss - 0.3481248986619629, Learning Rate - 0.003125, magnitude of gradient - 0.048800247467546\n",
      "Step - 4504, Loss - 0.3589001203335853, Learning Rate - 0.003125, magnitude of gradient - 0.020741391262032706\n",
      "Step - 4505, Loss - 0.4027635353552099, Learning Rate - 0.003125, magnitude of gradient - 0.08730499994490944\n",
      "Step - 4506, Loss - 0.38584104109517253, Learning Rate - 0.003125, magnitude of gradient - 0.07458256837951482\n",
      "Step - 4507, Loss - 0.32461294971668697, Learning Rate - 0.003125, magnitude of gradient - 0.027945766154019117\n",
      "Step - 4508, Loss - 0.3553608096027554, Learning Rate - 0.003125, magnitude of gradient - 0.023304328584951847\n",
      "Step - 4509, Loss - 0.329366259944959, Learning Rate - 0.003125, magnitude of gradient - 0.02699610102426173\n",
      "Step - 4510, Loss - 0.32617671976354534, Learning Rate - 0.003125, magnitude of gradient - 0.06217682250815181\n",
      "Step - 4511, Loss - 0.33417046647990045, Learning Rate - 0.003125, magnitude of gradient - 0.060520040996038894\n",
      "Step - 4512, Loss - 0.35713436597377674, Learning Rate - 0.003125, magnitude of gradient - 0.02612747985244721\n",
      "Step - 4513, Loss - 0.30852683866159014, Learning Rate - 0.003125, magnitude of gradient - 0.03432466220608366\n",
      "Step - 4514, Loss - 0.30285608875683, Learning Rate - 0.003125, magnitude of gradient - 0.05116648662706831\n",
      "Step - 4515, Loss - 0.4308041433770783, Learning Rate - 0.003125, magnitude of gradient - 0.06358385814126528\n",
      "Step - 4516, Loss - 0.29364481077157745, Learning Rate - 0.003125, magnitude of gradient - 0.05551404955968845\n",
      "Step - 4517, Loss - 0.35410307239476707, Learning Rate - 0.003125, magnitude of gradient - 0.13504874230394048\n",
      "Step - 4518, Loss - 0.3245928024273734, Learning Rate - 0.003125, magnitude of gradient - 0.08554979328328086\n",
      "Step - 4519, Loss - 0.4252245433669559, Learning Rate - 0.003125, magnitude of gradient - 0.10093054200690763\n",
      "Step - 4520, Loss - 0.31942668398570895, Learning Rate - 0.003125, magnitude of gradient - 0.05851130842300846\n",
      "Step - 4521, Loss - 0.3214141538556645, Learning Rate - 0.003125, magnitude of gradient - 0.0442400524990972\n",
      "Step - 4522, Loss - 0.27866850966866263, Learning Rate - 0.003125, magnitude of gradient - 0.051935248130893764\n",
      "Step - 4523, Loss - 0.3564835778979868, Learning Rate - 0.003125, magnitude of gradient - 0.0747805256098389\n",
      "Step - 4524, Loss - 0.3143744600838135, Learning Rate - 0.003125, magnitude of gradient - 0.034586226699266566\n",
      "Step - 4525, Loss - 0.43146755444752666, Learning Rate - 0.003125, magnitude of gradient - 0.07250651410871212\n",
      "Step - 4526, Loss - 0.3798032135661049, Learning Rate - 0.003125, magnitude of gradient - 0.04836973878416709\n",
      "Step - 4527, Loss - 0.3970078487822099, Learning Rate - 0.003125, magnitude of gradient - 0.03203003280225085\n",
      "Step - 4528, Loss - 0.3480402240640519, Learning Rate - 0.003125, magnitude of gradient - 0.0733185706491113\n",
      "Step - 4529, Loss - 0.40825262662025913, Learning Rate - 0.003125, magnitude of gradient - 0.05048891578990504\n",
      "Step - 4530, Loss - 0.34168864943914296, Learning Rate - 0.003125, magnitude of gradient - 0.05694656435476673\n",
      "Step - 4531, Loss - 0.4113044151156355, Learning Rate - 0.003125, magnitude of gradient - 0.023402872372722885\n",
      "Step - 4532, Loss - 0.2785390534598452, Learning Rate - 0.003125, magnitude of gradient - 0.02142039360607416\n",
      "Step - 4533, Loss - 0.31004166134499406, Learning Rate - 0.003125, magnitude of gradient - 0.0469306874183275\n",
      "Step - 4534, Loss - 0.3064572880993556, Learning Rate - 0.003125, magnitude of gradient - 0.04417281312593773\n",
      "Step - 4535, Loss - 0.3500306902316607, Learning Rate - 0.003125, magnitude of gradient - 0.02253605009687742\n",
      "Step - 4536, Loss - 0.2974392341628579, Learning Rate - 0.003125, magnitude of gradient - 0.04713887101923722\n",
      "Step - 4537, Loss - 0.23129048031166144, Learning Rate - 0.003125, magnitude of gradient - 0.026205553670615323\n",
      "Step - 4538, Loss - 0.2558906951074214, Learning Rate - 0.003125, magnitude of gradient - 0.048929821384648166\n",
      "Step - 4539, Loss - 0.41738747078761845, Learning Rate - 0.003125, magnitude of gradient - 0.06924784543569384\n",
      "Step - 4540, Loss - 0.3517638536938228, Learning Rate - 0.003125, magnitude of gradient - 0.07123586038577025\n",
      "Step - 4541, Loss - 0.3467811816901305, Learning Rate - 0.003125, magnitude of gradient - 0.06208898786993497\n",
      "Step - 4542, Loss - 0.3677678432646838, Learning Rate - 0.003125, magnitude of gradient - 0.015254896681941393\n",
      "Step - 4543, Loss - 0.3374144562756405, Learning Rate - 0.003125, magnitude of gradient - 0.09839262694775261\n",
      "Step - 4544, Loss - 0.27231309567713496, Learning Rate - 0.003125, magnitude of gradient - 0.11276300789339189\n",
      "Step - 4545, Loss - 0.289167711630592, Learning Rate - 0.003125, magnitude of gradient - 0.06587180517918986\n",
      "Step - 4546, Loss - 0.2785350241300527, Learning Rate - 0.003125, magnitude of gradient - 0.008088002512903087\n",
      "Step - 4547, Loss - 0.3759597942385002, Learning Rate - 0.003125, magnitude of gradient - 0.06288957259615893\n",
      "Step - 4548, Loss - 0.30597504538063613, Learning Rate - 0.003125, magnitude of gradient - 0.12743763959226445\n",
      "Step - 4549, Loss - 0.34678441291055223, Learning Rate - 0.003125, magnitude of gradient - 0.04731296378570054\n",
      "Step - 4550, Loss - 0.3637109175386769, Learning Rate - 0.003125, magnitude of gradient - 0.055976011708367635\n",
      "Step - 4551, Loss - 0.34706347722262226, Learning Rate - 0.003125, magnitude of gradient - 0.052230096775109223\n",
      "Step - 4552, Loss - 0.33632717040480326, Learning Rate - 0.003125, magnitude of gradient - 0.0852347293791406\n",
      "Step - 4553, Loss - 0.3282627891176375, Learning Rate - 0.003125, magnitude of gradient - 0.03509006350198958\n",
      "Step - 4554, Loss - 0.28828168725536546, Learning Rate - 0.003125, magnitude of gradient - 0.042482008256339486\n",
      "Step - 4555, Loss - 0.23088799807185212, Learning Rate - 0.003125, magnitude of gradient - 0.014143415275350648\n",
      "Step - 4556, Loss - 0.2587045361494332, Learning Rate - 0.003125, magnitude of gradient - 0.048019887610812204\n",
      "Step - 4557, Loss - 0.26676058061177405, Learning Rate - 0.003125, magnitude of gradient - 0.028530636872929453\n",
      "Step - 4558, Loss - 0.3195406710490719, Learning Rate - 0.003125, magnitude of gradient - 0.04709954719083764\n",
      "Step - 4559, Loss - 0.266235331804882, Learning Rate - 0.003125, magnitude of gradient - 0.054381098163033284\n",
      "Step - 4560, Loss - 0.29855147877748, Learning Rate - 0.003125, magnitude of gradient - 0.06647660577161459\n",
      "Step - 4561, Loss - 0.3298256374601803, Learning Rate - 0.003125, magnitude of gradient - 0.026986028533924115\n",
      "Step - 4562, Loss - 0.29694388585877324, Learning Rate - 0.003125, magnitude of gradient - 0.02297439243215677\n",
      "Step - 4563, Loss - 0.2998663332837954, Learning Rate - 0.003125, magnitude of gradient - 0.05977550047446593\n",
      "Step - 4564, Loss - 0.3241421539234394, Learning Rate - 0.003125, magnitude of gradient - 0.03992476792119523\n",
      "Step - 4565, Loss - 0.3494246232396544, Learning Rate - 0.003125, magnitude of gradient - 0.07324087417173034\n",
      "Step - 4566, Loss - 0.3032393328958647, Learning Rate - 0.003125, magnitude of gradient - 0.026523876036045307\n",
      "Step - 4567, Loss - 0.3049091230954486, Learning Rate - 0.003125, magnitude of gradient - 0.04614914939303226\n",
      "Step - 4568, Loss - 0.2799026040846165, Learning Rate - 0.003125, magnitude of gradient - 0.1021161091521173\n",
      "Step - 4569, Loss - 0.31345385409902227, Learning Rate - 0.003125, magnitude of gradient - 0.06300618008518406\n",
      "Step - 4570, Loss - 0.3270344026930486, Learning Rate - 0.003125, magnitude of gradient - 0.053403970416814085\n",
      "Step - 4571, Loss - 0.3130513891278519, Learning Rate - 0.003125, magnitude of gradient - 0.03036448604631795\n",
      "Step - 4572, Loss - 0.3623944163751739, Learning Rate - 0.003125, magnitude of gradient - 0.036769501474116735\n",
      "Step - 4573, Loss - 0.380069818429533, Learning Rate - 0.003125, magnitude of gradient - 0.028045876980355797\n",
      "Step - 4574, Loss - 0.3513930437087126, Learning Rate - 0.003125, magnitude of gradient - 0.03114379953657834\n",
      "Step - 4575, Loss - 0.3643872698061456, Learning Rate - 0.003125, magnitude of gradient - 0.01620313914862161\n",
      "Step - 4576, Loss - 0.41708615582943437, Learning Rate - 0.003125, magnitude of gradient - 0.12226275873166137\n",
      "Step - 4577, Loss - 0.37124761416216695, Learning Rate - 0.003125, magnitude of gradient - 0.03546903305065481\n",
      "Step - 4578, Loss - 0.35578812909159174, Learning Rate - 0.003125, magnitude of gradient - 0.07653157273030274\n",
      "Step - 4579, Loss - 0.27878621770323947, Learning Rate - 0.003125, magnitude of gradient - 0.035284211640559435\n",
      "Step - 4580, Loss - 0.32187663994484994, Learning Rate - 0.003125, magnitude of gradient - 0.06552241279877184\n",
      "Step - 4581, Loss - 0.31094338399210514, Learning Rate - 0.003125, magnitude of gradient - 0.01937115885734446\n",
      "Step - 4582, Loss - 0.260561860084622, Learning Rate - 0.003125, magnitude of gradient - 0.03351311962170609\n",
      "Step - 4583, Loss - 0.4103253537151146, Learning Rate - 0.003125, magnitude of gradient - 0.039201027355482164\n",
      "Step - 4584, Loss - 0.3283335099394206, Learning Rate - 0.003125, magnitude of gradient - 0.10117984394513822\n",
      "Step - 4585, Loss - 0.31792786447767063, Learning Rate - 0.003125, magnitude of gradient - 0.07815103028418918\n",
      "Step - 4586, Loss - 0.3531022735488092, Learning Rate - 0.003125, magnitude of gradient - 0.07533528409420324\n",
      "Step - 4587, Loss - 0.3045963376938578, Learning Rate - 0.003125, magnitude of gradient - 0.04187900892896256\n",
      "Step - 4588, Loss - 0.3429366067482429, Learning Rate - 0.003125, magnitude of gradient - 0.027633995743152638\n",
      "Step - 4589, Loss - 0.29039831798616866, Learning Rate - 0.003125, magnitude of gradient - 0.1016937761745364\n",
      "Step - 4590, Loss - 0.3173757542726795, Learning Rate - 0.003125, magnitude of gradient - 0.0971157886903796\n",
      "Step - 4591, Loss - 0.36287840530416465, Learning Rate - 0.003125, magnitude of gradient - 0.018772278292718943\n",
      "Step - 4592, Loss - 0.4441473426463636, Learning Rate - 0.003125, magnitude of gradient - 0.06159536979874671\n",
      "Step - 4593, Loss - 0.3020923015869578, Learning Rate - 0.003125, magnitude of gradient - 0.03727994252051918\n",
      "Step - 4594, Loss - 0.3059605220052209, Learning Rate - 0.003125, magnitude of gradient - 0.09589356730850926\n",
      "Step - 4595, Loss - 0.39053477756024724, Learning Rate - 0.003125, magnitude of gradient - 0.037986655543066704\n",
      "Step - 4596, Loss - 0.28933730213303266, Learning Rate - 0.003125, magnitude of gradient - 0.08170929873357027\n",
      "Step - 4597, Loss - 0.3520543653958345, Learning Rate - 0.003125, magnitude of gradient - 0.09059884156910457\n",
      "Step - 4598, Loss - 0.3535855537321771, Learning Rate - 0.003125, magnitude of gradient - 0.06221321728469426\n",
      "Step - 4599, Loss - 0.3415509203695844, Learning Rate - 0.003125, magnitude of gradient - 0.00861252635718969\n",
      "Step - 4600, Loss - 0.2671058986822304, Learning Rate - 0.003125, magnitude of gradient - 0.0297342528120347\n",
      "Step - 4601, Loss - 0.3100825266385464, Learning Rate - 0.003125, magnitude of gradient - 0.04319225804132373\n",
      "Step - 4602, Loss - 0.29829260482836806, Learning Rate - 0.003125, magnitude of gradient - 0.0423517479973377\n",
      "Step - 4603, Loss - 0.32401751843818577, Learning Rate - 0.003125, magnitude of gradient - 0.0754929315062346\n",
      "Step - 4604, Loss - 0.3530597452890751, Learning Rate - 0.003125, magnitude of gradient - 0.019646235465262556\n",
      "Step - 4605, Loss - 0.2500711905905567, Learning Rate - 0.003125, magnitude of gradient - 0.007598359957307766\n",
      "Step - 4606, Loss - 0.2584874425382925, Learning Rate - 0.003125, magnitude of gradient - 0.04497257164151162\n",
      "Step - 4607, Loss - 0.31436162319418554, Learning Rate - 0.003125, magnitude of gradient - 0.05204674922152089\n",
      "Step - 4608, Loss - 0.2678150843516905, Learning Rate - 0.003125, magnitude of gradient - 0.03270555901154401\n",
      "Step - 4609, Loss - 0.28065445707847336, Learning Rate - 0.003125, magnitude of gradient - 0.057544974797078735\n",
      "Step - 4610, Loss - 0.38933521619993233, Learning Rate - 0.003125, magnitude of gradient - 0.04162980694064369\n",
      "Step - 4611, Loss - 0.3303277780130426, Learning Rate - 0.003125, magnitude of gradient - 0.05380447981457427\n",
      "Step - 4612, Loss - 0.2233257855480839, Learning Rate - 0.003125, magnitude of gradient - 0.07412413691294752\n",
      "Step - 4613, Loss - 0.33122521229971574, Learning Rate - 0.003125, magnitude of gradient - 0.030747263288766882\n",
      "Step - 4614, Loss - 0.3414927104182161, Learning Rate - 0.003125, magnitude of gradient - 0.07332274990392787\n",
      "Step - 4615, Loss - 0.38857906944475723, Learning Rate - 0.003125, magnitude of gradient - 0.03400532569907935\n",
      "Step - 4616, Loss - 0.3291214447811659, Learning Rate - 0.003125, magnitude of gradient - 0.09042660600682129\n",
      "Step - 4617, Loss - 0.3005814671653234, Learning Rate - 0.003125, magnitude of gradient - 0.0352511005948945\n",
      "Step - 4618, Loss - 0.2743953479158384, Learning Rate - 0.003125, magnitude of gradient - 0.007232141672881461\n",
      "Step - 4619, Loss - 0.2675007204382794, Learning Rate - 0.003125, magnitude of gradient - 0.04015326722340482\n",
      "Step - 4620, Loss - 0.3380729707451794, Learning Rate - 0.003125, magnitude of gradient - 0.024845036813249885\n",
      "Step - 4621, Loss - 0.37581608702600305, Learning Rate - 0.003125, magnitude of gradient - 0.03846172996884408\n",
      "Step - 4622, Loss - 0.40630545239362426, Learning Rate - 0.003125, magnitude of gradient - 0.06299478637516956\n",
      "Step - 4623, Loss - 0.2664384217360415, Learning Rate - 0.003125, magnitude of gradient - 0.03262026928955074\n",
      "Step - 4624, Loss - 0.34423073760224404, Learning Rate - 0.003125, magnitude of gradient - 0.07191457946784983\n",
      "Step - 4625, Loss - 0.22042901434086348, Learning Rate - 0.003125, magnitude of gradient - 0.03131583771569277\n",
      "Step - 4626, Loss - 0.31225355851679576, Learning Rate - 0.003125, magnitude of gradient - 0.003339084638616594\n",
      "Step - 4627, Loss - 0.28053749197015193, Learning Rate - 0.003125, magnitude of gradient - 0.08725635199077615\n",
      "Step - 4628, Loss - 0.36725505760494065, Learning Rate - 0.003125, magnitude of gradient - 0.017757152623302918\n",
      "Step - 4629, Loss - 0.29979324605795815, Learning Rate - 0.003125, magnitude of gradient - 0.022227763969652845\n",
      "Step - 4630, Loss - 0.3137285959601086, Learning Rate - 0.003125, magnitude of gradient - 0.08610246889330639\n",
      "Step - 4631, Loss - 0.3477160547547048, Learning Rate - 0.003125, magnitude of gradient - 0.04332730693130906\n",
      "Step - 4632, Loss - 0.33350915428495453, Learning Rate - 0.003125, magnitude of gradient - 0.04021887215857429\n",
      "Step - 4633, Loss - 0.26414770686095324, Learning Rate - 0.003125, magnitude of gradient - 0.010823442387425546\n",
      "Step - 4634, Loss - 0.2910134832044002, Learning Rate - 0.003125, magnitude of gradient - 0.033709418850199874\n",
      "Step - 4635, Loss - 0.2606597441281185, Learning Rate - 0.003125, magnitude of gradient - 0.038540462263528266\n",
      "Step - 4636, Loss - 0.27796451740845707, Learning Rate - 0.003125, magnitude of gradient - 0.04725099119049471\n",
      "Step - 4637, Loss - 0.339164143133868, Learning Rate - 0.003125, magnitude of gradient - 0.08475162267866491\n",
      "Step - 4638, Loss - 0.30167577935857204, Learning Rate - 0.003125, magnitude of gradient - 0.0545225376112242\n",
      "Step - 4639, Loss - 0.3639109957255434, Learning Rate - 0.003125, magnitude of gradient - 0.04489666694549018\n",
      "Step - 4640, Loss - 0.3112783641969463, Learning Rate - 0.003125, magnitude of gradient - 0.13716906723666752\n",
      "Step - 4641, Loss - 0.3504962458382331, Learning Rate - 0.003125, magnitude of gradient - 0.0803973001426688\n",
      "Step - 4642, Loss - 0.3153019070618371, Learning Rate - 0.003125, magnitude of gradient - 0.0864610043036205\n",
      "Step - 4643, Loss - 0.4270966064054771, Learning Rate - 0.003125, magnitude of gradient - 0.01966231295342387\n",
      "Step - 4644, Loss - 0.41089108801060925, Learning Rate - 0.003125, magnitude of gradient - 0.04969826589917137\n",
      "Step - 4645, Loss - 0.3258884870027716, Learning Rate - 0.003125, magnitude of gradient - 0.024764452757245384\n",
      "Step - 4646, Loss - 0.3913667369724231, Learning Rate - 0.003125, magnitude of gradient - 0.024033213450621392\n",
      "Step - 4647, Loss - 0.39601253274912873, Learning Rate - 0.003125, magnitude of gradient - 0.03091803990834526\n",
      "Step - 4648, Loss - 0.35525418843859474, Learning Rate - 0.003125, magnitude of gradient - 0.08548726923404616\n",
      "Step - 4649, Loss - 0.29803482641986667, Learning Rate - 0.003125, magnitude of gradient - 0.05940244347283409\n",
      "Step - 4650, Loss - 0.36111845201016235, Learning Rate - 0.003125, magnitude of gradient - 0.0706209073138792\n",
      "Step - 4651, Loss - 0.43331461142724553, Learning Rate - 0.003125, magnitude of gradient - 0.041149800155205175\n",
      "Step - 4652, Loss - 0.29884543894056254, Learning Rate - 0.003125, magnitude of gradient - 0.03156594945892516\n",
      "Step - 4653, Loss - 0.3844267838545545, Learning Rate - 0.003125, magnitude of gradient - 0.09442996335157391\n",
      "Step - 4654, Loss - 0.27869030011447826, Learning Rate - 0.003125, magnitude of gradient - 0.046861999772657814\n",
      "Step - 4655, Loss - 0.3138898928905185, Learning Rate - 0.003125, magnitude of gradient - 0.032194999131259824\n",
      "Step - 4656, Loss - 0.3092986558603201, Learning Rate - 0.003125, magnitude of gradient - 0.09349096828025566\n",
      "Step - 4657, Loss - 0.3497498353678251, Learning Rate - 0.003125, magnitude of gradient - 0.04987394501660193\n",
      "Step - 4658, Loss - 0.24205734143204838, Learning Rate - 0.003125, magnitude of gradient - 0.07075822384359377\n",
      "Step - 4659, Loss - 0.27622132671183597, Learning Rate - 0.003125, magnitude of gradient - 0.05062701629886096\n",
      "Step - 4660, Loss - 0.3803260735123849, Learning Rate - 0.003125, magnitude of gradient - 0.04878450464868031\n",
      "Step - 4661, Loss - 0.3847576125923209, Learning Rate - 0.003125, magnitude of gradient - 0.00934645348783543\n",
      "Step - 4662, Loss - 0.33615538673396805, Learning Rate - 0.003125, magnitude of gradient - 0.06005290936551248\n",
      "Step - 4663, Loss - 0.31557164365785634, Learning Rate - 0.003125, magnitude of gradient - 0.042931347674029376\n",
      "Step - 4664, Loss - 0.3125440814604228, Learning Rate - 0.003125, magnitude of gradient - 0.04004078092382036\n",
      "Step - 4665, Loss - 0.347225571959294, Learning Rate - 0.003125, magnitude of gradient - 0.03596740513371872\n",
      "Step - 4666, Loss - 0.34860461146634486, Learning Rate - 0.003125, magnitude of gradient - 0.06771149388298814\n",
      "Step - 4667, Loss - 0.3897755244524756, Learning Rate - 0.003125, magnitude of gradient - 0.08787552182224032\n",
      "Step - 4668, Loss - 0.30955033118020836, Learning Rate - 0.003125, magnitude of gradient - 0.06293974917356081\n",
      "Step - 4669, Loss - 0.3895275119327266, Learning Rate - 0.003125, magnitude of gradient - 0.06810418561458229\n",
      "Step - 4670, Loss - 0.35163403751472344, Learning Rate - 0.003125, magnitude of gradient - 0.08406835556369695\n",
      "Step - 4671, Loss - 0.30998207579316683, Learning Rate - 0.003125, magnitude of gradient - 0.020003511085088754\n",
      "Step - 4672, Loss - 0.29318919385134645, Learning Rate - 0.003125, magnitude of gradient - 0.04399584535515046\n",
      "Step - 4673, Loss - 0.30232627116212485, Learning Rate - 0.003125, magnitude of gradient - 0.05654370391896375\n",
      "Step - 4674, Loss - 0.3851553470219575, Learning Rate - 0.003125, magnitude of gradient - 0.0361194911409457\n",
      "Step - 4675, Loss - 0.24807430282011628, Learning Rate - 0.003125, magnitude of gradient - 0.02687487566410475\n",
      "Step - 4676, Loss - 0.3163910494229602, Learning Rate - 0.003125, magnitude of gradient - 0.05475574500542454\n",
      "Step - 4677, Loss - 0.3064355641817009, Learning Rate - 0.003125, magnitude of gradient - 0.042941788102043146\n",
      "Step - 4678, Loss - 0.2984513323026473, Learning Rate - 0.003125, magnitude of gradient - 0.04678780427596103\n",
      "Step - 4679, Loss - 0.30805781313937314, Learning Rate - 0.003125, magnitude of gradient - 0.06470239887559748\n",
      "Step - 4680, Loss - 0.35430199256460737, Learning Rate - 0.003125, magnitude of gradient - 0.0642629992307691\n",
      "Step - 4681, Loss - 0.38652864055032693, Learning Rate - 0.003125, magnitude of gradient - 0.04518250754800496\n",
      "Step - 4682, Loss - 0.3444893791591471, Learning Rate - 0.003125, magnitude of gradient - 0.01226732666284685\n",
      "Step - 4683, Loss - 0.4094572197454961, Learning Rate - 0.003125, magnitude of gradient - 0.05021767671416469\n",
      "Step - 4684, Loss - 0.37999180671324684, Learning Rate - 0.003125, magnitude of gradient - 0.041021594228847054\n",
      "Step - 4685, Loss - 0.3059354011411942, Learning Rate - 0.003125, magnitude of gradient - 0.06229175425938312\n",
      "Step - 4686, Loss - 0.3703362848351924, Learning Rate - 0.003125, magnitude of gradient - 0.05135469730005752\n",
      "Step - 4687, Loss - 0.3393073189985269, Learning Rate - 0.003125, magnitude of gradient - 0.053659183803019636\n",
      "Step - 4688, Loss - 0.21485437030149024, Learning Rate - 0.003125, magnitude of gradient - 0.006992018010777086\n",
      "Step - 4689, Loss - 0.24293028647728634, Learning Rate - 0.003125, magnitude of gradient - 0.03505932930518342\n",
      "Step - 4690, Loss - 0.343895025894608, Learning Rate - 0.003125, magnitude of gradient - 0.025159640478521053\n",
      "Step - 4691, Loss - 0.23693906322616923, Learning Rate - 0.003125, magnitude of gradient - 0.09631002184549557\n",
      "Step - 4692, Loss - 0.33746850134500844, Learning Rate - 0.003125, magnitude of gradient - 0.10838832069437872\n",
      "Step - 4693, Loss - 0.4161919344159406, Learning Rate - 0.003125, magnitude of gradient - 0.07412776862769685\n",
      "Step - 4694, Loss - 0.2749576380673119, Learning Rate - 0.003125, magnitude of gradient - 0.055651475423287335\n",
      "Step - 4695, Loss - 0.39732468404505783, Learning Rate - 0.003125, magnitude of gradient - 0.03341748738949254\n",
      "Step - 4696, Loss - 0.3393222684696721, Learning Rate - 0.003125, magnitude of gradient - 0.023201800848406715\n",
      "Step - 4697, Loss - 0.32791045914407907, Learning Rate - 0.003125, magnitude of gradient - 0.05063652967341681\n",
      "Step - 4698, Loss - 0.3841132204215375, Learning Rate - 0.003125, magnitude of gradient - 0.03580058928799593\n",
      "Step - 4699, Loss - 0.31819258875968764, Learning Rate - 0.003125, magnitude of gradient - 0.018243432180113234\n",
      "Step - 4700, Loss - 0.3216837971248282, Learning Rate - 0.003125, magnitude of gradient - 0.03498124586619929\n",
      "Step - 4701, Loss - 0.32437430357163016, Learning Rate - 0.003125, magnitude of gradient - 0.064075490052062\n",
      "Step - 4702, Loss - 0.2809775826966475, Learning Rate - 0.003125, magnitude of gradient - 0.0590855238854849\n",
      "Step - 4703, Loss - 0.3170415250625644, Learning Rate - 0.003125, magnitude of gradient - 0.0919799066292403\n",
      "Step - 4704, Loss - 0.33279047170450266, Learning Rate - 0.003125, magnitude of gradient - 0.10581042844017649\n",
      "Step - 4705, Loss - 0.2834579720440503, Learning Rate - 0.003125, magnitude of gradient - 0.01879553395326894\n",
      "Step - 4706, Loss - 0.40583902399121463, Learning Rate - 0.003125, magnitude of gradient - 0.012158445919735195\n",
      "Step - 4707, Loss - 0.27288416604266835, Learning Rate - 0.003125, magnitude of gradient - 0.0847156370507818\n",
      "Step - 4708, Loss - 0.30738553483107406, Learning Rate - 0.003125, magnitude of gradient - 0.01536592746997183\n",
      "Step - 4709, Loss - 0.3128423958449516, Learning Rate - 0.003125, magnitude of gradient - 0.05495945222970538\n",
      "Step - 4710, Loss - 0.33279437834355236, Learning Rate - 0.003125, magnitude of gradient - 0.040342872462703135\n",
      "Step - 4711, Loss - 0.3604626549454625, Learning Rate - 0.003125, magnitude of gradient - 0.02361309248383096\n",
      "Step - 4712, Loss - 0.3843839426268654, Learning Rate - 0.003125, magnitude of gradient - 0.015341851238990938\n",
      "Step - 4713, Loss - 0.3300399746175729, Learning Rate - 0.003125, magnitude of gradient - 0.013577639498293303\n",
      "Step - 4714, Loss - 0.39541274022007505, Learning Rate - 0.003125, magnitude of gradient - 0.05245688459530364\n",
      "Step - 4715, Loss - 0.336257620694226, Learning Rate - 0.003125, magnitude of gradient - 0.04151938978587853\n",
      "Step - 4716, Loss - 0.26102941756222753, Learning Rate - 0.003125, magnitude of gradient - 0.013250980095314294\n",
      "Step - 4717, Loss - 0.3303921724800708, Learning Rate - 0.003125, magnitude of gradient - 0.05159096141288098\n",
      "Step - 4718, Loss - 0.3730760390350406, Learning Rate - 0.003125, magnitude of gradient - 0.05132013749204945\n",
      "Step - 4719, Loss - 0.3093514228668911, Learning Rate - 0.003125, magnitude of gradient - 0.020288472122818643\n",
      "Step - 4720, Loss - 0.38022008409197994, Learning Rate - 0.003125, magnitude of gradient - 0.026435896983761444\n",
      "Step - 4721, Loss - 0.2321897830296544, Learning Rate - 0.003125, magnitude of gradient - 0.06048703981696632\n",
      "Step - 4722, Loss - 0.3327580685641587, Learning Rate - 0.003125, magnitude of gradient - 0.02772096676672211\n",
      "Step - 4723, Loss - 0.3503296445147386, Learning Rate - 0.003125, magnitude of gradient - 0.024902613945152682\n",
      "Step - 4724, Loss - 0.3397768598303307, Learning Rate - 0.003125, magnitude of gradient - 0.06184410947585949\n",
      "Step - 4725, Loss - 0.3257476866615248, Learning Rate - 0.003125, magnitude of gradient - 0.10551438811609225\n",
      "Step - 4726, Loss - 0.3924475628668891, Learning Rate - 0.003125, magnitude of gradient - 0.05148753298844294\n",
      "Step - 4727, Loss - 0.2420075433158099, Learning Rate - 0.003125, magnitude of gradient - 0.024452322911974463\n",
      "Step - 4728, Loss - 0.39123765642256864, Learning Rate - 0.003125, magnitude of gradient - 0.028531044730261208\n",
      "Step - 4729, Loss - 0.25451607586219005, Learning Rate - 0.003125, magnitude of gradient - 0.05997779870791315\n",
      "Step - 4730, Loss - 0.3799844851490091, Learning Rate - 0.003125, magnitude of gradient - 0.010996517267667065\n",
      "Step - 4731, Loss - 0.3085114475656898, Learning Rate - 0.003125, magnitude of gradient - 0.04387876658859\n",
      "Step - 4732, Loss - 0.33695015002461937, Learning Rate - 0.003125, magnitude of gradient - 0.02721576597804218\n",
      "Step - 4733, Loss - 0.36821017287436303, Learning Rate - 0.003125, magnitude of gradient - 0.06611453532712232\n",
      "Step - 4734, Loss - 0.34148395398087106, Learning Rate - 0.003125, magnitude of gradient - 0.08466824259520013\n",
      "Step - 4735, Loss - 0.2939258790662411, Learning Rate - 0.003125, magnitude of gradient - 0.03049223717994711\n",
      "Step - 4736, Loss - 0.3194847497543929, Learning Rate - 0.003125, magnitude of gradient - 0.07834891264916598\n",
      "Step - 4737, Loss - 0.24650637373483608, Learning Rate - 0.003125, magnitude of gradient - 0.014096487426797722\n",
      "Step - 4738, Loss - 0.4068294210505599, Learning Rate - 0.003125, magnitude of gradient - 0.04298408396845756\n",
      "Step - 4739, Loss - 0.36513785963785456, Learning Rate - 0.003125, magnitude of gradient - 0.06434030842205489\n",
      "Step - 4740, Loss - 0.33641109347749854, Learning Rate - 0.003125, magnitude of gradient - 0.060602178456617695\n",
      "Step - 4741, Loss - 0.33545783810499663, Learning Rate - 0.003125, magnitude of gradient - 0.05807437678409743\n",
      "Step - 4742, Loss - 0.33246453290548345, Learning Rate - 0.003125, magnitude of gradient - 0.09730545952555215\n",
      "Step - 4743, Loss - 0.27100482356546707, Learning Rate - 0.003125, magnitude of gradient - 0.023992006459005833\n",
      "Step - 4744, Loss - 0.3149246115140628, Learning Rate - 0.003125, magnitude of gradient - 0.03731069120595363\n",
      "Step - 4745, Loss - 0.3462946154612474, Learning Rate - 0.003125, magnitude of gradient - 0.0393097680475217\n",
      "Step - 4746, Loss - 0.36059348822442044, Learning Rate - 0.003125, magnitude of gradient - 0.07986037528016424\n",
      "Step - 4747, Loss - 0.36165457151321617, Learning Rate - 0.003125, magnitude of gradient - 0.013748394603956588\n",
      "Step - 4748, Loss - 0.31701331273821753, Learning Rate - 0.003125, magnitude of gradient - 0.030415903954378008\n",
      "Step - 4749, Loss - 0.3204936059090979, Learning Rate - 0.003125, magnitude of gradient - 0.07267085493198393\n",
      "Step - 4750, Loss - 0.34610414460863304, Learning Rate - 0.003125, magnitude of gradient - 0.11950853384620413\n",
      "Step - 4751, Loss - 0.3992909856741241, Learning Rate - 0.003125, magnitude of gradient - 0.07300726920175298\n",
      "Step - 4752, Loss - 0.33294566408347037, Learning Rate - 0.003125, magnitude of gradient - 0.0886437761396153\n",
      "Step - 4753, Loss - 0.39373461473514215, Learning Rate - 0.003125, magnitude of gradient - 0.038168667560533956\n",
      "Step - 4754, Loss - 0.26305316646539545, Learning Rate - 0.003125, magnitude of gradient - 0.06343081836344676\n",
      "Step - 4755, Loss - 0.26320802323106174, Learning Rate - 0.003125, magnitude of gradient - 0.10278637307521436\n",
      "Step - 4756, Loss - 0.36029479091455874, Learning Rate - 0.003125, magnitude of gradient - 0.04738431847641797\n",
      "Step - 4757, Loss - 0.28703676007865503, Learning Rate - 0.003125, magnitude of gradient - 0.04834445781354543\n",
      "Step - 4758, Loss - 0.3305608402638491, Learning Rate - 0.003125, magnitude of gradient - 0.053561468161689905\n",
      "Step - 4759, Loss - 0.3305184124053705, Learning Rate - 0.003125, magnitude of gradient - 0.044168292337518296\n",
      "Step - 4760, Loss - 0.2947987672347661, Learning Rate - 0.003125, magnitude of gradient - 0.02988336841474558\n",
      "Step - 4761, Loss - 0.3330781977618038, Learning Rate - 0.003125, magnitude of gradient - 0.026119089699716717\n",
      "Step - 4762, Loss - 0.35176369098586646, Learning Rate - 0.003125, magnitude of gradient - 0.012751211842183885\n",
      "Step - 4763, Loss - 0.35810835948426245, Learning Rate - 0.003125, magnitude of gradient - 0.075593983811811\n",
      "Step - 4764, Loss - 0.3089115670964788, Learning Rate - 0.003125, magnitude of gradient - 0.04409834501096588\n",
      "Step - 4765, Loss - 0.30908334624249467, Learning Rate - 0.003125, magnitude of gradient - 0.012705641936798896\n",
      "Step - 4766, Loss - 0.31546720060214556, Learning Rate - 0.003125, magnitude of gradient - 0.04291954717626958\n",
      "Step - 4767, Loss - 0.33699457902029145, Learning Rate - 0.003125, magnitude of gradient - 0.09842663390440867\n",
      "Step - 4768, Loss - 0.285640813243594, Learning Rate - 0.003125, magnitude of gradient - 0.05381815959027647\n",
      "Step - 4769, Loss - 0.3525007867307067, Learning Rate - 0.003125, magnitude of gradient - 0.06886340102906498\n",
      "Step - 4770, Loss - 0.29863505112874766, Learning Rate - 0.003125, magnitude of gradient - 0.029758805645406495\n",
      "Step - 4771, Loss - 0.39152650242860787, Learning Rate - 0.003125, magnitude of gradient - 0.03566789407657562\n",
      "Step - 4772, Loss - 0.3613558374949728, Learning Rate - 0.003125, magnitude of gradient - 0.022402148890382098\n",
      "Step - 4773, Loss - 0.3648941874843437, Learning Rate - 0.003125, magnitude of gradient - 0.01577091803676848\n",
      "Step - 4774, Loss - 0.2792560050857724, Learning Rate - 0.003125, magnitude of gradient - 0.024527405393816166\n",
      "Step - 4775, Loss - 0.3300282067918511, Learning Rate - 0.003125, magnitude of gradient - 0.030548543020254445\n",
      "Step - 4776, Loss - 0.2271916715082308, Learning Rate - 0.003125, magnitude of gradient - 0.029674061089302463\n",
      "Step - 4777, Loss - 0.40187156579552924, Learning Rate - 0.003125, magnitude of gradient - 0.04129607014219744\n",
      "Step - 4778, Loss - 0.3101052570669074, Learning Rate - 0.003125, magnitude of gradient - 0.022777389327964397\n",
      "Step - 4779, Loss - 0.35858529867739297, Learning Rate - 0.003125, magnitude of gradient - 0.021390213849296607\n",
      "Step - 4780, Loss - 0.3163631013451105, Learning Rate - 0.003125, magnitude of gradient - 0.040237256350579495\n",
      "Step - 4781, Loss - 0.36621503809596256, Learning Rate - 0.003125, magnitude of gradient - 0.0812843780292317\n",
      "Step - 4782, Loss - 0.3265843325011207, Learning Rate - 0.003125, magnitude of gradient - 0.09211440281818194\n",
      "Step - 4783, Loss - 0.2540216269773214, Learning Rate - 0.003125, magnitude of gradient - 0.014654579932916608\n",
      "Step - 4784, Loss - 0.38060570352629364, Learning Rate - 0.003125, magnitude of gradient - 0.10827780791923454\n",
      "Step - 4785, Loss - 0.3185263552045096, Learning Rate - 0.003125, magnitude of gradient - 0.07196096160312322\n",
      "Step - 4786, Loss - 0.3040497307311375, Learning Rate - 0.003125, magnitude of gradient - 0.02068193178182486\n",
      "Step - 4787, Loss - 0.318371342236471, Learning Rate - 0.003125, magnitude of gradient - 0.03169076952748311\n",
      "Step - 4788, Loss - 0.31795324772456796, Learning Rate - 0.003125, magnitude of gradient - 0.07887623623735167\n",
      "Step - 4789, Loss - 0.275978439309444, Learning Rate - 0.003125, magnitude of gradient - 0.09194785593000776\n",
      "Step - 4790, Loss - 0.3467615691781316, Learning Rate - 0.003125, magnitude of gradient - 0.04567756990347661\n",
      "Step - 4791, Loss - 0.29233109921766454, Learning Rate - 0.003125, magnitude of gradient - 0.021140081788342695\n",
      "Step - 4792, Loss - 0.2876837691572345, Learning Rate - 0.003125, magnitude of gradient - 0.082424769064042\n",
      "Step - 4793, Loss - 0.35293800496419614, Learning Rate - 0.003125, magnitude of gradient - 0.008772540814414038\n",
      "Step - 4794, Loss - 0.37083949177976083, Learning Rate - 0.003125, magnitude of gradient - 0.03746575755793763\n",
      "Step - 4795, Loss - 0.36454502469777594, Learning Rate - 0.003125, magnitude of gradient - 0.01915484000510327\n",
      "Step - 4796, Loss - 0.33248199016595703, Learning Rate - 0.003125, magnitude of gradient - 0.09524280100335952\n",
      "Step - 4797, Loss - 0.3476757645933207, Learning Rate - 0.003125, magnitude of gradient - 0.08518353406104462\n",
      "Step - 4798, Loss - 0.34819064509999853, Learning Rate - 0.003125, magnitude of gradient - 0.07038546000183209\n",
      "Step - 4799, Loss - 0.3483602557841642, Learning Rate - 0.003125, magnitude of gradient - 0.08363957161126517\n",
      "Step - 4800, Loss - 0.2603493065874692, Learning Rate - 0.003125, magnitude of gradient - 0.05507257748000712\n",
      "Step - 4801, Loss - 0.3215196891938699, Learning Rate - 0.003125, magnitude of gradient - 0.1108435813958506\n",
      "Step - 4802, Loss - 0.3560151875158292, Learning Rate - 0.003125, magnitude of gradient - 0.10553951566855246\n",
      "Step - 4803, Loss - 0.32409811878270406, Learning Rate - 0.003125, magnitude of gradient - 0.02576097391394029\n",
      "Step - 4804, Loss - 0.26652567622361745, Learning Rate - 0.003125, magnitude of gradient - 0.016406039096617055\n",
      "Step - 4805, Loss - 0.32581948966873964, Learning Rate - 0.003125, magnitude of gradient - 0.043607917568361965\n",
      "Step - 4806, Loss - 0.3012037840655802, Learning Rate - 0.003125, magnitude of gradient - 0.06470566530503694\n",
      "Step - 4807, Loss - 0.3070203537313191, Learning Rate - 0.003125, magnitude of gradient - 0.04172886991942724\n",
      "Step - 4808, Loss - 0.37781730049070145, Learning Rate - 0.003125, magnitude of gradient - 0.02309503483656014\n",
      "Step - 4809, Loss - 0.3643289918048347, Learning Rate - 0.003125, magnitude of gradient - 0.04112076669116577\n",
      "Step - 4810, Loss - 0.25145474906214327, Learning Rate - 0.003125, magnitude of gradient - 0.022237602129533383\n",
      "Step - 4811, Loss - 0.33723818023426005, Learning Rate - 0.003125, magnitude of gradient - 0.03843767481617179\n",
      "Step - 4812, Loss - 0.3441903655917812, Learning Rate - 0.003125, magnitude of gradient - 0.027747492125672353\n",
      "Step - 4813, Loss - 0.3259344173616693, Learning Rate - 0.003125, magnitude of gradient - 0.06759629040694343\n",
      "Step - 4814, Loss - 0.3098617574374809, Learning Rate - 0.003125, magnitude of gradient - 0.04140340700665321\n",
      "Step - 4815, Loss - 0.31932481380495725, Learning Rate - 0.003125, magnitude of gradient - 0.034296122573820385\n",
      "Step - 4816, Loss - 0.2550359770956032, Learning Rate - 0.003125, magnitude of gradient - 0.06760048446828683\n",
      "Step - 4817, Loss - 0.24588514964790087, Learning Rate - 0.003125, magnitude of gradient - 0.03991833069375207\n",
      "Step - 4818, Loss - 0.33869100270815694, Learning Rate - 0.003125, magnitude of gradient - 0.0476969246295428\n",
      "Step - 4819, Loss - 0.38956383146589757, Learning Rate - 0.003125, magnitude of gradient - 0.04498791717618865\n",
      "Step - 4820, Loss - 0.3402430144034566, Learning Rate - 0.003125, magnitude of gradient - 0.06213312704739796\n",
      "Step - 4821, Loss - 0.27840908416959087, Learning Rate - 0.003125, magnitude of gradient - 0.02667802803177965\n",
      "Step - 4822, Loss - 0.2911364628581572, Learning Rate - 0.003125, magnitude of gradient - 0.011404904693426645\n",
      "Step - 4823, Loss - 0.2525414105043811, Learning Rate - 0.003125, magnitude of gradient - 0.07052720362861271\n",
      "Step - 4824, Loss - 0.32667563733256777, Learning Rate - 0.003125, magnitude of gradient - 0.05738499929063751\n",
      "Step - 4825, Loss - 0.3449887691077087, Learning Rate - 0.003125, magnitude of gradient - 0.03968690823344191\n",
      "Step - 4826, Loss - 0.3685600204928958, Learning Rate - 0.003125, magnitude of gradient - 0.08486533831785671\n",
      "Step - 4827, Loss - 0.2926092070802332, Learning Rate - 0.003125, magnitude of gradient - 0.028328836660077465\n",
      "Step - 4828, Loss - 0.3689878351063963, Learning Rate - 0.003125, magnitude of gradient - 0.03993171816227038\n",
      "Step - 4829, Loss - 0.36516674951772565, Learning Rate - 0.003125, magnitude of gradient - 0.03022287318516079\n",
      "Step - 4830, Loss - 0.36412504015607816, Learning Rate - 0.003125, magnitude of gradient - 0.07707357371559309\n",
      "Step - 4831, Loss - 0.27801167045511627, Learning Rate - 0.003125, magnitude of gradient - 0.044408899496698316\n",
      "Step - 4832, Loss - 0.3526128440168203, Learning Rate - 0.003125, magnitude of gradient - 0.038768504084991814\n",
      "Step - 4833, Loss - 0.2423093533083182, Learning Rate - 0.003125, magnitude of gradient - 0.09132321914784478\n",
      "Step - 4834, Loss - 0.3193622285925243, Learning Rate - 0.003125, magnitude of gradient - 0.06767607629606397\n",
      "Step - 4835, Loss - 0.3149608181433233, Learning Rate - 0.003125, magnitude of gradient - 0.02746412563423389\n",
      "Step - 4836, Loss - 0.3315657449944433, Learning Rate - 0.003125, magnitude of gradient - 0.03557831141352142\n",
      "Step - 4837, Loss - 0.287289768182177, Learning Rate - 0.003125, magnitude of gradient - 0.07795496399969222\n",
      "Step - 4838, Loss - 0.37631805668629476, Learning Rate - 0.003125, magnitude of gradient - 0.0680235015265355\n",
      "Step - 4839, Loss - 0.3306708596730908, Learning Rate - 0.003125, magnitude of gradient - 0.10382605468449968\n",
      "Step - 4840, Loss - 0.30550419489073316, Learning Rate - 0.003125, magnitude of gradient - 0.07896234342280335\n",
      "Step - 4841, Loss - 0.32598790856840143, Learning Rate - 0.003125, magnitude of gradient - 0.004845022702179125\n",
      "Step - 4842, Loss - 0.2964339054776943, Learning Rate - 0.003125, magnitude of gradient - 0.046072275021664534\n",
      "Step - 4843, Loss - 0.3151324942343474, Learning Rate - 0.003125, magnitude of gradient - 0.017143699562452114\n",
      "Step - 4844, Loss - 0.2672784992714677, Learning Rate - 0.003125, magnitude of gradient - 0.02250184761508437\n",
      "Step - 4845, Loss - 0.296002928971488, Learning Rate - 0.003125, magnitude of gradient - 0.05311423406789166\n",
      "Step - 4846, Loss - 0.36004699659716666, Learning Rate - 0.003125, magnitude of gradient - 0.009154134417781505\n",
      "Step - 4847, Loss - 0.2457441363082634, Learning Rate - 0.003125, magnitude of gradient - 0.030870758138412897\n",
      "Step - 4848, Loss - 0.3096391319160183, Learning Rate - 0.003125, magnitude of gradient - 0.026801185857219113\n",
      "Step - 4849, Loss - 0.36880009648303824, Learning Rate - 0.003125, magnitude of gradient - 0.051284018650596085\n",
      "Step - 4850, Loss - 0.23688701652641675, Learning Rate - 0.003125, magnitude of gradient - 0.03302145042632504\n",
      "Step - 4851, Loss - 0.28714140817364764, Learning Rate - 0.003125, magnitude of gradient - 0.06820193231378663\n",
      "Step - 4852, Loss - 0.28187884942904456, Learning Rate - 0.003125, magnitude of gradient - 0.03880326416021869\n",
      "Step - 4853, Loss - 0.22822958898784237, Learning Rate - 0.003125, magnitude of gradient - 0.03980691127968741\n",
      "Step - 4854, Loss - 0.231413643943233, Learning Rate - 0.003125, magnitude of gradient - 0.0636307073059305\n",
      "Step - 4855, Loss - 0.2855769547571094, Learning Rate - 0.003125, magnitude of gradient - 0.008644209147776152\n",
      "Step - 4856, Loss - 0.24900782146170197, Learning Rate - 0.003125, magnitude of gradient - 0.08033165659658972\n",
      "Step - 4857, Loss - 0.3329076409915097, Learning Rate - 0.003125, magnitude of gradient - 0.05537952458561978\n",
      "Step - 4858, Loss - 0.3472790140267212, Learning Rate - 0.003125, magnitude of gradient - 0.0797775069488067\n",
      "Step - 4859, Loss - 0.27390418982522724, Learning Rate - 0.003125, magnitude of gradient - 0.0407445600850039\n",
      "Step - 4860, Loss - 0.35061363828375736, Learning Rate - 0.003125, magnitude of gradient - 0.04837724641291363\n",
      "Step - 4861, Loss - 0.3777968625355105, Learning Rate - 0.003125, magnitude of gradient - 0.03254737423505363\n",
      "Step - 4862, Loss - 0.269770027988511, Learning Rate - 0.003125, magnitude of gradient - 0.018696597783964864\n",
      "Step - 4863, Loss - 0.2390426469759767, Learning Rate - 0.003125, magnitude of gradient - 0.07255188649167275\n",
      "Step - 4864, Loss - 0.3242629206093891, Learning Rate - 0.003125, magnitude of gradient - 0.05212237466072895\n",
      "Step - 4865, Loss - 0.31409297098729294, Learning Rate - 0.003125, magnitude of gradient - 0.016471626189180428\n",
      "Step - 4866, Loss - 0.36357109442303087, Learning Rate - 0.003125, magnitude of gradient - 0.044951034916141545\n",
      "Step - 4867, Loss - 0.30634411657930805, Learning Rate - 0.003125, magnitude of gradient - 0.05969943333589176\n",
      "Step - 4868, Loss - 0.2839970884369285, Learning Rate - 0.003125, magnitude of gradient - 0.026927258171455744\n",
      "Step - 4869, Loss - 0.31776074117347125, Learning Rate - 0.003125, magnitude of gradient - 0.04320071142669805\n",
      "Step - 4870, Loss - 0.3266420641893086, Learning Rate - 0.003125, magnitude of gradient - 0.07249662117729083\n",
      "Step - 4871, Loss - 0.3472456747401462, Learning Rate - 0.003125, magnitude of gradient - 0.04890852156844706\n",
      "Step - 4872, Loss - 0.3471744164120929, Learning Rate - 0.003125, magnitude of gradient - 0.05524911645733788\n",
      "Step - 4873, Loss - 0.31243821206934863, Learning Rate - 0.003125, magnitude of gradient - 0.05375298017478962\n",
      "Step - 4874, Loss - 0.37692469682733765, Learning Rate - 0.003125, magnitude of gradient - 0.01679606908152395\n",
      "Step - 4875, Loss - 0.2794632496041525, Learning Rate - 0.003125, magnitude of gradient - 0.0773266581167646\n",
      "Step - 4876, Loss - 0.29847643063621465, Learning Rate - 0.003125, magnitude of gradient - 0.04809312908712041\n",
      "Step - 4877, Loss - 0.31795902260583414, Learning Rate - 0.003125, magnitude of gradient - 0.06542736271562602\n",
      "Step - 4878, Loss - 0.22032436466390226, Learning Rate - 0.003125, magnitude of gradient - 0.043967033460607735\n",
      "Step - 4879, Loss - 0.419825292860521, Learning Rate - 0.003125, magnitude of gradient - 0.06913566194538538\n",
      "Step - 4880, Loss - 0.32342177628430957, Learning Rate - 0.003125, magnitude of gradient - 0.06056786098046662\n",
      "Step - 4881, Loss - 0.3197291882870672, Learning Rate - 0.003125, magnitude of gradient - 0.03442582294604245\n",
      "Step - 4882, Loss - 0.35086834291397706, Learning Rate - 0.003125, magnitude of gradient - 0.04477762144463785\n",
      "Step - 4883, Loss - 0.32386567244981523, Learning Rate - 0.003125, magnitude of gradient - 0.014925847515498695\n",
      "Step - 4884, Loss - 0.3618706769123031, Learning Rate - 0.003125, magnitude of gradient - 0.04747837273123141\n",
      "Step - 4885, Loss - 0.25395716707273186, Learning Rate - 0.003125, magnitude of gradient - 0.02089260128139254\n",
      "Step - 4886, Loss - 0.343902888572508, Learning Rate - 0.003125, magnitude of gradient - 0.02314585811421864\n",
      "Step - 4887, Loss - 0.30592953109581955, Learning Rate - 0.003125, magnitude of gradient - 0.09829445747913962\n",
      "Step - 4888, Loss - 0.2795553298168172, Learning Rate - 0.003125, magnitude of gradient - 0.035903882441456696\n",
      "Step - 4889, Loss - 0.2728509531474456, Learning Rate - 0.003125, magnitude of gradient - 0.09308745882224215\n",
      "Step - 4890, Loss - 0.2231341278425846, Learning Rate - 0.003125, magnitude of gradient - 0.03496901619033792\n",
      "Step - 4891, Loss - 0.3338605175768417, Learning Rate - 0.003125, magnitude of gradient - 0.007234932229147727\n",
      "Step - 4892, Loss - 0.30433716024815904, Learning Rate - 0.003125, magnitude of gradient - 0.05041085675135728\n",
      "Step - 4893, Loss - 0.3587118544063562, Learning Rate - 0.003125, magnitude of gradient - 0.025121465251534383\n",
      "Step - 4894, Loss - 0.37172044172032015, Learning Rate - 0.003125, magnitude of gradient - 0.09551051714546653\n",
      "Step - 4895, Loss - 0.2664316888968081, Learning Rate - 0.003125, magnitude of gradient - 0.02290653565641837\n",
      "Step - 4896, Loss - 0.3775277594856404, Learning Rate - 0.003125, magnitude of gradient - 0.08896812883079526\n",
      "Step - 4897, Loss - 0.342978746843824, Learning Rate - 0.003125, magnitude of gradient - 0.04245984066569933\n",
      "Step - 4898, Loss - 0.2974425741267697, Learning Rate - 0.003125, magnitude of gradient - 0.025232495068920592\n",
      "Step - 4899, Loss - 0.3398409136235758, Learning Rate - 0.003125, magnitude of gradient - 0.06414904869559387\n",
      "Step - 4900, Loss - 0.2725630925274106, Learning Rate - 0.003125, magnitude of gradient - 0.050166010943342866\n",
      "Step - 4901, Loss - 0.323447103714763, Learning Rate - 0.003125, magnitude of gradient - 0.07923928625192053\n",
      "Step - 4902, Loss - 0.3572612036993203, Learning Rate - 0.003125, magnitude of gradient - 0.027475829902246066\n",
      "Step - 4903, Loss - 0.3949490077247622, Learning Rate - 0.003125, magnitude of gradient - 0.03027334060559922\n",
      "Step - 4904, Loss - 0.28990382845533075, Learning Rate - 0.003125, magnitude of gradient - 0.03654211121663718\n",
      "Step - 4905, Loss - 0.35948051683488325, Learning Rate - 0.003125, magnitude of gradient - 0.07251849342213268\n",
      "Step - 4906, Loss - 0.2875823130124629, Learning Rate - 0.003125, magnitude of gradient - 0.023810490934203597\n",
      "Step - 4907, Loss - 0.3007156600951873, Learning Rate - 0.003125, magnitude of gradient - 0.011290944248771808\n",
      "Step - 4908, Loss - 0.39773082870534965, Learning Rate - 0.003125, magnitude of gradient - 0.06333685502152414\n",
      "Step - 4909, Loss - 0.3284515098181399, Learning Rate - 0.003125, magnitude of gradient - 0.09426961741739351\n",
      "Step - 4910, Loss - 0.3027052498091543, Learning Rate - 0.003125, magnitude of gradient - 0.020165472045914738\n",
      "Step - 4911, Loss - 0.2914792578062937, Learning Rate - 0.003125, magnitude of gradient - 0.07966691851222352\n",
      "Step - 4912, Loss - 0.31347006710146486, Learning Rate - 0.003125, magnitude of gradient - 0.020479115007651736\n",
      "Step - 4913, Loss - 0.3175709200932615, Learning Rate - 0.003125, magnitude of gradient - 0.09969783868241934\n",
      "Step - 4914, Loss - 0.3003085784070096, Learning Rate - 0.003125, magnitude of gradient - 0.07235223351393374\n",
      "Step - 4915, Loss - 0.32020304839287583, Learning Rate - 0.003125, magnitude of gradient - 0.037167151042656416\n",
      "Step - 4916, Loss - 0.3353988857534385, Learning Rate - 0.003125, magnitude of gradient - 0.0613729439130027\n",
      "Step - 4917, Loss - 0.31670181994332514, Learning Rate - 0.003125, magnitude of gradient - 0.05963747653450506\n",
      "Step - 4918, Loss - 0.3601070382715285, Learning Rate - 0.003125, magnitude of gradient - 0.049557400326646524\n",
      "Step - 4919, Loss - 0.3071005309626601, Learning Rate - 0.003125, magnitude of gradient - 0.04139908154835142\n",
      "Step - 4920, Loss - 0.26003806429595844, Learning Rate - 0.003125, magnitude of gradient - 0.05357856162391981\n",
      "Step - 4921, Loss - 0.2675167956682277, Learning Rate - 0.003125, magnitude of gradient - 0.08058105207285995\n",
      "Step - 4922, Loss - 0.29084869562035287, Learning Rate - 0.003125, magnitude of gradient - 0.0021183079463159716\n",
      "Step - 4923, Loss - 0.28416025434095743, Learning Rate - 0.003125, magnitude of gradient - 0.009402622454673085\n",
      "Step - 4924, Loss - 0.2437119217444858, Learning Rate - 0.003125, magnitude of gradient - 0.055714988458926924\n",
      "Step - 4925, Loss - 0.30076799066785653, Learning Rate - 0.003125, magnitude of gradient - 0.0911908025522606\n",
      "Step - 4926, Loss - 0.34977748145933907, Learning Rate - 0.003125, magnitude of gradient - 0.09493015896812598\n",
      "Step - 4927, Loss - 0.3162238320443589, Learning Rate - 0.003125, magnitude of gradient - 0.06363462763858957\n",
      "Step - 4928, Loss - 0.28994475868989844, Learning Rate - 0.003125, magnitude of gradient - 0.07952227644354108\n",
      "Step - 4929, Loss - 0.3546557066354102, Learning Rate - 0.003125, magnitude of gradient - 0.030086178276097515\n",
      "Step - 4930, Loss - 0.33796264582654945, Learning Rate - 0.003125, magnitude of gradient - 0.05342291665162361\n",
      "Step - 4931, Loss - 0.29479426886108734, Learning Rate - 0.003125, magnitude of gradient - 0.012100975680427761\n",
      "Step - 4932, Loss - 0.2603390166621052, Learning Rate - 0.003125, magnitude of gradient - 0.0702344342153451\n",
      "Step - 4933, Loss - 0.3594404183447991, Learning Rate - 0.003125, magnitude of gradient - 0.01540991583737493\n",
      "Step - 4934, Loss - 0.2867585963675825, Learning Rate - 0.003125, magnitude of gradient - 0.047966872579953974\n",
      "Step - 4935, Loss - 0.30235833430415093, Learning Rate - 0.003125, magnitude of gradient - 0.07569298258435385\n",
      "Step - 4936, Loss - 0.25664900270009294, Learning Rate - 0.003125, magnitude of gradient - 0.05844881799293019\n",
      "Step - 4937, Loss - 0.2831577224162471, Learning Rate - 0.003125, magnitude of gradient - 0.028789777343991775\n",
      "Step - 4938, Loss - 0.27645323334742644, Learning Rate - 0.003125, magnitude of gradient - 0.06282164007937516\n",
      "Step - 4939, Loss - 0.32015164497734755, Learning Rate - 0.003125, magnitude of gradient - 0.06576805730190728\n",
      "Step - 4940, Loss - 0.3167378052492603, Learning Rate - 0.003125, magnitude of gradient - 0.053408056204156534\n",
      "Step - 4941, Loss - 0.2539855043473082, Learning Rate - 0.003125, magnitude of gradient - 0.045300198434478446\n",
      "Step - 4942, Loss - 0.25380614227214093, Learning Rate - 0.003125, magnitude of gradient - 0.00844483197921067\n",
      "Step - 4943, Loss - 0.2355146639495573, Learning Rate - 0.003125, magnitude of gradient - 0.04608242597605758\n",
      "Step - 4944, Loss - 0.32596258439207737, Learning Rate - 0.003125, magnitude of gradient - 0.03605567294957575\n",
      "Step - 4945, Loss - 0.2703681738930689, Learning Rate - 0.003125, magnitude of gradient - 0.07173340881747682\n",
      "Step - 4946, Loss - 0.306438210420724, Learning Rate - 0.003125, magnitude of gradient - 0.0951992844219237\n",
      "Step - 4947, Loss - 0.2853085639839999, Learning Rate - 0.003125, magnitude of gradient - 0.06757099522303278\n",
      "Step - 4948, Loss - 0.37915276915971674, Learning Rate - 0.003125, magnitude of gradient - 0.09174395230061579\n",
      "Step - 4949, Loss - 0.23680438947310506, Learning Rate - 0.003125, magnitude of gradient - 0.027707819085427333\n",
      "Step - 4950, Loss - 0.25931994267253894, Learning Rate - 0.003125, magnitude of gradient - 0.056881114294546176\n",
      "Step - 4951, Loss - 0.25933126793509637, Learning Rate - 0.003125, magnitude of gradient - 0.035679517815964304\n",
      "Step - 4952, Loss - 0.30987090936627326, Learning Rate - 0.003125, magnitude of gradient - 0.019785103141279174\n",
      "Step - 4953, Loss - 0.28984708008914384, Learning Rate - 0.003125, magnitude of gradient - 0.04726371400083419\n",
      "Step - 4954, Loss - 0.22547503601856483, Learning Rate - 0.003125, magnitude of gradient - 0.05240626043807844\n",
      "Step - 4955, Loss - 0.3316243625041089, Learning Rate - 0.003125, magnitude of gradient - 0.1071220239013017\n",
      "Step - 4956, Loss - 0.36107023017736567, Learning Rate - 0.003125, magnitude of gradient - 0.04193722067221119\n",
      "Step - 4957, Loss - 0.38666431192207257, Learning Rate - 0.003125, magnitude of gradient - 0.06550386216171307\n",
      "Step - 4958, Loss - 0.24149073870961768, Learning Rate - 0.003125, magnitude of gradient - 0.055267967314693446\n",
      "Step - 4959, Loss - 0.3173368887061253, Learning Rate - 0.003125, magnitude of gradient - 0.01669439095790205\n",
      "Step - 4960, Loss - 0.28118219805100475, Learning Rate - 0.003125, magnitude of gradient - 0.056713887155587055\n",
      "Step - 4961, Loss - 0.3190745383181051, Learning Rate - 0.003125, magnitude of gradient - 0.05550929324206821\n",
      "Step - 4962, Loss - 0.31628732038690033, Learning Rate - 0.003125, magnitude of gradient - 0.08463059253031346\n",
      "Step - 4963, Loss - 0.28709391027799375, Learning Rate - 0.003125, magnitude of gradient - 0.08777158365701246\n",
      "Step - 4964, Loss - 0.26672141695161583, Learning Rate - 0.003125, magnitude of gradient - 0.02375690861427577\n",
      "Step - 4965, Loss - 0.26799964574320184, Learning Rate - 0.003125, magnitude of gradient - 0.030565923110005198\n",
      "Step - 4966, Loss - 0.2612495850091113, Learning Rate - 0.003125, magnitude of gradient - 0.029997634886966548\n",
      "Step - 4967, Loss - 0.26984532384683746, Learning Rate - 0.003125, magnitude of gradient - 0.02442983383127469\n",
      "Step - 4968, Loss - 0.43868036395625953, Learning Rate - 0.003125, magnitude of gradient - 0.014886185693848775\n",
      "Step - 4969, Loss - 0.3172141063343159, Learning Rate - 0.003125, magnitude of gradient - 0.04142475681525084\n",
      "Step - 4970, Loss - 0.24740047961982814, Learning Rate - 0.003125, magnitude of gradient - 0.03762541656658183\n",
      "Step - 4971, Loss - 0.2604487055158004, Learning Rate - 0.003125, magnitude of gradient - 0.05877965849930553\n",
      "Step - 4972, Loss - 0.2994545526629971, Learning Rate - 0.003125, magnitude of gradient - 0.040296562199584225\n",
      "Step - 4973, Loss - 0.29367193570812833, Learning Rate - 0.003125, magnitude of gradient - 0.050683549505841745\n",
      "Step - 4974, Loss - 0.3369366039118209, Learning Rate - 0.003125, magnitude of gradient - 0.025107831777243104\n",
      "Step - 4975, Loss - 0.33613016886896274, Learning Rate - 0.003125, magnitude of gradient - 0.003752086820672659\n",
      "Step - 4976, Loss - 0.3008257036969054, Learning Rate - 0.003125, magnitude of gradient - 0.019944937623348726\n",
      "Step - 4977, Loss - 0.34175134290359943, Learning Rate - 0.003125, magnitude of gradient - 0.04358066678203651\n",
      "Step - 4978, Loss - 0.2615322714458016, Learning Rate - 0.003125, magnitude of gradient - 0.018278573457916954\n",
      "Step - 4979, Loss - 0.36124700777242036, Learning Rate - 0.003125, magnitude of gradient - 0.07682837453355403\n",
      "Step - 4980, Loss - 0.36304236504266174, Learning Rate - 0.003125, magnitude of gradient - 0.052257775068400056\n",
      "Step - 4981, Loss - 0.2960844820174265, Learning Rate - 0.003125, magnitude of gradient - 0.0526477099241677\n",
      "Step - 4982, Loss - 0.36961330089267186, Learning Rate - 0.003125, magnitude of gradient - 0.0880880274488458\n",
      "Step - 4983, Loss - 0.3608630454729465, Learning Rate - 0.003125, magnitude of gradient - 0.08707663422700482\n",
      "Step - 4984, Loss - 0.3302622128589654, Learning Rate - 0.003125, magnitude of gradient - 0.026867611634746675\n",
      "Step - 4985, Loss - 0.3265962910069206, Learning Rate - 0.003125, magnitude of gradient - 0.025251695431787485\n",
      "Step - 4986, Loss - 0.29508674491335984, Learning Rate - 0.003125, magnitude of gradient - 0.03272711861415812\n",
      "Step - 4987, Loss - 0.2657769861875737, Learning Rate - 0.003125, magnitude of gradient - 0.05250528458341006\n",
      "Step - 4988, Loss - 0.275681690051744, Learning Rate - 0.003125, magnitude of gradient - 0.07343058918471583\n",
      "Step - 4989, Loss - 0.27582152858670134, Learning Rate - 0.003125, magnitude of gradient - 0.053513032907494665\n",
      "Step - 4990, Loss - 0.4222325723341378, Learning Rate - 0.003125, magnitude of gradient - 0.05999988205628933\n",
      "Step - 4991, Loss - 0.34637273119815426, Learning Rate - 0.003125, magnitude of gradient - 0.039604615185476456\n",
      "Step - 4992, Loss - 0.27811671394082993, Learning Rate - 0.003125, magnitude of gradient - 0.03510330285178602\n",
      "Step - 4993, Loss - 0.3384018061378646, Learning Rate - 0.003125, magnitude of gradient - 0.06825582994632753\n",
      "Step - 4994, Loss - 0.33820779432323284, Learning Rate - 0.003125, magnitude of gradient - 0.04694597585330607\n",
      "Step - 4995, Loss - 0.3033189254807386, Learning Rate - 0.003125, magnitude of gradient - 0.06758687996815463\n",
      "Step - 4996, Loss - 0.2712907518342711, Learning Rate - 0.003125, magnitude of gradient - 0.10468477339049244\n",
      "Step - 4997, Loss - 0.2555487515356132, Learning Rate - 0.003125, magnitude of gradient - 0.061859898804606045\n",
      "Step - 4998, Loss - 0.3450642789805489, Learning Rate - 0.003125, magnitude of gradient - 0.03946648080867622\n",
      "Step - 4999, Loss - 0.28312060592819266, Learning Rate - 0.003125, magnitude of gradient - 0.041620922063308294\n",
      "Step - 5000, Loss - 0.2946356956702801, Learning Rate - 0.003125, magnitude of gradient - 0.026148167334305222\n",
      "Step - 5001, Loss - 0.3194985665990768, Learning Rate - 0.0015625, magnitude of gradient - 0.02637538197480293\n",
      "Step - 5002, Loss - 0.3581264455311143, Learning Rate - 0.0015625, magnitude of gradient - 0.08681893927772673\n",
      "Step - 5003, Loss - 0.32229209887825205, Learning Rate - 0.0015625, magnitude of gradient - 0.029834100522735052\n",
      "Step - 5004, Loss - 0.35656445864527353, Learning Rate - 0.0015625, magnitude of gradient - 0.054695588080064445\n",
      "Step - 5005, Loss - 0.21256124236640017, Learning Rate - 0.0015625, magnitude of gradient - 0.02495627639979973\n",
      "Step - 5006, Loss - 0.334627850935172, Learning Rate - 0.0015625, magnitude of gradient - 0.02911254575665239\n",
      "Step - 5007, Loss - 0.26258605088390585, Learning Rate - 0.0015625, magnitude of gradient - 0.029676689371359852\n",
      "Step - 5008, Loss - 0.28721572919776933, Learning Rate - 0.0015625, magnitude of gradient - 0.01722370596261067\n",
      "Step - 5009, Loss - 0.2599409212056392, Learning Rate - 0.0015625, magnitude of gradient - 0.024775473129484672\n",
      "Step - 5010, Loss - 0.3405730537717714, Learning Rate - 0.0015625, magnitude of gradient - 0.0424435771572656\n",
      "Step - 5011, Loss - 0.35384707388869197, Learning Rate - 0.0015625, magnitude of gradient - 0.05007427007491137\n",
      "Step - 5012, Loss - 0.3264292614838858, Learning Rate - 0.0015625, magnitude of gradient - 0.06390325765248252\n",
      "Step - 5013, Loss - 0.31038151505049477, Learning Rate - 0.0015625, magnitude of gradient - 0.06033091375190237\n",
      "Step - 5014, Loss - 0.3231644033670094, Learning Rate - 0.0015625, magnitude of gradient - 0.048023582103651506\n",
      "Step - 5015, Loss - 0.29230172448555614, Learning Rate - 0.0015625, magnitude of gradient - 0.06448607806720642\n",
      "Step - 5016, Loss - 0.28869644908191516, Learning Rate - 0.0015625, magnitude of gradient - 0.03963778909142241\n",
      "Step - 5017, Loss - 0.3033230669420213, Learning Rate - 0.0015625, magnitude of gradient - 0.007808818845938488\n",
      "Step - 5018, Loss - 0.2980875145865378, Learning Rate - 0.0015625, magnitude of gradient - 0.07921384841133787\n",
      "Step - 5019, Loss - 0.3419024814988728, Learning Rate - 0.0015625, magnitude of gradient - 0.022584298788367232\n",
      "Step - 5020, Loss - 0.30760596471139495, Learning Rate - 0.0015625, magnitude of gradient - 0.03640701896159557\n",
      "Step - 5021, Loss - 0.3440831286591452, Learning Rate - 0.0015625, magnitude of gradient - 0.03471527435359193\n",
      "Step - 5022, Loss - 0.3588568979963958, Learning Rate - 0.0015625, magnitude of gradient - 0.03384183926977671\n",
      "Step - 5023, Loss - 0.315071428213259, Learning Rate - 0.0015625, magnitude of gradient - 0.03129309323709941\n",
      "Step - 5024, Loss - 0.3654700773230549, Learning Rate - 0.0015625, magnitude of gradient - 0.08312475764778117\n",
      "Step - 5025, Loss - 0.360702857117492, Learning Rate - 0.0015625, magnitude of gradient - 0.017220104440139054\n",
      "Step - 5026, Loss - 0.27332978445670464, Learning Rate - 0.0015625, magnitude of gradient - 0.04328555068849042\n",
      "Step - 5027, Loss - 0.35955297312958295, Learning Rate - 0.0015625, magnitude of gradient - 0.0172737240972012\n",
      "Step - 5028, Loss - 0.30937348070345183, Learning Rate - 0.0015625, magnitude of gradient - 0.0787468858896048\n",
      "Step - 5029, Loss - 0.2697976506104763, Learning Rate - 0.0015625, magnitude of gradient - 0.072023236970246\n",
      "Step - 5030, Loss - 0.28980202521274523, Learning Rate - 0.0015625, magnitude of gradient - 0.024810461510804502\n",
      "Step - 5031, Loss - 0.31263281426728773, Learning Rate - 0.0015625, magnitude of gradient - 0.03816680382859528\n",
      "Step - 5032, Loss - 0.37281468672554696, Learning Rate - 0.0015625, magnitude of gradient - 0.015240594683898715\n",
      "Step - 5033, Loss - 0.38604457883616233, Learning Rate - 0.0015625, magnitude of gradient - 0.05821461597041278\n",
      "Step - 5034, Loss - 0.372267631993412, Learning Rate - 0.0015625, magnitude of gradient - 0.08441839043281629\n",
      "Step - 5035, Loss - 0.3515735703045945, Learning Rate - 0.0015625, magnitude of gradient - 0.05987370145121396\n",
      "Step - 5036, Loss - 0.26120646866500075, Learning Rate - 0.0015625, magnitude of gradient - 0.02060151970362745\n",
      "Step - 5037, Loss - 0.35386850727728525, Learning Rate - 0.0015625, magnitude of gradient - 0.07250791514711605\n",
      "Step - 5038, Loss - 0.28152093113042365, Learning Rate - 0.0015625, magnitude of gradient - 0.054539866446126115\n",
      "Step - 5039, Loss - 0.3492972291860942, Learning Rate - 0.0015625, magnitude of gradient - 0.017166209338419763\n",
      "Step - 5040, Loss - 0.28744286428360694, Learning Rate - 0.0015625, magnitude of gradient - 0.026183909547402046\n",
      "Step - 5041, Loss - 0.3663235313350997, Learning Rate - 0.0015625, magnitude of gradient - 0.04580781127345129\n",
      "Step - 5042, Loss - 0.34868217655389855, Learning Rate - 0.0015625, magnitude of gradient - 0.07290219232335306\n",
      "Step - 5043, Loss - 0.3957293963468121, Learning Rate - 0.0015625, magnitude of gradient - 0.08236819398674375\n",
      "Step - 5044, Loss - 0.3054051467960539, Learning Rate - 0.0015625, magnitude of gradient - 0.03541586469289133\n",
      "Step - 5045, Loss - 0.36999960824924216, Learning Rate - 0.0015625, magnitude of gradient - 0.05046156840460751\n",
      "Step - 5046, Loss - 0.373326706869323, Learning Rate - 0.0015625, magnitude of gradient - 0.023641118105191764\n",
      "Step - 5047, Loss - 0.2714843098312253, Learning Rate - 0.0015625, magnitude of gradient - 0.04131468376122001\n",
      "Step - 5048, Loss - 0.31499344620355735, Learning Rate - 0.0015625, magnitude of gradient - 0.07309454066605926\n",
      "Step - 5049, Loss - 0.27018931512760763, Learning Rate - 0.0015625, magnitude of gradient - 0.09791809924663329\n",
      "Step - 5050, Loss - 0.3341505149318943, Learning Rate - 0.0015625, magnitude of gradient - 0.11571623792819435\n",
      "Step - 5051, Loss - 0.318834060865883, Learning Rate - 0.0015625, magnitude of gradient - 0.04561617197667251\n",
      "Step - 5052, Loss - 0.3208334914804857, Learning Rate - 0.0015625, magnitude of gradient - 0.07326278278746748\n",
      "Step - 5053, Loss - 0.30628252556777263, Learning Rate - 0.0015625, magnitude of gradient - 0.012207349562963543\n",
      "Step - 5054, Loss - 0.3000208378068813, Learning Rate - 0.0015625, magnitude of gradient - 0.030794966092573525\n",
      "Step - 5055, Loss - 0.3300329562403971, Learning Rate - 0.0015625, magnitude of gradient - 0.034830836565394784\n",
      "Step - 5056, Loss - 0.21049062072755537, Learning Rate - 0.0015625, magnitude of gradient - 0.023893973403607202\n",
      "Step - 5057, Loss - 0.34914309088715445, Learning Rate - 0.0015625, magnitude of gradient - 0.016014733052060377\n",
      "Step - 5058, Loss - 0.35408918586047033, Learning Rate - 0.0015625, magnitude of gradient - 0.015965559110518654\n",
      "Step - 5059, Loss - 0.3511310033534195, Learning Rate - 0.0015625, magnitude of gradient - 0.08087955239102222\n",
      "Step - 5060, Loss - 0.38080976885676054, Learning Rate - 0.0015625, magnitude of gradient - 0.012870826456540656\n",
      "Step - 5061, Loss - 0.2786082673491238, Learning Rate - 0.0015625, magnitude of gradient - 0.01226724483331007\n",
      "Step - 5062, Loss - 0.3509428025989642, Learning Rate - 0.0015625, magnitude of gradient - 0.07005416390338905\n",
      "Step - 5063, Loss - 0.3367789978198743, Learning Rate - 0.0015625, magnitude of gradient - 0.0444025715831942\n",
      "Step - 5064, Loss - 0.3007484146109552, Learning Rate - 0.0015625, magnitude of gradient - 0.026126775412856216\n",
      "Step - 5065, Loss - 0.3560814685008409, Learning Rate - 0.0015625, magnitude of gradient - 0.020837379310468356\n",
      "Step - 5066, Loss - 0.3546086747699385, Learning Rate - 0.0015625, magnitude of gradient - 0.04994113495430366\n",
      "Step - 5067, Loss - 0.27352542843265937, Learning Rate - 0.0015625, magnitude of gradient - 0.07828560082384027\n",
      "Step - 5068, Loss - 0.4183172605512716, Learning Rate - 0.0015625, magnitude of gradient - 0.013495366227069025\n",
      "Step - 5069, Loss - 0.3090673394882193, Learning Rate - 0.0015625, magnitude of gradient - 0.010252579758779412\n",
      "Step - 5070, Loss - 0.34598919875677325, Learning Rate - 0.0015625, magnitude of gradient - 0.013241489744422472\n",
      "Step - 5071, Loss - 0.3008936802049065, Learning Rate - 0.0015625, magnitude of gradient - 0.0486676635453741\n",
      "Step - 5072, Loss - 0.34864060027437266, Learning Rate - 0.0015625, magnitude of gradient - 0.013489145026244402\n",
      "Step - 5073, Loss - 0.3411684656345897, Learning Rate - 0.0015625, magnitude of gradient - 0.045277270741648605\n",
      "Step - 5074, Loss - 0.3338302005102422, Learning Rate - 0.0015625, magnitude of gradient - 0.07485600620549972\n",
      "Step - 5075, Loss - 0.3168691009662391, Learning Rate - 0.0015625, magnitude of gradient - 0.029102411165505868\n",
      "Step - 5076, Loss - 0.31383285758022006, Learning Rate - 0.0015625, magnitude of gradient - 0.029009044248380182\n",
      "Step - 5077, Loss - 0.32954091842449434, Learning Rate - 0.0015625, magnitude of gradient - 0.03183984533961348\n",
      "Step - 5078, Loss - 0.39033228047728863, Learning Rate - 0.0015625, magnitude of gradient - 0.03244707906688206\n",
      "Step - 5079, Loss - 0.28034967401576133, Learning Rate - 0.0015625, magnitude of gradient - 0.012827393907516152\n",
      "Step - 5080, Loss - 0.34376080234876427, Learning Rate - 0.0015625, magnitude of gradient - 0.03872001009509296\n",
      "Step - 5081, Loss - 0.33851865865787006, Learning Rate - 0.0015625, magnitude of gradient - 0.014069601855681362\n",
      "Step - 5082, Loss - 0.30072720316062246, Learning Rate - 0.0015625, magnitude of gradient - 0.061271108383299445\n",
      "Step - 5083, Loss - 0.35028459264661294, Learning Rate - 0.0015625, magnitude of gradient - 0.023755524663522497\n",
      "Step - 5084, Loss - 0.30656066410915167, Learning Rate - 0.0015625, magnitude of gradient - 0.053519006769139836\n",
      "Step - 5085, Loss - 0.3329151610945294, Learning Rate - 0.0015625, magnitude of gradient - 0.07439712228913464\n",
      "Step - 5086, Loss - 0.2485587467556301, Learning Rate - 0.0015625, magnitude of gradient - 0.05882347589258237\n",
      "Step - 5087, Loss - 0.30326211912571616, Learning Rate - 0.0015625, magnitude of gradient - 0.03629921079226727\n",
      "Step - 5088, Loss - 0.344617035483746, Learning Rate - 0.0015625, magnitude of gradient - 0.06412074742588536\n",
      "Step - 5089, Loss - 0.3153963141105025, Learning Rate - 0.0015625, magnitude of gradient - 0.037969895079829064\n",
      "Step - 5090, Loss - 0.279050457486893, Learning Rate - 0.0015625, magnitude of gradient - 0.03889233673691657\n",
      "Step - 5091, Loss - 0.27225054578417646, Learning Rate - 0.0015625, magnitude of gradient - 0.09150656046314262\n",
      "Step - 5092, Loss - 0.2554502199508014, Learning Rate - 0.0015625, magnitude of gradient - 0.10068265298743025\n",
      "Step - 5093, Loss - 0.31287553000492263, Learning Rate - 0.0015625, magnitude of gradient - 0.06913162496157292\n",
      "Step - 5094, Loss - 0.31163051447655155, Learning Rate - 0.0015625, magnitude of gradient - 0.04054819800305004\n",
      "Step - 5095, Loss - 0.36647982527638984, Learning Rate - 0.0015625, magnitude of gradient - 0.044497287796414785\n",
      "Step - 5096, Loss - 0.2733436659869898, Learning Rate - 0.0015625, magnitude of gradient - 0.08475897528954131\n",
      "Step - 5097, Loss - 0.2862267091319054, Learning Rate - 0.0015625, magnitude of gradient - 0.05423159738955251\n",
      "Step - 5098, Loss - 0.27491033986115576, Learning Rate - 0.0015625, magnitude of gradient - 0.02849384645117485\n",
      "Step - 5099, Loss - 0.2854064833552908, Learning Rate - 0.0015625, magnitude of gradient - 0.03128706749836965\n",
      "Step - 5100, Loss - 0.28563773142758375, Learning Rate - 0.0015625, magnitude of gradient - 0.027668840819773118\n",
      "Step - 5101, Loss - 0.346024444073531, Learning Rate - 0.0015625, magnitude of gradient - 0.04794454050962123\n",
      "Step - 5102, Loss - 0.3371243528155733, Learning Rate - 0.0015625, magnitude of gradient - 0.05401271851176591\n",
      "Step - 5103, Loss - 0.27785961487481525, Learning Rate - 0.0015625, magnitude of gradient - 0.02257044790386205\n",
      "Step - 5104, Loss - 0.33036691153808656, Learning Rate - 0.0015625, magnitude of gradient - 0.07773696033241083\n",
      "Step - 5105, Loss - 0.3069851791327992, Learning Rate - 0.0015625, magnitude of gradient - 0.017225389442878672\n",
      "Step - 5106, Loss - 0.3986066763983393, Learning Rate - 0.0015625, magnitude of gradient - 0.02670176771894223\n",
      "Step - 5107, Loss - 0.2692046053670332, Learning Rate - 0.0015625, magnitude of gradient - 0.03378482322407063\n",
      "Step - 5108, Loss - 0.36394515425165974, Learning Rate - 0.0015625, magnitude of gradient - 0.02044642728756917\n",
      "Step - 5109, Loss - 0.37583606279796455, Learning Rate - 0.0015625, magnitude of gradient - 0.07954953290247536\n",
      "Step - 5110, Loss - 0.347467679893525, Learning Rate - 0.0015625, magnitude of gradient - 0.0375011721249171\n",
      "Step - 5111, Loss - 0.31626934090185777, Learning Rate - 0.0015625, magnitude of gradient - 0.015843153743016834\n",
      "Step - 5112, Loss - 0.31814180438503914, Learning Rate - 0.0015625, magnitude of gradient - 0.03488478199853315\n",
      "Step - 5113, Loss - 0.28090204906242777, Learning Rate - 0.0015625, magnitude of gradient - 0.057314528820750284\n",
      "Step - 5114, Loss - 0.3407128552775952, Learning Rate - 0.0015625, magnitude of gradient - 0.03338154705721312\n",
      "Step - 5115, Loss - 0.34679457018504956, Learning Rate - 0.0015625, magnitude of gradient - 0.03665642584748685\n",
      "Step - 5116, Loss - 0.3878376437893328, Learning Rate - 0.0015625, magnitude of gradient - 0.08345274454923468\n",
      "Step - 5117, Loss - 0.34940259697422604, Learning Rate - 0.0015625, magnitude of gradient - 0.07683233913987342\n",
      "Step - 5118, Loss - 0.3118620248700933, Learning Rate - 0.0015625, magnitude of gradient - 0.06091732720306003\n",
      "Step - 5119, Loss - 0.27979633401610504, Learning Rate - 0.0015625, magnitude of gradient - 0.03380423002743859\n",
      "Step - 5120, Loss - 0.2776906185195266, Learning Rate - 0.0015625, magnitude of gradient - 0.029574358722314616\n",
      "Step - 5121, Loss - 0.25031624020066867, Learning Rate - 0.0015625, magnitude of gradient - 0.0563949342417879\n",
      "Step - 5122, Loss - 0.3442764676299737, Learning Rate - 0.0015625, magnitude of gradient - 0.061462233080407415\n",
      "Step - 5123, Loss - 0.39179533280559514, Learning Rate - 0.0015625, magnitude of gradient - 0.031338072983943435\n",
      "Step - 5124, Loss - 0.32055209496589787, Learning Rate - 0.0015625, magnitude of gradient - 0.046259319071117966\n",
      "Step - 5125, Loss - 0.37389257204415155, Learning Rate - 0.0015625, magnitude of gradient - 0.01933636456949299\n",
      "Step - 5126, Loss - 0.4250893284316448, Learning Rate - 0.0015625, magnitude of gradient - 0.046104743062692966\n",
      "Step - 5127, Loss - 0.2789973299359501, Learning Rate - 0.0015625, magnitude of gradient - 0.081994963559243\n",
      "Step - 5128, Loss - 0.3554893123364341, Learning Rate - 0.0015625, magnitude of gradient - 0.04707054156378142\n",
      "Step - 5129, Loss - 0.274378786612302, Learning Rate - 0.0015625, magnitude of gradient - 0.030862491533107116\n",
      "Step - 5130, Loss - 0.2930139332410503, Learning Rate - 0.0015625, magnitude of gradient - 0.04164596171836858\n",
      "Step - 5131, Loss - 0.3825684720268632, Learning Rate - 0.0015625, magnitude of gradient - 0.02555823968553611\n",
      "Step - 5132, Loss - 0.311730132207102, Learning Rate - 0.0015625, magnitude of gradient - 0.020831591046168513\n",
      "Step - 5133, Loss - 0.304177930761834, Learning Rate - 0.0015625, magnitude of gradient - 0.009892700497193534\n",
      "Step - 5134, Loss - 0.305212016562287, Learning Rate - 0.0015625, magnitude of gradient - 0.027933576669946884\n",
      "Step - 5135, Loss - 0.303442413854457, Learning Rate - 0.0015625, magnitude of gradient - 0.060424058058468434\n",
      "Step - 5136, Loss - 0.37903186650270926, Learning Rate - 0.0015625, magnitude of gradient - 0.0605451803926851\n",
      "Step - 5137, Loss - 0.35800501582295574, Learning Rate - 0.0015625, magnitude of gradient - 0.017402441314943798\n",
      "Step - 5138, Loss - 0.34873434276246323, Learning Rate - 0.0015625, magnitude of gradient - 0.060243495128694015\n",
      "Step - 5139, Loss - 0.3609297419521422, Learning Rate - 0.0015625, magnitude of gradient - 0.047236866711431624\n",
      "Step - 5140, Loss - 0.25364457858586814, Learning Rate - 0.0015625, magnitude of gradient - 0.04283442845252693\n",
      "Step - 5141, Loss - 0.28844140519894335, Learning Rate - 0.0015625, magnitude of gradient - 0.07808917600501458\n",
      "Step - 5142, Loss - 0.3707558900170706, Learning Rate - 0.0015625, magnitude of gradient - 0.061287195197655284\n",
      "Step - 5143, Loss - 0.3632917239027223, Learning Rate - 0.0015625, magnitude of gradient - 0.05025006911074636\n",
      "Step - 5144, Loss - 0.3840120135278412, Learning Rate - 0.0015625, magnitude of gradient - 0.04993428560573803\n",
      "Step - 5145, Loss - 0.2950589325110753, Learning Rate - 0.0015625, magnitude of gradient - 0.036656865235966396\n",
      "Step - 5146, Loss - 0.3445758410324933, Learning Rate - 0.0015625, magnitude of gradient - 0.08528684545935843\n",
      "Step - 5147, Loss - 0.3187356048450477, Learning Rate - 0.0015625, magnitude of gradient - 0.05549863799038297\n",
      "Step - 5148, Loss - 0.3603356541328613, Learning Rate - 0.0015625, magnitude of gradient - 0.09931276463010048\n",
      "Step - 5149, Loss - 0.3302882781319513, Learning Rate - 0.0015625, magnitude of gradient - 0.060610665958251826\n",
      "Step - 5150, Loss - 0.27286150221698324, Learning Rate - 0.0015625, magnitude of gradient - 0.04307890206318498\n",
      "Step - 5151, Loss - 0.2800424861471686, Learning Rate - 0.0015625, magnitude of gradient - 0.09676483891018936\n",
      "Step - 5152, Loss - 0.32960760230652847, Learning Rate - 0.0015625, magnitude of gradient - 0.05737406191206082\n",
      "Step - 5153, Loss - 0.3523330121242849, Learning Rate - 0.0015625, magnitude of gradient - 0.06182556764142978\n",
      "Step - 5154, Loss - 0.39266007213405446, Learning Rate - 0.0015625, magnitude of gradient - 0.016602596891107053\n",
      "Step - 5155, Loss - 0.34799941817552504, Learning Rate - 0.0015625, magnitude of gradient - 0.043644966007863766\n",
      "Step - 5156, Loss - 0.37425077046149796, Learning Rate - 0.0015625, magnitude of gradient - 0.03028829777205558\n",
      "Step - 5157, Loss - 0.37882599549841506, Learning Rate - 0.0015625, magnitude of gradient - 0.06271226503905547\n",
      "Step - 5158, Loss - 0.29353078184643416, Learning Rate - 0.0015625, magnitude of gradient - 0.026922283328950216\n",
      "Step - 5159, Loss - 0.38445162257063664, Learning Rate - 0.0015625, magnitude of gradient - 0.014793675340832643\n",
      "Step - 5160, Loss - 0.2879789606354095, Learning Rate - 0.0015625, magnitude of gradient - 0.03557576246731819\n",
      "Step - 5161, Loss - 0.33223629176527647, Learning Rate - 0.0015625, magnitude of gradient - 0.07261387942065631\n",
      "Step - 5162, Loss - 0.34465102091054234, Learning Rate - 0.0015625, magnitude of gradient - 0.08605030849893815\n",
      "Step - 5163, Loss - 0.3382907201908511, Learning Rate - 0.0015625, magnitude of gradient - 0.008548384425452278\n",
      "Step - 5164, Loss - 0.2939419505490629, Learning Rate - 0.0015625, magnitude of gradient - 0.0750723395311084\n",
      "Step - 5165, Loss - 0.3124395778440404, Learning Rate - 0.0015625, magnitude of gradient - 0.07141700118995425\n",
      "Step - 5166, Loss - 0.3329474847557534, Learning Rate - 0.0015625, magnitude of gradient - 0.06610037413654793\n",
      "Step - 5167, Loss - 0.35319667782067043, Learning Rate - 0.0015625, magnitude of gradient - 0.07390253984845531\n",
      "Step - 5168, Loss - 0.3429183137235909, Learning Rate - 0.0015625, magnitude of gradient - 0.09227915707166166\n",
      "Step - 5169, Loss - 0.3940028126247686, Learning Rate - 0.0015625, magnitude of gradient - 0.13297256182509362\n",
      "Step - 5170, Loss - 0.318121058387173, Learning Rate - 0.0015625, magnitude of gradient - 0.05898520199251941\n",
      "Step - 5171, Loss - 0.38237615664641644, Learning Rate - 0.0015625, magnitude of gradient - 0.052479026899995534\n",
      "Step - 5172, Loss - 0.32507752971094983, Learning Rate - 0.0015625, magnitude of gradient - 0.06444732254989632\n",
      "Step - 5173, Loss - 0.4729261803134931, Learning Rate - 0.0015625, magnitude of gradient - 0.151155459242992\n",
      "Step - 5174, Loss - 0.32930568906527885, Learning Rate - 0.0015625, magnitude of gradient - 0.011060620219070316\n",
      "Step - 5175, Loss - 0.25638141623646865, Learning Rate - 0.0015625, magnitude of gradient - 0.07647148980074985\n",
      "Step - 5176, Loss - 0.3312452327664756, Learning Rate - 0.0015625, magnitude of gradient - 0.0438889661880969\n",
      "Step - 5177, Loss - 0.25159116463833997, Learning Rate - 0.0015625, magnitude of gradient - 0.04419559588343796\n",
      "Step - 5178, Loss - 0.3529632098877134, Learning Rate - 0.0015625, magnitude of gradient - 0.0222456261591204\n",
      "Step - 5179, Loss - 0.32063067886271346, Learning Rate - 0.0015625, magnitude of gradient - 0.07389401379202992\n",
      "Step - 5180, Loss - 0.2539471290069225, Learning Rate - 0.0015625, magnitude of gradient - 0.00952876037954178\n",
      "Step - 5181, Loss - 0.29372406089229375, Learning Rate - 0.0015625, magnitude of gradient - 0.017840657858193065\n",
      "Step - 5182, Loss - 0.3183941873631891, Learning Rate - 0.0015625, magnitude of gradient - 0.01940890195138143\n",
      "Step - 5183, Loss - 0.26467829267132525, Learning Rate - 0.0015625, magnitude of gradient - 0.007485098367017178\n",
      "Step - 5184, Loss - 0.2865013634077461, Learning Rate - 0.0015625, magnitude of gradient - 0.01684592193616661\n",
      "Step - 5185, Loss - 0.30431530811806634, Learning Rate - 0.0015625, magnitude of gradient - 0.0661774150870279\n",
      "Step - 5186, Loss - 0.3382023146895788, Learning Rate - 0.0015625, magnitude of gradient - 0.058317736656971986\n",
      "Step - 5187, Loss - 0.38361887263708094, Learning Rate - 0.0015625, magnitude of gradient - 0.05480737679663684\n",
      "Step - 5188, Loss - 0.32641900413446967, Learning Rate - 0.0015625, magnitude of gradient - 0.0765829996442664\n",
      "Step - 5189, Loss - 0.28055720799933104, Learning Rate - 0.0015625, magnitude of gradient - 0.017576755371452087\n",
      "Step - 5190, Loss - 0.32465369281510503, Learning Rate - 0.0015625, magnitude of gradient - 0.07247557479936034\n",
      "Step - 5191, Loss - 0.30833174981800415, Learning Rate - 0.0015625, magnitude of gradient - 0.019420088483167407\n",
      "Step - 5192, Loss - 0.32411654501144066, Learning Rate - 0.0015625, magnitude of gradient - 0.02761931288158217\n",
      "Step - 5193, Loss - 0.2711348184589629, Learning Rate - 0.0015625, magnitude of gradient - 0.014354320492230082\n",
      "Step - 5194, Loss - 0.32657253806683023, Learning Rate - 0.0015625, magnitude of gradient - 0.033810661455032635\n",
      "Step - 5195, Loss - 0.3634384968408768, Learning Rate - 0.0015625, magnitude of gradient - 0.07948296372204629\n",
      "Step - 5196, Loss - 0.27334277496318193, Learning Rate - 0.0015625, magnitude of gradient - 0.011802444595535254\n",
      "Step - 5197, Loss - 0.4077607490668862, Learning Rate - 0.0015625, magnitude of gradient - 0.03069607477411515\n",
      "Step - 5198, Loss - 0.33777768707092654, Learning Rate - 0.0015625, magnitude of gradient - 0.042712404488367675\n",
      "Step - 5199, Loss - 0.22899542588763175, Learning Rate - 0.0015625, magnitude of gradient - 0.08243172004532662\n",
      "Step - 5200, Loss - 0.4168941393923145, Learning Rate - 0.0015625, magnitude of gradient - 0.04607230706468741\n",
      "Step - 5201, Loss - 0.348309651246204, Learning Rate - 0.0015625, magnitude of gradient - 0.061384438062786234\n",
      "Step - 5202, Loss - 0.3260826787159953, Learning Rate - 0.0015625, magnitude of gradient - 0.08231124170414139\n",
      "Step - 5203, Loss - 0.4288037339900389, Learning Rate - 0.0015625, magnitude of gradient - 0.07827899723238388\n",
      "Step - 5204, Loss - 0.3744183751536875, Learning Rate - 0.0015625, magnitude of gradient - 0.011750195602575083\n",
      "Step - 5205, Loss - 0.28105609199799025, Learning Rate - 0.0015625, magnitude of gradient - 0.02758312390668292\n",
      "Step - 5206, Loss - 0.29581170935208767, Learning Rate - 0.0015625, magnitude of gradient - 0.022877772200111775\n",
      "Step - 5207, Loss - 0.36525497412460345, Learning Rate - 0.0015625, magnitude of gradient - 0.014011352960592352\n",
      "Step - 5208, Loss - 0.3480159271085942, Learning Rate - 0.0015625, magnitude of gradient - 0.08751972945876073\n",
      "Step - 5209, Loss - 0.2500434924989587, Learning Rate - 0.0015625, magnitude of gradient - 0.058147350732624545\n",
      "Step - 5210, Loss - 0.32038207243570477, Learning Rate - 0.0015625, magnitude of gradient - 0.06024737484240408\n",
      "Step - 5211, Loss - 0.37680339749514546, Learning Rate - 0.0015625, magnitude of gradient - 0.030941525212478366\n",
      "Step - 5212, Loss - 0.3151219758655699, Learning Rate - 0.0015625, magnitude of gradient - 0.01950989616185534\n",
      "Step - 5213, Loss - 0.34007351587467194, Learning Rate - 0.0015625, magnitude of gradient - 0.03600025583730347\n",
      "Step - 5214, Loss - 0.27619933247601935, Learning Rate - 0.0015625, magnitude of gradient - 0.038433593688005266\n",
      "Step - 5215, Loss - 0.40558354912145833, Learning Rate - 0.0015625, magnitude of gradient - 0.05033135970826371\n",
      "Step - 5216, Loss - 0.374221620715775, Learning Rate - 0.0015625, magnitude of gradient - 0.06321112788672643\n",
      "Step - 5217, Loss - 0.2670863306401421, Learning Rate - 0.0015625, magnitude of gradient - 0.06796681869808423\n",
      "Step - 5218, Loss - 0.3127602094082955, Learning Rate - 0.0015625, magnitude of gradient - 0.06783864174058572\n",
      "Step - 5219, Loss - 0.3056887960836888, Learning Rate - 0.0015625, magnitude of gradient - 0.04229980187812234\n",
      "Step - 5220, Loss - 0.26761613335364426, Learning Rate - 0.0015625, magnitude of gradient - 0.039578097914691725\n",
      "Step - 5221, Loss - 0.3167797294452661, Learning Rate - 0.0015625, magnitude of gradient - 0.07384696006857353\n",
      "Step - 5222, Loss - 0.25680758581328533, Learning Rate - 0.0015625, magnitude of gradient - 0.01962671439774585\n",
      "Step - 5223, Loss - 0.33283407171195956, Learning Rate - 0.0015625, magnitude of gradient - 0.03273563947620379\n",
      "Step - 5224, Loss - 0.3538591102588677, Learning Rate - 0.0015625, magnitude of gradient - 0.029356774137469552\n",
      "Step - 5225, Loss - 0.2913074556178758, Learning Rate - 0.0015625, magnitude of gradient - 0.042936429904920075\n",
      "Step - 5226, Loss - 0.25283667943171495, Learning Rate - 0.0015625, magnitude of gradient - 0.08690041495293821\n",
      "Step - 5227, Loss - 0.33906411636847245, Learning Rate - 0.0015625, magnitude of gradient - 0.02421605750956578\n",
      "Step - 5228, Loss - 0.32033702784673856, Learning Rate - 0.0015625, magnitude of gradient - 0.04937419086297846\n",
      "Step - 5229, Loss - 0.3041784224726275, Learning Rate - 0.0015625, magnitude of gradient - 0.11491517525320007\n",
      "Step - 5230, Loss - 0.275794460211439, Learning Rate - 0.0015625, magnitude of gradient - 0.04126763385809102\n",
      "Step - 5231, Loss - 0.3747044281952449, Learning Rate - 0.0015625, magnitude of gradient - 0.15794732284547852\n",
      "Step - 5232, Loss - 0.36902657741250344, Learning Rate - 0.0015625, magnitude of gradient - 0.032662802266388354\n",
      "Step - 5233, Loss - 0.34387536722520223, Learning Rate - 0.0015625, magnitude of gradient - 0.018476248394646764\n",
      "Step - 5234, Loss - 0.29477706086290234, Learning Rate - 0.0015625, magnitude of gradient - 0.037241702276566126\n",
      "Step - 5235, Loss - 0.2993324802879711, Learning Rate - 0.0015625, magnitude of gradient - 0.05434430722180123\n",
      "Step - 5236, Loss - 0.3615948247636536, Learning Rate - 0.0015625, magnitude of gradient - 0.08051155918981089\n",
      "Step - 5237, Loss - 0.2281933899897841, Learning Rate - 0.0015625, magnitude of gradient - 0.06527381001387184\n",
      "Step - 5238, Loss - 0.33500847527335825, Learning Rate - 0.0015625, magnitude of gradient - 0.051640015916324625\n",
      "Step - 5239, Loss - 0.34054270697815214, Learning Rate - 0.0015625, magnitude of gradient - 0.07524637638391095\n",
      "Step - 5240, Loss - 0.3300852582783933, Learning Rate - 0.0015625, magnitude of gradient - 0.04982568379868734\n",
      "Step - 5241, Loss - 0.32505854110383237, Learning Rate - 0.0015625, magnitude of gradient - 0.08707220809359831\n",
      "Step - 5242, Loss - 0.31134447601209736, Learning Rate - 0.0015625, magnitude of gradient - 0.060784393939656074\n",
      "Step - 5243, Loss - 0.3172433935407566, Learning Rate - 0.0015625, magnitude of gradient - 0.05337127755422603\n",
      "Step - 5244, Loss - 0.3224657843151276, Learning Rate - 0.0015625, magnitude of gradient - 0.042932968096229596\n",
      "Step - 5245, Loss - 0.3244416470375301, Learning Rate - 0.0015625, magnitude of gradient - 0.01773433371433274\n",
      "Step - 5246, Loss - 0.3325872943142266, Learning Rate - 0.0015625, magnitude of gradient - 0.05129836165782916\n",
      "Step - 5247, Loss - 0.28398681347172794, Learning Rate - 0.0015625, magnitude of gradient - 0.02024273818126569\n",
      "Step - 5248, Loss - 0.3128900877694412, Learning Rate - 0.0015625, magnitude of gradient - 0.055295715036326805\n",
      "Step - 5249, Loss - 0.2838131266894532, Learning Rate - 0.0015625, magnitude of gradient - 0.07909675495940957\n",
      "Step - 5250, Loss - 0.3405848741617392, Learning Rate - 0.0015625, magnitude of gradient - 0.011477980373387764\n",
      "Step - 5251, Loss - 0.3769409164564501, Learning Rate - 0.0015625, magnitude of gradient - 0.05012715520517214\n",
      "Step - 5252, Loss - 0.3286234856176282, Learning Rate - 0.0015625, magnitude of gradient - 0.05707390835446596\n",
      "Step - 5253, Loss - 0.28985649636243, Learning Rate - 0.0015625, magnitude of gradient - 0.0677573558372971\n",
      "Step - 5254, Loss - 0.32000351645947644, Learning Rate - 0.0015625, magnitude of gradient - 0.0815082328774182\n",
      "Step - 5255, Loss - 0.3337344336114021, Learning Rate - 0.0015625, magnitude of gradient - 0.032036105017974055\n",
      "Step - 5256, Loss - 0.32637429215357683, Learning Rate - 0.0015625, magnitude of gradient - 0.036044565866913944\n",
      "Step - 5257, Loss - 0.33554128640581143, Learning Rate - 0.0015625, magnitude of gradient - 0.037525200380142815\n",
      "Step - 5258, Loss - 0.35629375825175463, Learning Rate - 0.0015625, magnitude of gradient - 0.05361881306357147\n",
      "Step - 5259, Loss - 0.34532211411837216, Learning Rate - 0.0015625, magnitude of gradient - 0.02176681528818634\n",
      "Step - 5260, Loss - 0.39015702129628077, Learning Rate - 0.0015625, magnitude of gradient - 0.0375138871830599\n",
      "Step - 5261, Loss - 0.2963287026923197, Learning Rate - 0.0015625, magnitude of gradient - 0.04887830820633679\n",
      "Step - 5262, Loss - 0.357112932639916, Learning Rate - 0.0015625, magnitude of gradient - 0.03666363463969919\n",
      "Step - 5263, Loss - 0.2623733897413611, Learning Rate - 0.0015625, magnitude of gradient - 0.0812700637507157\n",
      "Step - 5264, Loss - 0.34785083065230166, Learning Rate - 0.0015625, magnitude of gradient - 0.05181781362075048\n",
      "Step - 5265, Loss - 0.3217720302667227, Learning Rate - 0.0015625, magnitude of gradient - 0.040762488250095534\n",
      "Step - 5266, Loss - 0.27090689063156403, Learning Rate - 0.0015625, magnitude of gradient - 0.020629395696130204\n",
      "Step - 5267, Loss - 0.279716759895813, Learning Rate - 0.0015625, magnitude of gradient - 0.09048928096101319\n",
      "Step - 5268, Loss - 0.3229931172633828, Learning Rate - 0.0015625, magnitude of gradient - 0.05108805798372851\n",
      "Step - 5269, Loss - 0.350778161503814, Learning Rate - 0.0015625, magnitude of gradient - 0.030820667837882548\n",
      "Step - 5270, Loss - 0.31894819975674604, Learning Rate - 0.0015625, magnitude of gradient - 0.033316962130915756\n",
      "Step - 5271, Loss - 0.2695808438746836, Learning Rate - 0.0015625, magnitude of gradient - 0.03374383920460332\n",
      "Step - 5272, Loss - 0.3113369210072335, Learning Rate - 0.0015625, magnitude of gradient - 0.04913921146991611\n",
      "Step - 5273, Loss - 0.3685222832810822, Learning Rate - 0.0015625, magnitude of gradient - 0.03917779353446705\n",
      "Step - 5274, Loss - 0.33481062244562787, Learning Rate - 0.0015625, magnitude of gradient - 0.0635708491191181\n",
      "Step - 5275, Loss - 0.3055077142935254, Learning Rate - 0.0015625, magnitude of gradient - 0.0820035109689296\n",
      "Step - 5276, Loss - 0.2683530506706871, Learning Rate - 0.0015625, magnitude of gradient - 0.03402864835510587\n",
      "Step - 5277, Loss - 0.3562785670007962, Learning Rate - 0.0015625, magnitude of gradient - 0.048653326412955465\n",
      "Step - 5278, Loss - 0.34554756803305814, Learning Rate - 0.0015625, magnitude of gradient - 0.05607468721662426\n",
      "Step - 5279, Loss - 0.3232572819836125, Learning Rate - 0.0015625, magnitude of gradient - 0.06747819686408195\n",
      "Step - 5280, Loss - 0.31106024595577025, Learning Rate - 0.0015625, magnitude of gradient - 0.024886732777388062\n",
      "Step - 5281, Loss - 0.30729915269787134, Learning Rate - 0.0015625, magnitude of gradient - 0.05088054557720895\n",
      "Step - 5282, Loss - 0.2971159852625854, Learning Rate - 0.0015625, magnitude of gradient - 0.06638456327763627\n",
      "Step - 5283, Loss - 0.36913206769479967, Learning Rate - 0.0015625, magnitude of gradient - 0.10507388988945474\n",
      "Step - 5284, Loss - 0.3349502247179951, Learning Rate - 0.0015625, magnitude of gradient - 0.051579486917702574\n",
      "Step - 5285, Loss - 0.33631466053007264, Learning Rate - 0.0015625, magnitude of gradient - 0.047625407434766856\n",
      "Step - 5286, Loss - 0.349641355626144, Learning Rate - 0.0015625, magnitude of gradient - 0.05509206005836018\n",
      "Step - 5287, Loss - 0.33238197348373194, Learning Rate - 0.0015625, magnitude of gradient - 0.07045427809149259\n",
      "Step - 5288, Loss - 0.2814256437974779, Learning Rate - 0.0015625, magnitude of gradient - 0.035851737795192615\n",
      "Step - 5289, Loss - 0.33599940015469953, Learning Rate - 0.0015625, magnitude of gradient - 0.09371419163388908\n",
      "Step - 5290, Loss - 0.2679689526268263, Learning Rate - 0.0015625, magnitude of gradient - 0.06747991953162308\n",
      "Step - 5291, Loss - 0.23861714520529587, Learning Rate - 0.0015625, magnitude of gradient - 0.04561337203577199\n",
      "Step - 5292, Loss - 0.35063528672996164, Learning Rate - 0.0015625, magnitude of gradient - 0.04409136190417918\n",
      "Step - 5293, Loss - 0.35121207579541985, Learning Rate - 0.0015625, magnitude of gradient - 0.07346900594508299\n",
      "Step - 5294, Loss - 0.29751364091428995, Learning Rate - 0.0015625, magnitude of gradient - 0.058063431513892254\n",
      "Step - 5295, Loss - 0.42656820805638307, Learning Rate - 0.0015625, magnitude of gradient - 0.021051495723801127\n",
      "Step - 5296, Loss - 0.3176865819159108, Learning Rate - 0.0015625, magnitude of gradient - 0.020759411291595163\n",
      "Step - 5297, Loss - 0.3781790716053308, Learning Rate - 0.0015625, magnitude of gradient - 0.04345857728503337\n",
      "Step - 5298, Loss - 0.3199337383153511, Learning Rate - 0.0015625, magnitude of gradient - 0.09775150656805451\n",
      "Step - 5299, Loss - 0.33713172544007775, Learning Rate - 0.0015625, magnitude of gradient - 0.08641928431034768\n",
      "Step - 5300, Loss - 0.21492411427265684, Learning Rate - 0.0015625, magnitude of gradient - 0.06966520314574756\n",
      "Step - 5301, Loss - 0.4230385982144387, Learning Rate - 0.0015625, magnitude of gradient - 0.11218807661163975\n",
      "Step - 5302, Loss - 0.3180619715863148, Learning Rate - 0.0015625, magnitude of gradient - 0.09884478083048348\n",
      "Step - 5303, Loss - 0.32678231905059746, Learning Rate - 0.0015625, magnitude of gradient - 0.07200034388533061\n",
      "Step - 5304, Loss - 0.3572448335563677, Learning Rate - 0.0015625, magnitude of gradient - 0.07788154282396001\n",
      "Step - 5305, Loss - 0.24698485080594806, Learning Rate - 0.0015625, magnitude of gradient - 0.014653380224176577\n",
      "Step - 5306, Loss - 0.3456363906622156, Learning Rate - 0.0015625, magnitude of gradient - 0.04747937808352238\n",
      "Step - 5307, Loss - 0.3817711724167187, Learning Rate - 0.0015625, magnitude of gradient - 0.055308141068672344\n",
      "Step - 5308, Loss - 0.35823049298062637, Learning Rate - 0.0015625, magnitude of gradient - 0.05156114431393687\n",
      "Step - 5309, Loss - 0.3773793837484249, Learning Rate - 0.0015625, magnitude of gradient - 0.008752417281385347\n",
      "Step - 5310, Loss - 0.33995917606446624, Learning Rate - 0.0015625, magnitude of gradient - 0.06360512727033811\n",
      "Step - 5311, Loss - 0.2671864247973246, Learning Rate - 0.0015625, magnitude of gradient - 0.042634363329979504\n",
      "Step - 5312, Loss - 0.34977773669225876, Learning Rate - 0.0015625, magnitude of gradient - 0.07156235191850537\n",
      "Step - 5313, Loss - 0.31310099780802325, Learning Rate - 0.0015625, magnitude of gradient - 0.03708003379609749\n",
      "Step - 5314, Loss - 0.3262342294339944, Learning Rate - 0.0015625, magnitude of gradient - 0.020211882047326615\n",
      "Step - 5315, Loss - 0.3188341817712542, Learning Rate - 0.0015625, magnitude of gradient - 0.05997450830641414\n",
      "Step - 5316, Loss - 0.3473655946056402, Learning Rate - 0.0015625, magnitude of gradient - 0.10941956502208734\n",
      "Step - 5317, Loss - 0.3091909019156157, Learning Rate - 0.0015625, magnitude of gradient - 0.048287349744564965\n",
      "Step - 5318, Loss - 0.30471151372404676, Learning Rate - 0.0015625, magnitude of gradient - 0.07753718638423909\n",
      "Step - 5319, Loss - 0.3339194721702308, Learning Rate - 0.0015625, magnitude of gradient - 0.059880624446555834\n",
      "Step - 5320, Loss - 0.2621915377498357, Learning Rate - 0.0015625, magnitude of gradient - 0.0790207880447405\n",
      "Step - 5321, Loss - 0.3294897990769954, Learning Rate - 0.0015625, magnitude of gradient - 0.07768034732510172\n",
      "Step - 5322, Loss - 0.3664258234863154, Learning Rate - 0.0015625, magnitude of gradient - 0.027467796504391923\n",
      "Step - 5323, Loss - 0.37826031951498373, Learning Rate - 0.0015625, magnitude of gradient - 0.014311534719907986\n",
      "Step - 5324, Loss - 0.36726393897887555, Learning Rate - 0.0015625, magnitude of gradient - 0.0282441840753327\n",
      "Step - 5325, Loss - 0.32841901708631055, Learning Rate - 0.0015625, magnitude of gradient - 0.0714966633227681\n",
      "Step - 5326, Loss - 0.33798767300930216, Learning Rate - 0.0015625, magnitude of gradient - 0.04035368543879382\n",
      "Step - 5327, Loss - 0.3874223561111066, Learning Rate - 0.0015625, magnitude of gradient - 0.02280705002578439\n",
      "Step - 5328, Loss - 0.2745315955341503, Learning Rate - 0.0015625, magnitude of gradient - 0.04312126830575005\n",
      "Step - 5329, Loss - 0.3521959351261168, Learning Rate - 0.0015625, magnitude of gradient - 0.03607058164443082\n",
      "Step - 5330, Loss - 0.3331818247494988, Learning Rate - 0.0015625, magnitude of gradient - 0.016147507795838643\n",
      "Step - 5331, Loss - 0.3396072375180389, Learning Rate - 0.0015625, magnitude of gradient - 0.03684158198400042\n",
      "Step - 5332, Loss - 0.32359737338065364, Learning Rate - 0.0015625, magnitude of gradient - 0.061480547837539215\n",
      "Step - 5333, Loss - 0.2712379771199296, Learning Rate - 0.0015625, magnitude of gradient - 0.049432212622882124\n",
      "Step - 5334, Loss - 0.24098349554194692, Learning Rate - 0.0015625, magnitude of gradient - 0.09338579832215635\n",
      "Step - 5335, Loss - 0.329120559407185, Learning Rate - 0.0015625, magnitude of gradient - 0.05442638124377497\n",
      "Step - 5336, Loss - 0.3436896857089116, Learning Rate - 0.0015625, magnitude of gradient - 0.0529909662510418\n",
      "Step - 5337, Loss - 0.26897832341104677, Learning Rate - 0.0015625, magnitude of gradient - 0.01091858812841188\n",
      "Step - 5338, Loss - 0.37578070989882323, Learning Rate - 0.0015625, magnitude of gradient - 0.08237488930463116\n",
      "Step - 5339, Loss - 0.3524327317579231, Learning Rate - 0.0015625, magnitude of gradient - 0.03972382223452869\n",
      "Step - 5340, Loss - 0.2677122779470959, Learning Rate - 0.0015625, magnitude of gradient - 0.06344201411394505\n",
      "Step - 5341, Loss - 0.33567489809012857, Learning Rate - 0.0015625, magnitude of gradient - 0.11342884034462479\n",
      "Step - 5342, Loss - 0.2763268723433547, Learning Rate - 0.0015625, magnitude of gradient - 0.0743807154571734\n",
      "Step - 5343, Loss - 0.3214297214236809, Learning Rate - 0.0015625, magnitude of gradient - 0.05952265867085741\n",
      "Step - 5344, Loss - 0.30502567950220116, Learning Rate - 0.0015625, magnitude of gradient - 0.05832141404258912\n",
      "Step - 5345, Loss - 0.3253050324058675, Learning Rate - 0.0015625, magnitude of gradient - 0.09319756444882824\n",
      "Step - 5346, Loss - 0.4222616536944116, Learning Rate - 0.0015625, magnitude of gradient - 0.0668150105743292\n",
      "Step - 5347, Loss - 0.3619944228522684, Learning Rate - 0.0015625, magnitude of gradient - 0.04630907481347894\n",
      "Step - 5348, Loss - 0.3523551253749253, Learning Rate - 0.0015625, magnitude of gradient - 0.032790926052041064\n",
      "Step - 5349, Loss - 0.33469267223199095, Learning Rate - 0.0015625, magnitude of gradient - 0.05637816964506734\n",
      "Step - 5350, Loss - 0.3623293891645777, Learning Rate - 0.0015625, magnitude of gradient - 0.060018255328220436\n",
      "Step - 5351, Loss - 0.28354106771505244, Learning Rate - 0.0015625, magnitude of gradient - 0.020658549807731123\n",
      "Step - 5352, Loss - 0.2586873642489804, Learning Rate - 0.0015625, magnitude of gradient - 0.13266592790903456\n",
      "Step - 5353, Loss - 0.329713671736346, Learning Rate - 0.0015625, magnitude of gradient - 0.009677774081737677\n",
      "Step - 5354, Loss - 0.31276808926564603, Learning Rate - 0.0015625, magnitude of gradient - 0.039042274943200284\n",
      "Step - 5355, Loss - 0.24597026718887616, Learning Rate - 0.0015625, magnitude of gradient - 0.06288113640505097\n",
      "Step - 5356, Loss - 0.2586241478582423, Learning Rate - 0.0015625, magnitude of gradient - 0.03541220608994508\n",
      "Step - 5357, Loss - 0.38456241483611814, Learning Rate - 0.0015625, magnitude of gradient - 0.10022047735630522\n",
      "Step - 5358, Loss - 0.31628185206894005, Learning Rate - 0.0015625, magnitude of gradient - 0.058785819136276264\n",
      "Step - 5359, Loss - 0.2765004897553093, Learning Rate - 0.0015625, magnitude of gradient - 0.030180773865440973\n",
      "Step - 5360, Loss - 0.30985324327480634, Learning Rate - 0.0015625, magnitude of gradient - 0.014916193916083847\n",
      "Step - 5361, Loss - 0.3244407253976533, Learning Rate - 0.0015625, magnitude of gradient - 0.053956845250338975\n",
      "Step - 5362, Loss - 0.30216541512116, Learning Rate - 0.0015625, magnitude of gradient - 0.05981694606358626\n",
      "Step - 5363, Loss - 0.3967703697440215, Learning Rate - 0.0015625, magnitude of gradient - 0.04827424591036989\n",
      "Step - 5364, Loss - 0.2424683352133017, Learning Rate - 0.0015625, magnitude of gradient - 0.05215789701341974\n",
      "Step - 5365, Loss - 0.3680360217476628, Learning Rate - 0.0015625, magnitude of gradient - 0.03301887189380073\n",
      "Step - 5366, Loss - 0.32591553837802, Learning Rate - 0.0015625, magnitude of gradient - 0.056951123306592424\n",
      "Step - 5367, Loss - 0.3528293351135717, Learning Rate - 0.0015625, magnitude of gradient - 0.06728866780433657\n",
      "Step - 5368, Loss - 0.27853459122053714, Learning Rate - 0.0015625, magnitude of gradient - 0.054549850603506235\n",
      "Step - 5369, Loss - 0.3072951543723759, Learning Rate - 0.0015625, magnitude of gradient - 0.05492010190037041\n",
      "Step - 5370, Loss - 0.33173696370128064, Learning Rate - 0.0015625, magnitude of gradient - 0.021948776038454874\n",
      "Step - 5371, Loss - 0.3470558350910173, Learning Rate - 0.0015625, magnitude of gradient - 0.04873230923172705\n",
      "Step - 5372, Loss - 0.24367340751711186, Learning Rate - 0.0015625, magnitude of gradient - 0.013420862748160404\n",
      "Step - 5373, Loss - 0.3569221424886965, Learning Rate - 0.0015625, magnitude of gradient - 0.07558597392513239\n",
      "Step - 5374, Loss - 0.24430333619344208, Learning Rate - 0.0015625, magnitude of gradient - 0.0035005207477887355\n",
      "Step - 5375, Loss - 0.3247389902459915, Learning Rate - 0.0015625, magnitude of gradient - 0.0638556017316401\n",
      "Step - 5376, Loss - 0.34295343229048736, Learning Rate - 0.0015625, magnitude of gradient - 0.02470731367420925\n",
      "Step - 5377, Loss - 0.26930809279334217, Learning Rate - 0.0015625, magnitude of gradient - 0.03757016243589638\n",
      "Step - 5378, Loss - 0.3013294501523336, Learning Rate - 0.0015625, magnitude of gradient - 0.02935326953858119\n",
      "Step - 5379, Loss - 0.37605159470251204, Learning Rate - 0.0015625, magnitude of gradient - 0.07697770179462629\n",
      "Step - 5380, Loss - 0.2550975385828747, Learning Rate - 0.0015625, magnitude of gradient - 0.07470446759979975\n",
      "Step - 5381, Loss - 0.2747917447258499, Learning Rate - 0.0015625, magnitude of gradient - 0.042072159714064666\n",
      "Step - 5382, Loss - 0.31817137771063114, Learning Rate - 0.0015625, magnitude of gradient - 0.09514738962670129\n",
      "Step - 5383, Loss - 0.3338029434335904, Learning Rate - 0.0015625, magnitude of gradient - 0.06286743562680544\n",
      "Step - 5384, Loss - 0.36319717777811433, Learning Rate - 0.0015625, magnitude of gradient - 0.08658960542414551\n",
      "Step - 5385, Loss - 0.20634148487317766, Learning Rate - 0.0015625, magnitude of gradient - 0.07542371543838652\n",
      "Step - 5386, Loss - 0.332729595195819, Learning Rate - 0.0015625, magnitude of gradient - 0.10084499374969755\n",
      "Step - 5387, Loss - 0.2842818230182812, Learning Rate - 0.0015625, magnitude of gradient - 0.03262266837982095\n",
      "Step - 5388, Loss - 0.2936532319957603, Learning Rate - 0.0015625, magnitude of gradient - 0.059075143430855015\n",
      "Step - 5389, Loss - 0.39281887454720543, Learning Rate - 0.0015625, magnitude of gradient - 0.08275908739035999\n",
      "Step - 5390, Loss - 0.33719856244834223, Learning Rate - 0.0015625, magnitude of gradient - 0.0401508052812486\n",
      "Step - 5391, Loss - 0.265690819465066, Learning Rate - 0.0015625, magnitude of gradient - 0.009423479107719408\n",
      "Step - 5392, Loss - 0.4169483081919345, Learning Rate - 0.0015625, magnitude of gradient - 0.006107365197249828\n",
      "Step - 5393, Loss - 0.34947609055052253, Learning Rate - 0.0015625, magnitude of gradient - 0.06367403436599565\n",
      "Step - 5394, Loss - 0.43082957924803766, Learning Rate - 0.0015625, magnitude of gradient - 0.05222351811383898\n",
      "Step - 5395, Loss - 0.3088604132017242, Learning Rate - 0.0015625, magnitude of gradient - 0.0648163330107063\n",
      "Step - 5396, Loss - 0.32191406560869645, Learning Rate - 0.0015625, magnitude of gradient - 0.09479177547450952\n",
      "Step - 5397, Loss - 0.36342196946753147, Learning Rate - 0.0015625, magnitude of gradient - 0.009676711523504324\n",
      "Step - 5398, Loss - 0.359663538538183, Learning Rate - 0.0015625, magnitude of gradient - 0.04968949779335956\n",
      "Step - 5399, Loss - 0.41416407250298676, Learning Rate - 0.0015625, magnitude of gradient - 0.0310849673055566\n",
      "Step - 5400, Loss - 0.32661033966069647, Learning Rate - 0.0015625, magnitude of gradient - 0.04082802758305885\n",
      "Step - 5401, Loss - 0.23960124019219478, Learning Rate - 0.0015625, magnitude of gradient - 0.045834104497868566\n",
      "Step - 5402, Loss - 0.40220565072027376, Learning Rate - 0.0015625, magnitude of gradient - 0.013169233541040541\n",
      "Step - 5403, Loss - 0.34894734786763315, Learning Rate - 0.0015625, magnitude of gradient - 0.04916814251219331\n",
      "Step - 5404, Loss - 0.3999986579317963, Learning Rate - 0.0015625, magnitude of gradient - 0.0865089288497958\n",
      "Step - 5405, Loss - 0.3344769671984993, Learning Rate - 0.0015625, magnitude of gradient - 0.08017927006075205\n",
      "Step - 5406, Loss - 0.2580288630367983, Learning Rate - 0.0015625, magnitude of gradient - 0.11137233441745618\n",
      "Step - 5407, Loss - 0.35477891813832263, Learning Rate - 0.0015625, magnitude of gradient - 0.030351680348664912\n",
      "Step - 5408, Loss - 0.3513943677918683, Learning Rate - 0.0015625, magnitude of gradient - 0.04770182213993148\n",
      "Step - 5409, Loss - 0.32748464386451115, Learning Rate - 0.0015625, magnitude of gradient - 0.03841443068094527\n",
      "Step - 5410, Loss - 0.37391937351920923, Learning Rate - 0.0015625, magnitude of gradient - 0.023569516157300328\n",
      "Step - 5411, Loss - 0.35315613324708967, Learning Rate - 0.0015625, magnitude of gradient - 0.002852801087655982\n",
      "Step - 5412, Loss - 0.2989344475122261, Learning Rate - 0.0015625, magnitude of gradient - 0.0901698259846079\n",
      "Step - 5413, Loss - 0.3374340332592895, Learning Rate - 0.0015625, magnitude of gradient - 0.053961458074390185\n",
      "Step - 5414, Loss - 0.3213657080531091, Learning Rate - 0.0015625, magnitude of gradient - 0.053845098812796525\n",
      "Step - 5415, Loss - 0.26836976634901555, Learning Rate - 0.0015625, magnitude of gradient - 0.02482305353332687\n",
      "Step - 5416, Loss - 0.33506515614177806, Learning Rate - 0.0015625, magnitude of gradient - 0.012496290451966752\n",
      "Step - 5417, Loss - 0.31254364162534853, Learning Rate - 0.0015625, magnitude of gradient - 0.09530619346056028\n",
      "Step - 5418, Loss - 0.32163983030638976, Learning Rate - 0.0015625, magnitude of gradient - 0.016674619853900282\n",
      "Step - 5419, Loss - 0.33450159692080217, Learning Rate - 0.0015625, magnitude of gradient - 0.03148562419514601\n",
      "Step - 5420, Loss - 0.3412703529105585, Learning Rate - 0.0015625, magnitude of gradient - 0.04646930921313596\n",
      "Step - 5421, Loss - 0.3370003039310289, Learning Rate - 0.0015625, magnitude of gradient - 0.08121096920327694\n",
      "Step - 5422, Loss - 0.37536239249502806, Learning Rate - 0.0015625, magnitude of gradient - 0.020178835941477277\n",
      "Step - 5423, Loss - 0.23742251175426715, Learning Rate - 0.0015625, magnitude of gradient - 0.057265467675018805\n",
      "Step - 5424, Loss - 0.39320498319824126, Learning Rate - 0.0015625, magnitude of gradient - 0.05460042573404969\n",
      "Step - 5425, Loss - 0.3077013578034674, Learning Rate - 0.0015625, magnitude of gradient - 0.027300454503494394\n",
      "Step - 5426, Loss - 0.26657861907651303, Learning Rate - 0.0015625, magnitude of gradient - 0.03933452544342538\n",
      "Step - 5427, Loss - 0.2827999160992345, Learning Rate - 0.0015625, magnitude of gradient - 0.06563694407085151\n",
      "Step - 5428, Loss - 0.34637573669735167, Learning Rate - 0.0015625, magnitude of gradient - 0.005825037549392882\n",
      "Step - 5429, Loss - 0.3897985154382749, Learning Rate - 0.0015625, magnitude of gradient - 0.07641917079994313\n",
      "Step - 5430, Loss - 0.32404806751281456, Learning Rate - 0.0015625, magnitude of gradient - 0.07353653348955233\n",
      "Step - 5431, Loss - 0.37465057720503736, Learning Rate - 0.0015625, magnitude of gradient - 0.060753514394034244\n",
      "Step - 5432, Loss - 0.286257072913656, Learning Rate - 0.0015625, magnitude of gradient - 0.043644715350926684\n",
      "Step - 5433, Loss - 0.30752361895709346, Learning Rate - 0.0015625, magnitude of gradient - 0.0884038163278601\n",
      "Step - 5434, Loss - 0.30317413230760853, Learning Rate - 0.0015625, magnitude of gradient - 0.03629582864379703\n",
      "Step - 5435, Loss - 0.35242540888964125, Learning Rate - 0.0015625, magnitude of gradient - 0.06306312631626093\n",
      "Step - 5436, Loss - 0.31515432571188984, Learning Rate - 0.0015625, magnitude of gradient - 0.05200086870413432\n",
      "Step - 5437, Loss - 0.3542460822397975, Learning Rate - 0.0015625, magnitude of gradient - 0.09392070128298724\n",
      "Step - 5438, Loss - 0.361326315997618, Learning Rate - 0.0015625, magnitude of gradient - 0.029067324785307276\n",
      "Step - 5439, Loss - 0.29176846258578293, Learning Rate - 0.0015625, magnitude of gradient - 0.06358304435671339\n",
      "Step - 5440, Loss - 0.4072498704113857, Learning Rate - 0.0015625, magnitude of gradient - 0.05225458687131987\n",
      "Step - 5441, Loss - 0.31516910681956223, Learning Rate - 0.0015625, magnitude of gradient - 0.047738047204030284\n",
      "Step - 5442, Loss - 0.3067625270732226, Learning Rate - 0.0015625, magnitude of gradient - 0.037532825616630384\n",
      "Step - 5443, Loss - 0.30910306217056605, Learning Rate - 0.0015625, magnitude of gradient - 0.06594460509996516\n",
      "Step - 5444, Loss - 0.3469361240241237, Learning Rate - 0.0015625, magnitude of gradient - 0.03869226563372654\n",
      "Step - 5445, Loss - 0.3459862883460058, Learning Rate - 0.0015625, magnitude of gradient - 0.045298652354627024\n",
      "Step - 5446, Loss - 0.30052264858704325, Learning Rate - 0.0015625, magnitude of gradient - 0.05771181425524329\n",
      "Step - 5447, Loss - 0.34097773265729664, Learning Rate - 0.0015625, magnitude of gradient - 0.024807665548149834\n",
      "Step - 5448, Loss - 0.2712955242851039, Learning Rate - 0.0015625, magnitude of gradient - 0.018396411077336597\n",
      "Step - 5449, Loss - 0.38817983059630506, Learning Rate - 0.0015625, magnitude of gradient - 0.06952341672749612\n",
      "Step - 5450, Loss - 0.3076021277115021, Learning Rate - 0.0015625, magnitude of gradient - 0.08122574046517884\n",
      "Step - 5451, Loss - 0.3405726731837068, Learning Rate - 0.0015625, magnitude of gradient - 0.055418666291844026\n",
      "Step - 5452, Loss - 0.3739590621214514, Learning Rate - 0.0015625, magnitude of gradient - 0.04112823372014601\n",
      "Step - 5453, Loss - 0.32079806358469387, Learning Rate - 0.0015625, magnitude of gradient - 0.09889613568089656\n",
      "Step - 5454, Loss - 0.33797648038548334, Learning Rate - 0.0015625, magnitude of gradient - 0.038956741133847055\n",
      "Step - 5455, Loss - 0.31732989874354117, Learning Rate - 0.0015625, magnitude of gradient - 0.036317605422331665\n",
      "Step - 5456, Loss - 0.3107823594561021, Learning Rate - 0.0015625, magnitude of gradient - 0.03463748795819246\n",
      "Step - 5457, Loss - 0.2917353792489179, Learning Rate - 0.0015625, magnitude of gradient - 0.011557964068210736\n",
      "Step - 5458, Loss - 0.26529146701829964, Learning Rate - 0.0015625, magnitude of gradient - 0.045760091438605516\n",
      "Step - 5459, Loss - 0.30644198765537595, Learning Rate - 0.0015625, magnitude of gradient - 0.03431825651991563\n",
      "Step - 5460, Loss - 0.29830991796825973, Learning Rate - 0.0015625, magnitude of gradient - 0.03185026919028529\n",
      "Step - 5461, Loss - 0.3437819458680733, Learning Rate - 0.0015625, magnitude of gradient - 0.08303044161821571\n",
      "Step - 5462, Loss - 0.369765424765401, Learning Rate - 0.0015625, magnitude of gradient - 0.030022903749379624\n",
      "Step - 5463, Loss - 0.3568937737664319, Learning Rate - 0.0015625, magnitude of gradient - 0.02958107167033052\n",
      "Step - 5464, Loss - 0.32721392727057064, Learning Rate - 0.0015625, magnitude of gradient - 0.03555390126254727\n",
      "Step - 5465, Loss - 0.2839021246167359, Learning Rate - 0.0015625, magnitude of gradient - 0.05778897269533704\n",
      "Step - 5466, Loss - 0.3020735569458742, Learning Rate - 0.0015625, magnitude of gradient - 0.026599996710808167\n",
      "Step - 5467, Loss - 0.327543873345775, Learning Rate - 0.0015625, magnitude of gradient - 0.040658273948796377\n",
      "Step - 5468, Loss - 0.36912019660145223, Learning Rate - 0.0015625, magnitude of gradient - 0.0036533302727786033\n",
      "Step - 5469, Loss - 0.3080131137804108, Learning Rate - 0.0015625, magnitude of gradient - 0.05391350657874901\n",
      "Step - 5470, Loss - 0.34329970503661444, Learning Rate - 0.0015625, magnitude of gradient - 0.08192920592622965\n",
      "Step - 5471, Loss - 0.3234675294185597, Learning Rate - 0.0015625, magnitude of gradient - 0.1174447638536614\n",
      "Step - 5472, Loss - 0.2682092191143724, Learning Rate - 0.0015625, magnitude of gradient - 0.02735181345738208\n",
      "Step - 5473, Loss - 0.3551358639290335, Learning Rate - 0.0015625, magnitude of gradient - 0.04993461626447525\n",
      "Step - 5474, Loss - 0.3292746095355042, Learning Rate - 0.0015625, magnitude of gradient - 0.059482881011528065\n",
      "Step - 5475, Loss - 0.33008242570481755, Learning Rate - 0.0015625, magnitude of gradient - 0.06395138599232124\n",
      "Step - 5476, Loss - 0.28682036850963166, Learning Rate - 0.0015625, magnitude of gradient - 0.06286203799259392\n",
      "Step - 5477, Loss - 0.33835456891765264, Learning Rate - 0.0015625, magnitude of gradient - 0.03579659756638172\n",
      "Step - 5478, Loss - 0.3342480771627253, Learning Rate - 0.0015625, magnitude of gradient - 0.043354584617604414\n",
      "Step - 5479, Loss - 0.3085722772925623, Learning Rate - 0.0015625, magnitude of gradient - 0.046072452615948674\n",
      "Step - 5480, Loss - 0.35636624113297105, Learning Rate - 0.0015625, magnitude of gradient - 0.1177263360570933\n",
      "Step - 5481, Loss - 0.40191127949719596, Learning Rate - 0.0015625, magnitude of gradient - 0.09846097949062622\n",
      "Step - 5482, Loss - 0.37248633192868974, Learning Rate - 0.0015625, magnitude of gradient - 0.09434361193661299\n",
      "Step - 5483, Loss - 0.3114940468369557, Learning Rate - 0.0015625, magnitude of gradient - 0.054913486453183814\n",
      "Step - 5484, Loss - 0.27045159178507694, Learning Rate - 0.0015625, magnitude of gradient - 0.037670158236051125\n",
      "Step - 5485, Loss - 0.30050173796631285, Learning Rate - 0.0015625, magnitude of gradient - 0.05546213691924185\n",
      "Step - 5486, Loss - 0.24174341209993183, Learning Rate - 0.0015625, magnitude of gradient - 0.06538614909029444\n",
      "Step - 5487, Loss - 0.2486036147074328, Learning Rate - 0.0015625, magnitude of gradient - 0.01136990938314086\n",
      "Step - 5488, Loss - 0.27607379163897444, Learning Rate - 0.0015625, magnitude of gradient - 0.037753665469565593\n",
      "Step - 5489, Loss - 0.3571126313401749, Learning Rate - 0.0015625, magnitude of gradient - 0.06912942518538476\n",
      "Step - 5490, Loss - 0.33539808923046427, Learning Rate - 0.0015625, magnitude of gradient - 0.018515376481183395\n",
      "Step - 5491, Loss - 0.2975670719198955, Learning Rate - 0.0015625, magnitude of gradient - 0.008908019672746592\n",
      "Step - 5492, Loss - 0.311839619924512, Learning Rate - 0.0015625, magnitude of gradient - 0.054070208423814395\n",
      "Step - 5493, Loss - 0.299572149674475, Learning Rate - 0.0015625, magnitude of gradient - 0.08713822245537135\n",
      "Step - 5494, Loss - 0.27246895299294993, Learning Rate - 0.0015625, magnitude of gradient - 0.04075226446548693\n",
      "Step - 5495, Loss - 0.2953401157603272, Learning Rate - 0.0015625, magnitude of gradient - 0.07867228650255197\n",
      "Step - 5496, Loss - 0.39864488306651563, Learning Rate - 0.0015625, magnitude of gradient - 0.06083662471600388\n",
      "Step - 5497, Loss - 0.3338072254451153, Learning Rate - 0.0015625, magnitude of gradient - 0.05737974543617411\n",
      "Step - 5498, Loss - 0.3069577431560756, Learning Rate - 0.0015625, magnitude of gradient - 0.09059464642271631\n",
      "Step - 5499, Loss - 0.32752003384137823, Learning Rate - 0.0015625, magnitude of gradient - 0.042763150737860314\n",
      "Step - 5500, Loss - 0.31031258000608075, Learning Rate - 0.0015625, magnitude of gradient - 0.04912411260121134\n",
      "Step - 5501, Loss - 0.30068063745513696, Learning Rate - 0.0015625, magnitude of gradient - 0.09278680949675126\n",
      "Step - 5502, Loss - 0.31139318424937723, Learning Rate - 0.0015625, magnitude of gradient - 0.05397140884076767\n",
      "Step - 5503, Loss - 0.3153731702835607, Learning Rate - 0.0015625, magnitude of gradient - 0.0365069137291305\n",
      "Step - 5504, Loss - 0.24953691177382498, Learning Rate - 0.0015625, magnitude of gradient - 0.04555225499671701\n",
      "Step - 5505, Loss - 0.26432038235427485, Learning Rate - 0.0015625, magnitude of gradient - 0.08280362179636999\n",
      "Step - 5506, Loss - 0.3707171528001887, Learning Rate - 0.0015625, magnitude of gradient - 0.07013846419949181\n",
      "Step - 5507, Loss - 0.31834359436182175, Learning Rate - 0.0015625, magnitude of gradient - 0.05153416006821553\n",
      "Step - 5508, Loss - 0.26747200640962054, Learning Rate - 0.0015625, magnitude of gradient - 0.10042645459939019\n",
      "Step - 5509, Loss - 0.3807648291128102, Learning Rate - 0.0015625, magnitude of gradient - 0.04077155528859179\n",
      "Step - 5510, Loss - 0.3012113964434801, Learning Rate - 0.0015625, magnitude of gradient - 0.033631090728494045\n",
      "Step - 5511, Loss - 0.3152189615517089, Learning Rate - 0.0015625, magnitude of gradient - 0.029621603674072267\n",
      "Step - 5512, Loss - 0.355974794456548, Learning Rate - 0.0015625, magnitude of gradient - 0.05311484619901937\n",
      "Step - 5513, Loss - 0.2898725202873167, Learning Rate - 0.0015625, magnitude of gradient - 0.02498736916882824\n",
      "Step - 5514, Loss - 0.39091718418265414, Learning Rate - 0.0015625, magnitude of gradient - 0.024835130670800357\n",
      "Step - 5515, Loss - 0.3148140667248587, Learning Rate - 0.0015625, magnitude of gradient - 0.018205162416774237\n",
      "Step - 5516, Loss - 0.2952838390852979, Learning Rate - 0.0015625, magnitude of gradient - 0.006893625408055527\n",
      "Step - 5517, Loss - 0.35470655629783765, Learning Rate - 0.0015625, magnitude of gradient - 0.09746008172258343\n",
      "Step - 5518, Loss - 0.3664564045519549, Learning Rate - 0.0015625, magnitude of gradient - 0.09752697163997939\n",
      "Step - 5519, Loss - 0.2713421206741864, Learning Rate - 0.0015625, magnitude of gradient - 0.0547801759694051\n",
      "Step - 5520, Loss - 0.3145363491020836, Learning Rate - 0.0015625, magnitude of gradient - 0.0492807233527026\n",
      "Step - 5521, Loss - 0.31239734798874874, Learning Rate - 0.0015625, magnitude of gradient - 0.08321677909060182\n",
      "Step - 5522, Loss - 0.3114322528879009, Learning Rate - 0.0015625, magnitude of gradient - 0.07319172202077524\n",
      "Step - 5523, Loss - 0.42530684891747494, Learning Rate - 0.0015625, magnitude of gradient - 0.014814499282942956\n",
      "Step - 5524, Loss - 0.29420894604872894, Learning Rate - 0.0015625, magnitude of gradient - 0.06330713157472764\n",
      "Step - 5525, Loss - 0.36906095542320666, Learning Rate - 0.0015625, magnitude of gradient - 0.030442858230604834\n",
      "Step - 5526, Loss - 0.3180913169145845, Learning Rate - 0.0015625, magnitude of gradient - 0.04694159648321855\n",
      "Step - 5527, Loss - 0.3014776243188075, Learning Rate - 0.0015625, magnitude of gradient - 0.02250171718886788\n",
      "Step - 5528, Loss - 0.3933981885167645, Learning Rate - 0.0015625, magnitude of gradient - 0.018888179242661503\n",
      "Step - 5529, Loss - 0.32497784093717164, Learning Rate - 0.0015625, magnitude of gradient - 0.07606881535480928\n",
      "Step - 5530, Loss - 0.3362944692612606, Learning Rate - 0.0015625, magnitude of gradient - 0.09863155526944055\n",
      "Step - 5531, Loss - 0.3367930909810028, Learning Rate - 0.0015625, magnitude of gradient - 0.0422328461190931\n",
      "Step - 5532, Loss - 0.39810540261551264, Learning Rate - 0.0015625, magnitude of gradient - 0.053138489451472044\n",
      "Step - 5533, Loss - 0.3278060110366424, Learning Rate - 0.0015625, magnitude of gradient - 0.03753452706152302\n",
      "Step - 5534, Loss - 0.391278097283813, Learning Rate - 0.0015625, magnitude of gradient - 0.007999243018919576\n",
      "Step - 5535, Loss - 0.41468028709011795, Learning Rate - 0.0015625, magnitude of gradient - 0.05847331698697797\n",
      "Step - 5536, Loss - 0.2449746856842609, Learning Rate - 0.0015625, magnitude of gradient - 0.08298911484875454\n",
      "Step - 5537, Loss - 0.4269428644097358, Learning Rate - 0.0015625, magnitude of gradient - 0.05499738058862568\n",
      "Step - 5538, Loss - 0.30285216853541586, Learning Rate - 0.0015625, magnitude of gradient - 0.03693527910162069\n",
      "Step - 5539, Loss - 0.36061319010959414, Learning Rate - 0.0015625, magnitude of gradient - 0.013964558745154463\n",
      "Step - 5540, Loss - 0.403335246465507, Learning Rate - 0.0015625, magnitude of gradient - 0.06437153843994153\n",
      "Step - 5541, Loss - 0.4085457614504298, Learning Rate - 0.0015625, magnitude of gradient - 0.09223695981612073\n",
      "Step - 5542, Loss - 0.30888959920664344, Learning Rate - 0.0015625, magnitude of gradient - 0.015406612765899837\n",
      "Step - 5543, Loss - 0.35001258731900886, Learning Rate - 0.0015625, magnitude of gradient - 0.033376927632541166\n",
      "Step - 5544, Loss - 0.29744801206461047, Learning Rate - 0.0015625, magnitude of gradient - 0.07647177173506867\n",
      "Step - 5545, Loss - 0.38322377668058294, Learning Rate - 0.0015625, magnitude of gradient - 0.02885156888503188\n",
      "Step - 5546, Loss - 0.416289130339508, Learning Rate - 0.0015625, magnitude of gradient - 0.02290199066687588\n",
      "Step - 5547, Loss - 0.30870863523623254, Learning Rate - 0.0015625, magnitude of gradient - 0.07363333940882671\n",
      "Step - 5548, Loss - 0.31590519608222645, Learning Rate - 0.0015625, magnitude of gradient - 0.09668756324974873\n",
      "Step - 5549, Loss - 0.24406025849415838, Learning Rate - 0.0015625, magnitude of gradient - 0.041838143156270234\n",
      "Step - 5550, Loss - 0.322445008739643, Learning Rate - 0.0015625, magnitude of gradient - 0.03862079081324788\n",
      "Step - 5551, Loss - 0.317170310980868, Learning Rate - 0.0015625, magnitude of gradient - 0.04514081051396169\n",
      "Step - 5552, Loss - 0.31281817998565553, Learning Rate - 0.0015625, magnitude of gradient - 0.047455650248310666\n",
      "Step - 5553, Loss - 0.2733048544771081, Learning Rate - 0.0015625, magnitude of gradient - 0.0981974601981116\n",
      "Step - 5554, Loss - 0.3973969271542146, Learning Rate - 0.0015625, magnitude of gradient - 0.1053516140889399\n",
      "Step - 5555, Loss - 0.3253117706236487, Learning Rate - 0.0015625, magnitude of gradient - 0.043857289666112416\n",
      "Step - 5556, Loss - 0.2699474585247723, Learning Rate - 0.0015625, magnitude of gradient - 0.043484055989710814\n",
      "Step - 5557, Loss - 0.428732903990934, Learning Rate - 0.0015625, magnitude of gradient - 0.030870502800379618\n",
      "Step - 5558, Loss - 0.3318265888085367, Learning Rate - 0.0015625, magnitude of gradient - 0.01927020410226809\n",
      "Step - 5559, Loss - 0.3144555031484002, Learning Rate - 0.0015625, magnitude of gradient - 0.040137500791414454\n",
      "Step - 5560, Loss - 0.24457696580814026, Learning Rate - 0.0015625, magnitude of gradient - 0.06133126912199717\n",
      "Step - 5561, Loss - 0.34969664609821927, Learning Rate - 0.0015625, magnitude of gradient - 0.046638488268090965\n",
      "Step - 5562, Loss - 0.365474822926155, Learning Rate - 0.0015625, magnitude of gradient - 0.02672808935512021\n",
      "Step - 5563, Loss - 0.3601567355061627, Learning Rate - 0.0015625, magnitude of gradient - 0.08740017338191296\n",
      "Step - 5564, Loss - 0.32235609689254324, Learning Rate - 0.0015625, magnitude of gradient - 0.09516234067671091\n",
      "Step - 5565, Loss - 0.3038420716931445, Learning Rate - 0.0015625, magnitude of gradient - 0.05116513012817866\n",
      "Step - 5566, Loss - 0.4182693686269913, Learning Rate - 0.0015625, magnitude of gradient - 0.012811292713358225\n",
      "Step - 5567, Loss - 0.3118749584753721, Learning Rate - 0.0015625, magnitude of gradient - 0.059420215679406405\n",
      "Step - 5568, Loss - 0.2339027719217142, Learning Rate - 0.0015625, magnitude of gradient - 0.04479382301579215\n",
      "Step - 5569, Loss - 0.30804731281563497, Learning Rate - 0.0015625, magnitude of gradient - 0.06609713904014781\n",
      "Step - 5570, Loss - 0.36431757617946836, Learning Rate - 0.0015625, magnitude of gradient - 0.06771882931765615\n",
      "Step - 5571, Loss - 0.3307957836612796, Learning Rate - 0.0015625, magnitude of gradient - 0.05428156865927486\n",
      "Step - 5572, Loss - 0.41919760347027146, Learning Rate - 0.0015625, magnitude of gradient - 0.05958720525467291\n",
      "Step - 5573, Loss - 0.33066397944315773, Learning Rate - 0.0015625, magnitude of gradient - 0.0826165484021548\n",
      "Step - 5574, Loss - 0.3337257011146879, Learning Rate - 0.0015625, magnitude of gradient - 0.06982185811107773\n",
      "Step - 5575, Loss - 0.3625751570616889, Learning Rate - 0.0015625, magnitude of gradient - 0.06370961795351573\n",
      "Step - 5576, Loss - 0.28307842923533666, Learning Rate - 0.0015625, magnitude of gradient - 0.052014456624495775\n",
      "Step - 5577, Loss - 0.31140171313709275, Learning Rate - 0.0015625, magnitude of gradient - 0.04341290005427697\n",
      "Step - 5578, Loss - 0.30098887592710416, Learning Rate - 0.0015625, magnitude of gradient - 0.031860318732646546\n",
      "Step - 5579, Loss - 0.35089789636085655, Learning Rate - 0.0015625, magnitude of gradient - 0.053302739001111765\n",
      "Step - 5580, Loss - 0.19425694688456685, Learning Rate - 0.0015625, magnitude of gradient - 0.038993175098697895\n",
      "Step - 5581, Loss - 0.26335497556250376, Learning Rate - 0.0015625, magnitude of gradient - 0.07288928570343875\n",
      "Step - 5582, Loss - 0.29656464653107323, Learning Rate - 0.0015625, magnitude of gradient - 0.014467609415346343\n",
      "Step - 5583, Loss - 0.36359410234789485, Learning Rate - 0.0015625, magnitude of gradient - 0.022683855058298147\n",
      "Step - 5584, Loss - 0.3493961940583712, Learning Rate - 0.0015625, magnitude of gradient - 0.08412480675245955\n",
      "Step - 5585, Loss - 0.3023098484580675, Learning Rate - 0.0015625, magnitude of gradient - 0.05366694143934136\n",
      "Step - 5586, Loss - 0.37092636783598393, Learning Rate - 0.0015625, magnitude of gradient - 0.06901821830444028\n",
      "Step - 5587, Loss - 0.30060690208008023, Learning Rate - 0.0015625, magnitude of gradient - 0.014804388242002756\n",
      "Step - 5588, Loss - 0.3624272381899922, Learning Rate - 0.0015625, magnitude of gradient - 0.0517825118076853\n",
      "Step - 5589, Loss - 0.2990120001112707, Learning Rate - 0.0015625, magnitude of gradient - 0.018910040584093502\n",
      "Step - 5590, Loss - 0.2852154002634157, Learning Rate - 0.0015625, magnitude of gradient - 0.049971140931062225\n",
      "Step - 5591, Loss - 0.33102914884945267, Learning Rate - 0.0015625, magnitude of gradient - 0.06530607518809128\n",
      "Step - 5592, Loss - 0.40528452115843683, Learning Rate - 0.0015625, magnitude of gradient - 0.015867761465534\n",
      "Step - 5593, Loss - 0.29959384033837466, Learning Rate - 0.0015625, magnitude of gradient - 0.05200884567770078\n",
      "Step - 5594, Loss - 0.243237748695813, Learning Rate - 0.0015625, magnitude of gradient - 0.023228876524178675\n",
      "Step - 5595, Loss - 0.32600214430908225, Learning Rate - 0.0015625, magnitude of gradient - 0.032423227211437936\n",
      "Step - 5596, Loss - 0.2994443877755946, Learning Rate - 0.0015625, magnitude of gradient - 0.03530737334212456\n",
      "Step - 5597, Loss - 0.32701079675979383, Learning Rate - 0.0015625, magnitude of gradient - 0.05630348741083139\n",
      "Step - 5598, Loss - 0.43095037195764035, Learning Rate - 0.0015625, magnitude of gradient - 0.025338599556045463\n",
      "Step - 5599, Loss - 0.3219270353765261, Learning Rate - 0.0015625, magnitude of gradient - 0.09362830570310936\n",
      "Step - 5600, Loss - 0.24311518630885248, Learning Rate - 0.0015625, magnitude of gradient - 0.01654248673806869\n",
      "Step - 5601, Loss - 0.32801242041427864, Learning Rate - 0.0015625, magnitude of gradient - 0.043809907605328546\n",
      "Step - 5602, Loss - 0.3300314810026783, Learning Rate - 0.0015625, magnitude of gradient - 0.0377314112729988\n",
      "Step - 5603, Loss - 0.3839872109658997, Learning Rate - 0.0015625, magnitude of gradient - 0.03458179611181579\n",
      "Step - 5604, Loss - 0.3301441298280214, Learning Rate - 0.0015625, magnitude of gradient - 0.07259683504376238\n",
      "Step - 5605, Loss - 0.39499423492084296, Learning Rate - 0.0015625, magnitude of gradient - 0.09978230135227117\n",
      "Step - 5606, Loss - 0.3381405550815245, Learning Rate - 0.0015625, magnitude of gradient - 0.009188641287862882\n",
      "Step - 5607, Loss - 0.3013659768503797, Learning Rate - 0.0015625, magnitude of gradient - 0.05719150023085817\n",
      "Step - 5608, Loss - 0.3412542169876754, Learning Rate - 0.0015625, magnitude of gradient - 0.08117351654022707\n",
      "Step - 5609, Loss - 0.30088509854066253, Learning Rate - 0.0015625, magnitude of gradient - 0.08425793289232532\n",
      "Step - 5610, Loss - 0.38069561533006596, Learning Rate - 0.0015625, magnitude of gradient - 0.038543508032335076\n",
      "Step - 5611, Loss - 0.2955599329045351, Learning Rate - 0.0015625, magnitude of gradient - 0.020614473432601137\n",
      "Step - 5612, Loss - 0.2846183918666076, Learning Rate - 0.0015625, magnitude of gradient - 0.04973049119849238\n",
      "Step - 5613, Loss - 0.27981717806059225, Learning Rate - 0.0015625, magnitude of gradient - 0.08090118377484494\n",
      "Step - 5614, Loss - 0.32575642203529054, Learning Rate - 0.0015625, magnitude of gradient - 0.012124644880138825\n",
      "Step - 5615, Loss - 0.3326863505558882, Learning Rate - 0.0015625, magnitude of gradient - 0.0478213342554563\n",
      "Step - 5616, Loss - 0.2913235822645647, Learning Rate - 0.0015625, magnitude of gradient - 0.08348052896936513\n",
      "Step - 5617, Loss - 0.3276667748538513, Learning Rate - 0.0015625, magnitude of gradient - 0.043191927055747086\n",
      "Step - 5618, Loss - 0.4037010735811043, Learning Rate - 0.0015625, magnitude of gradient - 0.05862617486275852\n",
      "Step - 5619, Loss - 0.38689919253831156, Learning Rate - 0.0015625, magnitude of gradient - 0.0157959705520759\n",
      "Step - 5620, Loss - 0.3075634306125259, Learning Rate - 0.0015625, magnitude of gradient - 0.04269924554926281\n",
      "Step - 5621, Loss - 0.29991908267354694, Learning Rate - 0.0015625, magnitude of gradient - 0.0366068294849691\n",
      "Step - 5622, Loss - 0.3990375442461809, Learning Rate - 0.0015625, magnitude of gradient - 0.019799177567569284\n",
      "Step - 5623, Loss - 0.36349175048997334, Learning Rate - 0.0015625, magnitude of gradient - 0.07523648897830987\n",
      "Step - 5624, Loss - 0.31005159343836747, Learning Rate - 0.0015625, magnitude of gradient - 0.07728135265975417\n",
      "Step - 5625, Loss - 0.4129008270097541, Learning Rate - 0.0015625, magnitude of gradient - 0.04709711349241453\n",
      "Step - 5626, Loss - 0.3168484949616596, Learning Rate - 0.0015625, magnitude of gradient - 0.07204183359732835\n",
      "Step - 5627, Loss - 0.32760125028509474, Learning Rate - 0.0015625, magnitude of gradient - 0.040037797077584425\n",
      "Step - 5628, Loss - 0.296413963065437, Learning Rate - 0.0015625, magnitude of gradient - 0.02681725846829417\n",
      "Step - 5629, Loss - 0.3759755552292132, Learning Rate - 0.0015625, magnitude of gradient - 0.05695327639691757\n",
      "Step - 5630, Loss - 0.32290209276033066, Learning Rate - 0.0015625, magnitude of gradient - 0.07890058851902425\n",
      "Step - 5631, Loss - 0.2937775949069966, Learning Rate - 0.0015625, magnitude of gradient - 0.052712991153590065\n",
      "Step - 5632, Loss - 0.2889006377073963, Learning Rate - 0.0015625, magnitude of gradient - 0.06600888635452505\n",
      "Step - 5633, Loss - 0.3100839107281281, Learning Rate - 0.0015625, magnitude of gradient - 0.06237257148956825\n",
      "Step - 5634, Loss - 0.3311849149665989, Learning Rate - 0.0015625, magnitude of gradient - 0.053180058051952396\n",
      "Step - 5635, Loss - 0.4677647760210024, Learning Rate - 0.0015625, magnitude of gradient - 0.1314205094400765\n",
      "Step - 5636, Loss - 0.376158660317322, Learning Rate - 0.0015625, magnitude of gradient - 0.03833837483570698\n",
      "Step - 5637, Loss - 0.3517325456614271, Learning Rate - 0.0015625, magnitude of gradient - 0.09960524039484749\n",
      "Step - 5638, Loss - 0.32000090683213966, Learning Rate - 0.0015625, magnitude of gradient - 0.03195049757644189\n",
      "Step - 5639, Loss - 0.3555890048385962, Learning Rate - 0.0015625, magnitude of gradient - 0.02830519376407878\n",
      "Step - 5640, Loss - 0.2906064546402378, Learning Rate - 0.0015625, magnitude of gradient - 0.03418138990983275\n",
      "Step - 5641, Loss - 0.2649661631184761, Learning Rate - 0.0015625, magnitude of gradient - 0.039951698580953966\n",
      "Step - 5642, Loss - 0.24878611873159134, Learning Rate - 0.0015625, magnitude of gradient - 0.049832151599443736\n",
      "Step - 5643, Loss - 0.2967225374204561, Learning Rate - 0.0015625, magnitude of gradient - 0.050205027727524595\n",
      "Step - 5644, Loss - 0.3402192495177865, Learning Rate - 0.0015625, magnitude of gradient - 0.05306126582612372\n",
      "Step - 5645, Loss - 0.3115556029578691, Learning Rate - 0.0015625, magnitude of gradient - 0.05675323872688808\n",
      "Step - 5646, Loss - 0.2647803627176475, Learning Rate - 0.0015625, magnitude of gradient - 0.028375402013114007\n",
      "Step - 5647, Loss - 0.3720173537589058, Learning Rate - 0.0015625, magnitude of gradient - 0.020349001538036388\n",
      "Step - 5648, Loss - 0.3319890946809521, Learning Rate - 0.0015625, magnitude of gradient - 0.07994123195945613\n",
      "Step - 5649, Loss - 0.38989161321939003, Learning Rate - 0.0015625, magnitude of gradient - 0.03431961901682349\n",
      "Step - 5650, Loss - 0.3030178848733268, Learning Rate - 0.0015625, magnitude of gradient - 0.12139621293919664\n",
      "Step - 5651, Loss - 0.3617219002599234, Learning Rate - 0.0015625, magnitude of gradient - 0.09490482124630185\n",
      "Step - 5652, Loss - 0.264932222472572, Learning Rate - 0.0015625, magnitude of gradient - 0.059251058038460494\n",
      "Step - 5653, Loss - 0.3836892419164281, Learning Rate - 0.0015625, magnitude of gradient - 0.05131995496890341\n",
      "Step - 5654, Loss - 0.23213475791681515, Learning Rate - 0.0015625, magnitude of gradient - 0.03231046849327758\n",
      "Step - 5655, Loss - 0.38367164098859813, Learning Rate - 0.0015625, magnitude of gradient - 0.05159478040926304\n",
      "Step - 5656, Loss - 0.26710654529806316, Learning Rate - 0.0015625, magnitude of gradient - 0.029774749907037296\n",
      "Step - 5657, Loss - 0.32428090547351796, Learning Rate - 0.0015625, magnitude of gradient - 0.06073759157009395\n",
      "Step - 5658, Loss - 0.23900452503427264, Learning Rate - 0.0015625, magnitude of gradient - 0.03458796007273822\n",
      "Step - 5659, Loss - 0.2686009748217483, Learning Rate - 0.0015625, magnitude of gradient - 0.04145363361426516\n",
      "Step - 5660, Loss - 0.3311571105093441, Learning Rate - 0.0015625, magnitude of gradient - 0.08201249854705933\n",
      "Step - 5661, Loss - 0.2626030394844914, Learning Rate - 0.0015625, magnitude of gradient - 0.07862869647563112\n",
      "Step - 5662, Loss - 0.27427737670479035, Learning Rate - 0.0015625, magnitude of gradient - 0.06720865778152056\n",
      "Step - 5663, Loss - 0.33965633498653963, Learning Rate - 0.0015625, magnitude of gradient - 0.06731438538817812\n",
      "Step - 5664, Loss - 0.33851690996932776, Learning Rate - 0.0015625, magnitude of gradient - 0.03172018470421762\n",
      "Step - 5665, Loss - 0.3117346746667301, Learning Rate - 0.0015625, magnitude of gradient - 0.06167868688280271\n",
      "Step - 5666, Loss - 0.302910417448036, Learning Rate - 0.0015625, magnitude of gradient - 0.03895569766282524\n",
      "Step - 5667, Loss - 0.3488686800295673, Learning Rate - 0.0015625, magnitude of gradient - 0.01378653486842113\n",
      "Step - 5668, Loss - 0.438636122701035, Learning Rate - 0.0015625, magnitude of gradient - 0.13860791140484532\n",
      "Step - 5669, Loss - 0.26079284965389454, Learning Rate - 0.0015625, magnitude of gradient - 0.04079244517548809\n",
      "Step - 5670, Loss - 0.26792701065716207, Learning Rate - 0.0015625, magnitude of gradient - 0.050503064058477556\n",
      "Step - 5671, Loss - 0.27663644445862884, Learning Rate - 0.0015625, magnitude of gradient - 0.059219648326553014\n",
      "Step - 5672, Loss - 0.3454693353290291, Learning Rate - 0.0015625, magnitude of gradient - 0.015860240098863297\n",
      "Step - 5673, Loss - 0.2875139447718729, Learning Rate - 0.0015625, magnitude of gradient - 0.05735822304949399\n",
      "Step - 5674, Loss - 0.28870388793466123, Learning Rate - 0.0015625, magnitude of gradient - 0.05946253518901474\n",
      "Step - 5675, Loss - 0.3366953733295299, Learning Rate - 0.0015625, magnitude of gradient - 0.06617266293546385\n",
      "Step - 5676, Loss - 0.2979351622552559, Learning Rate - 0.0015625, magnitude of gradient - 0.025748178288097744\n",
      "Step - 5677, Loss - 0.36741917110980643, Learning Rate - 0.0015625, magnitude of gradient - 0.04701594711770696\n",
      "Step - 5678, Loss - 0.26386506929149767, Learning Rate - 0.0015625, magnitude of gradient - 0.03190787518960113\n",
      "Step - 5679, Loss - 0.27045713653880965, Learning Rate - 0.0015625, magnitude of gradient - 0.06927282224768547\n",
      "Step - 5680, Loss - 0.34715742692958895, Learning Rate - 0.0015625, magnitude of gradient - 0.0802322722444662\n",
      "Step - 5681, Loss - 0.24471271631414346, Learning Rate - 0.0015625, magnitude of gradient - 0.06786938748545963\n",
      "Step - 5682, Loss - 0.3946178480984961, Learning Rate - 0.0015625, magnitude of gradient - 0.024547509236937656\n",
      "Step - 5683, Loss - 0.30599221290275674, Learning Rate - 0.0015625, magnitude of gradient - 0.042787586567215916\n",
      "Step - 5684, Loss - 0.3674883115985493, Learning Rate - 0.0015625, magnitude of gradient - 0.031840340901199256\n",
      "Step - 5685, Loss - 0.30962487883981427, Learning Rate - 0.0015625, magnitude of gradient - 0.06013896924248001\n",
      "Step - 5686, Loss - 0.27252402573216494, Learning Rate - 0.0015625, magnitude of gradient - 0.08056320878291459\n",
      "Step - 5687, Loss - 0.34660637618176127, Learning Rate - 0.0015625, magnitude of gradient - 0.08191214118252199\n",
      "Step - 5688, Loss - 0.261962818690019, Learning Rate - 0.0015625, magnitude of gradient - 0.05154409468948077\n",
      "Step - 5689, Loss - 0.357760925143139, Learning Rate - 0.0015625, magnitude of gradient - 0.021438873356307883\n",
      "Step - 5690, Loss - 0.3171121917949023, Learning Rate - 0.0015625, magnitude of gradient - 0.026462170733544736\n",
      "Step - 5691, Loss - 0.3464040917087249, Learning Rate - 0.0015625, magnitude of gradient - 0.05686091348575496\n",
      "Step - 5692, Loss - 0.4047218396146157, Learning Rate - 0.0015625, magnitude of gradient - 0.050373992109434156\n",
      "Step - 5693, Loss - 0.359232757760813, Learning Rate - 0.0015625, magnitude of gradient - 0.02936710305947751\n",
      "Step - 5694, Loss - 0.3324350150850752, Learning Rate - 0.0015625, magnitude of gradient - 0.025688210499994698\n",
      "Step - 5695, Loss - 0.27058674954844253, Learning Rate - 0.0015625, magnitude of gradient - 0.03624249968754955\n",
      "Step - 5696, Loss - 0.3748105731962099, Learning Rate - 0.0015625, magnitude of gradient - 0.03981605652886764\n",
      "Step - 5697, Loss - 0.34260566817226157, Learning Rate - 0.0015625, magnitude of gradient - 0.07097498724448603\n",
      "Step - 5698, Loss - 0.2837621388210392, Learning Rate - 0.0015625, magnitude of gradient - 0.12469031418593386\n",
      "Step - 5699, Loss - 0.3662114921389768, Learning Rate - 0.0015625, magnitude of gradient - 0.08268303857903725\n",
      "Step - 5700, Loss - 0.3444343459468926, Learning Rate - 0.0015625, magnitude of gradient - 0.02797498920490701\n",
      "Step - 5701, Loss - 0.3417621669640057, Learning Rate - 0.0015625, magnitude of gradient - 0.05148546057019417\n",
      "Step - 5702, Loss - 0.35774455515856557, Learning Rate - 0.0015625, magnitude of gradient - 0.07034727094887004\n",
      "Step - 5703, Loss - 0.35951856969567303, Learning Rate - 0.0015625, magnitude of gradient - 0.048130887560375854\n",
      "Step - 5704, Loss - 0.30301237802828734, Learning Rate - 0.0015625, magnitude of gradient - 0.05080374747622897\n",
      "Step - 5705, Loss - 0.3077961039343003, Learning Rate - 0.0015625, magnitude of gradient - 0.04753201577732839\n",
      "Step - 5706, Loss - 0.3641655480174584, Learning Rate - 0.0015625, magnitude of gradient - 0.06672450835957806\n",
      "Step - 5707, Loss - 0.24518934923662616, Learning Rate - 0.0015625, magnitude of gradient - 0.10119227264941223\n",
      "Step - 5708, Loss - 0.3266467442584784, Learning Rate - 0.0015625, magnitude of gradient - 0.07073414491565333\n",
      "Step - 5709, Loss - 0.28733536437472557, Learning Rate - 0.0015625, magnitude of gradient - 0.03426908903872919\n",
      "Step - 5710, Loss - 0.29915438816562284, Learning Rate - 0.0015625, magnitude of gradient - 0.03835598472541657\n",
      "Step - 5711, Loss - 0.3440613206915945, Learning Rate - 0.0015625, magnitude of gradient - 0.0433479856895231\n",
      "Step - 5712, Loss - 0.321074978296887, Learning Rate - 0.0015625, magnitude of gradient - 0.028499945371001895\n",
      "Step - 5713, Loss - 0.2717321173006604, Learning Rate - 0.0015625, magnitude of gradient - 0.020587497456391737\n",
      "Step - 5714, Loss - 0.2982861544035636, Learning Rate - 0.0015625, magnitude of gradient - 0.05235899007691512\n",
      "Step - 5715, Loss - 0.33416518695144315, Learning Rate - 0.0015625, magnitude of gradient - 0.07469992355467751\n",
      "Step - 5716, Loss - 0.3986006363566325, Learning Rate - 0.0015625, magnitude of gradient - 0.07038797545921839\n",
      "Step - 5717, Loss - 0.31728322199132986, Learning Rate - 0.0015625, magnitude of gradient - 0.04500012319916101\n",
      "Step - 5718, Loss - 0.299598232600086, Learning Rate - 0.0015625, magnitude of gradient - 0.030390152030571344\n",
      "Step - 5719, Loss - 0.3656668354416888, Learning Rate - 0.0015625, magnitude of gradient - 0.01653614374777367\n",
      "Step - 5720, Loss - 0.32901255007679353, Learning Rate - 0.0015625, magnitude of gradient - 0.04021404758478412\n",
      "Step - 5721, Loss - 0.3807370106460735, Learning Rate - 0.0015625, magnitude of gradient - 0.014000843187666\n",
      "Step - 5722, Loss - 0.38272345457558243, Learning Rate - 0.0015625, magnitude of gradient - 0.054264053915543356\n",
      "Step - 5723, Loss - 0.39116493635600774, Learning Rate - 0.0015625, magnitude of gradient - 0.08582779825105046\n",
      "Step - 5724, Loss - 0.34088141489369717, Learning Rate - 0.0015625, magnitude of gradient - 0.021921403591495654\n",
      "Step - 5725, Loss - 0.29223782435878287, Learning Rate - 0.0015625, magnitude of gradient - 0.06822659127923243\n",
      "Step - 5726, Loss - 0.3592563267111421, Learning Rate - 0.0015625, magnitude of gradient - 0.09622136948802645\n",
      "Step - 5727, Loss - 0.34788289786164284, Learning Rate - 0.0015625, magnitude of gradient - 0.0653600023383563\n",
      "Step - 5728, Loss - 0.34941706657013605, Learning Rate - 0.0015625, magnitude of gradient - 0.063963951259098\n",
      "Step - 5729, Loss - 0.3385886271099594, Learning Rate - 0.0015625, magnitude of gradient - 0.05096544440002628\n",
      "Step - 5730, Loss - 0.24307102486030568, Learning Rate - 0.0015625, magnitude of gradient - 0.08405181794808794\n",
      "Step - 5731, Loss - 0.32344815648800146, Learning Rate - 0.0015625, magnitude of gradient - 0.0701999087877091\n",
      "Step - 5732, Loss - 0.2836349621941296, Learning Rate - 0.0015625, magnitude of gradient - 0.03126698815522911\n",
      "Step - 5733, Loss - 0.3318211373222306, Learning Rate - 0.0015625, magnitude of gradient - 0.09104766429753908\n",
      "Step - 5734, Loss - 0.34513512156773335, Learning Rate - 0.0015625, magnitude of gradient - 0.07914254025978971\n",
      "Step - 5735, Loss - 0.37480014482882545, Learning Rate - 0.0015625, magnitude of gradient - 0.12195508591371311\n",
      "Step - 5736, Loss - 0.38109576030919606, Learning Rate - 0.0015625, magnitude of gradient - 0.0302442191246507\n",
      "Step - 5737, Loss - 0.2908836530795129, Learning Rate - 0.0015625, magnitude of gradient - 0.05868999946433902\n",
      "Step - 5738, Loss - 0.3425197109618052, Learning Rate - 0.0015625, magnitude of gradient - 0.04733688448896581\n",
      "Step - 5739, Loss - 0.34722045353224856, Learning Rate - 0.0015625, magnitude of gradient - 0.025732924862976944\n",
      "Step - 5740, Loss - 0.26354841655573535, Learning Rate - 0.0015625, magnitude of gradient - 0.044441998833930456\n",
      "Step - 5741, Loss - 0.29024765818213555, Learning Rate - 0.0015625, magnitude of gradient - 0.06874476820797176\n",
      "Step - 5742, Loss - 0.3253542123566756, Learning Rate - 0.0015625, magnitude of gradient - 0.029543080884778878\n",
      "Step - 5743, Loss - 0.29019184786013247, Learning Rate - 0.0015625, magnitude of gradient - 0.09192351413616437\n",
      "Step - 5744, Loss - 0.35719156126848484, Learning Rate - 0.0015625, magnitude of gradient - 0.11125918243512184\n",
      "Step - 5745, Loss - 0.3447943004144556, Learning Rate - 0.0015625, magnitude of gradient - 0.0696307607423687\n",
      "Step - 5746, Loss - 0.3482155788052196, Learning Rate - 0.0015625, magnitude of gradient - 0.04202332871706231\n",
      "Step - 5747, Loss - 0.2589936737747015, Learning Rate - 0.0015625, magnitude of gradient - 0.0910485586213155\n",
      "Step - 5748, Loss - 0.28350027181917165, Learning Rate - 0.0015625, magnitude of gradient - 0.09799372674049875\n",
      "Step - 5749, Loss - 0.3492733917922414, Learning Rate - 0.0015625, magnitude of gradient - 0.02384320878797398\n",
      "Step - 5750, Loss - 0.3629035265847673, Learning Rate - 0.0015625, magnitude of gradient - 0.02298010163073108\n",
      "Step - 5751, Loss - 0.2925412082177129, Learning Rate - 0.0015625, magnitude of gradient - 0.05915363910957952\n",
      "Step - 5752, Loss - 0.3614967021825135, Learning Rate - 0.0015625, magnitude of gradient - 0.05981483376028392\n",
      "Step - 5753, Loss - 0.37128194049091356, Learning Rate - 0.0015625, magnitude of gradient - 0.012767212244187658\n",
      "Step - 5754, Loss - 0.26301297122539075, Learning Rate - 0.0015625, magnitude of gradient - 0.04768924870647221\n",
      "Step - 5755, Loss - 0.2732828637151243, Learning Rate - 0.0015625, magnitude of gradient - 0.05446988813759085\n",
      "Step - 5756, Loss - 0.24852957514258978, Learning Rate - 0.0015625, magnitude of gradient - 0.08532949830730178\n",
      "Step - 5757, Loss - 0.31967964735162485, Learning Rate - 0.0015625, magnitude of gradient - 0.029461838217351106\n",
      "Step - 5758, Loss - 0.2918780462525955, Learning Rate - 0.0015625, magnitude of gradient - 0.034834457975452746\n",
      "Step - 5759, Loss - 0.32309871311501803, Learning Rate - 0.0015625, magnitude of gradient - 0.050637684371349044\n",
      "Step - 5760, Loss - 0.37188133341153873, Learning Rate - 0.0015625, magnitude of gradient - 0.0647477147410522\n",
      "Step - 5761, Loss - 0.29653701809345145, Learning Rate - 0.0015625, magnitude of gradient - 0.08782552653784675\n",
      "Step - 5762, Loss - 0.37125383991922933, Learning Rate - 0.0015625, magnitude of gradient - 0.048005070670044205\n",
      "Step - 5763, Loss - 0.2984517158543504, Learning Rate - 0.0015625, magnitude of gradient - 0.06976621581552198\n",
      "Step - 5764, Loss - 0.32965218993030865, Learning Rate - 0.0015625, magnitude of gradient - 0.06635599859130863\n",
      "Step - 5765, Loss - 0.4103887772259425, Learning Rate - 0.0015625, magnitude of gradient - 0.022095021793659457\n",
      "Step - 5766, Loss - 0.3510196508446852, Learning Rate - 0.0015625, magnitude of gradient - 0.022640083012355927\n",
      "Step - 5767, Loss - 0.379546294996846, Learning Rate - 0.0015625, magnitude of gradient - 0.05990624008435791\n",
      "Step - 5768, Loss - 0.3377478722054734, Learning Rate - 0.0015625, magnitude of gradient - 0.03227411974242009\n",
      "Step - 5769, Loss - 0.284546212709918, Learning Rate - 0.0015625, magnitude of gradient - 0.059170203987771175\n",
      "Step - 5770, Loss - 0.3334520328194085, Learning Rate - 0.0015625, magnitude of gradient - 0.028919023778593893\n",
      "Step - 5771, Loss - 0.32239537764225545, Learning Rate - 0.0015625, magnitude of gradient - 0.024907805660624283\n",
      "Step - 5772, Loss - 0.25031565956755064, Learning Rate - 0.0015625, magnitude of gradient - 0.01587724541839284\n",
      "Step - 5773, Loss - 0.2533186633524685, Learning Rate - 0.0015625, magnitude of gradient - 0.0558755538097311\n",
      "Step - 5774, Loss - 0.28678394926712836, Learning Rate - 0.0015625, magnitude of gradient - 0.044083908108342576\n",
      "Step - 5775, Loss - 0.28191864228687563, Learning Rate - 0.0015625, magnitude of gradient - 0.045011450175508644\n",
      "Step - 5776, Loss - 0.27178953833164765, Learning Rate - 0.0015625, magnitude of gradient - 0.04058322827492736\n",
      "Step - 5777, Loss - 0.273101977042203, Learning Rate - 0.0015625, magnitude of gradient - 0.0444065952992119\n",
      "Step - 5778, Loss - 0.32023999493279465, Learning Rate - 0.0015625, magnitude of gradient - 0.03355823350399531\n",
      "Step - 5779, Loss - 0.31614811106457835, Learning Rate - 0.0015625, magnitude of gradient - 0.033689624136202384\n",
      "Step - 5780, Loss - 0.272673206614584, Learning Rate - 0.0015625, magnitude of gradient - 0.0826305588998511\n",
      "Step - 5781, Loss - 0.3771379079772823, Learning Rate - 0.0015625, magnitude of gradient - 0.08406575495990677\n",
      "Step - 5782, Loss - 0.2958666176548654, Learning Rate - 0.0015625, magnitude of gradient - 0.040390952066464585\n",
      "Step - 5783, Loss - 0.32279439583693825, Learning Rate - 0.0015625, magnitude of gradient - 0.025872999891519775\n",
      "Step - 5784, Loss - 0.2941777846132615, Learning Rate - 0.0015625, magnitude of gradient - 0.057321055202458185\n",
      "Step - 5785, Loss - 0.3223458110397396, Learning Rate - 0.0015625, magnitude of gradient - 0.01290307529635964\n",
      "Step - 5786, Loss - 0.3380545703773137, Learning Rate - 0.0015625, magnitude of gradient - 0.04596557793650146\n",
      "Step - 5787, Loss - 0.30426616812306617, Learning Rate - 0.0015625, magnitude of gradient - 0.06432019549299704\n",
      "Step - 5788, Loss - 0.32432681075901276, Learning Rate - 0.0015625, magnitude of gradient - 0.06560133569115148\n",
      "Step - 5789, Loss - 0.2871140263090367, Learning Rate - 0.0015625, magnitude of gradient - 0.07596703890519486\n",
      "Step - 5790, Loss - 0.35205292381023057, Learning Rate - 0.0015625, magnitude of gradient - 0.08824705809938631\n",
      "Step - 5791, Loss - 0.2755271222045228, Learning Rate - 0.0015625, magnitude of gradient - 0.030201230652897378\n",
      "Step - 5792, Loss - 0.33459815158342826, Learning Rate - 0.0015625, magnitude of gradient - 0.029982320624770225\n",
      "Step - 5793, Loss - 0.3106495823996091, Learning Rate - 0.0015625, magnitude of gradient - 0.025259231713597532\n",
      "Step - 5794, Loss - 0.34721150744265417, Learning Rate - 0.0015625, magnitude of gradient - 0.006988704911725467\n",
      "Step - 5795, Loss - 0.3745765632005087, Learning Rate - 0.0015625, magnitude of gradient - 0.042516412779747347\n",
      "Step - 5796, Loss - 0.2479408447111766, Learning Rate - 0.0015625, magnitude of gradient - 0.01247931325860509\n",
      "Step - 5797, Loss - 0.37090732335196763, Learning Rate - 0.0015625, magnitude of gradient - 0.023732411584322422\n",
      "Step - 5798, Loss - 0.38033451873111174, Learning Rate - 0.0015625, magnitude of gradient - 0.023865947181648764\n",
      "Step - 5799, Loss - 0.3585518086943483, Learning Rate - 0.0015625, magnitude of gradient - 0.034696885004841874\n",
      "Step - 5800, Loss - 0.3312702861838527, Learning Rate - 0.0015625, magnitude of gradient - 0.016927820460598406\n",
      "Step - 5801, Loss - 0.32490062395685626, Learning Rate - 0.0015625, magnitude of gradient - 0.06363290221373702\n",
      "Step - 5802, Loss - 0.3149851825327407, Learning Rate - 0.0015625, magnitude of gradient - 0.03188420068647551\n",
      "Step - 5803, Loss - 0.3480564381830394, Learning Rate - 0.0015625, magnitude of gradient - 0.06208054154131223\n",
      "Step - 5804, Loss - 0.3016173414314641, Learning Rate - 0.0015625, magnitude of gradient - 0.047524992942189793\n",
      "Step - 5805, Loss - 0.2577837850969211, Learning Rate - 0.0015625, magnitude of gradient - 0.010597889968565009\n",
      "Step - 5806, Loss - 0.2724622033061808, Learning Rate - 0.0015625, magnitude of gradient - 0.08338901744352344\n",
      "Step - 5807, Loss - 0.3018371242479907, Learning Rate - 0.0015625, magnitude of gradient - 0.04210103302341533\n",
      "Step - 5808, Loss - 0.26261837172231783, Learning Rate - 0.0015625, magnitude of gradient - 0.06564802183606754\n",
      "Step - 5809, Loss - 0.3448952080048959, Learning Rate - 0.0015625, magnitude of gradient - 0.043087509790581936\n",
      "Step - 5810, Loss - 0.3437567530387505, Learning Rate - 0.0015625, magnitude of gradient - 0.0800375319869293\n",
      "Step - 5811, Loss - 0.26287519123969766, Learning Rate - 0.0015625, magnitude of gradient - 0.025748214944134287\n",
      "Step - 5812, Loss - 0.34358646469974374, Learning Rate - 0.0015625, magnitude of gradient - 0.09345815793214138\n",
      "Step - 5813, Loss - 0.3387008047545835, Learning Rate - 0.0015625, magnitude of gradient - 0.08216942719528722\n",
      "Step - 5814, Loss - 0.3011296355544503, Learning Rate - 0.0015625, magnitude of gradient - 0.044962610528702866\n",
      "Step - 5815, Loss - 0.34430339476099214, Learning Rate - 0.0015625, magnitude of gradient - 0.0616776866742731\n",
      "Step - 5816, Loss - 0.2785565284515219, Learning Rate - 0.0015625, magnitude of gradient - 0.013539597978192862\n",
      "Step - 5817, Loss - 0.25376304841450453, Learning Rate - 0.0015625, magnitude of gradient - 0.03203531676251868\n",
      "Step - 5818, Loss - 0.25768385225434304, Learning Rate - 0.0015625, magnitude of gradient - 0.025279255035195843\n",
      "Step - 5819, Loss - 0.421201619387002, Learning Rate - 0.0015625, magnitude of gradient - 0.0866077591654803\n",
      "Step - 5820, Loss - 0.2714743222312102, Learning Rate - 0.0015625, magnitude of gradient - 0.09051708106537973\n",
      "Step - 5821, Loss - 0.28741262063059636, Learning Rate - 0.0015625, magnitude of gradient - 0.007035033190200556\n",
      "Step - 5822, Loss - 0.3628580604441225, Learning Rate - 0.0015625, magnitude of gradient - 0.05811650892138327\n",
      "Step - 5823, Loss - 0.34102905682754286, Learning Rate - 0.0015625, magnitude of gradient - 0.033964105645482974\n",
      "Step - 5824, Loss - 0.3773371237847969, Learning Rate - 0.0015625, magnitude of gradient - 0.0641199856334172\n",
      "Step - 5825, Loss - 0.3139922307416589, Learning Rate - 0.0015625, magnitude of gradient - 0.005809894693682798\n",
      "Step - 5826, Loss - 0.33936385734821206, Learning Rate - 0.0015625, magnitude of gradient - 0.059584537367011005\n",
      "Step - 5827, Loss - 0.3959978079136228, Learning Rate - 0.0015625, magnitude of gradient - 0.059752028081680716\n",
      "Step - 5828, Loss - 0.36469711624770407, Learning Rate - 0.0015625, magnitude of gradient - 0.052756027020852594\n",
      "Step - 5829, Loss - 0.33153103793467564, Learning Rate - 0.0015625, magnitude of gradient - 0.044611768069901026\n",
      "Step - 5830, Loss - 0.3245938015446713, Learning Rate - 0.0015625, magnitude of gradient - 0.040235124100261636\n",
      "Step - 5831, Loss - 0.2863780189355207, Learning Rate - 0.0015625, magnitude of gradient - 0.006462681442386505\n",
      "Step - 5832, Loss - 0.2923908872681259, Learning Rate - 0.0015625, magnitude of gradient - 0.01031440289439865\n",
      "Step - 5833, Loss - 0.4082490040569646, Learning Rate - 0.0015625, magnitude of gradient - 0.02689154473013817\n",
      "Step - 5834, Loss - 0.25720636386148044, Learning Rate - 0.0015625, magnitude of gradient - 0.07508753238464186\n",
      "Step - 5835, Loss - 0.24526391655930524, Learning Rate - 0.0015625, magnitude of gradient - 0.026274216568653336\n",
      "Step - 5836, Loss - 0.4290606175302898, Learning Rate - 0.0015625, magnitude of gradient - 0.04546008826792857\n",
      "Step - 5837, Loss - 0.2825731647640435, Learning Rate - 0.0015625, magnitude of gradient - 0.029819637912993115\n",
      "Step - 5838, Loss - 0.25708142716849325, Learning Rate - 0.0015625, magnitude of gradient - 0.06101376346419021\n",
      "Step - 5839, Loss - 0.29632293219395534, Learning Rate - 0.0015625, magnitude of gradient - 0.05282098403124444\n",
      "Step - 5840, Loss - 0.3345396731867134, Learning Rate - 0.0015625, magnitude of gradient - 0.04984554309774028\n",
      "Step - 5841, Loss - 0.25332621608278605, Learning Rate - 0.0015625, magnitude of gradient - 0.03700973088202502\n",
      "Step - 5842, Loss - 0.3053197187359518, Learning Rate - 0.0015625, magnitude of gradient - 0.0069180586689652716\n",
      "Step - 5843, Loss - 0.30708384152186796, Learning Rate - 0.0015625, magnitude of gradient - 0.039160274090395936\n",
      "Step - 5844, Loss - 0.28518035640451667, Learning Rate - 0.0015625, magnitude of gradient - 0.04056453552448225\n",
      "Step - 5845, Loss - 0.39619109604392444, Learning Rate - 0.0015625, magnitude of gradient - 0.06282264939944572\n",
      "Step - 5846, Loss - 0.32341199991998903, Learning Rate - 0.0015625, magnitude of gradient - 0.07391434552700805\n",
      "Step - 5847, Loss - 0.34452293072331075, Learning Rate - 0.0015625, magnitude of gradient - 0.0919667702558746\n",
      "Step - 5848, Loss - 0.3080686807811026, Learning Rate - 0.0015625, magnitude of gradient - 0.08365174910687759\n",
      "Step - 5849, Loss - 0.40717004509651306, Learning Rate - 0.0015625, magnitude of gradient - 0.07407617300576967\n",
      "Step - 5850, Loss - 0.30295788818039254, Learning Rate - 0.0015625, magnitude of gradient - 0.027253340439722974\n",
      "Step - 5851, Loss - 0.2190432049143035, Learning Rate - 0.0015625, magnitude of gradient - 0.07012017153974283\n",
      "Step - 5852, Loss - 0.2771465012171307, Learning Rate - 0.0015625, magnitude of gradient - 0.019866020331866775\n",
      "Step - 5853, Loss - 0.3774304026072177, Learning Rate - 0.0015625, magnitude of gradient - 0.04608901811244587\n",
      "Step - 5854, Loss - 0.3287136900660118, Learning Rate - 0.0015625, magnitude of gradient - 0.051840549910127584\n",
      "Step - 5855, Loss - 0.3041722478947467, Learning Rate - 0.0015625, magnitude of gradient - 0.03923212417013197\n",
      "Step - 5856, Loss - 0.34358335495418973, Learning Rate - 0.0015625, magnitude of gradient - 0.01929673257184413\n",
      "Step - 5857, Loss - 0.3049902880275535, Learning Rate - 0.0015625, magnitude of gradient - 0.07486887226575946\n",
      "Step - 5858, Loss - 0.3236490158852582, Learning Rate - 0.0015625, magnitude of gradient - 0.025457984382456406\n",
      "Step - 5859, Loss - 0.2623957802980278, Learning Rate - 0.0015625, magnitude of gradient - 0.08900936397068779\n",
      "Step - 5860, Loss - 0.3681174682364041, Learning Rate - 0.0015625, magnitude of gradient - 0.02749144878413964\n",
      "Step - 5861, Loss - 0.3798036384231828, Learning Rate - 0.0015625, magnitude of gradient - 0.031569244188290074\n",
      "Step - 5862, Loss - 0.25504454122285225, Learning Rate - 0.0015625, magnitude of gradient - 0.12325660919843828\n",
      "Step - 5863, Loss - 0.30342760210772046, Learning Rate - 0.0015625, magnitude of gradient - 0.00595678388225332\n",
      "Step - 5864, Loss - 0.2914780116698375, Learning Rate - 0.0015625, magnitude of gradient - 0.06370669370337163\n",
      "Step - 5865, Loss - 0.25263489217359886, Learning Rate - 0.0015625, magnitude of gradient - 0.03141252129923875\n",
      "Step - 5866, Loss - 0.2618092966135998, Learning Rate - 0.0015625, magnitude of gradient - 0.1109539832665316\n",
      "Step - 5867, Loss - 0.34352383054540825, Learning Rate - 0.0015625, magnitude of gradient - 0.020051738608909388\n",
      "Step - 5868, Loss - 0.3196198731366549, Learning Rate - 0.0015625, magnitude of gradient - 0.07501046368994672\n",
      "Step - 5869, Loss - 0.3309250467344707, Learning Rate - 0.0015625, magnitude of gradient - 0.017666217082382068\n",
      "Step - 5870, Loss - 0.26868350110279304, Learning Rate - 0.0015625, magnitude of gradient - 0.023940273112986233\n",
      "Step - 5871, Loss - 0.37544582457411524, Learning Rate - 0.0015625, magnitude of gradient - 0.09110838568599036\n",
      "Step - 5872, Loss - 0.32552976904845415, Learning Rate - 0.0015625, magnitude of gradient - 0.011682366686362727\n",
      "Step - 5873, Loss - 0.3313730863230353, Learning Rate - 0.0015625, magnitude of gradient - 0.08683051398568886\n",
      "Step - 5874, Loss - 0.2506635585015278, Learning Rate - 0.0015625, magnitude of gradient - 0.021784868029403253\n",
      "Step - 5875, Loss - 0.26749349298519753, Learning Rate - 0.0015625, magnitude of gradient - 0.05854159581686059\n",
      "Step - 5876, Loss - 0.29756303740623463, Learning Rate - 0.0015625, magnitude of gradient - 0.0819238109830269\n",
      "Step - 5877, Loss - 0.28322228901345003, Learning Rate - 0.0015625, magnitude of gradient - 0.008687661715667816\n",
      "Step - 5878, Loss - 0.3242102491109897, Learning Rate - 0.0015625, magnitude of gradient - 0.04459307938011726\n",
      "Step - 5879, Loss - 0.27281987964692384, Learning Rate - 0.0015625, magnitude of gradient - 0.06378947940584388\n",
      "Step - 5880, Loss - 0.3760084673060852, Learning Rate - 0.0015625, magnitude of gradient - 0.08765967990598032\n",
      "Step - 5881, Loss - 0.2640500020560853, Learning Rate - 0.0015625, magnitude of gradient - 0.031919785468126106\n",
      "Step - 5882, Loss - 0.3382624981563253, Learning Rate - 0.0015625, magnitude of gradient - 0.02964047178918714\n",
      "Step - 5883, Loss - 0.3385766496967912, Learning Rate - 0.0015625, magnitude of gradient - 0.045646059114752904\n",
      "Step - 5884, Loss - 0.370243225121991, Learning Rate - 0.0015625, magnitude of gradient - 0.05016087827842426\n",
      "Step - 5885, Loss - 0.2996599581470049, Learning Rate - 0.0015625, magnitude of gradient - 0.062165847857534144\n",
      "Step - 5886, Loss - 0.2993421124378415, Learning Rate - 0.0015625, magnitude of gradient - 0.058995422800084685\n",
      "Step - 5887, Loss - 0.3306398861964363, Learning Rate - 0.0015625, magnitude of gradient - 0.09362956084609245\n",
      "Step - 5888, Loss - 0.3272032093334124, Learning Rate - 0.0015625, magnitude of gradient - 0.09722932098844178\n",
      "Step - 5889, Loss - 0.3040129425696745, Learning Rate - 0.0015625, magnitude of gradient - 0.06255122239958145\n",
      "Step - 5890, Loss - 0.2860895124506657, Learning Rate - 0.0015625, magnitude of gradient - 0.011504930854518878\n",
      "Step - 5891, Loss - 0.3203498648678621, Learning Rate - 0.0015625, magnitude of gradient - 0.05456514527359909\n",
      "Step - 5892, Loss - 0.38367173041776337, Learning Rate - 0.0015625, magnitude of gradient - 0.0815251690803503\n",
      "Step - 5893, Loss - 0.3218944290406189, Learning Rate - 0.0015625, magnitude of gradient - 0.09264978886038717\n",
      "Step - 5894, Loss - 0.30107419786018313, Learning Rate - 0.0015625, magnitude of gradient - 0.056827272241934464\n",
      "Step - 5895, Loss - 0.34906166464967125, Learning Rate - 0.0015625, magnitude of gradient - 0.052691132217938794\n",
      "Step - 5896, Loss - 0.36191280504121937, Learning Rate - 0.0015625, magnitude of gradient - 0.04533986676437401\n",
      "Step - 5897, Loss - 0.34548803706586995, Learning Rate - 0.0015625, magnitude of gradient - 0.04946683291709778\n",
      "Step - 5898, Loss - 0.31004938568918733, Learning Rate - 0.0015625, magnitude of gradient - 0.03589359709411116\n",
      "Step - 5899, Loss - 0.35314100576570445, Learning Rate - 0.0015625, magnitude of gradient - 0.02324454645234337\n",
      "Step - 5900, Loss - 0.24653655228887714, Learning Rate - 0.0015625, magnitude of gradient - 0.0864462755897334\n",
      "Step - 5901, Loss - 0.229095066067167, Learning Rate - 0.0015625, magnitude of gradient - 0.0330876510825059\n",
      "Step - 5902, Loss - 0.2739519936234212, Learning Rate - 0.0015625, magnitude of gradient - 0.03396799893970157\n",
      "Step - 5903, Loss - 0.35933521310413297, Learning Rate - 0.0015625, magnitude of gradient - 0.03519827083391969\n",
      "Step - 5904, Loss - 0.30285536659621487, Learning Rate - 0.0015625, magnitude of gradient - 0.06296205573587942\n",
      "Step - 5905, Loss - 0.26338699739075794, Learning Rate - 0.0015625, magnitude of gradient - 0.05285888989546346\n",
      "Step - 5906, Loss - 0.3377694645129847, Learning Rate - 0.0015625, magnitude of gradient - 0.05574048763784327\n",
      "Step - 5907, Loss - 0.3199423484770396, Learning Rate - 0.0015625, magnitude of gradient - 0.05739935520939264\n",
      "Step - 5908, Loss - 0.34551149310425716, Learning Rate - 0.0015625, magnitude of gradient - 0.07601981450868495\n",
      "Step - 5909, Loss - 0.3656123188475368, Learning Rate - 0.0015625, magnitude of gradient - 0.013698588325317838\n",
      "Step - 5910, Loss - 0.32562925158363154, Learning Rate - 0.0015625, magnitude of gradient - 0.0847131241165975\n",
      "Step - 5911, Loss - 0.41954986354560747, Learning Rate - 0.0015625, magnitude of gradient - 0.012839869206608822\n",
      "Step - 5912, Loss - 0.31896945020406425, Learning Rate - 0.0015625, magnitude of gradient - 0.011568169713571974\n",
      "Step - 5913, Loss - 0.32362273088045623, Learning Rate - 0.0015625, magnitude of gradient - 0.0701209597290342\n",
      "Step - 5914, Loss - 0.3201359105993458, Learning Rate - 0.0015625, magnitude of gradient - 0.046082166153829957\n",
      "Step - 5915, Loss - 0.27981491632045985, Learning Rate - 0.0015625, magnitude of gradient - 0.0993239842139931\n",
      "Step - 5916, Loss - 0.32605305517352856, Learning Rate - 0.0015625, magnitude of gradient - 0.021849894738586527\n",
      "Step - 5917, Loss - 0.35512562853312585, Learning Rate - 0.0015625, magnitude of gradient - 0.02722156716632809\n",
      "Step - 5918, Loss - 0.29764438849513647, Learning Rate - 0.0015625, magnitude of gradient - 0.034966595791448736\n",
      "Step - 5919, Loss - 0.3407873068184954, Learning Rate - 0.0015625, magnitude of gradient - 0.03187298415582664\n",
      "Step - 5920, Loss - 0.3243200062655546, Learning Rate - 0.0015625, magnitude of gradient - 0.05049008380746142\n",
      "Step - 5921, Loss - 0.3109599342538867, Learning Rate - 0.0015625, magnitude of gradient - 0.0438046657014371\n",
      "Step - 5922, Loss - 0.2762880638871905, Learning Rate - 0.0015625, magnitude of gradient - 0.05540989968539852\n",
      "Step - 5923, Loss - 0.2661411885411186, Learning Rate - 0.0015625, magnitude of gradient - 0.036619421001060096\n",
      "Step - 5924, Loss - 0.27369529631740697, Learning Rate - 0.0015625, magnitude of gradient - 0.08309011428008078\n",
      "Step - 5925, Loss - 0.2912823689336582, Learning Rate - 0.0015625, magnitude of gradient - 0.05001809453802714\n",
      "Step - 5926, Loss - 0.30371895475196403, Learning Rate - 0.0015625, magnitude of gradient - 0.03527435421497594\n",
      "Step - 5927, Loss - 0.3494320436704242, Learning Rate - 0.0015625, magnitude of gradient - 0.0592726590290974\n",
      "Step - 5928, Loss - 0.334057204231778, Learning Rate - 0.0015625, magnitude of gradient - 0.05582466381447785\n",
      "Step - 5929, Loss - 0.3039081358138755, Learning Rate - 0.0015625, magnitude of gradient - 0.029375140831685343\n",
      "Step - 5930, Loss - 0.3183489835096367, Learning Rate - 0.0015625, magnitude of gradient - 0.04456717741129616\n",
      "Step - 5931, Loss - 0.33173947202636567, Learning Rate - 0.0015625, magnitude of gradient - 0.05142209390311527\n",
      "Step - 5932, Loss - 0.26217998144880106, Learning Rate - 0.0015625, magnitude of gradient - 0.028492357096015934\n",
      "Step - 5933, Loss - 0.31239608294791454, Learning Rate - 0.0015625, magnitude of gradient - 0.039278683574987525\n",
      "Step - 5934, Loss - 0.33949509988940196, Learning Rate - 0.0015625, magnitude of gradient - 0.03473517133070861\n",
      "Step - 5935, Loss - 0.2743954282699568, Learning Rate - 0.0015625, magnitude of gradient - 0.03937872716430066\n",
      "Step - 5936, Loss - 0.31199965585400785, Learning Rate - 0.0015625, magnitude of gradient - 0.04889538185453661\n",
      "Step - 5937, Loss - 0.3393498815433221, Learning Rate - 0.0015625, magnitude of gradient - 0.024991221633779193\n",
      "Step - 5938, Loss - 0.30722279508950434, Learning Rate - 0.0015625, magnitude of gradient - 0.06297977458260173\n",
      "Step - 5939, Loss - 0.305664995275811, Learning Rate - 0.0015625, magnitude of gradient - 0.04539140297588367\n",
      "Step - 5940, Loss - 0.32833510228948537, Learning Rate - 0.0015625, magnitude of gradient - 0.13106780095088166\n",
      "Step - 5941, Loss - 0.29138395716624155, Learning Rate - 0.0015625, magnitude of gradient - 0.02687641876170434\n",
      "Step - 5942, Loss - 0.34011906601938824, Learning Rate - 0.0015625, magnitude of gradient - 0.04218131570860393\n",
      "Step - 5943, Loss - 0.3005149730918133, Learning Rate - 0.0015625, magnitude of gradient - 0.03977031746475749\n",
      "Step - 5944, Loss - 0.30420098858622924, Learning Rate - 0.0015625, magnitude of gradient - 0.025542558194566373\n",
      "Step - 5945, Loss - 0.2533377721520236, Learning Rate - 0.0015625, magnitude of gradient - 0.02909629269408697\n",
      "Step - 5946, Loss - 0.3800988263710494, Learning Rate - 0.0015625, magnitude of gradient - 0.11675363884944295\n",
      "Step - 5947, Loss - 0.26679249108672237, Learning Rate - 0.0015625, magnitude of gradient - 0.022589682978059415\n",
      "Step - 5948, Loss - 0.34111372757149894, Learning Rate - 0.0015625, magnitude of gradient - 0.04717666959673411\n",
      "Step - 5949, Loss - 0.4134301532347545, Learning Rate - 0.0015625, magnitude of gradient - 0.10229892048436819\n",
      "Step - 5950, Loss - 0.34067190155982735, Learning Rate - 0.0015625, magnitude of gradient - 0.013953214031078028\n",
      "Step - 5951, Loss - 0.3646324868720292, Learning Rate - 0.0015625, magnitude of gradient - 0.048406228205173436\n",
      "Step - 5952, Loss - 0.34660876628269893, Learning Rate - 0.0015625, magnitude of gradient - 0.01027541023936524\n",
      "Step - 5953, Loss - 0.3151686349578539, Learning Rate - 0.0015625, magnitude of gradient - 0.05291146692747823\n",
      "Step - 5954, Loss - 0.269766184468811, Learning Rate - 0.0015625, magnitude of gradient - 0.052771275773549425\n",
      "Step - 5955, Loss - 0.2856801189270387, Learning Rate - 0.0015625, magnitude of gradient - 0.019537506834566207\n",
      "Step - 5956, Loss - 0.28435206649118294, Learning Rate - 0.0015625, magnitude of gradient - 0.08451536627295615\n",
      "Step - 5957, Loss - 0.36866345631182734, Learning Rate - 0.0015625, magnitude of gradient - 0.045767196604314025\n",
      "Step - 5958, Loss - 0.30805622374727026, Learning Rate - 0.0015625, magnitude of gradient - 0.04381596938468629\n",
      "Step - 5959, Loss - 0.26397819157795066, Learning Rate - 0.0015625, magnitude of gradient - 0.040005633470603465\n",
      "Step - 5960, Loss - 0.3269470458700979, Learning Rate - 0.0015625, magnitude of gradient - 0.06736852952277074\n",
      "Step - 5961, Loss - 0.34041579191905125, Learning Rate - 0.0015625, magnitude of gradient - 0.05415550545125899\n",
      "Step - 5962, Loss - 0.39005336834223325, Learning Rate - 0.0015625, magnitude of gradient - 0.04607926975278614\n",
      "Step - 5963, Loss - 0.297890466633137, Learning Rate - 0.0015625, magnitude of gradient - 0.020430385474518635\n",
      "Step - 5964, Loss - 0.330564379015327, Learning Rate - 0.0015625, magnitude of gradient - 0.04592246690913671\n",
      "Step - 5965, Loss - 0.3526976256449945, Learning Rate - 0.0015625, magnitude of gradient - 0.051387883027190065\n",
      "Step - 5966, Loss - 0.33129372413071106, Learning Rate - 0.0015625, magnitude of gradient - 0.05287101995109092\n",
      "Step - 5967, Loss - 0.3411165801769276, Learning Rate - 0.0015625, magnitude of gradient - 0.037104504074432415\n",
      "Step - 5968, Loss - 0.27035957021794343, Learning Rate - 0.0015625, magnitude of gradient - 0.08610452738034235\n",
      "Step - 5969, Loss - 0.29310670785759774, Learning Rate - 0.0015625, magnitude of gradient - 0.023272968726836537\n",
      "Step - 5970, Loss - 0.2915326658652412, Learning Rate - 0.0015625, magnitude of gradient - 0.08528819927785539\n",
      "Step - 5971, Loss - 0.312418852732627, Learning Rate - 0.0015625, magnitude of gradient - 0.07686982308506855\n",
      "Step - 5972, Loss - 0.3239839405132551, Learning Rate - 0.0015625, magnitude of gradient - 0.034259097099711144\n",
      "Step - 5973, Loss - 0.26560656587298126, Learning Rate - 0.0015625, magnitude of gradient - 0.0391731788220965\n",
      "Step - 5974, Loss - 0.2828091674410136, Learning Rate - 0.0015625, magnitude of gradient - 0.025956545058916848\n",
      "Step - 5975, Loss - 0.3701094635308483, Learning Rate - 0.0015625, magnitude of gradient - 0.030870387633425214\n",
      "Step - 5976, Loss - 0.3650834226064846, Learning Rate - 0.0015625, magnitude of gradient - 0.028178057040774595\n",
      "Step - 5977, Loss - 0.31660757924012, Learning Rate - 0.0015625, magnitude of gradient - 0.09669661560730615\n",
      "Step - 5978, Loss - 0.33407363768193826, Learning Rate - 0.0015625, magnitude of gradient - 0.02220769394959507\n",
      "Step - 5979, Loss - 0.2988170709057757, Learning Rate - 0.0015625, magnitude of gradient - 0.07852242678003778\n",
      "Step - 5980, Loss - 0.3031030138977526, Learning Rate - 0.0015625, magnitude of gradient - 0.04995408288877172\n",
      "Step - 5981, Loss - 0.30755751435380874, Learning Rate - 0.0015625, magnitude of gradient - 0.09685246381750452\n",
      "Step - 5982, Loss - 0.2729607869113112, Learning Rate - 0.0015625, magnitude of gradient - 0.03935499129784925\n",
      "Step - 5983, Loss - 0.34216126340817343, Learning Rate - 0.0015625, magnitude of gradient - 0.029941971336512923\n",
      "Step - 5984, Loss - 0.31727127534720745, Learning Rate - 0.0015625, magnitude of gradient - 0.04682842898799957\n",
      "Step - 5985, Loss - 0.3063595086424204, Learning Rate - 0.0015625, magnitude of gradient - 0.02997325816354788\n",
      "Step - 5986, Loss - 0.3399803920931983, Learning Rate - 0.0015625, magnitude of gradient - 0.024344612484476183\n",
      "Step - 5987, Loss - 0.29230644215285295, Learning Rate - 0.0015625, magnitude of gradient - 0.046314338589967446\n",
      "Step - 5988, Loss - 0.33411608746558175, Learning Rate - 0.0015625, magnitude of gradient - 0.082182660845543\n",
      "Step - 5989, Loss - 0.29694366980879344, Learning Rate - 0.0015625, magnitude of gradient - 0.06586734182176437\n",
      "Step - 5990, Loss - 0.3541110740853152, Learning Rate - 0.0015625, magnitude of gradient - 0.04143364285410831\n",
      "Step - 5991, Loss - 0.28564791608116236, Learning Rate - 0.0015625, magnitude of gradient - 0.056671039815208234\n",
      "Step - 5992, Loss - 0.326286723736247, Learning Rate - 0.0015625, magnitude of gradient - 0.021748686465247823\n",
      "Step - 5993, Loss - 0.2952665455866915, Learning Rate - 0.0015625, magnitude of gradient - 0.0066868024089184585\n",
      "Step - 5994, Loss - 0.29323726140681505, Learning Rate - 0.0015625, magnitude of gradient - 0.01872677445505128\n",
      "Step - 5995, Loss - 0.2990317069771023, Learning Rate - 0.0015625, magnitude of gradient - 0.01141066063063148\n",
      "Step - 5996, Loss - 0.3581395762652002, Learning Rate - 0.0015625, magnitude of gradient - 0.03864878559868805\n",
      "Step - 5997, Loss - 0.3444144018382297, Learning Rate - 0.0015625, magnitude of gradient - 0.05653535453418975\n",
      "Step - 5998, Loss - 0.36197127456906325, Learning Rate - 0.0015625, magnitude of gradient - 0.03971734046322498\n",
      "Step - 5999, Loss - 0.30207562583164604, Learning Rate - 0.0015625, magnitude of gradient - 0.033484662379710844\n",
      "Step - 6000, Loss - 0.3392948152550738, Learning Rate - 0.0015625, magnitude of gradient - 0.05295307902923944\n",
      "Step - 6001, Loss - 0.3716502626000102, Learning Rate - 0.00078125, magnitude of gradient - 0.05046934753699654\n",
      "Step - 6002, Loss - 0.3655447755442168, Learning Rate - 0.00078125, magnitude of gradient - 0.07239649849372151\n",
      "Step - 6003, Loss - 0.34682171373298787, Learning Rate - 0.00078125, magnitude of gradient - 0.01809535631739489\n",
      "Step - 6004, Loss - 0.2526089986767788, Learning Rate - 0.00078125, magnitude of gradient - 0.013050996499959519\n",
      "Step - 6005, Loss - 0.3402389861300986, Learning Rate - 0.00078125, magnitude of gradient - 0.03638324605024108\n",
      "Step - 6006, Loss - 0.3058383608340265, Learning Rate - 0.00078125, magnitude of gradient - 0.01967453919651711\n",
      "Step - 6007, Loss - 0.3127754579737449, Learning Rate - 0.00078125, magnitude of gradient - 0.04682584210578289\n",
      "Step - 6008, Loss - 0.30631718852372625, Learning Rate - 0.00078125, magnitude of gradient - 0.0279382935792931\n",
      "Step - 6009, Loss - 0.3043381137774646, Learning Rate - 0.00078125, magnitude of gradient - 0.05618852590265835\n",
      "Step - 6010, Loss - 0.2794385641727935, Learning Rate - 0.00078125, magnitude of gradient - 0.05614668868582271\n",
      "Step - 6011, Loss - 0.2809664827830153, Learning Rate - 0.00078125, magnitude of gradient - 0.06257268134858658\n",
      "Step - 6012, Loss - 0.3128551593110068, Learning Rate - 0.00078125, magnitude of gradient - 0.048563999284385574\n",
      "Step - 6013, Loss - 0.34046366019698393, Learning Rate - 0.00078125, magnitude of gradient - 0.026151056113479555\n",
      "Step - 6014, Loss - 0.36129660115481493, Learning Rate - 0.00078125, magnitude of gradient - 0.01731247858672559\n",
      "Step - 6015, Loss - 0.24927632076487577, Learning Rate - 0.00078125, magnitude of gradient - 0.06196537532253192\n",
      "Step - 6016, Loss - 0.2893599696685497, Learning Rate - 0.00078125, magnitude of gradient - 0.024179371944013237\n",
      "Step - 6017, Loss - 0.40620611594993006, Learning Rate - 0.00078125, magnitude of gradient - 0.09472888238127404\n",
      "Step - 6018, Loss - 0.34633504337941123, Learning Rate - 0.00078125, magnitude of gradient - 0.01436938079983261\n",
      "Step - 6019, Loss - 0.25489515764702236, Learning Rate - 0.00078125, magnitude of gradient - 0.07601903508502708\n",
      "Step - 6020, Loss - 0.3753743288424134, Learning Rate - 0.00078125, magnitude of gradient - 0.032550625790037646\n",
      "Step - 6021, Loss - 0.23414423778083127, Learning Rate - 0.00078125, magnitude of gradient - 0.04939844655675571\n",
      "Step - 6022, Loss - 0.34612324459192423, Learning Rate - 0.00078125, magnitude of gradient - 0.005774016440278318\n",
      "Step - 6023, Loss - 0.30606860429144767, Learning Rate - 0.00078125, magnitude of gradient - 0.02910368379448981\n",
      "Step - 6024, Loss - 0.2232875489548528, Learning Rate - 0.00078125, magnitude of gradient - 0.06909348136780726\n",
      "Step - 6025, Loss - 0.2918329048561087, Learning Rate - 0.00078125, magnitude of gradient - 0.03816272374338527\n",
      "Step - 6026, Loss - 0.2664482863802117, Learning Rate - 0.00078125, magnitude of gradient - 0.08001903329185589\n",
      "Step - 6027, Loss - 0.3787912820174222, Learning Rate - 0.00078125, magnitude of gradient - 0.021994096076877127\n",
      "Step - 6028, Loss - 0.25917200292694825, Learning Rate - 0.00078125, magnitude of gradient - 0.06288681497095396\n",
      "Step - 6029, Loss - 0.3256361898808608, Learning Rate - 0.00078125, magnitude of gradient - 0.07455667749481479\n",
      "Step - 6030, Loss - 0.36473208080212294, Learning Rate - 0.00078125, magnitude of gradient - 0.07569123134012963\n",
      "Step - 6031, Loss - 0.2976318561232958, Learning Rate - 0.00078125, magnitude of gradient - 0.03969331711893674\n",
      "Step - 6032, Loss - 0.3452250601950596, Learning Rate - 0.00078125, magnitude of gradient - 0.07304480909190562\n",
      "Step - 6033, Loss - 0.3410130382424634, Learning Rate - 0.00078125, magnitude of gradient - 0.03849515992281392\n",
      "Step - 6034, Loss - 0.34322358595939845, Learning Rate - 0.00078125, magnitude of gradient - 0.051209127819676264\n",
      "Step - 6035, Loss - 0.29162072135122175, Learning Rate - 0.00078125, magnitude of gradient - 0.04377469567154814\n",
      "Step - 6036, Loss - 0.34024960792288295, Learning Rate - 0.00078125, magnitude of gradient - 0.04886686222126151\n",
      "Step - 6037, Loss - 0.2842519792775911, Learning Rate - 0.00078125, magnitude of gradient - 0.05512832859895512\n",
      "Step - 6038, Loss - 0.23386726020535548, Learning Rate - 0.00078125, magnitude of gradient - 0.06414337738272208\n",
      "Step - 6039, Loss - 0.3780670605979739, Learning Rate - 0.00078125, magnitude of gradient - 0.015000111265882047\n",
      "Step - 6040, Loss - 0.2557113038411088, Learning Rate - 0.00078125, magnitude of gradient - 0.017234242010246894\n",
      "Step - 6041, Loss - 0.41071109468833994, Learning Rate - 0.00078125, magnitude of gradient - 0.014314149181039019\n",
      "Step - 6042, Loss - 0.3366461582702569, Learning Rate - 0.00078125, magnitude of gradient - 0.04091547669148758\n",
      "Step - 6043, Loss - 0.30787277461199647, Learning Rate - 0.00078125, magnitude of gradient - 0.02356638860420998\n",
      "Step - 6044, Loss - 0.26148980813169653, Learning Rate - 0.00078125, magnitude of gradient - 0.05101436929434602\n",
      "Step - 6045, Loss - 0.26670668419834465, Learning Rate - 0.00078125, magnitude of gradient - 0.02083020593910324\n",
      "Step - 6046, Loss - 0.37112591692639557, Learning Rate - 0.00078125, magnitude of gradient - 0.07363019384806604\n",
      "Step - 6047, Loss - 0.3002019534742372, Learning Rate - 0.00078125, magnitude of gradient - 0.062258988169356676\n",
      "Step - 6048, Loss - 0.3537566694172718, Learning Rate - 0.00078125, magnitude of gradient - 0.06218530602487909\n",
      "Step - 6049, Loss - 0.37934143492777317, Learning Rate - 0.00078125, magnitude of gradient - 0.06499274565909587\n",
      "Step - 6050, Loss - 0.24782551290902724, Learning Rate - 0.00078125, magnitude of gradient - 0.07289593481262502\n",
      "Step - 6051, Loss - 0.347873809632646, Learning Rate - 0.00078125, magnitude of gradient - 0.05568765165836542\n",
      "Step - 6052, Loss - 0.3079729463164162, Learning Rate - 0.00078125, magnitude of gradient - 0.0203051520579642\n",
      "Step - 6053, Loss - 0.31423797140691184, Learning Rate - 0.00078125, magnitude of gradient - 0.014457865894364815\n",
      "Step - 6054, Loss - 0.42869524879859516, Learning Rate - 0.00078125, magnitude of gradient - 0.07389935140138637\n",
      "Step - 6055, Loss - 0.33647378946227746, Learning Rate - 0.00078125, magnitude of gradient - 0.022012237798702806\n",
      "Step - 6056, Loss - 0.31927139198499604, Learning Rate - 0.00078125, magnitude of gradient - 0.01404940281097732\n",
      "Step - 6057, Loss - 0.3523647772389682, Learning Rate - 0.00078125, magnitude of gradient - 0.0412886722117443\n",
      "Step - 6058, Loss - 0.2995365933634154, Learning Rate - 0.00078125, magnitude of gradient - 0.09020653761272182\n",
      "Step - 6059, Loss - 0.39637110244061363, Learning Rate - 0.00078125, magnitude of gradient - 0.04000127785849497\n",
      "Step - 6060, Loss - 0.3651746459310208, Learning Rate - 0.00078125, magnitude of gradient - 0.01949035290719714\n",
      "Step - 6061, Loss - 0.2610158689082728, Learning Rate - 0.00078125, magnitude of gradient - 0.08901476550851443\n",
      "Step - 6062, Loss - 0.3116234521094525, Learning Rate - 0.00078125, magnitude of gradient - 0.037435630202779416\n",
      "Step - 6063, Loss - 0.2827844007848525, Learning Rate - 0.00078125, magnitude of gradient - 0.036772053823358304\n",
      "Step - 6064, Loss - 0.307131265540249, Learning Rate - 0.00078125, magnitude of gradient - 0.024228270539903698\n",
      "Step - 6065, Loss - 0.3688874205922088, Learning Rate - 0.00078125, magnitude of gradient - 0.04377684047785524\n",
      "Step - 6066, Loss - 0.28692452565373283, Learning Rate - 0.00078125, magnitude of gradient - 0.02847359508675362\n",
      "Step - 6067, Loss - 0.3241264038506968, Learning Rate - 0.00078125, magnitude of gradient - 0.05497507421873479\n",
      "Step - 6068, Loss - 0.33620194399830594, Learning Rate - 0.00078125, magnitude of gradient - 0.04706386792337215\n",
      "Step - 6069, Loss - 0.33564770014717304, Learning Rate - 0.00078125, magnitude of gradient - 0.05259442218908876\n",
      "Step - 6070, Loss - 0.39301425449554056, Learning Rate - 0.00078125, magnitude of gradient - 0.07250357757024693\n",
      "Step - 6071, Loss - 0.29972954744338715, Learning Rate - 0.00078125, magnitude of gradient - 0.04084045252802126\n",
      "Step - 6072, Loss - 0.28122682253443804, Learning Rate - 0.00078125, magnitude of gradient - 0.0693222897941616\n",
      "Step - 6073, Loss - 0.3222289212614706, Learning Rate - 0.00078125, magnitude of gradient - 0.0689657465176498\n",
      "Step - 6074, Loss - 0.258579814666105, Learning Rate - 0.00078125, magnitude of gradient - 0.047223532743105014\n",
      "Step - 6075, Loss - 0.3395148964991911, Learning Rate - 0.00078125, magnitude of gradient - 0.05705847163830027\n",
      "Step - 6076, Loss - 0.39318490675438456, Learning Rate - 0.00078125, magnitude of gradient - 0.09309986655292696\n",
      "Step - 6077, Loss - 0.41262846529810054, Learning Rate - 0.00078125, magnitude of gradient - 0.04784968282235808\n",
      "Step - 6078, Loss - 0.34156601074314763, Learning Rate - 0.00078125, magnitude of gradient - 0.035814736797467925\n",
      "Step - 6079, Loss - 0.35049806207944045, Learning Rate - 0.00078125, magnitude of gradient - 0.04449302224851834\n",
      "Step - 6080, Loss - 0.33086117294471395, Learning Rate - 0.00078125, magnitude of gradient - 0.1266769009421796\n",
      "Step - 6081, Loss - 0.3385142119082205, Learning Rate - 0.00078125, magnitude of gradient - 0.04831285627977665\n",
      "Step - 6082, Loss - 0.2719080688823692, Learning Rate - 0.00078125, magnitude of gradient - 0.06896673198953539\n",
      "Step - 6083, Loss - 0.3425596340718983, Learning Rate - 0.00078125, magnitude of gradient - 0.084188762056216\n",
      "Step - 6084, Loss - 0.317772032408567, Learning Rate - 0.00078125, magnitude of gradient - 0.06038438758616235\n",
      "Step - 6085, Loss - 0.41979899351338323, Learning Rate - 0.00078125, magnitude of gradient - 0.06621535872457188\n",
      "Step - 6086, Loss - 0.32914588819528096, Learning Rate - 0.00078125, magnitude of gradient - 0.0685434181481652\n",
      "Step - 6087, Loss - 0.3244923274572206, Learning Rate - 0.00078125, magnitude of gradient - 0.05091438731647407\n",
      "Step - 6088, Loss - 0.35678453229837953, Learning Rate - 0.00078125, magnitude of gradient - 0.04650810062137364\n",
      "Step - 6089, Loss - 0.20938183122048598, Learning Rate - 0.00078125, magnitude of gradient - 0.048055771027197644\n",
      "Step - 6090, Loss - 0.36627107533367204, Learning Rate - 0.00078125, magnitude of gradient - 0.0426133788220023\n",
      "Step - 6091, Loss - 0.30004623765055005, Learning Rate - 0.00078125, magnitude of gradient - 0.06294622964205984\n",
      "Step - 6092, Loss - 0.31834885065487933, Learning Rate - 0.00078125, magnitude of gradient - 0.03406022213199225\n",
      "Step - 6093, Loss - 0.27867053871730196, Learning Rate - 0.00078125, magnitude of gradient - 0.06504300077661\n",
      "Step - 6094, Loss - 0.31347563495667385, Learning Rate - 0.00078125, magnitude of gradient - 0.024370151806427355\n",
      "Step - 6095, Loss - 0.284794468471626, Learning Rate - 0.00078125, magnitude of gradient - 0.005451837189456799\n",
      "Step - 6096, Loss - 0.3194137568634714, Learning Rate - 0.00078125, magnitude of gradient - 0.05315656809477315\n",
      "Step - 6097, Loss - 0.3720355626433248, Learning Rate - 0.00078125, magnitude of gradient - 0.10769158318424017\n",
      "Step - 6098, Loss - 0.4000909828840187, Learning Rate - 0.00078125, magnitude of gradient - 0.026004712009813455\n",
      "Step - 6099, Loss - 0.32722645807452555, Learning Rate - 0.00078125, magnitude of gradient - 0.08269709449391421\n",
      "Step - 6100, Loss - 0.29407020918876464, Learning Rate - 0.00078125, magnitude of gradient - 0.06048335067148131\n",
      "Step - 6101, Loss - 0.2681051904600309, Learning Rate - 0.00078125, magnitude of gradient - 0.08362059779863473\n",
      "Step - 6102, Loss - 0.29668245690756057, Learning Rate - 0.00078125, magnitude of gradient - 0.03660343407504117\n",
      "Step - 6103, Loss - 0.2650088929727627, Learning Rate - 0.00078125, magnitude of gradient - 0.0060559002634943074\n",
      "Step - 6104, Loss - 0.31967133948068505, Learning Rate - 0.00078125, magnitude of gradient - 0.05114052645682559\n",
      "Step - 6105, Loss - 0.31665317339825566, Learning Rate - 0.00078125, magnitude of gradient - 0.02781261653947334\n",
      "Step - 6106, Loss - 0.3357790262864387, Learning Rate - 0.00078125, magnitude of gradient - 0.0550297662307834\n",
      "Step - 6107, Loss - 0.37956308760059015, Learning Rate - 0.00078125, magnitude of gradient - 0.019628623614103148\n",
      "Step - 6108, Loss - 0.4094019927442253, Learning Rate - 0.00078125, magnitude of gradient - 0.06526617786598768\n",
      "Step - 6109, Loss - 0.2982444667117825, Learning Rate - 0.00078125, magnitude of gradient - 0.0569002902486598\n",
      "Step - 6110, Loss - 0.30967571079595896, Learning Rate - 0.00078125, magnitude of gradient - 0.027460515118589223\n",
      "Step - 6111, Loss - 0.4226512265079273, Learning Rate - 0.00078125, magnitude of gradient - 0.0010529081250473212\n",
      "Step - 6112, Loss - 0.2964907332846473, Learning Rate - 0.00078125, magnitude of gradient - 0.03967443036381378\n",
      "Step - 6113, Loss - 0.2566738572493999, Learning Rate - 0.00078125, magnitude of gradient - 0.06870781303221166\n",
      "Step - 6114, Loss - 0.28241154639975113, Learning Rate - 0.00078125, magnitude of gradient - 0.01156091609995909\n",
      "Step - 6115, Loss - 0.274961811282151, Learning Rate - 0.00078125, magnitude of gradient - 0.10121502534076318\n",
      "Step - 6116, Loss - 0.3401616030395279, Learning Rate - 0.00078125, magnitude of gradient - 0.04896966369613095\n",
      "Step - 6117, Loss - 0.30109687517634814, Learning Rate - 0.00078125, magnitude of gradient - 0.08554385315411636\n",
      "Step - 6118, Loss - 0.29528765716615424, Learning Rate - 0.00078125, magnitude of gradient - 0.06364575749350082\n",
      "Step - 6119, Loss - 0.39953252516098825, Learning Rate - 0.00078125, magnitude of gradient - 0.08818665199042089\n",
      "Step - 6120, Loss - 0.34815626792555326, Learning Rate - 0.00078125, magnitude of gradient - 0.03687488297199669\n",
      "Step - 6121, Loss - 0.35592640103138856, Learning Rate - 0.00078125, magnitude of gradient - 0.06708353420272566\n",
      "Step - 6122, Loss - 0.2734120966804293, Learning Rate - 0.00078125, magnitude of gradient - 0.024518839962004917\n",
      "Step - 6123, Loss - 0.2734427097152578, Learning Rate - 0.00078125, magnitude of gradient - 0.03356037151266897\n",
      "Step - 6124, Loss - 0.3973374769273182, Learning Rate - 0.00078125, magnitude of gradient - 0.05527662958655977\n",
      "Step - 6125, Loss - 0.2927829666283642, Learning Rate - 0.00078125, magnitude of gradient - 0.05233965417231943\n",
      "Step - 6126, Loss - 0.3277611148492858, Learning Rate - 0.00078125, magnitude of gradient - 0.05071220594868312\n",
      "Step - 6127, Loss - 0.35969136317155703, Learning Rate - 0.00078125, magnitude of gradient - 0.08192148411025596\n",
      "Step - 6128, Loss - 0.33871225766084195, Learning Rate - 0.00078125, magnitude of gradient - 0.014782200072046926\n",
      "Step - 6129, Loss - 0.2676960949726833, Learning Rate - 0.00078125, magnitude of gradient - 0.056330870523514466\n",
      "Step - 6130, Loss - 0.3446761344476476, Learning Rate - 0.00078125, magnitude of gradient - 0.04339384958414641\n",
      "Step - 6131, Loss - 0.34404891779373303, Learning Rate - 0.00078125, magnitude of gradient - 0.07533590774572667\n",
      "Step - 6132, Loss - 0.3463918331323489, Learning Rate - 0.00078125, magnitude of gradient - 0.0362280156048238\n",
      "Step - 6133, Loss - 0.42327501524629985, Learning Rate - 0.00078125, magnitude of gradient - 0.07155761395530119\n",
      "Step - 6134, Loss - 0.2750009482425, Learning Rate - 0.00078125, magnitude of gradient - 0.0720749337904814\n",
      "Step - 6135, Loss - 0.2909085557742408, Learning Rate - 0.00078125, magnitude of gradient - 0.014102599558593687\n",
      "Step - 6136, Loss - 0.3459894250805967, Learning Rate - 0.00078125, magnitude of gradient - 0.0378376731902927\n",
      "Step - 6137, Loss - 0.39482689090354045, Learning Rate - 0.00078125, magnitude of gradient - 0.03982058554771272\n",
      "Step - 6138, Loss - 0.3044335290859749, Learning Rate - 0.00078125, magnitude of gradient - 0.0773333873247855\n",
      "Step - 6139, Loss - 0.31186925297593765, Learning Rate - 0.00078125, magnitude of gradient - 0.07874511459423593\n",
      "Step - 6140, Loss - 0.2852889489281828, Learning Rate - 0.00078125, magnitude of gradient - 0.04499215256096651\n",
      "Step - 6141, Loss - 0.3612303787617174, Learning Rate - 0.00078125, magnitude of gradient - 0.020022021173298127\n",
      "Step - 6142, Loss - 0.2989574784737967, Learning Rate - 0.00078125, magnitude of gradient - 0.042980695340260194\n",
      "Step - 6143, Loss - 0.40928217592882843, Learning Rate - 0.00078125, magnitude of gradient - 0.04084989260960447\n",
      "Step - 6144, Loss - 0.2972708820110111, Learning Rate - 0.00078125, magnitude of gradient - 0.11393136382393451\n",
      "Step - 6145, Loss - 0.37955053755191337, Learning Rate - 0.00078125, magnitude of gradient - 0.05476405829168742\n",
      "Step - 6146, Loss - 0.25039597118830415, Learning Rate - 0.00078125, magnitude of gradient - 0.023299484435366752\n",
      "Step - 6147, Loss - 0.3599009929233135, Learning Rate - 0.00078125, magnitude of gradient - 0.03416336241611543\n",
      "Step - 6148, Loss - 0.29918866256024884, Learning Rate - 0.00078125, magnitude of gradient - 0.060493074901301395\n",
      "Step - 6149, Loss - 0.3134853252038323, Learning Rate - 0.00078125, magnitude of gradient - 0.06004283251509164\n",
      "Step - 6150, Loss - 0.30008779509050554, Learning Rate - 0.00078125, magnitude of gradient - 0.06160446035051584\n",
      "Step - 6151, Loss - 0.3155173724606555, Learning Rate - 0.00078125, magnitude of gradient - 0.007648699170543503\n",
      "Step - 6152, Loss - 0.294672182913434, Learning Rate - 0.00078125, magnitude of gradient - 0.04231236042198629\n",
      "Step - 6153, Loss - 0.3504196175222451, Learning Rate - 0.00078125, magnitude of gradient - 0.08118044347794649\n",
      "Step - 6154, Loss - 0.2715858335813115, Learning Rate - 0.00078125, magnitude of gradient - 0.05864271540347915\n",
      "Step - 6155, Loss - 0.24808186737721172, Learning Rate - 0.00078125, magnitude of gradient - 0.10629009120130715\n",
      "Step - 6156, Loss - 0.30236413793244504, Learning Rate - 0.00078125, magnitude of gradient - 0.09228467480454602\n",
      "Step - 6157, Loss - 0.31438301631373566, Learning Rate - 0.00078125, magnitude of gradient - 0.06684112891338116\n",
      "Step - 6158, Loss - 0.3339265012730685, Learning Rate - 0.00078125, magnitude of gradient - 0.06763254485101682\n",
      "Step - 6159, Loss - 0.28820548045164407, Learning Rate - 0.00078125, magnitude of gradient - 0.03066189395737004\n",
      "Step - 6160, Loss - 0.27768769903032153, Learning Rate - 0.00078125, magnitude of gradient - 0.06568149775972985\n",
      "Step - 6161, Loss - 0.42181336818272663, Learning Rate - 0.00078125, magnitude of gradient - 0.0466945698852791\n",
      "Step - 6162, Loss - 0.2887781689243818, Learning Rate - 0.00078125, magnitude of gradient - 0.06773506197181711\n",
      "Step - 6163, Loss - 0.33557433499721395, Learning Rate - 0.00078125, magnitude of gradient - 0.07880517688146724\n",
      "Step - 6164, Loss - 0.40558626240113455, Learning Rate - 0.00078125, magnitude of gradient - 0.10554903511892547\n",
      "Step - 6165, Loss - 0.29462858118514557, Learning Rate - 0.00078125, magnitude of gradient - 0.015205136653945508\n",
      "Step - 6166, Loss - 0.392540696482818, Learning Rate - 0.00078125, magnitude of gradient - 0.014064650076031958\n",
      "Step - 6167, Loss - 0.3442156682123529, Learning Rate - 0.00078125, magnitude of gradient - 0.06481326088189157\n",
      "Step - 6168, Loss - 0.24937223734141176, Learning Rate - 0.00078125, magnitude of gradient - 0.026975066592798722\n",
      "Step - 6169, Loss - 0.2723966312169198, Learning Rate - 0.00078125, magnitude of gradient - 0.07895538043622853\n",
      "Step - 6170, Loss - 0.3469009944565012, Learning Rate - 0.00078125, magnitude of gradient - 0.02932673592379456\n",
      "Step - 6171, Loss - 0.32525202413096405, Learning Rate - 0.00078125, magnitude of gradient - 0.05711160375053322\n",
      "Step - 6172, Loss - 0.3499702209876651, Learning Rate - 0.00078125, magnitude of gradient - 0.01232894837684143\n",
      "Step - 6173, Loss - 0.3807447310191985, Learning Rate - 0.00078125, magnitude of gradient - 0.053368678451575086\n",
      "Step - 6174, Loss - 0.26854337703803366, Learning Rate - 0.00078125, magnitude of gradient - 0.016114462816849896\n",
      "Step - 6175, Loss - 0.33614218767714843, Learning Rate - 0.00078125, magnitude of gradient - 0.026499299826380343\n",
      "Step - 6176, Loss - 0.2765780589011726, Learning Rate - 0.00078125, magnitude of gradient - 0.055200426078327154\n",
      "Step - 6177, Loss - 0.31605401776694053, Learning Rate - 0.00078125, magnitude of gradient - 0.05238556935395847\n",
      "Step - 6178, Loss - 0.21096246478461383, Learning Rate - 0.00078125, magnitude of gradient - 0.06034039168568983\n",
      "Step - 6179, Loss - 0.3072596001829258, Learning Rate - 0.00078125, magnitude of gradient - 0.047575528702471864\n",
      "Step - 6180, Loss - 0.3074976954193455, Learning Rate - 0.00078125, magnitude of gradient - 0.07087592444240838\n",
      "Step - 6181, Loss - 0.337195536477916, Learning Rate - 0.00078125, magnitude of gradient - 0.05870407941784209\n",
      "Step - 6182, Loss - 0.3441822492484626, Learning Rate - 0.00078125, magnitude of gradient - 0.004505865155961095\n",
      "Step - 6183, Loss - 0.3251441593044872, Learning Rate - 0.00078125, magnitude of gradient - 0.015254833767380458\n",
      "Step - 6184, Loss - 0.31734960362994763, Learning Rate - 0.00078125, magnitude of gradient - 0.04149192372106357\n",
      "Step - 6185, Loss - 0.31787140875754427, Learning Rate - 0.00078125, magnitude of gradient - 0.05648585099862591\n",
      "Step - 6186, Loss - 0.3266934587388135, Learning Rate - 0.00078125, magnitude of gradient - 0.03453228061023361\n",
      "Step - 6187, Loss - 0.3482283005474864, Learning Rate - 0.00078125, magnitude of gradient - 0.05634637977620299\n",
      "Step - 6188, Loss - 0.31466120132099396, Learning Rate - 0.00078125, magnitude of gradient - 0.07294120046224715\n",
      "Step - 6189, Loss - 0.3275315885078566, Learning Rate - 0.00078125, magnitude of gradient - 0.042899117777881184\n",
      "Step - 6190, Loss - 0.3249777500994844, Learning Rate - 0.00078125, magnitude of gradient - 0.04320084625750577\n",
      "Step - 6191, Loss - 0.22535548075512335, Learning Rate - 0.00078125, magnitude of gradient - 0.048225276018417446\n",
      "Step - 6192, Loss - 0.30775213636558707, Learning Rate - 0.00078125, magnitude of gradient - 0.051300280427369144\n",
      "Step - 6193, Loss - 0.2982176574957307, Learning Rate - 0.00078125, magnitude of gradient - 0.02876010032644567\n",
      "Step - 6194, Loss - 0.24514475889563261, Learning Rate - 0.00078125, magnitude of gradient - 0.036903750319461\n",
      "Step - 6195, Loss - 0.3967502692662085, Learning Rate - 0.00078125, magnitude of gradient - 0.04955070009013741\n",
      "Step - 6196, Loss - 0.32519231121166037, Learning Rate - 0.00078125, magnitude of gradient - 0.027298457678493517\n",
      "Step - 6197, Loss - 0.26657769782058527, Learning Rate - 0.00078125, magnitude of gradient - 0.06539580298782692\n",
      "Step - 6198, Loss - 0.372906169165157, Learning Rate - 0.00078125, magnitude of gradient - 0.0158433555734186\n",
      "Step - 6199, Loss - 0.30686698841166643, Learning Rate - 0.00078125, magnitude of gradient - 0.008500363386166703\n",
      "Step - 6200, Loss - 0.29484151735289715, Learning Rate - 0.00078125, magnitude of gradient - 0.03880492391469378\n",
      "Step - 6201, Loss - 0.31183078193238284, Learning Rate - 0.00078125, magnitude of gradient - 0.061749506022235216\n",
      "Step - 6202, Loss - 0.3061132090504223, Learning Rate - 0.00078125, magnitude of gradient - 0.059421285948130814\n",
      "Step - 6203, Loss - 0.24333616048419154, Learning Rate - 0.00078125, magnitude of gradient - 0.05473028098802915\n",
      "Step - 6204, Loss - 0.42334316200781685, Learning Rate - 0.00078125, magnitude of gradient - 0.043239736265150985\n",
      "Step - 6205, Loss - 0.39746578307643315, Learning Rate - 0.00078125, magnitude of gradient - 0.05392305270629244\n",
      "Step - 6206, Loss - 0.3242864924276381, Learning Rate - 0.00078125, magnitude of gradient - 0.02888338956769324\n",
      "Step - 6207, Loss - 0.3718426567984288, Learning Rate - 0.00078125, magnitude of gradient - 0.05352688457801533\n",
      "Step - 6208, Loss - 0.3379037831102928, Learning Rate - 0.00078125, magnitude of gradient - 0.05751928675749033\n",
      "Step - 6209, Loss - 0.2682622534288832, Learning Rate - 0.00078125, magnitude of gradient - 0.055327332537195385\n",
      "Step - 6210, Loss - 0.33969932024838956, Learning Rate - 0.00078125, magnitude of gradient - 0.009540081237597161\n",
      "Step - 6211, Loss - 0.2546140391737768, Learning Rate - 0.00078125, magnitude of gradient - 0.04820524462552465\n",
      "Step - 6212, Loss - 0.3808348497206785, Learning Rate - 0.00078125, magnitude of gradient - 0.06391594510297398\n",
      "Step - 6213, Loss - 0.3972554401226064, Learning Rate - 0.00078125, magnitude of gradient - 0.018737107207604946\n",
      "Step - 6214, Loss - 0.3258314115407638, Learning Rate - 0.00078125, magnitude of gradient - 0.0806965918085268\n",
      "Step - 6215, Loss - 0.3084095108303689, Learning Rate - 0.00078125, magnitude of gradient - 0.009011104824951578\n",
      "Step - 6216, Loss - 0.3001342329242416, Learning Rate - 0.00078125, magnitude of gradient - 0.04568498305613285\n",
      "Step - 6217, Loss - 0.27760206249904607, Learning Rate - 0.00078125, magnitude of gradient - 0.07964547063542947\n",
      "Step - 6218, Loss - 0.3257089247422285, Learning Rate - 0.00078125, magnitude of gradient - 0.024290377698539395\n",
      "Step - 6219, Loss - 0.3688898506257795, Learning Rate - 0.00078125, magnitude of gradient - 0.09635593095912541\n",
      "Step - 6220, Loss - 0.30277679420448494, Learning Rate - 0.00078125, magnitude of gradient - 0.058430252801603945\n",
      "Step - 6221, Loss - 0.3551659479508239, Learning Rate - 0.00078125, magnitude of gradient - 0.004036756059451756\n",
      "Step - 6222, Loss - 0.3098950560011101, Learning Rate - 0.00078125, magnitude of gradient - 0.03726339260539116\n",
      "Step - 6223, Loss - 0.322031019614617, Learning Rate - 0.00078125, magnitude of gradient - 0.04334682179731169\n",
      "Step - 6224, Loss - 0.315100977678183, Learning Rate - 0.00078125, magnitude of gradient - 0.02125658075974797\n",
      "Step - 6225, Loss - 0.36186974504277836, Learning Rate - 0.00078125, magnitude of gradient - 0.09177378730532926\n",
      "Step - 6226, Loss - 0.3547191672202151, Learning Rate - 0.00078125, magnitude of gradient - 0.041050361170748485\n",
      "Step - 6227, Loss - 0.3906643333709255, Learning Rate - 0.00078125, magnitude of gradient - 0.03953643195481273\n",
      "Step - 6228, Loss - 0.2960217046274988, Learning Rate - 0.00078125, magnitude of gradient - 0.04519216090984085\n",
      "Step - 6229, Loss - 0.3167466393056396, Learning Rate - 0.00078125, magnitude of gradient - 0.019748025942426823\n",
      "Step - 6230, Loss - 0.27131791759419177, Learning Rate - 0.00078125, magnitude of gradient - 0.00430248751938343\n",
      "Step - 6231, Loss - 0.3031841871835916, Learning Rate - 0.00078125, magnitude of gradient - 0.05808011811180968\n",
      "Step - 6232, Loss - 0.4151195929206565, Learning Rate - 0.00078125, magnitude of gradient - 0.11552330764223663\n",
      "Step - 6233, Loss - 0.3098748593270076, Learning Rate - 0.00078125, magnitude of gradient - 0.04471533241048848\n",
      "Step - 6234, Loss - 0.34664949183113514, Learning Rate - 0.00078125, magnitude of gradient - 0.037122051218444974\n",
      "Step - 6235, Loss - 0.3153087737029648, Learning Rate - 0.00078125, magnitude of gradient - 0.011634854713505931\n",
      "Step - 6236, Loss - 0.3160028345059016, Learning Rate - 0.00078125, magnitude of gradient - 0.030922659466612597\n",
      "Step - 6237, Loss - 0.32297762690473664, Learning Rate - 0.00078125, magnitude of gradient - 0.05048859507237929\n",
      "Step - 6238, Loss - 0.2877322996301799, Learning Rate - 0.00078125, magnitude of gradient - 0.040488719128893534\n",
      "Step - 6239, Loss - 0.3030145545095805, Learning Rate - 0.00078125, magnitude of gradient - 0.02372582337556414\n",
      "Step - 6240, Loss - 0.3021892493072152, Learning Rate - 0.00078125, magnitude of gradient - 0.10569135344763\n",
      "Step - 6241, Loss - 0.30272045488259725, Learning Rate - 0.00078125, magnitude of gradient - 0.07207917173368002\n",
      "Step - 6242, Loss - 0.3243218649917833, Learning Rate - 0.00078125, magnitude of gradient - 0.05855318214996259\n",
      "Step - 6243, Loss - 0.3898474374303223, Learning Rate - 0.00078125, magnitude of gradient - 0.042762790793562745\n",
      "Step - 6244, Loss - 0.3212938866454974, Learning Rate - 0.00078125, magnitude of gradient - 0.022376000391941438\n",
      "Step - 6245, Loss - 0.3135925438457887, Learning Rate - 0.00078125, magnitude of gradient - 0.023218655451483283\n",
      "Step - 6246, Loss - 0.3507843012058555, Learning Rate - 0.00078125, magnitude of gradient - 0.025355707918493755\n",
      "Step - 6247, Loss - 0.2925482061047565, Learning Rate - 0.00078125, magnitude of gradient - 0.09469136796319114\n",
      "Step - 6248, Loss - 0.3037736180327185, Learning Rate - 0.00078125, magnitude of gradient - 0.016364350306605366\n",
      "Step - 6249, Loss - 0.2693889732987368, Learning Rate - 0.00078125, magnitude of gradient - 0.015982474898151883\n",
      "Step - 6250, Loss - 0.3745497261493692, Learning Rate - 0.00078125, magnitude of gradient - 0.038269634738641145\n",
      "Step - 6251, Loss - 0.3652639691594372, Learning Rate - 0.00078125, magnitude of gradient - 0.025459344991048363\n",
      "Step - 6252, Loss - 0.3241219216353136, Learning Rate - 0.00078125, magnitude of gradient - 0.06022495128732362\n",
      "Step - 6253, Loss - 0.32474507745417847, Learning Rate - 0.00078125, magnitude of gradient - 0.042903962428233786\n",
      "Step - 6254, Loss - 0.3360268366405705, Learning Rate - 0.00078125, magnitude of gradient - 0.037175206914548745\n",
      "Step - 6255, Loss - 0.30958194661546873, Learning Rate - 0.00078125, magnitude of gradient - 0.0509307471888415\n",
      "Step - 6256, Loss - 0.2935635652144648, Learning Rate - 0.00078125, magnitude of gradient - 0.013334187756192904\n",
      "Step - 6257, Loss - 0.3463716128644835, Learning Rate - 0.00078125, magnitude of gradient - 0.0058661040929012375\n",
      "Step - 6258, Loss - 0.3213157451292442, Learning Rate - 0.00078125, magnitude of gradient - 0.07655003389591535\n",
      "Step - 6259, Loss - 0.3569688008542582, Learning Rate - 0.00078125, magnitude of gradient - 0.0194084101324096\n",
      "Step - 6260, Loss - 0.28900871881531354, Learning Rate - 0.00078125, magnitude of gradient - 0.043605619051642314\n",
      "Step - 6261, Loss - 0.34058398748467966, Learning Rate - 0.00078125, magnitude of gradient - 0.05827808503174053\n",
      "Step - 6262, Loss - 0.331666983995331, Learning Rate - 0.00078125, magnitude of gradient - 0.09261304436998849\n",
      "Step - 6263, Loss - 0.3750954658370381, Learning Rate - 0.00078125, magnitude of gradient - 0.11996800649391773\n",
      "Step - 6264, Loss - 0.21788523084700784, Learning Rate - 0.00078125, magnitude of gradient - 0.05310793994855732\n",
      "Step - 6265, Loss - 0.2893033171844029, Learning Rate - 0.00078125, magnitude of gradient - 0.12256356233244853\n",
      "Step - 6266, Loss - 0.2509553290587937, Learning Rate - 0.00078125, magnitude of gradient - 0.045677916752201723\n",
      "Step - 6267, Loss - 0.3853248847716661, Learning Rate - 0.00078125, magnitude of gradient - 0.07812511613118854\n",
      "Step - 6268, Loss - 0.32941753777116356, Learning Rate - 0.00078125, magnitude of gradient - 0.08773516081548312\n",
      "Step - 6269, Loss - 0.27698183522570163, Learning Rate - 0.00078125, magnitude of gradient - 0.06137373917701789\n",
      "Step - 6270, Loss - 0.3333261742344927, Learning Rate - 0.00078125, magnitude of gradient - 0.028743612179420067\n",
      "Step - 6271, Loss - 0.41202259922457574, Learning Rate - 0.00078125, magnitude of gradient - 0.09396238493882345\n",
      "Step - 6272, Loss - 0.33306474331266966, Learning Rate - 0.00078125, magnitude of gradient - 0.027977248542394265\n",
      "Step - 6273, Loss - 0.3672251342582005, Learning Rate - 0.00078125, magnitude of gradient - 0.03161569649458275\n",
      "Step - 6274, Loss - 0.3108590388916772, Learning Rate - 0.00078125, magnitude of gradient - 0.08802861747164391\n",
      "Step - 6275, Loss - 0.3191808884103576, Learning Rate - 0.00078125, magnitude of gradient - 0.04423879533495875\n",
      "Step - 6276, Loss - 0.3040752526908204, Learning Rate - 0.00078125, magnitude of gradient - 0.004312845216165364\n",
      "Step - 6277, Loss - 0.2871408328168949, Learning Rate - 0.00078125, magnitude of gradient - 0.06624658555697936\n",
      "Step - 6278, Loss - 0.2567662000239776, Learning Rate - 0.00078125, magnitude of gradient - 0.006562465118758982\n",
      "Step - 6279, Loss - 0.2017337464493734, Learning Rate - 0.00078125, magnitude of gradient - 0.050768011207679105\n",
      "Step - 6280, Loss - 0.3714519188011078, Learning Rate - 0.00078125, magnitude of gradient - 0.021631226296042187\n",
      "Step - 6281, Loss - 0.43657642836700056, Learning Rate - 0.00078125, magnitude of gradient - 0.05432925317472313\n",
      "Step - 6282, Loss - 0.34588511989341675, Learning Rate - 0.00078125, magnitude of gradient - 0.03955342020255328\n",
      "Step - 6283, Loss - 0.33026549001138417, Learning Rate - 0.00078125, magnitude of gradient - 0.04906623900672993\n",
      "Step - 6284, Loss - 0.2940628827814428, Learning Rate - 0.00078125, magnitude of gradient - 0.04906636573341828\n",
      "Step - 6285, Loss - 0.2664321885012355, Learning Rate - 0.00078125, magnitude of gradient - 0.05580707072698547\n",
      "Step - 6286, Loss - 0.400489902993095, Learning Rate - 0.00078125, magnitude of gradient - 0.022475034892514956\n",
      "Step - 6287, Loss - 0.3272367659229242, Learning Rate - 0.00078125, magnitude of gradient - 0.05376914263385407\n",
      "Step - 6288, Loss - 0.30773427014055466, Learning Rate - 0.00078125, magnitude of gradient - 0.060576477555063644\n",
      "Step - 6289, Loss - 0.4070213956575155, Learning Rate - 0.00078125, magnitude of gradient - 0.0627372613268645\n",
      "Step - 6290, Loss - 0.3525666785209791, Learning Rate - 0.00078125, magnitude of gradient - 0.03847117114910054\n",
      "Step - 6291, Loss - 0.2940952503430928, Learning Rate - 0.00078125, magnitude of gradient - 0.036742462770749135\n",
      "Step - 6292, Loss - 0.3654646723160359, Learning Rate - 0.00078125, magnitude of gradient - 0.006625533725895793\n",
      "Step - 6293, Loss - 0.3480537337895424, Learning Rate - 0.00078125, magnitude of gradient - 0.019338018903392835\n",
      "Step - 6294, Loss - 0.3137067954288738, Learning Rate - 0.00078125, magnitude of gradient - 0.07412197659991379\n",
      "Step - 6295, Loss - 0.3234881468433456, Learning Rate - 0.00078125, magnitude of gradient - 0.03236431352585288\n",
      "Step - 6296, Loss - 0.4007702462479169, Learning Rate - 0.00078125, magnitude of gradient - 0.04077532745034837\n",
      "Step - 6297, Loss - 0.28687539925187777, Learning Rate - 0.00078125, magnitude of gradient - 0.055668196480273095\n",
      "Step - 6298, Loss - 0.30997405886187945, Learning Rate - 0.00078125, magnitude of gradient - 0.07362877241780892\n",
      "Step - 6299, Loss - 0.36860133591533556, Learning Rate - 0.00078125, magnitude of gradient - 0.07564911472100586\n",
      "Step - 6300, Loss - 0.22016227337225663, Learning Rate - 0.00078125, magnitude of gradient - 0.01423460344904331\n",
      "Step - 6301, Loss - 0.2843058038819326, Learning Rate - 0.00078125, magnitude of gradient - 0.030412964047429766\n",
      "Step - 6302, Loss - 0.2749866844026461, Learning Rate - 0.00078125, magnitude of gradient - 0.04401329337152785\n",
      "Step - 6303, Loss - 0.33139974029631347, Learning Rate - 0.00078125, magnitude of gradient - 0.015174012318925904\n",
      "Step - 6304, Loss - 0.4009340110914848, Learning Rate - 0.00078125, magnitude of gradient - 0.05987893999932495\n",
      "Step - 6305, Loss - 0.3537532626272574, Learning Rate - 0.00078125, magnitude of gradient - 0.05379446495793919\n",
      "Step - 6306, Loss - 0.2842236065349495, Learning Rate - 0.00078125, magnitude of gradient - 0.05147774881924668\n",
      "Step - 6307, Loss - 0.3884208534763793, Learning Rate - 0.00078125, magnitude of gradient - 0.03358759668988391\n",
      "Step - 6308, Loss - 0.3077730675079341, Learning Rate - 0.00078125, magnitude of gradient - 0.009976413362759498\n",
      "Step - 6309, Loss - 0.3668379366652771, Learning Rate - 0.00078125, magnitude of gradient - 0.04307949882974302\n",
      "Step - 6310, Loss - 0.3078470210602964, Learning Rate - 0.00078125, magnitude of gradient - 0.0075952392791911435\n",
      "Step - 6311, Loss - 0.31934270454555436, Learning Rate - 0.00078125, magnitude of gradient - 0.03479020561540625\n",
      "Step - 6312, Loss - 0.304545985855793, Learning Rate - 0.00078125, magnitude of gradient - 0.04304450845300255\n",
      "Step - 6313, Loss - 0.3277816800335292, Learning Rate - 0.00078125, magnitude of gradient - 0.041238157687584424\n",
      "Step - 6314, Loss - 0.33748686208100964, Learning Rate - 0.00078125, magnitude of gradient - 0.02231725508702564\n",
      "Step - 6315, Loss - 0.33304276033153823, Learning Rate - 0.00078125, magnitude of gradient - 0.09682327980752775\n",
      "Step - 6316, Loss - 0.3071589538968686, Learning Rate - 0.00078125, magnitude of gradient - 0.05349310327740225\n",
      "Step - 6317, Loss - 0.2904444310552584, Learning Rate - 0.00078125, magnitude of gradient - 0.06804469872367289\n",
      "Step - 6318, Loss - 0.30747157887832427, Learning Rate - 0.00078125, magnitude of gradient - 0.06723435015134568\n",
      "Step - 6319, Loss - 0.32862155299488827, Learning Rate - 0.00078125, magnitude of gradient - 0.03212595790014891\n",
      "Step - 6320, Loss - 0.35804634962756016, Learning Rate - 0.00078125, magnitude of gradient - 0.053204193524133116\n",
      "Step - 6321, Loss - 0.2818111272745436, Learning Rate - 0.00078125, magnitude of gradient - 0.04655530501079611\n",
      "Step - 6322, Loss - 0.3412366292542798, Learning Rate - 0.00078125, magnitude of gradient - 0.024817648262333628\n",
      "Step - 6323, Loss - 0.33810056512357456, Learning Rate - 0.00078125, magnitude of gradient - 0.04692000217614017\n",
      "Step - 6324, Loss - 0.36161025323069773, Learning Rate - 0.00078125, magnitude of gradient - 0.07375155294081369\n",
      "Step - 6325, Loss - 0.3627376206737626, Learning Rate - 0.00078125, magnitude of gradient - 0.08169483858466266\n",
      "Step - 6326, Loss - 0.29585186618483406, Learning Rate - 0.00078125, magnitude of gradient - 0.025866112812495456\n",
      "Step - 6327, Loss - 0.30325900434073394, Learning Rate - 0.00078125, magnitude of gradient - 0.06713705205176783\n",
      "Step - 6328, Loss - 0.27408849747122266, Learning Rate - 0.00078125, magnitude of gradient - 0.037678769172978445\n",
      "Step - 6329, Loss - 0.31733348684385, Learning Rate - 0.00078125, magnitude of gradient - 0.06306696153822054\n",
      "Step - 6330, Loss - 0.28368349010873845, Learning Rate - 0.00078125, magnitude of gradient - 0.022212664836222595\n",
      "Step - 6331, Loss - 0.3639351953678622, Learning Rate - 0.00078125, magnitude of gradient - 0.04069350906266559\n",
      "Step - 6332, Loss - 0.34163428930239037, Learning Rate - 0.00078125, magnitude of gradient - 0.024633883067626636\n",
      "Step - 6333, Loss - 0.27198694833599923, Learning Rate - 0.00078125, magnitude of gradient - 0.013309695910251766\n",
      "Step - 6334, Loss - 0.38089977330718794, Learning Rate - 0.00078125, magnitude of gradient - 0.058916683869790924\n",
      "Step - 6335, Loss - 0.28272923067319333, Learning Rate - 0.00078125, magnitude of gradient - 0.02591215657968055\n",
      "Step - 6336, Loss - 0.38374150059304396, Learning Rate - 0.00078125, magnitude of gradient - 0.0426449598165674\n",
      "Step - 6337, Loss - 0.41260592597958773, Learning Rate - 0.00078125, magnitude of gradient - 0.019063164350866815\n",
      "Step - 6338, Loss - 0.3248109898794438, Learning Rate - 0.00078125, magnitude of gradient - 0.012617704066261688\n",
      "Step - 6339, Loss - 0.29126884448084156, Learning Rate - 0.00078125, magnitude of gradient - 0.06705404863864117\n",
      "Step - 6340, Loss - 0.3009949106839497, Learning Rate - 0.00078125, magnitude of gradient - 0.06975780233922241\n",
      "Step - 6341, Loss - 0.32816826674853555, Learning Rate - 0.00078125, magnitude of gradient - 0.051005443109384645\n",
      "Step - 6342, Loss - 0.3770251697378881, Learning Rate - 0.00078125, magnitude of gradient - 0.039040400102508\n",
      "Step - 6343, Loss - 0.3080304841400189, Learning Rate - 0.00078125, magnitude of gradient - 0.021007890720801448\n",
      "Step - 6344, Loss - 0.3017083336202816, Learning Rate - 0.00078125, magnitude of gradient - 0.07390667437381582\n",
      "Step - 6345, Loss - 0.3541644294487865, Learning Rate - 0.00078125, magnitude of gradient - 0.1096839231872592\n",
      "Step - 6346, Loss - 0.33959985312089386, Learning Rate - 0.00078125, magnitude of gradient - 0.0589542968585711\n",
      "Step - 6347, Loss - 0.28222611629149474, Learning Rate - 0.00078125, magnitude of gradient - 0.07174252846312619\n",
      "Step - 6348, Loss - 0.29933499540077635, Learning Rate - 0.00078125, magnitude of gradient - 0.014668060342780696\n",
      "Step - 6349, Loss - 0.3451336542020893, Learning Rate - 0.00078125, magnitude of gradient - 0.038804694625239715\n",
      "Step - 6350, Loss - 0.256661671252496, Learning Rate - 0.00078125, magnitude of gradient - 0.00832117811536375\n",
      "Step - 6351, Loss - 0.3142035764767147, Learning Rate - 0.00078125, magnitude of gradient - 0.05230727404084032\n",
      "Step - 6352, Loss - 0.27615254498960934, Learning Rate - 0.00078125, magnitude of gradient - 0.06340793210678637\n",
      "Step - 6353, Loss - 0.36225620398892716, Learning Rate - 0.00078125, magnitude of gradient - 0.039133346170314685\n",
      "Step - 6354, Loss - 0.25240810005296105, Learning Rate - 0.00078125, magnitude of gradient - 0.0801561193546308\n",
      "Step - 6355, Loss - 0.34865831321294727, Learning Rate - 0.00078125, magnitude of gradient - 0.0430067474677918\n",
      "Step - 6356, Loss - 0.36386126661207374, Learning Rate - 0.00078125, magnitude of gradient - 0.07326151028122412\n",
      "Step - 6357, Loss - 0.34496577403818485, Learning Rate - 0.00078125, magnitude of gradient - 0.045918455296141185\n",
      "Step - 6358, Loss - 0.30061281515290506, Learning Rate - 0.00078125, magnitude of gradient - 0.005870191410117621\n",
      "Step - 6359, Loss - 0.4270473143473254, Learning Rate - 0.00078125, magnitude of gradient - 0.07666957445220793\n",
      "Step - 6360, Loss - 0.342208960854119, Learning Rate - 0.00078125, magnitude of gradient - 0.0535103091879435\n",
      "Step - 6361, Loss - 0.30785569101333216, Learning Rate - 0.00078125, magnitude of gradient - 0.005195817491074284\n",
      "Step - 6362, Loss - 0.35620681177105895, Learning Rate - 0.00078125, magnitude of gradient - 0.08238229928792072\n",
      "Step - 6363, Loss - 0.319209975872798, Learning Rate - 0.00078125, magnitude of gradient - 0.05343832053711011\n",
      "Step - 6364, Loss - 0.28236783746582206, Learning Rate - 0.00078125, magnitude of gradient - 0.05606962969118655\n",
      "Step - 6365, Loss - 0.3141835214186346, Learning Rate - 0.00078125, magnitude of gradient - 0.032843692951227806\n",
      "Step - 6366, Loss - 0.44166959537244, Learning Rate - 0.00078125, magnitude of gradient - 0.06253634087192231\n",
      "Step - 6367, Loss - 0.3337800874604831, Learning Rate - 0.00078125, magnitude of gradient - 0.009762686442242752\n",
      "Step - 6368, Loss - 0.27397604889574506, Learning Rate - 0.00078125, magnitude of gradient - 0.09247232808776833\n",
      "Step - 6369, Loss - 0.312134611696523, Learning Rate - 0.00078125, magnitude of gradient - 0.08048369676316093\n",
      "Step - 6370, Loss - 0.31106166468304375, Learning Rate - 0.00078125, magnitude of gradient - 0.03770615852936716\n",
      "Step - 6371, Loss - 0.2586938301182758, Learning Rate - 0.00078125, magnitude of gradient - 0.08341534245670355\n",
      "Step - 6372, Loss - 0.31619740249764344, Learning Rate - 0.00078125, magnitude of gradient - 0.02835294427346404\n",
      "Step - 6373, Loss - 0.44839838073790744, Learning Rate - 0.00078125, magnitude of gradient - 0.07950315057667022\n",
      "Step - 6374, Loss - 0.351186998848538, Learning Rate - 0.00078125, magnitude of gradient - 0.06699619244690456\n",
      "Step - 6375, Loss - 0.3021931584847405, Learning Rate - 0.00078125, magnitude of gradient - 0.01923311746324965\n",
      "Step - 6376, Loss - 0.3812752433813065, Learning Rate - 0.00078125, magnitude of gradient - 0.03647073801089351\n",
      "Step - 6377, Loss - 0.31409100471742524, Learning Rate - 0.00078125, magnitude of gradient - 0.034593502979252906\n",
      "Step - 6378, Loss - 0.28701889710002426, Learning Rate - 0.00078125, magnitude of gradient - 0.05216824866291251\n",
      "Step - 6379, Loss - 0.2899634395664654, Learning Rate - 0.00078125, magnitude of gradient - 0.059253401736380815\n",
      "Step - 6380, Loss - 0.34866171519158634, Learning Rate - 0.00078125, magnitude of gradient - 0.0196967808838639\n",
      "Step - 6381, Loss - 0.28648036215275907, Learning Rate - 0.00078125, magnitude of gradient - 0.06601237761690526\n",
      "Step - 6382, Loss - 0.3194507085534921, Learning Rate - 0.00078125, magnitude of gradient - 0.07993568462655798\n",
      "Step - 6383, Loss - 0.41428762783279216, Learning Rate - 0.00078125, magnitude of gradient - 0.03441066055810699\n",
      "Step - 6384, Loss - 0.3513280272217822, Learning Rate - 0.00078125, magnitude of gradient - 0.03858653529127156\n",
      "Step - 6385, Loss - 0.31161454301848157, Learning Rate - 0.00078125, magnitude of gradient - 0.06778173831639553\n",
      "Step - 6386, Loss - 0.31074459977996166, Learning Rate - 0.00078125, magnitude of gradient - 0.062000121198378784\n",
      "Step - 6387, Loss - 0.3852872143162216, Learning Rate - 0.00078125, magnitude of gradient - 0.06196348780633079\n",
      "Step - 6388, Loss - 0.29654306904470434, Learning Rate - 0.00078125, magnitude of gradient - 0.02254257299451349\n",
      "Step - 6389, Loss - 0.27966826255920274, Learning Rate - 0.00078125, magnitude of gradient - 0.06521263870490084\n",
      "Step - 6390, Loss - 0.4163793778850286, Learning Rate - 0.00078125, magnitude of gradient - 0.011038001289219421\n",
      "Step - 6391, Loss - 0.36183103203335565, Learning Rate - 0.00078125, magnitude of gradient - 0.032662184366765004\n",
      "Step - 6392, Loss - 0.31029206641847656, Learning Rate - 0.00078125, magnitude of gradient - 0.059404927137465154\n",
      "Step - 6393, Loss - 0.3324599399202738, Learning Rate - 0.00078125, magnitude of gradient - 0.0679334756910902\n",
      "Step - 6394, Loss - 0.35799764634400777, Learning Rate - 0.00078125, magnitude of gradient - 0.08255660907937273\n",
      "Step - 6395, Loss - 0.36754976402256945, Learning Rate - 0.00078125, magnitude of gradient - 0.012627351677416045\n",
      "Step - 6396, Loss - 0.26857135288140727, Learning Rate - 0.00078125, magnitude of gradient - 0.02065489812566833\n",
      "Step - 6397, Loss - 0.32747167886408285, Learning Rate - 0.00078125, magnitude of gradient - 0.030410278251156986\n",
      "Step - 6398, Loss - 0.30216765273709456, Learning Rate - 0.00078125, magnitude of gradient - 0.012976297527577186\n",
      "Step - 6399, Loss - 0.4356437374624639, Learning Rate - 0.00078125, magnitude of gradient - 0.08015052814492198\n",
      "Step - 6400, Loss - 0.33695840804064386, Learning Rate - 0.00078125, magnitude of gradient - 0.029773342312014357\n",
      "Step - 6401, Loss - 0.3347930608349237, Learning Rate - 0.00078125, magnitude of gradient - 0.04573645462225764\n",
      "Step - 6402, Loss - 0.25320495891852735, Learning Rate - 0.00078125, magnitude of gradient - 0.08605261363163104\n",
      "Step - 6403, Loss - 0.3667775977129676, Learning Rate - 0.00078125, magnitude of gradient - 0.028677479965134933\n",
      "Step - 6404, Loss - 0.3775844660052775, Learning Rate - 0.00078125, magnitude of gradient - 0.07214568365788593\n",
      "Step - 6405, Loss - 0.2821772127835499, Learning Rate - 0.00078125, magnitude of gradient - 0.032300405629759374\n",
      "Step - 6406, Loss - 0.2827336979953439, Learning Rate - 0.00078125, magnitude of gradient - 0.04346824958721611\n",
      "Step - 6407, Loss - 0.3104099296878975, Learning Rate - 0.00078125, magnitude of gradient - 0.03316545393923607\n",
      "Step - 6408, Loss - 0.3687832782029578, Learning Rate - 0.00078125, magnitude of gradient - 0.059733257081554886\n",
      "Step - 6409, Loss - 0.2860340114479617, Learning Rate - 0.00078125, magnitude of gradient - 0.010871833910569152\n",
      "Step - 6410, Loss - 0.35021912931573496, Learning Rate - 0.00078125, magnitude of gradient - 0.04828105085310378\n",
      "Step - 6411, Loss - 0.3564525332759322, Learning Rate - 0.00078125, magnitude of gradient - 0.05086458280319238\n",
      "Step - 6412, Loss - 0.3619601411354993, Learning Rate - 0.00078125, magnitude of gradient - 0.012424964412063371\n",
      "Step - 6413, Loss - 0.34873584611404895, Learning Rate - 0.00078125, magnitude of gradient - 0.03910356532902511\n",
      "Step - 6414, Loss - 0.32144180213411566, Learning Rate - 0.00078125, magnitude of gradient - 0.07216382492225328\n",
      "Step - 6415, Loss - 0.22388035744168286, Learning Rate - 0.00078125, magnitude of gradient - 0.0571173620896711\n",
      "Step - 6416, Loss - 0.30000590117448706, Learning Rate - 0.00078125, magnitude of gradient - 0.022354487841343376\n",
      "Step - 6417, Loss - 0.31160105956060724, Learning Rate - 0.00078125, magnitude of gradient - 0.0455098534674064\n",
      "Step - 6418, Loss - 0.3383873866875292, Learning Rate - 0.00078125, magnitude of gradient - 0.06414099620685007\n",
      "Step - 6419, Loss - 0.29218147343074424, Learning Rate - 0.00078125, magnitude of gradient - 0.030207965965723185\n",
      "Step - 6420, Loss - 0.3309548644948898, Learning Rate - 0.00078125, magnitude of gradient - 0.11235275921833998\n",
      "Step - 6421, Loss - 0.34845996091079673, Learning Rate - 0.00078125, magnitude of gradient - 0.01897266773973392\n",
      "Step - 6422, Loss - 0.3541593088847295, Learning Rate - 0.00078125, magnitude of gradient - 0.040703252314281596\n",
      "Step - 6423, Loss - 0.27522107843361654, Learning Rate - 0.00078125, magnitude of gradient - 0.04856507061908974\n",
      "Step - 6424, Loss - 0.35349481020561374, Learning Rate - 0.00078125, magnitude of gradient - 0.033360011568981884\n",
      "Step - 6425, Loss - 0.3413015963953513, Learning Rate - 0.00078125, magnitude of gradient - 0.07746982932878481\n",
      "Step - 6426, Loss - 0.3421029269272716, Learning Rate - 0.00078125, magnitude of gradient - 0.09313180692250773\n",
      "Step - 6427, Loss - 0.34618008356745456, Learning Rate - 0.00078125, magnitude of gradient - 0.030397967703434812\n",
      "Step - 6428, Loss - 0.33113786859983385, Learning Rate - 0.00078125, magnitude of gradient - 0.030794033217226662\n",
      "Step - 6429, Loss - 0.29291988139177283, Learning Rate - 0.00078125, magnitude of gradient - 0.021627285368811568\n",
      "Step - 6430, Loss - 0.3617075583410746, Learning Rate - 0.00078125, magnitude of gradient - 0.023658974896564995\n",
      "Step - 6431, Loss - 0.36897276348035407, Learning Rate - 0.00078125, magnitude of gradient - 0.03782536780788528\n",
      "Step - 6432, Loss - 0.29884944607625985, Learning Rate - 0.00078125, magnitude of gradient - 0.03217690141048874\n",
      "Step - 6433, Loss - 0.3234893581420814, Learning Rate - 0.00078125, magnitude of gradient - 0.04808207030408416\n",
      "Step - 6434, Loss - 0.4020477887145567, Learning Rate - 0.00078125, magnitude of gradient - 0.036167224583776594\n",
      "Step - 6435, Loss - 0.3054002900710892, Learning Rate - 0.00078125, magnitude of gradient - 0.007341523281498574\n",
      "Step - 6436, Loss - 0.24512093125141549, Learning Rate - 0.00078125, magnitude of gradient - 0.03198164491275831\n",
      "Step - 6437, Loss - 0.32769978908405506, Learning Rate - 0.00078125, magnitude of gradient - 0.016188934577055246\n",
      "Step - 6438, Loss - 0.3678218363387571, Learning Rate - 0.00078125, magnitude of gradient - 0.09179122622191177\n",
      "Step - 6439, Loss - 0.38062699423151414, Learning Rate - 0.00078125, magnitude of gradient - 0.08108422605435893\n",
      "Step - 6440, Loss - 0.38089072042759214, Learning Rate - 0.00078125, magnitude of gradient - 0.06857304702990177\n",
      "Step - 6441, Loss - 0.32860923255253816, Learning Rate - 0.00078125, magnitude of gradient - 0.03358892341434036\n",
      "Step - 6442, Loss - 0.31720208509080716, Learning Rate - 0.00078125, magnitude of gradient - 0.029541469758716903\n",
      "Step - 6443, Loss - 0.30065181551230025, Learning Rate - 0.00078125, magnitude of gradient - 0.06487506194713627\n",
      "Step - 6444, Loss - 0.31221531237143085, Learning Rate - 0.00078125, magnitude of gradient - 0.05107668998400292\n",
      "Step - 6445, Loss - 0.40204385856241714, Learning Rate - 0.00078125, magnitude of gradient - 0.08538753544781752\n",
      "Step - 6446, Loss - 0.3498207064524981, Learning Rate - 0.00078125, magnitude of gradient - 0.043669833888220766\n",
      "Step - 6447, Loss - 0.2316666836249356, Learning Rate - 0.00078125, magnitude of gradient - 0.03433269905286892\n",
      "Step - 6448, Loss - 0.327459478531009, Learning Rate - 0.00078125, magnitude of gradient - 0.04144764991984331\n",
      "Step - 6449, Loss - 0.3545899562063751, Learning Rate - 0.00078125, magnitude of gradient - 0.020446002689184\n",
      "Step - 6450, Loss - 0.40703758204577034, Learning Rate - 0.00078125, magnitude of gradient - 0.006151391312998507\n",
      "Step - 6451, Loss - 0.2752282180038065, Learning Rate - 0.00078125, magnitude of gradient - 0.04140937762124299\n",
      "Step - 6452, Loss - 0.3655452653281628, Learning Rate - 0.00078125, magnitude of gradient - 0.020922340144564098\n",
      "Step - 6453, Loss - 0.32082732473004816, Learning Rate - 0.00078125, magnitude of gradient - 0.008647792519259267\n",
      "Step - 6454, Loss - 0.3795459852950155, Learning Rate - 0.00078125, magnitude of gradient - 0.03527931819851556\n",
      "Step - 6455, Loss - 0.482049851501, Learning Rate - 0.00078125, magnitude of gradient - 0.09520357420429415\n",
      "Step - 6456, Loss - 0.19048464041921487, Learning Rate - 0.00078125, magnitude of gradient - 0.03218982298183271\n",
      "Step - 6457, Loss - 0.40333064606270846, Learning Rate - 0.00078125, magnitude of gradient - 0.012362530666471012\n",
      "Step - 6458, Loss - 0.2608870248611129, Learning Rate - 0.00078125, magnitude of gradient - 0.04091152734069844\n",
      "Step - 6459, Loss - 0.33265271090818666, Learning Rate - 0.00078125, magnitude of gradient - 0.021248162377076506\n",
      "Step - 6460, Loss - 0.27888885244650535, Learning Rate - 0.00078125, magnitude of gradient - 0.05139860466319596\n",
      "Step - 6461, Loss - 0.30241043825707986, Learning Rate - 0.00078125, magnitude of gradient - 0.019444688704938085\n",
      "Step - 6462, Loss - 0.3262207202845583, Learning Rate - 0.00078125, magnitude of gradient - 0.05705666390893229\n",
      "Step - 6463, Loss - 0.29929661218765546, Learning Rate - 0.00078125, magnitude of gradient - 0.0481671362692091\n",
      "Step - 6464, Loss - 0.350324544217027, Learning Rate - 0.00078125, magnitude of gradient - 0.024134007446221305\n",
      "Step - 6465, Loss - 0.26715826755082484, Learning Rate - 0.00078125, magnitude of gradient - 0.05806590206031556\n",
      "Step - 6466, Loss - 0.3493122140819802, Learning Rate - 0.00078125, magnitude of gradient - 0.11678546879167916\n",
      "Step - 6467, Loss - 0.2931252598351818, Learning Rate - 0.00078125, magnitude of gradient - 0.010877240779261116\n",
      "Step - 6468, Loss - 0.30531668895886466, Learning Rate - 0.00078125, magnitude of gradient - 0.01770735366572906\n",
      "Step - 6469, Loss - 0.2638065721340162, Learning Rate - 0.00078125, magnitude of gradient - 0.05503331266025694\n",
      "Step - 6470, Loss - 0.285592119094885, Learning Rate - 0.00078125, magnitude of gradient - 0.025145083837907034\n",
      "Step - 6471, Loss - 0.29063262944553814, Learning Rate - 0.00078125, magnitude of gradient - 0.09485044183188177\n",
      "Step - 6472, Loss - 0.31027130215743004, Learning Rate - 0.00078125, magnitude of gradient - 0.08358276588484356\n",
      "Step - 6473, Loss - 0.3783555308875236, Learning Rate - 0.00078125, magnitude of gradient - 0.03274357157480757\n",
      "Step - 6474, Loss - 0.3087164951716227, Learning Rate - 0.00078125, magnitude of gradient - 0.07662329848248545\n",
      "Step - 6475, Loss - 0.36016782383720924, Learning Rate - 0.00078125, magnitude of gradient - 0.060320636586018266\n",
      "Step - 6476, Loss - 0.38774311945315254, Learning Rate - 0.00078125, magnitude of gradient - 0.07580625472340603\n",
      "Step - 6477, Loss - 0.3509775821602217, Learning Rate - 0.00078125, magnitude of gradient - 0.018414628137457\n",
      "Step - 6478, Loss - 0.3227665973389994, Learning Rate - 0.00078125, magnitude of gradient - 0.0530351485114679\n",
      "Step - 6479, Loss - 0.2890462108844197, Learning Rate - 0.00078125, magnitude of gradient - 0.0438098908079412\n",
      "Step - 6480, Loss - 0.3196675772962927, Learning Rate - 0.00078125, magnitude of gradient - 0.05153805097603664\n",
      "Step - 6481, Loss - 0.29484911393473184, Learning Rate - 0.00078125, magnitude of gradient - 0.06703188105908287\n",
      "Step - 6482, Loss - 0.36563628818663224, Learning Rate - 0.00078125, magnitude of gradient - 0.032675220602693546\n",
      "Step - 6483, Loss - 0.32403510867136875, Learning Rate - 0.00078125, magnitude of gradient - 0.09440488982618918\n",
      "Step - 6484, Loss - 0.39812785751460905, Learning Rate - 0.00078125, magnitude of gradient - 0.11138808242121682\n",
      "Step - 6485, Loss - 0.3171338464303039, Learning Rate - 0.00078125, magnitude of gradient - 0.03738981265531504\n",
      "Step - 6486, Loss - 0.27997346284856783, Learning Rate - 0.00078125, magnitude of gradient - 0.02262838221959776\n",
      "Step - 6487, Loss - 0.2844606027606733, Learning Rate - 0.00078125, magnitude of gradient - 0.02024367745718525\n",
      "Step - 6488, Loss - 0.2858840335246793, Learning Rate - 0.00078125, magnitude of gradient - 0.01988954472546328\n",
      "Step - 6489, Loss - 0.3430364289831215, Learning Rate - 0.00078125, magnitude of gradient - 0.04314731710810913\n",
      "Step - 6490, Loss - 0.2916524850668753, Learning Rate - 0.00078125, magnitude of gradient - 0.0966866439557227\n",
      "Step - 6491, Loss - 0.2790144389741568, Learning Rate - 0.00078125, magnitude of gradient - 0.05194052663543162\n",
      "Step - 6492, Loss - 0.2575584141929689, Learning Rate - 0.00078125, magnitude of gradient - 0.09296836722662027\n",
      "Step - 6493, Loss - 0.3531395833860821, Learning Rate - 0.00078125, magnitude of gradient - 0.03579264845273311\n",
      "Step - 6494, Loss - 0.3344671317272422, Learning Rate - 0.00078125, magnitude of gradient - 0.02315773555184361\n",
      "Step - 6495, Loss - 0.32556347022129545, Learning Rate - 0.00078125, magnitude of gradient - 0.00781622384031843\n",
      "Step - 6496, Loss - 0.34395239480770534, Learning Rate - 0.00078125, magnitude of gradient - 0.1151807058109865\n",
      "Step - 6497, Loss - 0.3742741734686663, Learning Rate - 0.00078125, magnitude of gradient - 0.058753036653713314\n",
      "Step - 6498, Loss - 0.2945678122902382, Learning Rate - 0.00078125, magnitude of gradient - 0.06899517374551697\n",
      "Step - 6499, Loss - 0.3474734546667238, Learning Rate - 0.00078125, magnitude of gradient - 0.04275870707184599\n",
      "Step - 6500, Loss - 0.3243956584294365, Learning Rate - 0.00078125, magnitude of gradient - 0.0534698971062109\n",
      "Step - 6501, Loss - 0.2680361172750353, Learning Rate - 0.00078125, magnitude of gradient - 0.07483026486157408\n",
      "Step - 6502, Loss - 0.27536492455995043, Learning Rate - 0.00078125, magnitude of gradient - 0.06645071191009669\n",
      "Step - 6503, Loss - 0.2889568590641376, Learning Rate - 0.00078125, magnitude of gradient - 0.0692451938618977\n",
      "Step - 6504, Loss - 0.25416112228628285, Learning Rate - 0.00078125, magnitude of gradient - 0.02231692170685747\n",
      "Step - 6505, Loss - 0.38012183859864923, Learning Rate - 0.00078125, magnitude of gradient - 0.018167639846730504\n",
      "Step - 6506, Loss - 0.24021819350549473, Learning Rate - 0.00078125, magnitude of gradient - 0.11408336045223433\n",
      "Step - 6507, Loss - 0.33380576910963766, Learning Rate - 0.00078125, magnitude of gradient - 0.05437906090893911\n",
      "Step - 6508, Loss - 0.34111623919621864, Learning Rate - 0.00078125, magnitude of gradient - 0.0820289172732435\n",
      "Step - 6509, Loss - 0.2755562658560302, Learning Rate - 0.00078125, magnitude of gradient - 0.02736605752539246\n",
      "Step - 6510, Loss - 0.38841623936990577, Learning Rate - 0.00078125, magnitude of gradient - 0.04807987869776519\n",
      "Step - 6511, Loss - 0.3515701436612301, Learning Rate - 0.00078125, magnitude of gradient - 0.054609767777427376\n",
      "Step - 6512, Loss - 0.30306444844735014, Learning Rate - 0.00078125, magnitude of gradient - 0.01586058475317233\n",
      "Step - 6513, Loss - 0.3733312428189454, Learning Rate - 0.00078125, magnitude of gradient - 0.13792044529466838\n",
      "Step - 6514, Loss - 0.35533706605056825, Learning Rate - 0.00078125, magnitude of gradient - 0.06415019990338096\n",
      "Step - 6515, Loss - 0.330875736427189, Learning Rate - 0.00078125, magnitude of gradient - 0.02525621751347766\n",
      "Step - 6516, Loss - 0.34954048426517326, Learning Rate - 0.00078125, magnitude of gradient - 0.09389729406034918\n",
      "Step - 6517, Loss - 0.34285621290903456, Learning Rate - 0.00078125, magnitude of gradient - 0.046686132216313445\n",
      "Step - 6518, Loss - 0.3214566669959974, Learning Rate - 0.00078125, magnitude of gradient - 0.052910627663112046\n",
      "Step - 6519, Loss - 0.3204638601798954, Learning Rate - 0.00078125, magnitude of gradient - 0.02811555808834042\n",
      "Step - 6520, Loss - 0.3391136824959422, Learning Rate - 0.00078125, magnitude of gradient - 0.08408424875966987\n",
      "Step - 6521, Loss - 0.3002891134460781, Learning Rate - 0.00078125, magnitude of gradient - 0.06932683600970814\n",
      "Step - 6522, Loss - 0.2501932024662906, Learning Rate - 0.00078125, magnitude of gradient - 0.04328086072762705\n",
      "Step - 6523, Loss - 0.35109988788719604, Learning Rate - 0.00078125, magnitude of gradient - 0.02192214422283797\n",
      "Step - 6524, Loss - 0.2709103917849412, Learning Rate - 0.00078125, magnitude of gradient - 0.07323487874980136\n",
      "Step - 6525, Loss - 0.3129216196631871, Learning Rate - 0.00078125, magnitude of gradient - 0.04483179182937764\n",
      "Step - 6526, Loss - 0.3302795850927287, Learning Rate - 0.00078125, magnitude of gradient - 0.043695598326345064\n",
      "Step - 6527, Loss - 0.28788079576486447, Learning Rate - 0.00078125, magnitude of gradient - 0.04138229852200763\n",
      "Step - 6528, Loss - 0.3180038459071796, Learning Rate - 0.00078125, magnitude of gradient - 0.015386911604614193\n",
      "Step - 6529, Loss - 0.3973643575036455, Learning Rate - 0.00078125, magnitude of gradient - 0.09457001051626186\n",
      "Step - 6530, Loss - 0.24929562583990972, Learning Rate - 0.00078125, magnitude of gradient - 0.014067786151806427\n",
      "Step - 6531, Loss - 0.38869470159923264, Learning Rate - 0.00078125, magnitude of gradient - 0.03287464714664222\n",
      "Step - 6532, Loss - 0.3275828402150356, Learning Rate - 0.00078125, magnitude of gradient - 0.07127557286289796\n",
      "Step - 6533, Loss - 0.32077859238967493, Learning Rate - 0.00078125, magnitude of gradient - 0.03537850401038885\n",
      "Step - 6534, Loss - 0.34652198617079033, Learning Rate - 0.00078125, magnitude of gradient - 0.040230313533559624\n",
      "Step - 6535, Loss - 0.2794530323076474, Learning Rate - 0.00078125, magnitude of gradient - 0.032955312095460106\n",
      "Step - 6536, Loss - 0.3235036392187318, Learning Rate - 0.00078125, magnitude of gradient - 0.02539403630710488\n",
      "Step - 6537, Loss - 0.3703485799218008, Learning Rate - 0.00078125, magnitude of gradient - 0.06197457484688635\n",
      "Step - 6538, Loss - 0.39470694944961615, Learning Rate - 0.00078125, magnitude of gradient - 0.041902076341016246\n",
      "Step - 6539, Loss - 0.3890454813032291, Learning Rate - 0.00078125, magnitude of gradient - 0.09612573129839044\n",
      "Step - 6540, Loss - 0.3323312696945465, Learning Rate - 0.00078125, magnitude of gradient - 0.046036225877256684\n",
      "Step - 6541, Loss - 0.30659400143566173, Learning Rate - 0.00078125, magnitude of gradient - 0.05620290734677394\n",
      "Step - 6542, Loss - 0.2965590950692426, Learning Rate - 0.00078125, magnitude of gradient - 0.10640900083393022\n",
      "Step - 6543, Loss - 0.3754852160644493, Learning Rate - 0.00078125, magnitude of gradient - 0.07472814246587978\n",
      "Step - 6544, Loss - 0.36380722586398184, Learning Rate - 0.00078125, magnitude of gradient - 0.09578204381794088\n",
      "Step - 6545, Loss - 0.3666025279464136, Learning Rate - 0.00078125, magnitude of gradient - 0.04551145018619677\n",
      "Step - 6546, Loss - 0.38349809956925396, Learning Rate - 0.00078125, magnitude of gradient - 0.0983968551464909\n",
      "Step - 6547, Loss - 0.34100878735851536, Learning Rate - 0.00078125, magnitude of gradient - 0.0731569358572673\n",
      "Step - 6548, Loss - 0.28125257200985954, Learning Rate - 0.00078125, magnitude of gradient - 0.06419092252544924\n",
      "Step - 6549, Loss - 0.29449821486718025, Learning Rate - 0.00078125, magnitude of gradient - 0.05759876572798362\n",
      "Step - 6550, Loss - 0.3058373093683251, Learning Rate - 0.00078125, magnitude of gradient - 0.11777197959560777\n",
      "Step - 6551, Loss - 0.3262635182623471, Learning Rate - 0.00078125, magnitude of gradient - 0.04217524028130735\n",
      "Step - 6552, Loss - 0.2861476942682556, Learning Rate - 0.00078125, magnitude of gradient - 0.044623042876187395\n",
      "Step - 6553, Loss - 0.3203837408013369, Learning Rate - 0.00078125, magnitude of gradient - 0.017489969328424927\n",
      "Step - 6554, Loss - 0.30094181215383986, Learning Rate - 0.00078125, magnitude of gradient - 0.09103330183448433\n",
      "Step - 6555, Loss - 0.28533694565461676, Learning Rate - 0.00078125, magnitude of gradient - 0.02685606761641192\n",
      "Step - 6556, Loss - 0.26756067857367793, Learning Rate - 0.00078125, magnitude of gradient - 0.043917027632257744\n",
      "Step - 6557, Loss - 0.2722587563148684, Learning Rate - 0.00078125, magnitude of gradient - 0.028280297852722132\n",
      "Step - 6558, Loss - 0.30117336074578177, Learning Rate - 0.00078125, magnitude of gradient - 0.060934574781396224\n",
      "Step - 6559, Loss - 0.23866895747376055, Learning Rate - 0.00078125, magnitude of gradient - 0.01964560279124387\n",
      "Step - 6560, Loss - 0.30884577765335197, Learning Rate - 0.00078125, magnitude of gradient - 0.02561322614794686\n",
      "Step - 6561, Loss - 0.33019974452340134, Learning Rate - 0.00078125, magnitude of gradient - 0.02634203931592818\n",
      "Step - 6562, Loss - 0.42202552256310977, Learning Rate - 0.00078125, magnitude of gradient - 0.0391832879239156\n",
      "Step - 6563, Loss - 0.2833293165620129, Learning Rate - 0.00078125, magnitude of gradient - 0.04286615623808145\n",
      "Step - 6564, Loss - 0.27287472787996714, Learning Rate - 0.00078125, magnitude of gradient - 0.06833613500757477\n",
      "Step - 6565, Loss - 0.3511798046243694, Learning Rate - 0.00078125, magnitude of gradient - 0.05459416636530924\n",
      "Step - 6566, Loss - 0.26413238748029755, Learning Rate - 0.00078125, magnitude of gradient - 0.026591761275929337\n",
      "Step - 6567, Loss - 0.2992465775055403, Learning Rate - 0.00078125, magnitude of gradient - 0.046600151387696076\n",
      "Step - 6568, Loss - 0.30627274053924314, Learning Rate - 0.00078125, magnitude of gradient - 0.04109159041405249\n",
      "Step - 6569, Loss - 0.310369170110107, Learning Rate - 0.00078125, magnitude of gradient - 0.09079258079025365\n",
      "Step - 6570, Loss - 0.27563277690361687, Learning Rate - 0.00078125, magnitude of gradient - 0.005806112866631292\n",
      "Step - 6571, Loss - 0.30734047561971495, Learning Rate - 0.00078125, magnitude of gradient - 0.03263908694281094\n",
      "Step - 6572, Loss - 0.2712797212441014, Learning Rate - 0.00078125, magnitude of gradient - 0.07345224132371372\n",
      "Step - 6573, Loss - 0.3982303622737993, Learning Rate - 0.00078125, magnitude of gradient - 0.09339178954552818\n",
      "Step - 6574, Loss - 0.2836360709407128, Learning Rate - 0.00078125, magnitude of gradient - 0.05178958381013465\n",
      "Step - 6575, Loss - 0.257522436347886, Learning Rate - 0.00078125, magnitude of gradient - 0.0380729387180005\n",
      "Step - 6576, Loss - 0.3030694257058167, Learning Rate - 0.00078125, magnitude of gradient - 0.07563806825635461\n",
      "Step - 6577, Loss - 0.32257277765215325, Learning Rate - 0.00078125, magnitude of gradient - 0.027483401760948756\n",
      "Step - 6578, Loss - 0.3523662889412112, Learning Rate - 0.00078125, magnitude of gradient - 0.08883424498021773\n",
      "Step - 6579, Loss - 0.2964205560833226, Learning Rate - 0.00078125, magnitude of gradient - 0.024661136454435514\n",
      "Step - 6580, Loss - 0.2696938530913296, Learning Rate - 0.00078125, magnitude of gradient - 0.0670954509161907\n",
      "Step - 6581, Loss - 0.3018085219706321, Learning Rate - 0.00078125, magnitude of gradient - 0.06635399757134716\n",
      "Step - 6582, Loss - 0.31759589275137745, Learning Rate - 0.00078125, magnitude of gradient - 0.04256794012143619\n",
      "Step - 6583, Loss - 0.28673824095785777, Learning Rate - 0.00078125, magnitude of gradient - 0.018380857379834182\n",
      "Step - 6584, Loss - 0.3257755250121672, Learning Rate - 0.00078125, magnitude of gradient - 0.018268046168917435\n",
      "Step - 6585, Loss - 0.2689970961460367, Learning Rate - 0.00078125, magnitude of gradient - 0.025028616789419676\n",
      "Step - 6586, Loss - 0.32169158731336295, Learning Rate - 0.00078125, magnitude of gradient - 0.1036012085663176\n",
      "Step - 6587, Loss - 0.2624982671304421, Learning Rate - 0.00078125, magnitude of gradient - 0.05208351296370135\n",
      "Step - 6588, Loss - 0.22708332638363143, Learning Rate - 0.00078125, magnitude of gradient - 0.02759336345726826\n",
      "Step - 6589, Loss - 0.2637324793756186, Learning Rate - 0.00078125, magnitude of gradient - 0.030690451304580552\n",
      "Step - 6590, Loss - 0.3228043004885147, Learning Rate - 0.00078125, magnitude of gradient - 0.024242503120580485\n",
      "Step - 6591, Loss - 0.3762716494392879, Learning Rate - 0.00078125, magnitude of gradient - 0.05564586257066995\n",
      "Step - 6592, Loss - 0.3320667926930393, Learning Rate - 0.00078125, magnitude of gradient - 0.0300535046419374\n",
      "Step - 6593, Loss - 0.27776604748790307, Learning Rate - 0.00078125, magnitude of gradient - 0.06096545506017056\n",
      "Step - 6594, Loss - 0.32249313714815514, Learning Rate - 0.00078125, magnitude of gradient - 0.022575113705949595\n",
      "Step - 6595, Loss - 0.29135066154995404, Learning Rate - 0.00078125, magnitude of gradient - 0.06093175495562698\n",
      "Step - 6596, Loss - 0.3152378180798441, Learning Rate - 0.00078125, magnitude of gradient - 0.05667452484718586\n",
      "Step - 6597, Loss - 0.3171285053721956, Learning Rate - 0.00078125, magnitude of gradient - 0.06332286665910025\n",
      "Step - 6598, Loss - 0.34937883557283494, Learning Rate - 0.00078125, magnitude of gradient - 0.028326850871868845\n",
      "Step - 6599, Loss - 0.35506863582042514, Learning Rate - 0.00078125, magnitude of gradient - 0.03824722349570133\n",
      "Step - 6600, Loss - 0.2877445138585316, Learning Rate - 0.00078125, magnitude of gradient - 0.04102598997111408\n",
      "Step - 6601, Loss - 0.38216616626458455, Learning Rate - 0.00078125, magnitude of gradient - 0.03805155734040946\n",
      "Step - 6602, Loss - 0.3161039403279539, Learning Rate - 0.00078125, magnitude of gradient - 0.028041164354554494\n",
      "Step - 6603, Loss - 0.3074262851272869, Learning Rate - 0.00078125, magnitude of gradient - 0.05896583055484591\n",
      "Step - 6604, Loss - 0.3456827060612712, Learning Rate - 0.00078125, magnitude of gradient - 0.03403960939780673\n",
      "Step - 6605, Loss - 0.40906358430050055, Learning Rate - 0.00078125, magnitude of gradient - 0.08983749921133571\n",
      "Step - 6606, Loss - 0.31431167425678136, Learning Rate - 0.00078125, magnitude of gradient - 0.03545056101877312\n",
      "Step - 6607, Loss - 0.33647442786802817, Learning Rate - 0.00078125, magnitude of gradient - 0.043079966817609096\n",
      "Step - 6608, Loss - 0.2519703341638155, Learning Rate - 0.00078125, magnitude of gradient - 0.05532683599653206\n",
      "Step - 6609, Loss - 0.297263697375744, Learning Rate - 0.00078125, magnitude of gradient - 0.03764241148228805\n",
      "Step - 6610, Loss - 0.4414216003786718, Learning Rate - 0.00078125, magnitude of gradient - 0.012765773374127081\n",
      "Step - 6611, Loss - 0.3339131917333072, Learning Rate - 0.00078125, magnitude of gradient - 0.02540282481400569\n",
      "Step - 6612, Loss - 0.2944556388406252, Learning Rate - 0.00078125, magnitude of gradient - 0.036365573866284466\n",
      "Step - 6613, Loss - 0.32013378317748126, Learning Rate - 0.00078125, magnitude of gradient - 0.023041750035979698\n",
      "Step - 6614, Loss - 0.33870620196504414, Learning Rate - 0.00078125, magnitude of gradient - 0.07206898159694437\n",
      "Step - 6615, Loss - 0.30356944010995374, Learning Rate - 0.00078125, magnitude of gradient - 0.053469490882072\n",
      "Step - 6616, Loss - 0.2925868196978062, Learning Rate - 0.00078125, magnitude of gradient - 0.030547235653529826\n",
      "Step - 6617, Loss - 0.37595852635785004, Learning Rate - 0.00078125, magnitude of gradient - 0.05207237390845227\n",
      "Step - 6618, Loss - 0.285524491399687, Learning Rate - 0.00078125, magnitude of gradient - 0.0529986393573308\n",
      "Step - 6619, Loss - 0.32473495869748126, Learning Rate - 0.00078125, magnitude of gradient - 0.06679388349337881\n",
      "Step - 6620, Loss - 0.3722831294689902, Learning Rate - 0.00078125, magnitude of gradient - 0.049397026069757925\n",
      "Step - 6621, Loss - 0.28035697249380853, Learning Rate - 0.00078125, magnitude of gradient - 0.03428917444681059\n",
      "Step - 6622, Loss - 0.31952286001736474, Learning Rate - 0.00078125, magnitude of gradient - 0.07882679667810309\n",
      "Step - 6623, Loss - 0.2543739808220217, Learning Rate - 0.00078125, magnitude of gradient - 0.04287530734026602\n",
      "Step - 6624, Loss - 0.3636425042512685, Learning Rate - 0.00078125, magnitude of gradient - 0.03276470727032617\n",
      "Step - 6625, Loss - 0.34487441873764957, Learning Rate - 0.00078125, magnitude of gradient - 0.024273088604472415\n",
      "Step - 6626, Loss - 0.2592393961850791, Learning Rate - 0.00078125, magnitude of gradient - 0.0763490456418564\n",
      "Step - 6627, Loss - 0.3135319229511412, Learning Rate - 0.00078125, magnitude of gradient - 0.05192314225526157\n",
      "Step - 6628, Loss - 0.3898626385310662, Learning Rate - 0.00078125, magnitude of gradient - 0.021982356823403253\n",
      "Step - 6629, Loss - 0.32110063065480215, Learning Rate - 0.00078125, magnitude of gradient - 0.012162361621635661\n",
      "Step - 6630, Loss - 0.31914813133131037, Learning Rate - 0.00078125, magnitude of gradient - 0.10380560540289506\n",
      "Step - 6631, Loss - 0.3358247350755771, Learning Rate - 0.00078125, magnitude of gradient - 0.030806163528175234\n",
      "Step - 6632, Loss - 0.3931676404946905, Learning Rate - 0.00078125, magnitude of gradient - 0.036041617177994346\n",
      "Step - 6633, Loss - 0.2961800517328934, Learning Rate - 0.00078125, magnitude of gradient - 0.03511432585557687\n",
      "Step - 6634, Loss - 0.3228531957234173, Learning Rate - 0.00078125, magnitude of gradient - 0.01220428456161025\n",
      "Step - 6635, Loss - 0.38043337682580314, Learning Rate - 0.00078125, magnitude of gradient - 0.05024938553744723\n",
      "Step - 6636, Loss - 0.32463855763512645, Learning Rate - 0.00078125, magnitude of gradient - 0.04686026309021984\n",
      "Step - 6637, Loss - 0.38212081302739853, Learning Rate - 0.00078125, magnitude of gradient - 0.06544120302100101\n",
      "Step - 6638, Loss - 0.2719671560990176, Learning Rate - 0.00078125, magnitude of gradient - 0.030011026931295294\n",
      "Step - 6639, Loss - 0.23897897533001036, Learning Rate - 0.00078125, magnitude of gradient - 0.03235623402216467\n",
      "Step - 6640, Loss - 0.27023230398016224, Learning Rate - 0.00078125, magnitude of gradient - 0.06808674324329639\n",
      "Step - 6641, Loss - 0.2651850023622544, Learning Rate - 0.00078125, magnitude of gradient - 0.0499969213081583\n",
      "Step - 6642, Loss - 0.40515339376297077, Learning Rate - 0.00078125, magnitude of gradient - 0.030182227161893515\n",
      "Step - 6643, Loss - 0.3481223388880065, Learning Rate - 0.00078125, magnitude of gradient - 0.09687385937154795\n",
      "Step - 6644, Loss - 0.2914827698042381, Learning Rate - 0.00078125, magnitude of gradient - 0.03018728324097445\n",
      "Step - 6645, Loss - 0.36058008338426034, Learning Rate - 0.00078125, magnitude of gradient - 0.04398546650646166\n",
      "Step - 6646, Loss - 0.26848287370476165, Learning Rate - 0.00078125, magnitude of gradient - 0.05351686093833519\n",
      "Step - 6647, Loss - 0.33645860342361744, Learning Rate - 0.00078125, magnitude of gradient - 0.012970630581259439\n",
      "Step - 6648, Loss - 0.30485819825668214, Learning Rate - 0.00078125, magnitude of gradient - 0.0429020378504927\n",
      "Step - 6649, Loss - 0.34984336408764555, Learning Rate - 0.00078125, magnitude of gradient - 0.023830524228297727\n",
      "Step - 6650, Loss - 0.2950971907738533, Learning Rate - 0.00078125, magnitude of gradient - 0.07413595592458117\n",
      "Step - 6651, Loss - 0.3668022994637798, Learning Rate - 0.00078125, magnitude of gradient - 0.05164941542611933\n",
      "Step - 6652, Loss - 0.28725401135739587, Learning Rate - 0.00078125, magnitude of gradient - 0.038542689906948965\n",
      "Step - 6653, Loss - 0.34074975154851583, Learning Rate - 0.00078125, magnitude of gradient - 0.09613985685977011\n",
      "Step - 6654, Loss - 0.31356938080161256, Learning Rate - 0.00078125, magnitude of gradient - 0.06862579418365938\n",
      "Step - 6655, Loss - 0.27169372080770626, Learning Rate - 0.00078125, magnitude of gradient - 0.05350273978675051\n",
      "Step - 6656, Loss - 0.3072533491411636, Learning Rate - 0.00078125, magnitude of gradient - 0.008400462356211773\n",
      "Step - 6657, Loss - 0.32886408775869636, Learning Rate - 0.00078125, magnitude of gradient - 0.03696271952394939\n",
      "Step - 6658, Loss - 0.33136144320402705, Learning Rate - 0.00078125, magnitude of gradient - 0.05028361660745888\n",
      "Step - 6659, Loss - 0.4062659600703328, Learning Rate - 0.00078125, magnitude of gradient - 0.06603694880402608\n",
      "Step - 6660, Loss - 0.34714920883651046, Learning Rate - 0.00078125, magnitude of gradient - 0.03427526932154131\n",
      "Step - 6661, Loss - 0.3142636234436693, Learning Rate - 0.00078125, magnitude of gradient - 0.0222250095565699\n",
      "Step - 6662, Loss - 0.27844491983108405, Learning Rate - 0.00078125, magnitude of gradient - 0.04863871744789018\n",
      "Step - 6663, Loss - 0.41883708427524724, Learning Rate - 0.00078125, magnitude of gradient - 0.08284954364089032\n",
      "Step - 6664, Loss - 0.2856624625314755, Learning Rate - 0.00078125, magnitude of gradient - 0.05668141282007167\n",
      "Step - 6665, Loss - 0.3482535573907447, Learning Rate - 0.00078125, magnitude of gradient - 0.06634274144984009\n",
      "Step - 6666, Loss - 0.3238376613288643, Learning Rate - 0.00078125, magnitude of gradient - 0.06951207747371956\n",
      "Step - 6667, Loss - 0.3586846637438253, Learning Rate - 0.00078125, magnitude of gradient - 0.09247291621097452\n",
      "Step - 6668, Loss - 0.3538837824421451, Learning Rate - 0.00078125, magnitude of gradient - 0.06781960924935215\n",
      "Step - 6669, Loss - 0.33060535178127104, Learning Rate - 0.00078125, magnitude of gradient - 0.057519626183634445\n",
      "Step - 6670, Loss - 0.3420752861593073, Learning Rate - 0.00078125, magnitude of gradient - 0.07739971164812894\n",
      "Step - 6671, Loss - 0.26791090113028027, Learning Rate - 0.00078125, magnitude of gradient - 0.01957184768754124\n",
      "Step - 6672, Loss - 0.28404541743580064, Learning Rate - 0.00078125, magnitude of gradient - 0.07142457946792698\n",
      "Step - 6673, Loss - 0.31962977915337853, Learning Rate - 0.00078125, magnitude of gradient - 0.04742510668159783\n",
      "Step - 6674, Loss - 0.3372669209397077, Learning Rate - 0.00078125, magnitude of gradient - 0.08822275434622015\n",
      "Step - 6675, Loss - 0.2839565051096411, Learning Rate - 0.00078125, magnitude of gradient - 0.04738494414101306\n",
      "Step - 6676, Loss - 0.35389637334257124, Learning Rate - 0.00078125, magnitude of gradient - 0.09801059310118458\n",
      "Step - 6677, Loss - 0.3328123205488756, Learning Rate - 0.00078125, magnitude of gradient - 0.019986082393304978\n",
      "Step - 6678, Loss - 0.3682007765047992, Learning Rate - 0.00078125, magnitude of gradient - 0.01999101051660402\n",
      "Step - 6679, Loss - 0.349181465544973, Learning Rate - 0.00078125, magnitude of gradient - 0.08067558825748436\n",
      "Step - 6680, Loss - 0.3381546502301873, Learning Rate - 0.00078125, magnitude of gradient - 0.06523945269257364\n",
      "Step - 6681, Loss - 0.32885747780920355, Learning Rate - 0.00078125, magnitude of gradient - 0.005361918276173793\n",
      "Step - 6682, Loss - 0.32581238436768295, Learning Rate - 0.00078125, magnitude of gradient - 0.030751551447754044\n",
      "Step - 6683, Loss - 0.2434426230588201, Learning Rate - 0.00078125, magnitude of gradient - 0.07178229870422173\n",
      "Step - 6684, Loss - 0.4123340792860323, Learning Rate - 0.00078125, magnitude of gradient - 0.07509076720747893\n",
      "Step - 6685, Loss - 0.308426461109474, Learning Rate - 0.00078125, magnitude of gradient - 0.02482580895808731\n",
      "Step - 6686, Loss - 0.3301040129444501, Learning Rate - 0.00078125, magnitude of gradient - 0.11551988549114663\n",
      "Step - 6687, Loss - 0.2871525250972955, Learning Rate - 0.00078125, magnitude of gradient - 0.07050976724225215\n",
      "Step - 6688, Loss - 0.2520688109920668, Learning Rate - 0.00078125, magnitude of gradient - 0.03621802362510007\n",
      "Step - 6689, Loss - 0.3833405367299036, Learning Rate - 0.00078125, magnitude of gradient - 0.026021929643610227\n",
      "Step - 6690, Loss - 0.357041420080669, Learning Rate - 0.00078125, magnitude of gradient - 0.03507214709427649\n",
      "Step - 6691, Loss - 0.30174014799036936, Learning Rate - 0.00078125, magnitude of gradient - 0.05792281109546659\n",
      "Step - 6692, Loss - 0.24965348727710043, Learning Rate - 0.00078125, magnitude of gradient - 0.07567651381479683\n",
      "Step - 6693, Loss - 0.4020900119931009, Learning Rate - 0.00078125, magnitude of gradient - 0.021397689740285095\n",
      "Step - 6694, Loss - 0.26909931198998205, Learning Rate - 0.00078125, magnitude of gradient - 0.03235654769778081\n",
      "Step - 6695, Loss - 0.3269658028211885, Learning Rate - 0.00078125, magnitude of gradient - 0.03375929342052312\n",
      "Step - 6696, Loss - 0.38036854421576527, Learning Rate - 0.00078125, magnitude of gradient - 0.056683703775795846\n",
      "Step - 6697, Loss - 0.2873218141174156, Learning Rate - 0.00078125, magnitude of gradient - 0.021470343585572507\n",
      "Step - 6698, Loss - 0.32698610813537776, Learning Rate - 0.00078125, magnitude of gradient - 0.025626637315833724\n",
      "Step - 6699, Loss - 0.3185278532472052, Learning Rate - 0.00078125, magnitude of gradient - 0.06323039123605266\n",
      "Step - 6700, Loss - 0.3414974202267882, Learning Rate - 0.00078125, magnitude of gradient - 0.04886603360937736\n",
      "Step - 6701, Loss - 0.28398314030350763, Learning Rate - 0.00078125, magnitude of gradient - 0.048205964354640723\n",
      "Step - 6702, Loss - 0.2768294056695832, Learning Rate - 0.00078125, magnitude of gradient - 0.04390981049032511\n",
      "Step - 6703, Loss - 0.35565740375454846, Learning Rate - 0.00078125, magnitude of gradient - 0.017328325681580655\n",
      "Step - 6704, Loss - 0.36880017949570354, Learning Rate - 0.00078125, magnitude of gradient - 0.07299428828653383\n",
      "Step - 6705, Loss - 0.40310661330618336, Learning Rate - 0.00078125, magnitude of gradient - 0.06816199140283663\n",
      "Step - 6706, Loss - 0.27193959773325577, Learning Rate - 0.00078125, magnitude of gradient - 0.07775647531201269\n",
      "Step - 6707, Loss - 0.34437918222163494, Learning Rate - 0.00078125, magnitude of gradient - 0.09027829312091444\n",
      "Step - 6708, Loss - 0.3561379456050463, Learning Rate - 0.00078125, magnitude of gradient - 0.045646467488263474\n",
      "Step - 6709, Loss - 0.3126318005268571, Learning Rate - 0.00078125, magnitude of gradient - 0.02654520440482608\n",
      "Step - 6710, Loss - 0.35414150987707954, Learning Rate - 0.00078125, magnitude of gradient - 0.015804322162543245\n",
      "Step - 6711, Loss - 0.3092167218414381, Learning Rate - 0.00078125, magnitude of gradient - 0.021669058182402872\n",
      "Step - 6712, Loss - 0.28822677947254477, Learning Rate - 0.00078125, magnitude of gradient - 0.07726556320576164\n",
      "Step - 6713, Loss - 0.3476045028407254, Learning Rate - 0.00078125, magnitude of gradient - 0.04400355999519895\n",
      "Step - 6714, Loss - 0.4237443141853122, Learning Rate - 0.00078125, magnitude of gradient - 0.06018116183865004\n",
      "Step - 6715, Loss - 0.32813186924233373, Learning Rate - 0.00078125, magnitude of gradient - 0.022666792485179225\n",
      "Step - 6716, Loss - 0.26529987350886913, Learning Rate - 0.00078125, magnitude of gradient - 0.06463159484441616\n",
      "Step - 6717, Loss - 0.3804862964232176, Learning Rate - 0.00078125, magnitude of gradient - 0.0413482158063313\n",
      "Step - 6718, Loss - 0.36066318670604414, Learning Rate - 0.00078125, magnitude of gradient - 0.06070097724312164\n",
      "Step - 6719, Loss - 0.28898790379709904, Learning Rate - 0.00078125, magnitude of gradient - 0.0425099336746041\n",
      "Step - 6720, Loss - 0.33925709096763534, Learning Rate - 0.00078125, magnitude of gradient - 0.09922220413163858\n",
      "Step - 6721, Loss - 0.3439039539191532, Learning Rate - 0.00078125, magnitude of gradient - 0.08691541234584048\n",
      "Step - 6722, Loss - 0.3522575680289939, Learning Rate - 0.00078125, magnitude of gradient - 0.055463408935309017\n",
      "Step - 6723, Loss - 0.28157927745378153, Learning Rate - 0.00078125, magnitude of gradient - 0.059202315203010634\n",
      "Step - 6724, Loss - 0.36239075386446784, Learning Rate - 0.00078125, magnitude of gradient - 0.01983333771837764\n",
      "Step - 6725, Loss - 0.29518233147402584, Learning Rate - 0.00078125, magnitude of gradient - 0.05200303653672162\n",
      "Step - 6726, Loss - 0.3226264607788157, Learning Rate - 0.00078125, magnitude of gradient - 0.08270907975140225\n",
      "Step - 6727, Loss - 0.3069856545177637, Learning Rate - 0.00078125, magnitude of gradient - 0.04818898206280944\n",
      "Step - 6728, Loss - 0.3050881985767069, Learning Rate - 0.00078125, magnitude of gradient - 0.035415951537190016\n",
      "Step - 6729, Loss - 0.304938120640012, Learning Rate - 0.00078125, magnitude of gradient - 0.06123470576617231\n",
      "Step - 6730, Loss - 0.3119692074573978, Learning Rate - 0.00078125, magnitude of gradient - 0.01973808484986792\n",
      "Step - 6731, Loss - 0.3662472478543348, Learning Rate - 0.00078125, magnitude of gradient - 0.08529947475012342\n",
      "Step - 6732, Loss - 0.35872351902807964, Learning Rate - 0.00078125, magnitude of gradient - 0.058569482068542345\n",
      "Step - 6733, Loss - 0.33058086283523674, Learning Rate - 0.00078125, magnitude of gradient - 0.04630266301975128\n",
      "Step - 6734, Loss - 0.26013917824235, Learning Rate - 0.00078125, magnitude of gradient - 0.04154932823795281\n",
      "Step - 6735, Loss - 0.3485790836768342, Learning Rate - 0.00078125, magnitude of gradient - 0.011799478565830891\n",
      "Step - 6736, Loss - 0.34777580139512565, Learning Rate - 0.00078125, magnitude of gradient - 0.07623209144065102\n",
      "Step - 6737, Loss - 0.3216278164007453, Learning Rate - 0.00078125, magnitude of gradient - 0.07839896592125949\n",
      "Step - 6738, Loss - 0.3382116988355143, Learning Rate - 0.00078125, magnitude of gradient - 0.047692135763615744\n",
      "Step - 6739, Loss - 0.2889613162655221, Learning Rate - 0.00078125, magnitude of gradient - 0.04879857810126761\n",
      "Step - 6740, Loss - 0.3226084204567854, Learning Rate - 0.00078125, magnitude of gradient - 0.05951069491801785\n",
      "Step - 6741, Loss - 0.3055761198742886, Learning Rate - 0.00078125, magnitude of gradient - 0.030143839332739848\n",
      "Step - 6742, Loss - 0.3714512298369803, Learning Rate - 0.00078125, magnitude of gradient - 0.10391677908586648\n",
      "Step - 6743, Loss - 0.29329848921393886, Learning Rate - 0.00078125, magnitude of gradient - 0.0334799059903799\n",
      "Step - 6744, Loss - 0.36467684975894893, Learning Rate - 0.00078125, magnitude of gradient - 0.053421312336548156\n",
      "Step - 6745, Loss - 0.32103662756804313, Learning Rate - 0.00078125, magnitude of gradient - 0.08129883305747378\n",
      "Step - 6746, Loss - 0.4383809172375266, Learning Rate - 0.00078125, magnitude of gradient - 0.038536706551467015\n",
      "Step - 6747, Loss - 0.27741889673425657, Learning Rate - 0.00078125, magnitude of gradient - 0.03106540084173257\n",
      "Step - 6748, Loss - 0.33714284562938923, Learning Rate - 0.00078125, magnitude of gradient - 0.09638227854251355\n",
      "Step - 6749, Loss - 0.3394088372957025, Learning Rate - 0.00078125, magnitude of gradient - 0.06271435661948788\n",
      "Step - 6750, Loss - 0.3815790780157524, Learning Rate - 0.00078125, magnitude of gradient - 0.06287201883590281\n",
      "Step - 6751, Loss - 0.2911714705543881, Learning Rate - 0.00078125, magnitude of gradient - 0.1590850512397151\n",
      "Step - 6752, Loss - 0.34440317059613434, Learning Rate - 0.00078125, magnitude of gradient - 0.01753613984090332\n",
      "Step - 6753, Loss - 0.28649787502693946, Learning Rate - 0.00078125, magnitude of gradient - 0.014907905472640138\n",
      "Step - 6754, Loss - 0.37525535039305047, Learning Rate - 0.00078125, magnitude of gradient - 0.02485844299783609\n",
      "Step - 6755, Loss - 0.36185185363226335, Learning Rate - 0.00078125, magnitude of gradient - 0.019678875379996966\n",
      "Step - 6756, Loss - 0.2733352110705516, Learning Rate - 0.00078125, magnitude of gradient - 0.0470436825966005\n",
      "Step - 6757, Loss - 0.2780689474041736, Learning Rate - 0.00078125, magnitude of gradient - 0.013104295337274082\n",
      "Step - 6758, Loss - 0.36051358477180884, Learning Rate - 0.00078125, magnitude of gradient - 0.0598739826887423\n",
      "Step - 6759, Loss - 0.3165448965014652, Learning Rate - 0.00078125, magnitude of gradient - 0.03271920639394202\n",
      "Step - 6760, Loss - 0.3607085499198748, Learning Rate - 0.00078125, magnitude of gradient - 0.039263173667913834\n",
      "Step - 6761, Loss - 0.34290932479673175, Learning Rate - 0.00078125, magnitude of gradient - 0.022426473721927607\n",
      "Step - 6762, Loss - 0.3032382618389456, Learning Rate - 0.00078125, magnitude of gradient - 0.06196365176053927\n",
      "Step - 6763, Loss - 0.28862429187948885, Learning Rate - 0.00078125, magnitude of gradient - 0.11442685139153687\n",
      "Step - 6764, Loss - 0.3088787364698026, Learning Rate - 0.00078125, magnitude of gradient - 0.08816597865346444\n",
      "Step - 6765, Loss - 0.3177220290924911, Learning Rate - 0.00078125, magnitude of gradient - 0.020594633519377223\n",
      "Step - 6766, Loss - 0.2889568011711817, Learning Rate - 0.00078125, magnitude of gradient - 0.03439385476026215\n",
      "Step - 6767, Loss - 0.23695506681974587, Learning Rate - 0.00078125, magnitude of gradient - 0.01420190601012784\n",
      "Step - 6768, Loss - 0.3449421854946757, Learning Rate - 0.00078125, magnitude of gradient - 0.027376563945912473\n",
      "Step - 6769, Loss - 0.30028855772694907, Learning Rate - 0.00078125, magnitude of gradient - 0.0581917924153906\n",
      "Step - 6770, Loss - 0.3498163047993604, Learning Rate - 0.00078125, magnitude of gradient - 0.04062096465840959\n",
      "Step - 6771, Loss - 0.29343746532572906, Learning Rate - 0.00078125, magnitude of gradient - 0.08070607596264948\n",
      "Step - 6772, Loss - 0.34150784202066525, Learning Rate - 0.00078125, magnitude of gradient - 0.04281385247197636\n",
      "Step - 6773, Loss - 0.3413543660618286, Learning Rate - 0.00078125, magnitude of gradient - 0.022502360183180616\n",
      "Step - 6774, Loss - 0.28028732094754966, Learning Rate - 0.00078125, magnitude of gradient - 0.08947504903080813\n",
      "Step - 6775, Loss - 0.33922602911120325, Learning Rate - 0.00078125, magnitude of gradient - 0.05689797592126951\n",
      "Step - 6776, Loss - 0.391265322523597, Learning Rate - 0.00078125, magnitude of gradient - 0.08165999288763702\n",
      "Step - 6777, Loss - 0.34752549969057256, Learning Rate - 0.00078125, magnitude of gradient - 0.02487817491967503\n",
      "Step - 6778, Loss - 0.3317958324360348, Learning Rate - 0.00078125, magnitude of gradient - 0.03312354850530602\n",
      "Step - 6779, Loss - 0.33877574295253143, Learning Rate - 0.00078125, magnitude of gradient - 0.032813452880970886\n",
      "Step - 6780, Loss - 0.31718407086871925, Learning Rate - 0.00078125, magnitude of gradient - 0.10790674952734738\n",
      "Step - 6781, Loss - 0.30993926681801587, Learning Rate - 0.00078125, magnitude of gradient - 0.06732153468453557\n",
      "Step - 6782, Loss - 0.3505222772904091, Learning Rate - 0.00078125, magnitude of gradient - 0.058836376445816824\n",
      "Step - 6783, Loss - 0.29126476583824207, Learning Rate - 0.00078125, magnitude of gradient - 0.09661488905202915\n",
      "Step - 6784, Loss - 0.3079628361167152, Learning Rate - 0.00078125, magnitude of gradient - 0.03776205324655907\n",
      "Step - 6785, Loss - 0.3063084289921213, Learning Rate - 0.00078125, magnitude of gradient - 0.027396455484768885\n",
      "Step - 6786, Loss - 0.3337582688841901, Learning Rate - 0.00078125, magnitude of gradient - 0.05693149995365659\n",
      "Step - 6787, Loss - 0.31189167443865395, Learning Rate - 0.00078125, magnitude of gradient - 0.032452127546349054\n",
      "Step - 6788, Loss - 0.24560548257004944, Learning Rate - 0.00078125, magnitude of gradient - 0.018467164231283625\n",
      "Step - 6789, Loss - 0.331091013758504, Learning Rate - 0.00078125, magnitude of gradient - 0.004586332867786972\n",
      "Step - 6790, Loss - 0.3710578017464913, Learning Rate - 0.00078125, magnitude of gradient - 0.06390576419646715\n",
      "Step - 6791, Loss - 0.41606055662933683, Learning Rate - 0.00078125, magnitude of gradient - 0.07466547532614176\n",
      "Step - 6792, Loss - 0.376005739538845, Learning Rate - 0.00078125, magnitude of gradient - 0.042366924748466456\n",
      "Step - 6793, Loss - 0.3771438987682791, Learning Rate - 0.00078125, magnitude of gradient - 0.10140360102380896\n",
      "Step - 6794, Loss - 0.29416724827494917, Learning Rate - 0.00078125, magnitude of gradient - 0.04952130114984889\n",
      "Step - 6795, Loss - 0.35873114928229427, Learning Rate - 0.00078125, magnitude of gradient - 0.058379074868095626\n",
      "Step - 6796, Loss - 0.2913809585984869, Learning Rate - 0.00078125, magnitude of gradient - 0.0909248042420151\n",
      "Step - 6797, Loss - 0.3616899569137518, Learning Rate - 0.00078125, magnitude of gradient - 0.06108412803609074\n",
      "Step - 6798, Loss - 0.38020728692310557, Learning Rate - 0.00078125, magnitude of gradient - 0.06138089637388823\n",
      "Step - 6799, Loss - 0.3651104103179993, Learning Rate - 0.00078125, magnitude of gradient - 0.047248874488580395\n",
      "Step - 6800, Loss - 0.3188564942038215, Learning Rate - 0.00078125, magnitude of gradient - 0.09248711062392717\n",
      "Step - 6801, Loss - 0.3150248964688888, Learning Rate - 0.00078125, magnitude of gradient - 0.01606562699610666\n",
      "Step - 6802, Loss - 0.314105576341595, Learning Rate - 0.00078125, magnitude of gradient - 0.021443919072419396\n",
      "Step - 6803, Loss - 0.3563868016356336, Learning Rate - 0.00078125, magnitude of gradient - 0.04480741779928874\n",
      "Step - 6804, Loss - 0.30184327967124946, Learning Rate - 0.00078125, magnitude of gradient - 0.036114218336812345\n",
      "Step - 6805, Loss - 0.2964118273422742, Learning Rate - 0.00078125, magnitude of gradient - 0.023932013607654266\n",
      "Step - 6806, Loss - 0.29385684343099616, Learning Rate - 0.00078125, magnitude of gradient - 0.0468486336059027\n",
      "Step - 6807, Loss - 0.31147109565032466, Learning Rate - 0.00078125, magnitude of gradient - 0.011705193293477998\n",
      "Step - 6808, Loss - 0.3357845066991949, Learning Rate - 0.00078125, magnitude of gradient - 0.06413963899192975\n",
      "Step - 6809, Loss - 0.32811317548813895, Learning Rate - 0.00078125, magnitude of gradient - 0.05889458140107467\n",
      "Step - 6810, Loss - 0.3480638009917936, Learning Rate - 0.00078125, magnitude of gradient - 0.0843536589947258\n",
      "Step - 6811, Loss - 0.28746626250472834, Learning Rate - 0.00078125, magnitude of gradient - 0.06057613588012661\n",
      "Step - 6812, Loss - 0.31312304254301077, Learning Rate - 0.00078125, magnitude of gradient - 0.04978127665650024\n",
      "Step - 6813, Loss - 0.2675916016012218, Learning Rate - 0.00078125, magnitude of gradient - 0.037127158122839815\n",
      "Step - 6814, Loss - 0.344815283972852, Learning Rate - 0.00078125, magnitude of gradient - 0.015077406999930359\n",
      "Step - 6815, Loss - 0.3025814545834746, Learning Rate - 0.00078125, magnitude of gradient - 0.10365678768165545\n",
      "Step - 6816, Loss - 0.2865690919089932, Learning Rate - 0.00078125, magnitude of gradient - 0.029019737845213694\n",
      "Step - 6817, Loss - 0.31587746411441886, Learning Rate - 0.00078125, magnitude of gradient - 0.05946528360506554\n",
      "Step - 6818, Loss - 0.3017523810522661, Learning Rate - 0.00078125, magnitude of gradient - 0.020720093064996666\n",
      "Step - 6819, Loss - 0.3532066254859199, Learning Rate - 0.00078125, magnitude of gradient - 0.05429632383510862\n",
      "Step - 6820, Loss - 0.2941375423709685, Learning Rate - 0.00078125, magnitude of gradient - 0.04572352265717981\n",
      "Step - 6821, Loss - 0.3639653772048949, Learning Rate - 0.00078125, magnitude of gradient - 0.09310759555589031\n",
      "Step - 6822, Loss - 0.42886710152413005, Learning Rate - 0.00078125, magnitude of gradient - 0.021533204848921322\n",
      "Step - 6823, Loss - 0.287418787932159, Learning Rate - 0.00078125, magnitude of gradient - 0.058179981202301584\n",
      "Step - 6824, Loss - 0.3130005335967054, Learning Rate - 0.00078125, magnitude of gradient - 0.03256323415805552\n",
      "Step - 6825, Loss - 0.2982720980098609, Learning Rate - 0.00078125, magnitude of gradient - 0.035192826690812425\n",
      "Step - 6826, Loss - 0.3384897753307807, Learning Rate - 0.00078125, magnitude of gradient - 0.048685244288138994\n",
      "Step - 6827, Loss - 0.24747773408483492, Learning Rate - 0.00078125, magnitude of gradient - 0.04023192212381215\n",
      "Step - 6828, Loss - 0.31140165799367975, Learning Rate - 0.00078125, magnitude of gradient - 0.03545196255586718\n",
      "Step - 6829, Loss - 0.31322374387217944, Learning Rate - 0.00078125, magnitude of gradient - 0.011309385575723342\n",
      "Step - 6830, Loss - 0.31015254943013004, Learning Rate - 0.00078125, magnitude of gradient - 0.013101805181527178\n",
      "Step - 6831, Loss - 0.27465122850747054, Learning Rate - 0.00078125, magnitude of gradient - 0.07365353180290171\n",
      "Step - 6832, Loss - 0.3196167523222381, Learning Rate - 0.00078125, magnitude of gradient - 0.06809420798705168\n",
      "Step - 6833, Loss - 0.33628945694877727, Learning Rate - 0.00078125, magnitude of gradient - 0.051885004896093\n",
      "Step - 6834, Loss - 0.3174005679774107, Learning Rate - 0.00078125, magnitude of gradient - 0.028560122531927457\n",
      "Step - 6835, Loss - 0.297894207295081, Learning Rate - 0.00078125, magnitude of gradient - 0.056883769715829736\n",
      "Step - 6836, Loss - 0.45527997051087216, Learning Rate - 0.00078125, magnitude of gradient - 0.034295329426587896\n",
      "Step - 6837, Loss - 0.29470053826086373, Learning Rate - 0.00078125, magnitude of gradient - 0.035833748923283955\n",
      "Step - 6838, Loss - 0.380360965797114, Learning Rate - 0.00078125, magnitude of gradient - 0.011317574531914539\n",
      "Step - 6839, Loss - 0.3601696803825737, Learning Rate - 0.00078125, magnitude of gradient - 0.023137350825083967\n",
      "Step - 6840, Loss - 0.2975060641968157, Learning Rate - 0.00078125, magnitude of gradient - 0.047145615296041644\n",
      "Step - 6841, Loss - 0.4130047041940224, Learning Rate - 0.00078125, magnitude of gradient - 0.1153350729901029\n",
      "Step - 6842, Loss - 0.28571649283538164, Learning Rate - 0.00078125, magnitude of gradient - 0.10825439246310768\n",
      "Step - 6843, Loss - 0.3349410393418776, Learning Rate - 0.00078125, magnitude of gradient - 0.07441716794978663\n",
      "Step - 6844, Loss - 0.3372369788148613, Learning Rate - 0.00078125, magnitude of gradient - 0.03294471314153598\n",
      "Step - 6845, Loss - 0.3695417399876137, Learning Rate - 0.00078125, magnitude of gradient - 0.06233647033449404\n",
      "Step - 6846, Loss - 0.35077535496831824, Learning Rate - 0.00078125, magnitude of gradient - 0.08854758181780943\n",
      "Step - 6847, Loss - 0.3012475293818479, Learning Rate - 0.00078125, magnitude of gradient - 0.046227011042940684\n",
      "Step - 6848, Loss - 0.37492563438581405, Learning Rate - 0.00078125, magnitude of gradient - 0.07250667648634183\n",
      "Step - 6849, Loss - 0.3288586282741244, Learning Rate - 0.00078125, magnitude of gradient - 0.023584210141116665\n",
      "Step - 6850, Loss - 0.3779704430397426, Learning Rate - 0.00078125, magnitude of gradient - 0.030699448446137007\n",
      "Step - 6851, Loss - 0.2773068694750839, Learning Rate - 0.00078125, magnitude of gradient - 0.03043119437416086\n",
      "Step - 6852, Loss - 0.2601214717231732, Learning Rate - 0.00078125, magnitude of gradient - 0.07891530669861078\n",
      "Step - 6853, Loss - 0.2975486088683827, Learning Rate - 0.00078125, magnitude of gradient - 0.010216434235666275\n",
      "Step - 6854, Loss - 0.34475515766582776, Learning Rate - 0.00078125, magnitude of gradient - 0.027773466118723937\n",
      "Step - 6855, Loss - 0.36093386417882267, Learning Rate - 0.00078125, magnitude of gradient - 0.011023603671020427\n",
      "Step - 6856, Loss - 0.34146405982653116, Learning Rate - 0.00078125, magnitude of gradient - 0.06838684935796789\n",
      "Step - 6857, Loss - 0.31028269848353296, Learning Rate - 0.00078125, magnitude of gradient - 0.014590103149244013\n",
      "Step - 6858, Loss - 0.3497077032439082, Learning Rate - 0.00078125, magnitude of gradient - 0.044483799575216934\n",
      "Step - 6859, Loss - 0.3127403528524529, Learning Rate - 0.00078125, magnitude of gradient - 0.011220457162398615\n",
      "Step - 6860, Loss - 0.33880927816391887, Learning Rate - 0.00078125, magnitude of gradient - 0.08966919999546193\n",
      "Step - 6861, Loss - 0.25826367010005324, Learning Rate - 0.00078125, magnitude of gradient - 0.036363226122853025\n",
      "Step - 6862, Loss - 0.33721073222875464, Learning Rate - 0.00078125, magnitude of gradient - 0.03704747817230827\n",
      "Step - 6863, Loss - 0.3413518412648339, Learning Rate - 0.00078125, magnitude of gradient - 0.03523683354519887\n",
      "Step - 6864, Loss - 0.2823209266189353, Learning Rate - 0.00078125, magnitude of gradient - 0.030263949427903794\n",
      "Step - 6865, Loss - 0.25371256105504547, Learning Rate - 0.00078125, magnitude of gradient - 0.05769352334814903\n",
      "Step - 6866, Loss - 0.3399260390109727, Learning Rate - 0.00078125, magnitude of gradient - 0.058434760262910744\n",
      "Step - 6867, Loss - 0.3217361017312721, Learning Rate - 0.00078125, magnitude of gradient - 0.02702225424152651\n",
      "Step - 6868, Loss - 0.34686183483813127, Learning Rate - 0.00078125, magnitude of gradient - 0.09190158956761646\n",
      "Step - 6869, Loss - 0.21824689581968018, Learning Rate - 0.00078125, magnitude of gradient - 0.09005496334322521\n",
      "Step - 6870, Loss - 0.3578891442902324, Learning Rate - 0.00078125, magnitude of gradient - 0.02546489070731026\n",
      "Step - 6871, Loss - 0.3210478457465163, Learning Rate - 0.00078125, magnitude of gradient - 0.0724979958654928\n",
      "Step - 6872, Loss - 0.3450826047313657, Learning Rate - 0.00078125, magnitude of gradient - 0.024055235985062463\n",
      "Step - 6873, Loss - 0.269610770043678, Learning Rate - 0.00078125, magnitude of gradient - 0.056796525182093806\n",
      "Step - 6874, Loss - 0.3140420087101412, Learning Rate - 0.00078125, magnitude of gradient - 0.03586894071345205\n",
      "Step - 6875, Loss - 0.32718619244093905, Learning Rate - 0.00078125, magnitude of gradient - 0.021144578094290906\n",
      "Step - 6876, Loss - 0.303529396493757, Learning Rate - 0.00078125, magnitude of gradient - 0.048411863801294565\n",
      "Step - 6877, Loss - 0.2928902547524391, Learning Rate - 0.00078125, magnitude of gradient - 0.032531606122993294\n",
      "Step - 6878, Loss - 0.30641143003686516, Learning Rate - 0.00078125, magnitude of gradient - 0.014136226724409307\n",
      "Step - 6879, Loss - 0.27528703389892806, Learning Rate - 0.00078125, magnitude of gradient - 0.09741764870693423\n",
      "Step - 6880, Loss - 0.3729113939042621, Learning Rate - 0.00078125, magnitude of gradient - 0.01447443097075453\n",
      "Step - 6881, Loss - 0.30325287462769596, Learning Rate - 0.00078125, magnitude of gradient - 0.07114996501658273\n",
      "Step - 6882, Loss - 0.3428494637821712, Learning Rate - 0.00078125, magnitude of gradient - 0.05072069411051191\n",
      "Step - 6883, Loss - 0.2658612757760635, Learning Rate - 0.00078125, magnitude of gradient - 0.058315691072110154\n",
      "Step - 6884, Loss - 0.3966286258952493, Learning Rate - 0.00078125, magnitude of gradient - 0.04028022319816206\n",
      "Step - 6885, Loss - 0.3444701120242411, Learning Rate - 0.00078125, magnitude of gradient - 0.03598758542462208\n",
      "Step - 6886, Loss - 0.2925390946074989, Learning Rate - 0.00078125, magnitude of gradient - 0.07830417092607961\n",
      "Step - 6887, Loss - 0.4049813196542214, Learning Rate - 0.00078125, magnitude of gradient - 0.041537634934719084\n",
      "Step - 6888, Loss - 0.40053728857854576, Learning Rate - 0.00078125, magnitude of gradient - 0.07765590868535083\n",
      "Step - 6889, Loss - 0.3363984039040213, Learning Rate - 0.00078125, magnitude of gradient - 0.047463740923347646\n",
      "Step - 6890, Loss - 0.3556606105457802, Learning Rate - 0.00078125, magnitude of gradient - 0.043568362414336766\n",
      "Step - 6891, Loss - 0.2712821378590598, Learning Rate - 0.00078125, magnitude of gradient - 0.020246864513072657\n",
      "Step - 6892, Loss - 0.26893490212003923, Learning Rate - 0.00078125, magnitude of gradient - 0.04192678465700998\n",
      "Step - 6893, Loss - 0.38492224248939977, Learning Rate - 0.00078125, magnitude of gradient - 0.08166722223249534\n",
      "Step - 6894, Loss - 0.2838855232882258, Learning Rate - 0.00078125, magnitude of gradient - 0.011607704118146009\n",
      "Step - 6895, Loss - 0.28129010819967737, Learning Rate - 0.00078125, magnitude of gradient - 0.062143961695205356\n",
      "Step - 6896, Loss - 0.30308852509913237, Learning Rate - 0.00078125, magnitude of gradient - 0.1089588562607826\n",
      "Step - 6897, Loss - 0.2988342381644661, Learning Rate - 0.00078125, magnitude of gradient - 0.06606758267374128\n",
      "Step - 6898, Loss - 0.27748050636670335, Learning Rate - 0.00078125, magnitude of gradient - 0.027152222121363673\n",
      "Step - 6899, Loss - 0.3293282799459647, Learning Rate - 0.00078125, magnitude of gradient - 0.04763082396997246\n",
      "Step - 6900, Loss - 0.3545685573614649, Learning Rate - 0.00078125, magnitude of gradient - 0.061635410445984876\n",
      "Step - 6901, Loss - 0.42389833788134884, Learning Rate - 0.00078125, magnitude of gradient - 0.05174311567398646\n",
      "Step - 6902, Loss - 0.3414749053681219, Learning Rate - 0.00078125, magnitude of gradient - 0.07018505760867053\n",
      "Step - 6903, Loss - 0.35739593584319695, Learning Rate - 0.00078125, magnitude of gradient - 0.04706125817645455\n",
      "Step - 6904, Loss - 0.24813301446797445, Learning Rate - 0.00078125, magnitude of gradient - 0.01141016230561558\n",
      "Step - 6905, Loss - 0.2698575577601017, Learning Rate - 0.00078125, magnitude of gradient - 0.029052831244912775\n",
      "Step - 6906, Loss - 0.33957719662953756, Learning Rate - 0.00078125, magnitude of gradient - 0.10595237376988229\n",
      "Step - 6907, Loss - 0.3387615853235587, Learning Rate - 0.00078125, magnitude of gradient - 0.01412060038923109\n",
      "Step - 6908, Loss - 0.3467558690448244, Learning Rate - 0.00078125, magnitude of gradient - 0.04048762672687342\n",
      "Step - 6909, Loss - 0.3220222452942072, Learning Rate - 0.00078125, magnitude of gradient - 0.0675745385608236\n",
      "Step - 6910, Loss - 0.3435293577664474, Learning Rate - 0.00078125, magnitude of gradient - 0.07195712950858682\n",
      "Step - 6911, Loss - 0.31657453112184275, Learning Rate - 0.00078125, magnitude of gradient - 0.006121370250636287\n",
      "Step - 6912, Loss - 0.2478522018809312, Learning Rate - 0.00078125, magnitude of gradient - 0.00524021711571838\n",
      "Step - 6913, Loss - 0.2820716552057874, Learning Rate - 0.00078125, magnitude of gradient - 0.021135947371013025\n",
      "Step - 6914, Loss - 0.36709323965353663, Learning Rate - 0.00078125, magnitude of gradient - 0.05170637066877563\n",
      "Step - 6915, Loss - 0.36266123612347073, Learning Rate - 0.00078125, magnitude of gradient - 0.03677418099395731\n",
      "Step - 6916, Loss - 0.39354897626480667, Learning Rate - 0.00078125, magnitude of gradient - 0.04345896107188233\n",
      "Step - 6917, Loss - 0.27351408660703735, Learning Rate - 0.00078125, magnitude of gradient - 0.05888748261396615\n",
      "Step - 6918, Loss - 0.3814866139161145, Learning Rate - 0.00078125, magnitude of gradient - 0.0463992568115118\n",
      "Step - 6919, Loss - 0.28147847329443026, Learning Rate - 0.00078125, magnitude of gradient - 0.08372143055581227\n",
      "Step - 6920, Loss - 0.3362082329547832, Learning Rate - 0.00078125, magnitude of gradient - 0.06430520130484504\n",
      "Step - 6921, Loss - 0.3080157585130133, Learning Rate - 0.00078125, magnitude of gradient - 0.034441104539496246\n",
      "Step - 6922, Loss - 0.34219314635427733, Learning Rate - 0.00078125, magnitude of gradient - 0.08048756179543355\n",
      "Step - 6923, Loss - 0.30746425687012324, Learning Rate - 0.00078125, magnitude of gradient - 0.042454870325085665\n",
      "Step - 6924, Loss - 0.26077860084449433, Learning Rate - 0.00078125, magnitude of gradient - 0.012416565266986467\n",
      "Step - 6925, Loss - 0.3433614074401319, Learning Rate - 0.00078125, magnitude of gradient - 0.014009733698231795\n",
      "Step - 6926, Loss - 0.32659842314354554, Learning Rate - 0.00078125, magnitude of gradient - 0.07280415907760553\n",
      "Step - 6927, Loss - 0.2711312642949295, Learning Rate - 0.00078125, magnitude of gradient - 0.06749122932520309\n",
      "Step - 6928, Loss - 0.23642335150791596, Learning Rate - 0.00078125, magnitude of gradient - 0.024446497405609603\n",
      "Step - 6929, Loss - 0.3449037659021219, Learning Rate - 0.00078125, magnitude of gradient - 0.0625194621473536\n",
      "Step - 6930, Loss - 0.3414839106665206, Learning Rate - 0.00078125, magnitude of gradient - 0.03902681410734081\n",
      "Step - 6931, Loss - 0.40318808612277, Learning Rate - 0.00078125, magnitude of gradient - 0.06451996994309689\n",
      "Step - 6932, Loss - 0.3869464852133007, Learning Rate - 0.00078125, magnitude of gradient - 0.08393162454367026\n",
      "Step - 6933, Loss - 0.3035085077756521, Learning Rate - 0.00078125, magnitude of gradient - 0.02761779302784706\n",
      "Step - 6934, Loss - 0.27449107074286416, Learning Rate - 0.00078125, magnitude of gradient - 0.05205772196066763\n",
      "Step - 6935, Loss - 0.28287444109318793, Learning Rate - 0.00078125, magnitude of gradient - 0.11963690226171889\n",
      "Step - 6936, Loss - 0.36055361220208065, Learning Rate - 0.00078125, magnitude of gradient - 0.089099149945406\n",
      "Step - 6937, Loss - 0.34211677097505183, Learning Rate - 0.00078125, magnitude of gradient - 0.03389318391345906\n",
      "Step - 6938, Loss - 0.21769398987506944, Learning Rate - 0.00078125, magnitude of gradient - 0.044140372406931555\n",
      "Step - 6939, Loss - 0.2915421642725086, Learning Rate - 0.00078125, magnitude of gradient - 0.06012520392714849\n",
      "Step - 6940, Loss - 0.3656186151956241, Learning Rate - 0.00078125, magnitude of gradient - 0.018046802334396116\n",
      "Step - 6941, Loss - 0.32635136385751307, Learning Rate - 0.00078125, magnitude of gradient - 0.04588072958877954\n",
      "Step - 6942, Loss - 0.34391476427737133, Learning Rate - 0.00078125, magnitude of gradient - 0.024791919754123724\n",
      "Step - 6943, Loss - 0.30100599626001856, Learning Rate - 0.00078125, magnitude of gradient - 0.02282440459645214\n",
      "Step - 6944, Loss - 0.2761902022952052, Learning Rate - 0.00078125, magnitude of gradient - 0.037313604557908116\n",
      "Step - 6945, Loss - 0.3243601713784944, Learning Rate - 0.00078125, magnitude of gradient - 0.04213235995518422\n",
      "Step - 6946, Loss - 0.25797412121173363, Learning Rate - 0.00078125, magnitude of gradient - 0.01114326598737605\n",
      "Step - 6947, Loss - 0.31189894351122066, Learning Rate - 0.00078125, magnitude of gradient - 0.046024742821002315\n",
      "Step - 6948, Loss - 0.34319704484301705, Learning Rate - 0.00078125, magnitude of gradient - 0.09625568504332589\n",
      "Step - 6949, Loss - 0.31800641553285175, Learning Rate - 0.00078125, magnitude of gradient - 0.039965536753549226\n",
      "Step - 6950, Loss - 0.31907678044859306, Learning Rate - 0.00078125, magnitude of gradient - 0.05165666495439269\n",
      "Step - 6951, Loss - 0.24547866078567593, Learning Rate - 0.00078125, magnitude of gradient - 0.0308253433906922\n",
      "Step - 6952, Loss - 0.3167644762120444, Learning Rate - 0.00078125, magnitude of gradient - 0.017568805706121876\n",
      "Step - 6953, Loss - 0.31787243054163544, Learning Rate - 0.00078125, magnitude of gradient - 0.028620688830203255\n",
      "Step - 6954, Loss - 0.32500405493881124, Learning Rate - 0.00078125, magnitude of gradient - 0.026521984545082596\n",
      "Step - 6955, Loss - 0.33779856702770333, Learning Rate - 0.00078125, magnitude of gradient - 0.025804113826051207\n",
      "Step - 6956, Loss - 0.3394833376054064, Learning Rate - 0.00078125, magnitude of gradient - 0.09342999557807673\n",
      "Step - 6957, Loss - 0.2996387404592704, Learning Rate - 0.00078125, magnitude of gradient - 0.07043723933777352\n",
      "Step - 6958, Loss - 0.24817208188127182, Learning Rate - 0.00078125, magnitude of gradient - 0.02262358418767862\n",
      "Step - 6959, Loss - 0.3889622577937775, Learning Rate - 0.00078125, magnitude of gradient - 0.0655591043013544\n",
      "Step - 6960, Loss - 0.27391400355688383, Learning Rate - 0.00078125, magnitude of gradient - 0.045508307361091224\n",
      "Step - 6961, Loss - 0.3708553269504157, Learning Rate - 0.00078125, magnitude of gradient - 0.027350572714793155\n",
      "Step - 6962, Loss - 0.2807669239865682, Learning Rate - 0.00078125, magnitude of gradient - 0.008212000848863215\n",
      "Step - 6963, Loss - 0.33830921608438813, Learning Rate - 0.00078125, magnitude of gradient - 0.058986669484533696\n",
      "Step - 6964, Loss - 0.286366412638756, Learning Rate - 0.00078125, magnitude of gradient - 0.06496874231041543\n",
      "Step - 6965, Loss - 0.3116407931873696, Learning Rate - 0.00078125, magnitude of gradient - 0.07116983032115175\n",
      "Step - 6966, Loss - 0.3257403035260069, Learning Rate - 0.00078125, magnitude of gradient - 0.0947864351067311\n",
      "Step - 6967, Loss - 0.22103968046145986, Learning Rate - 0.00078125, magnitude of gradient - 0.04822077046793176\n",
      "Step - 6968, Loss - 0.3560969638531342, Learning Rate - 0.00078125, magnitude of gradient - 0.046127431302823596\n",
      "Step - 6969, Loss - 0.30927476864755266, Learning Rate - 0.00078125, magnitude of gradient - 0.11746236995443796\n",
      "Step - 6970, Loss - 0.2613478168685752, Learning Rate - 0.00078125, magnitude of gradient - 0.07861142857984707\n",
      "Step - 6971, Loss - 0.3741259580145684, Learning Rate - 0.00078125, magnitude of gradient - 0.09369869840987294\n",
      "Step - 6972, Loss - 0.3280002112085052, Learning Rate - 0.00078125, magnitude of gradient - 0.033010328562586476\n",
      "Step - 6973, Loss - 0.4067528136772179, Learning Rate - 0.00078125, magnitude of gradient - 0.06258851616497522\n",
      "Step - 6974, Loss - 0.26344191249776805, Learning Rate - 0.00078125, magnitude of gradient - 0.05218671311001195\n",
      "Step - 6975, Loss - 0.3293910456725013, Learning Rate - 0.00078125, magnitude of gradient - 0.03118363151336074\n",
      "Step - 6976, Loss - 0.3560279586706808, Learning Rate - 0.00078125, magnitude of gradient - 0.02112463025368475\n",
      "Step - 6977, Loss - 0.35513530538562577, Learning Rate - 0.00078125, magnitude of gradient - 0.0684376327135321\n",
      "Step - 6978, Loss - 0.2652003879108254, Learning Rate - 0.00078125, magnitude of gradient - 0.08368408141433799\n",
      "Step - 6979, Loss - 0.3420231314922034, Learning Rate - 0.00078125, magnitude of gradient - 0.07159109418550474\n",
      "Step - 6980, Loss - 0.36149402811942677, Learning Rate - 0.00078125, magnitude of gradient - 0.028426183247789078\n",
      "Step - 6981, Loss - 0.32264112898906255, Learning Rate - 0.00078125, magnitude of gradient - 0.015635293918402623\n",
      "Step - 6982, Loss - 0.3209354857437584, Learning Rate - 0.00078125, magnitude of gradient - 0.06732425956179791\n",
      "Step - 6983, Loss - 0.3479166756189815, Learning Rate - 0.00078125, magnitude of gradient - 0.04117012475876015\n",
      "Step - 6984, Loss - 0.31851491232614293, Learning Rate - 0.00078125, magnitude of gradient - 0.026928772878143305\n",
      "Step - 6985, Loss - 0.3319335914126693, Learning Rate - 0.00078125, magnitude of gradient - 0.042171542502626495\n",
      "Step - 6986, Loss - 0.37369602896206683, Learning Rate - 0.00078125, magnitude of gradient - 0.0752078985684237\n",
      "Step - 6987, Loss - 0.2732253257258418, Learning Rate - 0.00078125, magnitude of gradient - 0.060129805731826615\n",
      "Step - 6988, Loss - 0.3155802638749443, Learning Rate - 0.00078125, magnitude of gradient - 0.03883709767426874\n",
      "Step - 6989, Loss - 0.3169501587752878, Learning Rate - 0.00078125, magnitude of gradient - 0.04046640969739544\n",
      "Step - 6990, Loss - 0.2275714834997981, Learning Rate - 0.00078125, magnitude of gradient - 0.01386510880495903\n",
      "Step - 6991, Loss - 0.2768191374401709, Learning Rate - 0.00078125, magnitude of gradient - 0.037492135918679434\n",
      "Step - 6992, Loss - 0.27362367372214336, Learning Rate - 0.00078125, magnitude of gradient - 0.02246637595867886\n",
      "Step - 6993, Loss - 0.34816764743293654, Learning Rate - 0.00078125, magnitude of gradient - 0.024233356961936788\n",
      "Step - 6994, Loss - 0.3536398432360999, Learning Rate - 0.00078125, magnitude of gradient - 0.054404989096710425\n",
      "Step - 6995, Loss - 0.37761190171320647, Learning Rate - 0.00078125, magnitude of gradient - 0.04082659554239973\n",
      "Step - 6996, Loss - 0.3822994226450735, Learning Rate - 0.00078125, magnitude of gradient - 0.030102160457098848\n",
      "Step - 6997, Loss - 0.33014472899355124, Learning Rate - 0.00078125, magnitude of gradient - 0.05819055668171011\n",
      "Step - 6998, Loss - 0.2974617522205769, Learning Rate - 0.00078125, magnitude of gradient - 0.0940275974041305\n",
      "Step - 6999, Loss - 0.3202554292835035, Learning Rate - 0.00078125, magnitude of gradient - 0.07406942320630763\n",
      "Step - 7000, Loss - 0.29783105064270043, Learning Rate - 0.00078125, magnitude of gradient - 0.105260961171692\n",
      "Step - 7001, Loss - 0.3124727092177131, Learning Rate - 0.000390625, magnitude of gradient - 0.040951278576822576\n",
      "Step - 7002, Loss - 0.3774217684359567, Learning Rate - 0.000390625, magnitude of gradient - 0.032674772632958114\n",
      "Step - 7003, Loss - 0.30408043696917886, Learning Rate - 0.000390625, magnitude of gradient - 0.04061589485717874\n",
      "Step - 7004, Loss - 0.3305000443284398, Learning Rate - 0.000390625, magnitude of gradient - 0.056824345125293024\n",
      "Step - 7005, Loss - 0.3245120571234031, Learning Rate - 0.000390625, magnitude of gradient - 0.11466712009769449\n",
      "Step - 7006, Loss - 0.24962810144391373, Learning Rate - 0.000390625, magnitude of gradient - 0.04034570454700208\n",
      "Step - 7007, Loss - 0.31776976231095977, Learning Rate - 0.000390625, magnitude of gradient - 0.05398979685673471\n",
      "Step - 7008, Loss - 0.3062869949959607, Learning Rate - 0.000390625, magnitude of gradient - 0.06898469251526793\n",
      "Step - 7009, Loss - 0.34991974188379926, Learning Rate - 0.000390625, magnitude of gradient - 0.033079845577415896\n",
      "Step - 7010, Loss - 0.3279280565830348, Learning Rate - 0.000390625, magnitude of gradient - 0.04209360268070632\n",
      "Step - 7011, Loss - 0.3563359007797843, Learning Rate - 0.000390625, magnitude of gradient - 0.046731115924702554\n",
      "Step - 7012, Loss - 0.39089500754840834, Learning Rate - 0.000390625, magnitude of gradient - 0.06530366492528228\n",
      "Step - 7013, Loss - 0.2611344455110242, Learning Rate - 0.000390625, magnitude of gradient - 0.02429328739251485\n",
      "Step - 7014, Loss - 0.4118277892969797, Learning Rate - 0.000390625, magnitude of gradient - 0.09377318879433412\n",
      "Step - 7015, Loss - 0.3682024739136377, Learning Rate - 0.000390625, magnitude of gradient - 0.03233847335335077\n",
      "Step - 7016, Loss - 0.349655598080035, Learning Rate - 0.000390625, magnitude of gradient - 0.04750424014242389\n",
      "Step - 7017, Loss - 0.29783228843855303, Learning Rate - 0.000390625, magnitude of gradient - 0.054263285689006374\n",
      "Step - 7018, Loss - 0.4537433965353231, Learning Rate - 0.000390625, magnitude of gradient - 0.03171933188958193\n",
      "Step - 7019, Loss - 0.30042277222170877, Learning Rate - 0.000390625, magnitude of gradient - 0.046148636212039684\n",
      "Step - 7020, Loss - 0.33161664065244545, Learning Rate - 0.000390625, magnitude of gradient - 0.024833387616365575\n",
      "Step - 7021, Loss - 0.366915426787197, Learning Rate - 0.000390625, magnitude of gradient - 0.061145738711072116\n",
      "Step - 7022, Loss - 0.3574154111054468, Learning Rate - 0.000390625, magnitude of gradient - 0.02503448017196996\n",
      "Step - 7023, Loss - 0.37444984884269694, Learning Rate - 0.000390625, magnitude of gradient - 0.007900278718569517\n",
      "Step - 7024, Loss - 0.374298395656189, Learning Rate - 0.000390625, magnitude of gradient - 0.11724308123331684\n",
      "Step - 7025, Loss - 0.37140224865587124, Learning Rate - 0.000390625, magnitude of gradient - 0.054377036558220494\n",
      "Step - 7026, Loss - 0.2815080592540139, Learning Rate - 0.000390625, magnitude of gradient - 0.0978059952837619\n",
      "Step - 7027, Loss - 0.390427079419288, Learning Rate - 0.000390625, magnitude of gradient - 0.03622394073234632\n",
      "Step - 7028, Loss - 0.35614293213853, Learning Rate - 0.000390625, magnitude of gradient - 0.10613639653728363\n",
      "Step - 7029, Loss - 0.33891493447074794, Learning Rate - 0.000390625, magnitude of gradient - 0.05335918306868406\n",
      "Step - 7030, Loss - 0.350201668932722, Learning Rate - 0.000390625, magnitude of gradient - 0.05295567852687267\n",
      "Step - 7031, Loss - 0.3965434224730247, Learning Rate - 0.000390625, magnitude of gradient - 0.025292054594757106\n",
      "Step - 7032, Loss - 0.2131731337281447, Learning Rate - 0.000390625, magnitude of gradient - 0.015911066024304622\n",
      "Step - 7033, Loss - 0.2845444182176155, Learning Rate - 0.000390625, magnitude of gradient - 0.0413283979658313\n",
      "Step - 7034, Loss - 0.37916493299539555, Learning Rate - 0.000390625, magnitude of gradient - 0.07105293751597125\n",
      "Step - 7035, Loss - 0.27641954172572897, Learning Rate - 0.000390625, magnitude of gradient - 0.022482112862995382\n",
      "Step - 7036, Loss - 0.32015160457908176, Learning Rate - 0.000390625, magnitude of gradient - 0.006711239104799033\n",
      "Step - 7037, Loss - 0.3531131679131959, Learning Rate - 0.000390625, magnitude of gradient - 0.06831367270814317\n",
      "Step - 7038, Loss - 0.2641435284552713, Learning Rate - 0.000390625, magnitude of gradient - 0.0811323414485684\n",
      "Step - 7039, Loss - 0.26746550163194577, Learning Rate - 0.000390625, magnitude of gradient - 0.04834632050937996\n",
      "Step - 7040, Loss - 0.403992517692337, Learning Rate - 0.000390625, magnitude of gradient - 0.026868723779746778\n",
      "Step - 7041, Loss - 0.28172152563530745, Learning Rate - 0.000390625, magnitude of gradient - 0.024124618694445685\n",
      "Step - 7042, Loss - 0.3343447430471294, Learning Rate - 0.000390625, magnitude of gradient - 0.04863886472902437\n",
      "Step - 7043, Loss - 0.40229992145591265, Learning Rate - 0.000390625, magnitude of gradient - 0.11529612437373589\n",
      "Step - 7044, Loss - 0.3430202284410739, Learning Rate - 0.000390625, magnitude of gradient - 0.0764919202717038\n",
      "Step - 7045, Loss - 0.38321344336601526, Learning Rate - 0.000390625, magnitude of gradient - 0.02621566332279827\n",
      "Step - 7046, Loss - 0.3902119001382851, Learning Rate - 0.000390625, magnitude of gradient - 0.1193621072117867\n",
      "Step - 7047, Loss - 0.3011345193669047, Learning Rate - 0.000390625, magnitude of gradient - 0.030660351727344878\n",
      "Step - 7048, Loss - 0.31234941853546044, Learning Rate - 0.000390625, magnitude of gradient - 0.04162367534523835\n",
      "Step - 7049, Loss - 0.2914474833296222, Learning Rate - 0.000390625, magnitude of gradient - 0.03540080988201162\n",
      "Step - 7050, Loss - 0.2671989422242895, Learning Rate - 0.000390625, magnitude of gradient - 0.019087095469037824\n",
      "Step - 7051, Loss - 0.43944754613187254, Learning Rate - 0.000390625, magnitude of gradient - 0.0650394215299829\n",
      "Step - 7052, Loss - 0.36887733899667935, Learning Rate - 0.000390625, magnitude of gradient - 0.0614370194740009\n",
      "Step - 7053, Loss - 0.3831144913749249, Learning Rate - 0.000390625, magnitude of gradient - 0.05310635888533153\n",
      "Step - 7054, Loss - 0.2959207579974086, Learning Rate - 0.000390625, magnitude of gradient - 0.07435824217365987\n",
      "Step - 7055, Loss - 0.332301684510063, Learning Rate - 0.000390625, magnitude of gradient - 0.021491149199259336\n",
      "Step - 7056, Loss - 0.2978931771796444, Learning Rate - 0.000390625, magnitude of gradient - 0.08805916474285452\n",
      "Step - 7057, Loss - 0.23970239458817838, Learning Rate - 0.000390625, magnitude of gradient - 0.05735845525360252\n",
      "Step - 7058, Loss - 0.2829987233550514, Learning Rate - 0.000390625, magnitude of gradient - 0.05949270845930126\n",
      "Step - 7059, Loss - 0.3534226590683062, Learning Rate - 0.000390625, magnitude of gradient - 0.0529595907299231\n",
      "Step - 7060, Loss - 0.3269328267681141, Learning Rate - 0.000390625, magnitude of gradient - 0.018363683968718705\n",
      "Step - 7061, Loss - 0.3082505316404303, Learning Rate - 0.000390625, magnitude of gradient - 0.016103048082187765\n",
      "Step - 7062, Loss - 0.31601232650632954, Learning Rate - 0.000390625, magnitude of gradient - 0.033140803640119856\n",
      "Step - 7063, Loss - 0.2995928381263964, Learning Rate - 0.000390625, magnitude of gradient - 0.04617097301755315\n",
      "Step - 7064, Loss - 0.3439234357494496, Learning Rate - 0.000390625, magnitude of gradient - 0.057444416691805646\n",
      "Step - 7065, Loss - 0.3395004425239266, Learning Rate - 0.000390625, magnitude of gradient - 0.07778805953479971\n",
      "Step - 7066, Loss - 0.37242112176047154, Learning Rate - 0.000390625, magnitude of gradient - 0.025177704093641604\n",
      "Step - 7067, Loss - 0.32135645807456525, Learning Rate - 0.000390625, magnitude of gradient - 0.05149756506341625\n",
      "Step - 7068, Loss - 0.35582056633089837, Learning Rate - 0.000390625, magnitude of gradient - 0.10622795563795355\n",
      "Step - 7069, Loss - 0.3179867598729437, Learning Rate - 0.000390625, magnitude of gradient - 0.059295627375177226\n",
      "Step - 7070, Loss - 0.3104703476085612, Learning Rate - 0.000390625, magnitude of gradient - 0.049789098878487176\n",
      "Step - 7071, Loss - 0.32595353021399626, Learning Rate - 0.000390625, magnitude of gradient - 0.06362292485350618\n",
      "Step - 7072, Loss - 0.29973067756183713, Learning Rate - 0.000390625, magnitude of gradient - 0.07270244178827363\n",
      "Step - 7073, Loss - 0.4032142279096652, Learning Rate - 0.000390625, magnitude of gradient - 0.06493987139884551\n",
      "Step - 7074, Loss - 0.25994572402570315, Learning Rate - 0.000390625, magnitude of gradient - 0.09851341448455224\n",
      "Step - 7075, Loss - 0.3283956799179163, Learning Rate - 0.000390625, magnitude of gradient - 0.014207754903033413\n",
      "Step - 7076, Loss - 0.3402138076098913, Learning Rate - 0.000390625, magnitude of gradient - 0.011166545999762996\n",
      "Step - 7077, Loss - 0.28856157590368714, Learning Rate - 0.000390625, magnitude of gradient - 0.12723718223081426\n",
      "Step - 7078, Loss - 0.28899449756216306, Learning Rate - 0.000390625, magnitude of gradient - 0.01802995178526398\n",
      "Step - 7079, Loss - 0.35241746190861, Learning Rate - 0.000390625, magnitude of gradient - 0.04796359529121236\n",
      "Step - 7080, Loss - 0.3458814272743881, Learning Rate - 0.000390625, magnitude of gradient - 0.05090050581101073\n",
      "Step - 7081, Loss - 0.3098280996292053, Learning Rate - 0.000390625, magnitude of gradient - 0.0667062150379091\n",
      "Step - 7082, Loss - 0.355184312024165, Learning Rate - 0.000390625, magnitude of gradient - 0.006801317241551767\n",
      "Step - 7083, Loss - 0.35970808687300104, Learning Rate - 0.000390625, magnitude of gradient - 0.09188932498975051\n",
      "Step - 7084, Loss - 0.28149923288807144, Learning Rate - 0.000390625, magnitude of gradient - 0.09621503749917011\n",
      "Step - 7085, Loss - 0.34923068076269587, Learning Rate - 0.000390625, magnitude of gradient - 0.05770525916843035\n",
      "Step - 7086, Loss - 0.36448241245621193, Learning Rate - 0.000390625, magnitude of gradient - 0.007081763093113101\n",
      "Step - 7087, Loss - 0.34002077271400266, Learning Rate - 0.000390625, magnitude of gradient - 0.11942096484236812\n",
      "Step - 7088, Loss - 0.30965958940149574, Learning Rate - 0.000390625, magnitude of gradient - 0.057478529998001314\n",
      "Step - 7089, Loss - 0.2836828588274852, Learning Rate - 0.000390625, magnitude of gradient - 0.003567455228863232\n",
      "Step - 7090, Loss - 0.3340698536185955, Learning Rate - 0.000390625, magnitude of gradient - 0.012775902079447244\n",
      "Step - 7091, Loss - 0.32125219970747165, Learning Rate - 0.000390625, magnitude of gradient - 0.03401285442511542\n",
      "Step - 7092, Loss - 0.32870802559197404, Learning Rate - 0.000390625, magnitude of gradient - 0.02855638818845529\n",
      "Step - 7093, Loss - 0.3364384004304559, Learning Rate - 0.000390625, magnitude of gradient - 0.061074239331406106\n",
      "Step - 7094, Loss - 0.2959259186691119, Learning Rate - 0.000390625, magnitude of gradient - 0.09029697985731105\n",
      "Step - 7095, Loss - 0.2871111406143535, Learning Rate - 0.000390625, magnitude of gradient - 0.007101929356690032\n",
      "Step - 7096, Loss - 0.285895109346425, Learning Rate - 0.000390625, magnitude of gradient - 0.00714100338120343\n",
      "Step - 7097, Loss - 0.3233302738977006, Learning Rate - 0.000390625, magnitude of gradient - 0.054644059035930614\n",
      "Step - 7098, Loss - 0.33386592366810813, Learning Rate - 0.000390625, magnitude of gradient - 0.04049638132410376\n",
      "Step - 7099, Loss - 0.25629417431922585, Learning Rate - 0.000390625, magnitude of gradient - 0.043233449878962736\n",
      "Step - 7100, Loss - 0.3097812880706759, Learning Rate - 0.000390625, magnitude of gradient - 0.056648225994296225\n",
      "Step - 7101, Loss - 0.3314468288345283, Learning Rate - 0.000390625, magnitude of gradient - 0.07878641863324692\n",
      "Step - 7102, Loss - 0.3697773136503339, Learning Rate - 0.000390625, magnitude of gradient - 0.09714098609233773\n",
      "Step - 7103, Loss - 0.3272911595461907, Learning Rate - 0.000390625, magnitude of gradient - 0.07567376620628882\n",
      "Step - 7104, Loss - 0.3272757091393036, Learning Rate - 0.000390625, magnitude of gradient - 0.08563153482191221\n",
      "Step - 7105, Loss - 0.3394744851219636, Learning Rate - 0.000390625, magnitude of gradient - 0.03371265164919919\n",
      "Step - 7106, Loss - 0.35915566285150297, Learning Rate - 0.000390625, magnitude of gradient - 0.08355667446196247\n",
      "Step - 7107, Loss - 0.34960728493835014, Learning Rate - 0.000390625, magnitude of gradient - 0.1253188797831629\n",
      "Step - 7108, Loss - 0.3022745274339001, Learning Rate - 0.000390625, magnitude of gradient - 0.054710943745044906\n",
      "Step - 7109, Loss - 0.3038416670656831, Learning Rate - 0.000390625, magnitude of gradient - 0.06224670461584516\n",
      "Step - 7110, Loss - 0.31255212250707043, Learning Rate - 0.000390625, magnitude of gradient - 0.031628259065722746\n",
      "Step - 7111, Loss - 0.32871798843522637, Learning Rate - 0.000390625, magnitude of gradient - 0.052552848100995864\n",
      "Step - 7112, Loss - 0.2906183762317643, Learning Rate - 0.000390625, magnitude of gradient - 0.07552251538825715\n",
      "Step - 7113, Loss - 0.3159973290830157, Learning Rate - 0.000390625, magnitude of gradient - 0.07633628783819874\n",
      "Step - 7114, Loss - 0.2603911992781418, Learning Rate - 0.000390625, magnitude of gradient - 0.06328256459302968\n",
      "Step - 7115, Loss - 0.31340074517259986, Learning Rate - 0.000390625, magnitude of gradient - 0.02939146071337283\n",
      "Step - 7116, Loss - 0.27302612401358806, Learning Rate - 0.000390625, magnitude of gradient - 0.043149473242614055\n",
      "Step - 7117, Loss - 0.33392631810272655, Learning Rate - 0.000390625, magnitude of gradient - 0.08784305687618751\n",
      "Step - 7118, Loss - 0.28095166214961464, Learning Rate - 0.000390625, magnitude of gradient - 0.030515313929804715\n",
      "Step - 7119, Loss - 0.2990733710947783, Learning Rate - 0.000390625, magnitude of gradient - 0.009955130456221902\n",
      "Step - 7120, Loss - 0.30300809421470565, Learning Rate - 0.000390625, magnitude of gradient - 0.0569088775945353\n",
      "Step - 7121, Loss - 0.28662762141098924, Learning Rate - 0.000390625, magnitude of gradient - 0.03419691230060353\n",
      "Step - 7122, Loss - 0.33437915958426484, Learning Rate - 0.000390625, magnitude of gradient - 0.020179524790721746\n",
      "Step - 7123, Loss - 0.24655535388606398, Learning Rate - 0.000390625, magnitude of gradient - 0.058077182627384\n",
      "Step - 7124, Loss - 0.32656080703419843, Learning Rate - 0.000390625, magnitude of gradient - 0.06106772822590891\n",
      "Step - 7125, Loss - 0.380885052284854, Learning Rate - 0.000390625, magnitude of gradient - 0.06105034051062526\n",
      "Step - 7126, Loss - 0.30272890515337497, Learning Rate - 0.000390625, magnitude of gradient - 0.03213588589876625\n",
      "Step - 7127, Loss - 0.2800250975863987, Learning Rate - 0.000390625, magnitude of gradient - 0.02297178740435959\n",
      "Step - 7128, Loss - 0.3801779924895616, Learning Rate - 0.000390625, magnitude of gradient - 0.0461131492319318\n",
      "Step - 7129, Loss - 0.26158132052396293, Learning Rate - 0.000390625, magnitude of gradient - 0.053165111497112595\n",
      "Step - 7130, Loss - 0.3012683869475532, Learning Rate - 0.000390625, magnitude of gradient - 0.026644181591776915\n",
      "Step - 7131, Loss - 0.3018926140421272, Learning Rate - 0.000390625, magnitude of gradient - 0.0321516048088126\n",
      "Step - 7132, Loss - 0.27065973515871206, Learning Rate - 0.000390625, magnitude of gradient - 0.052577871614882206\n",
      "Step - 7133, Loss - 0.32200484238204685, Learning Rate - 0.000390625, magnitude of gradient - 0.06091645647417738\n",
      "Step - 7134, Loss - 0.24527797282530142, Learning Rate - 0.000390625, magnitude of gradient - 0.06222585254950656\n",
      "Step - 7135, Loss - 0.3287363384670457, Learning Rate - 0.000390625, magnitude of gradient - 0.03836261876989943\n",
      "Step - 7136, Loss - 0.3260788502197015, Learning Rate - 0.000390625, magnitude of gradient - 0.06261035901207235\n",
      "Step - 7137, Loss - 0.30252210893723985, Learning Rate - 0.000390625, magnitude of gradient - 0.031143405180960634\n",
      "Step - 7138, Loss - 0.36579122972246003, Learning Rate - 0.000390625, magnitude of gradient - 0.06741669539505149\n",
      "Step - 7139, Loss - 0.4173678068889675, Learning Rate - 0.000390625, magnitude of gradient - 0.035370170504436554\n",
      "Step - 7140, Loss - 0.2758893983550701, Learning Rate - 0.000390625, magnitude of gradient - 0.007341228560461695\n",
      "Step - 7141, Loss - 0.31180375717543485, Learning Rate - 0.000390625, magnitude of gradient - 0.09427813578833849\n",
      "Step - 7142, Loss - 0.2725758088124668, Learning Rate - 0.000390625, magnitude of gradient - 0.013487059910632402\n",
      "Step - 7143, Loss - 0.24606253323690325, Learning Rate - 0.000390625, magnitude of gradient - 0.06843114150100088\n",
      "Step - 7144, Loss - 0.3081408928047319, Learning Rate - 0.000390625, magnitude of gradient - 0.11658929974140313\n",
      "Step - 7145, Loss - 0.3593524475832754, Learning Rate - 0.000390625, magnitude of gradient - 0.02650354329932366\n",
      "Step - 7146, Loss - 0.2700333767745403, Learning Rate - 0.000390625, magnitude of gradient - 0.04032961521887774\n",
      "Step - 7147, Loss - 0.2775128273924565, Learning Rate - 0.000390625, magnitude of gradient - 0.025863601354416108\n",
      "Step - 7148, Loss - 0.3057742126660268, Learning Rate - 0.000390625, magnitude of gradient - 0.07564929717762658\n",
      "Step - 7149, Loss - 0.38783639040235435, Learning Rate - 0.000390625, magnitude of gradient - 0.04983857060693367\n",
      "Step - 7150, Loss - 0.3274171088007414, Learning Rate - 0.000390625, magnitude of gradient - 0.025161799467925513\n",
      "Step - 7151, Loss - 0.393910493288139, Learning Rate - 0.000390625, magnitude of gradient - 0.02749936907968023\n",
      "Step - 7152, Loss - 0.31625200114148894, Learning Rate - 0.000390625, magnitude of gradient - 0.0859731323818894\n",
      "Step - 7153, Loss - 0.35745899480037624, Learning Rate - 0.000390625, magnitude of gradient - 0.06541917938727608\n",
      "Step - 7154, Loss - 0.4391257499122098, Learning Rate - 0.000390625, magnitude of gradient - 0.024664807367651368\n",
      "Step - 7155, Loss - 0.3684341546456423, Learning Rate - 0.000390625, magnitude of gradient - 0.08442142053939829\n",
      "Step - 7156, Loss - 0.3321265165432101, Learning Rate - 0.000390625, magnitude of gradient - 0.03197561928843605\n",
      "Step - 7157, Loss - 0.2871838286543684, Learning Rate - 0.000390625, magnitude of gradient - 0.010532164139470196\n",
      "Step - 7158, Loss - 0.3399381738907818, Learning Rate - 0.000390625, magnitude of gradient - 0.0455963638661014\n",
      "Step - 7159, Loss - 0.31366969448742843, Learning Rate - 0.000390625, magnitude of gradient - 0.030060782247189968\n",
      "Step - 7160, Loss - 0.3412506463915443, Learning Rate - 0.000390625, magnitude of gradient - 0.052144442991760326\n",
      "Step - 7161, Loss - 0.37201256016481815, Learning Rate - 0.000390625, magnitude of gradient - 0.05181535448495956\n",
      "Step - 7162, Loss - 0.3638852854111741, Learning Rate - 0.000390625, magnitude of gradient - 0.035681805757396885\n",
      "Step - 7163, Loss - 0.38253859074174507, Learning Rate - 0.000390625, magnitude of gradient - 0.027833639567898368\n",
      "Step - 7164, Loss - 0.28885278124204206, Learning Rate - 0.000390625, magnitude of gradient - 0.06864837740445046\n",
      "Step - 7165, Loss - 0.264544680190375, Learning Rate - 0.000390625, magnitude of gradient - 0.04925942138116874\n",
      "Step - 7166, Loss - 0.31824532124661004, Learning Rate - 0.000390625, magnitude of gradient - 0.06507336535762133\n",
      "Step - 7167, Loss - 0.3215784981633021, Learning Rate - 0.000390625, magnitude of gradient - 0.09911661032574168\n",
      "Step - 7168, Loss - 0.3607028504219748, Learning Rate - 0.000390625, magnitude of gradient - 0.06836737605232587\n",
      "Step - 7169, Loss - 0.33177430435771527, Learning Rate - 0.000390625, magnitude of gradient - 0.054667683161587086\n",
      "Step - 7170, Loss - 0.3372992696371814, Learning Rate - 0.000390625, magnitude of gradient - 0.05275123931723529\n",
      "Step - 7171, Loss - 0.3226391593398735, Learning Rate - 0.000390625, magnitude of gradient - 0.031652732251360845\n",
      "Step - 7172, Loss - 0.3678507144673424, Learning Rate - 0.000390625, magnitude of gradient - 0.009177532801654975\n",
      "Step - 7173, Loss - 0.3076096754446902, Learning Rate - 0.000390625, magnitude of gradient - 0.11517771423670968\n",
      "Step - 7174, Loss - 0.28144615050541144, Learning Rate - 0.000390625, magnitude of gradient - 0.024713497388518538\n",
      "Step - 7175, Loss - 0.2902363572036811, Learning Rate - 0.000390625, magnitude of gradient - 0.03370455245830285\n",
      "Step - 7176, Loss - 0.3118980622744908, Learning Rate - 0.000390625, magnitude of gradient - 0.061280729164151296\n",
      "Step - 7177, Loss - 0.3580815339530721, Learning Rate - 0.000390625, magnitude of gradient - 0.03812026087286271\n",
      "Step - 7178, Loss - 0.3646589239526805, Learning Rate - 0.000390625, magnitude of gradient - 0.12212105835361788\n",
      "Step - 7179, Loss - 0.25289161414156547, Learning Rate - 0.000390625, magnitude of gradient - 0.03985226985264702\n",
      "Step - 7180, Loss - 0.37397618936441474, Learning Rate - 0.000390625, magnitude of gradient - 0.05696335044098634\n",
      "Step - 7181, Loss - 0.36680055854488813, Learning Rate - 0.000390625, magnitude of gradient - 0.04661228645392443\n",
      "Step - 7182, Loss - 0.2468834793381677, Learning Rate - 0.000390625, magnitude of gradient - 0.049729984426977146\n",
      "Step - 7183, Loss - 0.38589662904815886, Learning Rate - 0.000390625, magnitude of gradient - 0.007466565436719748\n",
      "Step - 7184, Loss - 0.2793771423331537, Learning Rate - 0.000390625, magnitude of gradient - 0.06641483058617488\n",
      "Step - 7185, Loss - 0.3616923129840932, Learning Rate - 0.000390625, magnitude of gradient - 0.06764800714934098\n",
      "Step - 7186, Loss - 0.33908533583916106, Learning Rate - 0.000390625, magnitude of gradient - 0.03718326306874212\n",
      "Step - 7187, Loss - 0.30613310764764956, Learning Rate - 0.000390625, magnitude of gradient - 0.034547896839771\n",
      "Step - 7188, Loss - 0.3713816445697357, Learning Rate - 0.000390625, magnitude of gradient - 0.08711670890630395\n",
      "Step - 7189, Loss - 0.3020683185634734, Learning Rate - 0.000390625, magnitude of gradient - 0.05343969230287288\n",
      "Step - 7190, Loss - 0.39921288951473083, Learning Rate - 0.000390625, magnitude of gradient - 0.044465350085087264\n",
      "Step - 7191, Loss - 0.271850875033788, Learning Rate - 0.000390625, magnitude of gradient - 0.07687741297409031\n",
      "Step - 7192, Loss - 0.2835347918417156, Learning Rate - 0.000390625, magnitude of gradient - 0.053858982635821256\n",
      "Step - 7193, Loss - 0.29206322412655555, Learning Rate - 0.000390625, magnitude of gradient - 0.030080617623672107\n",
      "Step - 7194, Loss - 0.33274970529566983, Learning Rate - 0.000390625, magnitude of gradient - 0.030683208230273745\n",
      "Step - 7195, Loss - 0.3111007611409106, Learning Rate - 0.000390625, magnitude of gradient - 0.01511188653476883\n",
      "Step - 7196, Loss - 0.36292522593693005, Learning Rate - 0.000390625, magnitude of gradient - 0.021043915763699222\n",
      "Step - 7197, Loss - 0.30279495525687167, Learning Rate - 0.000390625, magnitude of gradient - 0.03387521953033368\n",
      "Step - 7198, Loss - 0.27668458198077733, Learning Rate - 0.000390625, magnitude of gradient - 0.023286328682299427\n",
      "Step - 7199, Loss - 0.32406364911995134, Learning Rate - 0.000390625, magnitude of gradient - 0.057838625668767615\n",
      "Step - 7200, Loss - 0.3142527243105618, Learning Rate - 0.000390625, magnitude of gradient - 0.07026355269359956\n",
      "Step - 7201, Loss - 0.292084117863488, Learning Rate - 0.000390625, magnitude of gradient - 0.008808863699132738\n",
      "Step - 7202, Loss - 0.30469630371446943, Learning Rate - 0.000390625, magnitude of gradient - 0.03872156758185513\n",
      "Step - 7203, Loss - 0.28165111168221546, Learning Rate - 0.000390625, magnitude of gradient - 0.03788717744669876\n",
      "Step - 7204, Loss - 0.33192599408237367, Learning Rate - 0.000390625, magnitude of gradient - 0.033420084934335806\n",
      "Step - 7205, Loss - 0.3618907894200515, Learning Rate - 0.000390625, magnitude of gradient - 0.033906122884470334\n",
      "Step - 7206, Loss - 0.2895213810981078, Learning Rate - 0.000390625, magnitude of gradient - 0.08994190073249761\n",
      "Step - 7207, Loss - 0.3310377485720569, Learning Rate - 0.000390625, magnitude of gradient - 0.05890091378808007\n",
      "Step - 7208, Loss - 0.34508918288208934, Learning Rate - 0.000390625, magnitude of gradient - 0.09140624275276348\n",
      "Step - 7209, Loss - 0.38721431260959194, Learning Rate - 0.000390625, magnitude of gradient - 0.038346100451579546\n",
      "Step - 7210, Loss - 0.3836102482229644, Learning Rate - 0.000390625, magnitude of gradient - 0.06631823872522608\n",
      "Step - 7211, Loss - 0.3285135167856501, Learning Rate - 0.000390625, magnitude of gradient - 0.07097155204467064\n",
      "Step - 7212, Loss - 0.3248604402833205, Learning Rate - 0.000390625, magnitude of gradient - 0.09506340423797824\n",
      "Step - 7213, Loss - 0.3670979461783436, Learning Rate - 0.000390625, magnitude of gradient - 0.030728883248750186\n",
      "Step - 7214, Loss - 0.3337038355552495, Learning Rate - 0.000390625, magnitude of gradient - 0.06279574891680441\n",
      "Step - 7215, Loss - 0.2648433504807922, Learning Rate - 0.000390625, magnitude of gradient - 0.03997874160927785\n",
      "Step - 7216, Loss - 0.28347919876091604, Learning Rate - 0.000390625, magnitude of gradient - 0.058997615064168465\n",
      "Step - 7217, Loss - 0.2783704118890759, Learning Rate - 0.000390625, magnitude of gradient - 0.03247772356551376\n",
      "Step - 7218, Loss - 0.2785099036828651, Learning Rate - 0.000390625, magnitude of gradient - 0.06223317456837772\n",
      "Step - 7219, Loss - 0.2955134326040893, Learning Rate - 0.000390625, magnitude of gradient - 0.012166993513274521\n",
      "Step - 7220, Loss - 0.36206354523627293, Learning Rate - 0.000390625, magnitude of gradient - 0.03536731560351549\n",
      "Step - 7221, Loss - 0.4239866446592163, Learning Rate - 0.000390625, magnitude of gradient - 0.07169635395230092\n",
      "Step - 7222, Loss - 0.3318462557055693, Learning Rate - 0.000390625, magnitude of gradient - 0.05464754205418703\n",
      "Step - 7223, Loss - 0.2970173912468892, Learning Rate - 0.000390625, magnitude of gradient - 0.08973330549280376\n",
      "Step - 7224, Loss - 0.34042119039123775, Learning Rate - 0.000390625, magnitude of gradient - 0.028211362345040705\n",
      "Step - 7225, Loss - 0.31941145241083596, Learning Rate - 0.000390625, magnitude of gradient - 0.04054176238375237\n",
      "Step - 7226, Loss - 0.3831912538237664, Learning Rate - 0.000390625, magnitude of gradient - 0.03896418526452219\n",
      "Step - 7227, Loss - 0.3645731664084122, Learning Rate - 0.000390625, magnitude of gradient - 0.017997298735636626\n",
      "Step - 7228, Loss - 0.3235871530626571, Learning Rate - 0.000390625, magnitude of gradient - 0.05423070980762655\n",
      "Step - 7229, Loss - 0.33870170588925075, Learning Rate - 0.000390625, magnitude of gradient - 0.07263988343601818\n",
      "Step - 7230, Loss - 0.29428547049686943, Learning Rate - 0.000390625, magnitude of gradient - 0.031672933086529645\n",
      "Step - 7231, Loss - 0.28041230940143075, Learning Rate - 0.000390625, magnitude of gradient - 0.08030480550878098\n",
      "Step - 7232, Loss - 0.3235964554194765, Learning Rate - 0.000390625, magnitude of gradient - 0.09742883082071492\n",
      "Step - 7233, Loss - 0.3223609787859197, Learning Rate - 0.000390625, magnitude of gradient - 0.016080242966057704\n",
      "Step - 7234, Loss - 0.30151222434816194, Learning Rate - 0.000390625, magnitude of gradient - 0.036066474398018515\n",
      "Step - 7235, Loss - 0.3626614151269669, Learning Rate - 0.000390625, magnitude of gradient - 0.09104443131942815\n",
      "Step - 7236, Loss - 0.3218908316230287, Learning Rate - 0.000390625, magnitude of gradient - 0.04836478830516962\n",
      "Step - 7237, Loss - 0.44281006581402865, Learning Rate - 0.000390625, magnitude of gradient - 0.05724726436046609\n",
      "Step - 7238, Loss - 0.2877840332339849, Learning Rate - 0.000390625, magnitude of gradient - 0.05299081979653485\n",
      "Step - 7239, Loss - 0.2863209425223816, Learning Rate - 0.000390625, magnitude of gradient - 0.04655146818911891\n",
      "Step - 7240, Loss - 0.3497281504930684, Learning Rate - 0.000390625, magnitude of gradient - 0.034136065874456126\n",
      "Step - 7241, Loss - 0.34297614171787827, Learning Rate - 0.000390625, magnitude of gradient - 0.028571549590068773\n",
      "Step - 7242, Loss - 0.29730422574779447, Learning Rate - 0.000390625, magnitude of gradient - 0.043243328521320235\n",
      "Step - 7243, Loss - 0.2907325256609186, Learning Rate - 0.000390625, magnitude of gradient - 0.04463934764543792\n",
      "Step - 7244, Loss - 0.30901526961066716, Learning Rate - 0.000390625, magnitude of gradient - 0.061654206022195825\n",
      "Step - 7245, Loss - 0.2993548189818208, Learning Rate - 0.000390625, magnitude of gradient - 0.05119266748197787\n",
      "Step - 7246, Loss - 0.3213390734933792, Learning Rate - 0.000390625, magnitude of gradient - 0.06603457290320451\n",
      "Step - 7247, Loss - 0.3459970168169114, Learning Rate - 0.000390625, magnitude of gradient - 0.017373878016796206\n",
      "Step - 7248, Loss - 0.39526379774143355, Learning Rate - 0.000390625, magnitude of gradient - 0.05937777830706765\n",
      "Step - 7249, Loss - 0.26488562774061347, Learning Rate - 0.000390625, magnitude of gradient - 0.05130517859747072\n",
      "Step - 7250, Loss - 0.3893548127614592, Learning Rate - 0.000390625, magnitude of gradient - 0.09061186871286961\n",
      "Step - 7251, Loss - 0.268066070494853, Learning Rate - 0.000390625, magnitude of gradient - 0.020343962246169786\n",
      "Step - 7252, Loss - 0.3459482277260533, Learning Rate - 0.000390625, magnitude of gradient - 0.05368268843962882\n",
      "Step - 7253, Loss - 0.2438932061828562, Learning Rate - 0.000390625, magnitude of gradient - 0.05062895500791853\n",
      "Step - 7254, Loss - 0.31788516689842233, Learning Rate - 0.000390625, magnitude of gradient - 0.024821311436179556\n",
      "Step - 7255, Loss - 0.3617492547840052, Learning Rate - 0.000390625, magnitude of gradient - 0.06289210417107409\n",
      "Step - 7256, Loss - 0.4432721989378269, Learning Rate - 0.000390625, magnitude of gradient - 0.07106744297834627\n",
      "Step - 7257, Loss - 0.2709797579177117, Learning Rate - 0.000390625, magnitude of gradient - 0.08258568423792581\n",
      "Step - 7258, Loss - 0.34639107289779036, Learning Rate - 0.000390625, magnitude of gradient - 0.04650433577072737\n",
      "Step - 7259, Loss - 0.3506109462752173, Learning Rate - 0.000390625, magnitude of gradient - 0.07972046661827115\n",
      "Step - 7260, Loss - 0.32889133758914124, Learning Rate - 0.000390625, magnitude of gradient - 0.02792131567395673\n",
      "Step - 7261, Loss - 0.27091124659448207, Learning Rate - 0.000390625, magnitude of gradient - 0.06792616553400815\n",
      "Step - 7262, Loss - 0.2819357971559058, Learning Rate - 0.000390625, magnitude of gradient - 0.03830194238833459\n",
      "Step - 7263, Loss - 0.3799541339577935, Learning Rate - 0.000390625, magnitude of gradient - 0.011601006280491889\n",
      "Step - 7264, Loss - 0.36913992192630457, Learning Rate - 0.000390625, magnitude of gradient - 0.03291169789253562\n",
      "Step - 7265, Loss - 0.3964422504837182, Learning Rate - 0.000390625, magnitude of gradient - 0.038262998656598296\n",
      "Step - 7266, Loss - 0.23646129396096072, Learning Rate - 0.000390625, magnitude of gradient - 0.05671159471257302\n",
      "Step - 7267, Loss - 0.34225696562416486, Learning Rate - 0.000390625, magnitude of gradient - 0.08123904355367022\n",
      "Step - 7268, Loss - 0.3407936424419621, Learning Rate - 0.000390625, magnitude of gradient - 0.029075411957104284\n",
      "Step - 7269, Loss - 0.3232202747906592, Learning Rate - 0.000390625, magnitude of gradient - 0.044817438561645356\n",
      "Step - 7270, Loss - 0.3014620776681043, Learning Rate - 0.000390625, magnitude of gradient - 0.03787895261759623\n",
      "Step - 7271, Loss - 0.3620326811679092, Learning Rate - 0.000390625, magnitude of gradient - 0.08659765254227289\n",
      "Step - 7272, Loss - 0.3001237738729678, Learning Rate - 0.000390625, magnitude of gradient - 0.05250154995592501\n",
      "Step - 7273, Loss - 0.24115097793665696, Learning Rate - 0.000390625, magnitude of gradient - 0.037304578870671394\n",
      "Step - 7274, Loss - 0.33840772295458255, Learning Rate - 0.000390625, magnitude of gradient - 0.026735497978567425\n",
      "Step - 7275, Loss - 0.35206443448397085, Learning Rate - 0.000390625, magnitude of gradient - 0.027993963057307836\n",
      "Step - 7276, Loss - 0.33909293758687625, Learning Rate - 0.000390625, magnitude of gradient - 0.0453059206253808\n",
      "Step - 7277, Loss - 0.24856462867790935, Learning Rate - 0.000390625, magnitude of gradient - 0.04244772380638718\n",
      "Step - 7278, Loss - 0.3346472364822228, Learning Rate - 0.000390625, magnitude of gradient - 0.04555530349114275\n",
      "Step - 7279, Loss - 0.2628116414663024, Learning Rate - 0.000390625, magnitude of gradient - 0.0774956645374229\n",
      "Step - 7280, Loss - 0.3291357457338462, Learning Rate - 0.000390625, magnitude of gradient - 0.03245460376803158\n",
      "Step - 7281, Loss - 0.2677714801367321, Learning Rate - 0.000390625, magnitude of gradient - 0.024720050759053987\n",
      "Step - 7282, Loss - 0.31230889029793885, Learning Rate - 0.000390625, magnitude of gradient - 0.0356443124741399\n",
      "Step - 7283, Loss - 0.2730487202595041, Learning Rate - 0.000390625, magnitude of gradient - 0.032222447672505754\n",
      "Step - 7284, Loss - 0.359616762342979, Learning Rate - 0.000390625, magnitude of gradient - 0.033677658457802545\n",
      "Step - 7285, Loss - 0.2727705260154162, Learning Rate - 0.000390625, magnitude of gradient - 0.06639770727937494\n",
      "Step - 7286, Loss - 0.3141027516589896, Learning Rate - 0.000390625, magnitude of gradient - 0.03962507291510174\n",
      "Step - 7287, Loss - 0.2913732848939725, Learning Rate - 0.000390625, magnitude of gradient - 0.019782836011243612\n",
      "Step - 7288, Loss - 0.3401169892826156, Learning Rate - 0.000390625, magnitude of gradient - 0.12687432558226158\n",
      "Step - 7289, Loss - 0.28363331963122757, Learning Rate - 0.000390625, magnitude of gradient - 0.056440613721552665\n",
      "Step - 7290, Loss - 0.26535553593282474, Learning Rate - 0.000390625, magnitude of gradient - 0.02467197155176915\n",
      "Step - 7291, Loss - 0.29339235474959785, Learning Rate - 0.000390625, magnitude of gradient - 0.06599224854980464\n",
      "Step - 7292, Loss - 0.3826241286844886, Learning Rate - 0.000390625, magnitude of gradient - 0.07700304306606409\n",
      "Step - 7293, Loss - 0.2873553261710428, Learning Rate - 0.000390625, magnitude of gradient - 0.018421047972358297\n",
      "Step - 7294, Loss - 0.2915519168159924, Learning Rate - 0.000390625, magnitude of gradient - 0.06008707145373705\n",
      "Step - 7295, Loss - 0.327357203599719, Learning Rate - 0.000390625, magnitude of gradient - 0.014573935914925298\n",
      "Step - 7296, Loss - 0.37680341552337926, Learning Rate - 0.000390625, magnitude of gradient - 0.025609758728249256\n",
      "Step - 7297, Loss - 0.29775474685314074, Learning Rate - 0.000390625, magnitude of gradient - 0.09013278391248608\n",
      "Step - 7298, Loss - 0.32202487246082606, Learning Rate - 0.000390625, magnitude of gradient - 0.044657720578170676\n",
      "Step - 7299, Loss - 0.3147746507056344, Learning Rate - 0.000390625, magnitude of gradient - 0.0774075198285448\n",
      "Step - 7300, Loss - 0.2791934595814487, Learning Rate - 0.000390625, magnitude of gradient - 0.01986022039966091\n",
      "Step - 7301, Loss - 0.2907268135000145, Learning Rate - 0.000390625, magnitude of gradient - 0.033028071418044654\n",
      "Step - 7302, Loss - 0.28438796261046206, Learning Rate - 0.000390625, magnitude of gradient - 0.05586300576543905\n",
      "Step - 7303, Loss - 0.4063762261197414, Learning Rate - 0.000390625, magnitude of gradient - 0.0692424617853122\n",
      "Step - 7304, Loss - 0.32616097010785894, Learning Rate - 0.000390625, magnitude of gradient - 0.051104349786015725\n",
      "Step - 7305, Loss - 0.30365425759005393, Learning Rate - 0.000390625, magnitude of gradient - 0.029445562917548188\n",
      "Step - 7306, Loss - 0.23132205599930822, Learning Rate - 0.000390625, magnitude of gradient - 0.009520756283031648\n",
      "Step - 7307, Loss - 0.3229122609230619, Learning Rate - 0.000390625, magnitude of gradient - 0.02025943561078705\n",
      "Step - 7308, Loss - 0.29870518622581804, Learning Rate - 0.000390625, magnitude of gradient - 0.02776130552302688\n",
      "Step - 7309, Loss - 0.3523916544124829, Learning Rate - 0.000390625, magnitude of gradient - 0.08874657708768668\n",
      "Step - 7310, Loss - 0.2934677436465967, Learning Rate - 0.000390625, magnitude of gradient - 0.06549934354006814\n",
      "Step - 7311, Loss - 0.3679743592351945, Learning Rate - 0.000390625, magnitude of gradient - 0.05719220956178383\n",
      "Step - 7312, Loss - 0.302473217707778, Learning Rate - 0.000390625, magnitude of gradient - 0.021748420788512843\n",
      "Step - 7313, Loss - 0.27976518479444584, Learning Rate - 0.000390625, magnitude of gradient - 0.03774402982510438\n",
      "Step - 7314, Loss - 0.3826686394025126, Learning Rate - 0.000390625, magnitude of gradient - 0.03698927134201166\n",
      "Step - 7315, Loss - 0.36872569633847285, Learning Rate - 0.000390625, magnitude of gradient - 0.07194713288235748\n",
      "Step - 7316, Loss - 0.30185204318019854, Learning Rate - 0.000390625, magnitude of gradient - 0.0668963145886603\n",
      "Step - 7317, Loss - 0.29956563352861476, Learning Rate - 0.000390625, magnitude of gradient - 0.07407682618814049\n",
      "Step - 7318, Loss - 0.40710575060907167, Learning Rate - 0.000390625, magnitude of gradient - 0.055616986007385884\n",
      "Step - 7319, Loss - 0.31574727921541024, Learning Rate - 0.000390625, magnitude of gradient - 0.08998742653905892\n",
      "Step - 7320, Loss - 0.27388318017902885, Learning Rate - 0.000390625, magnitude of gradient - 0.056985822212185414\n",
      "Step - 7321, Loss - 0.3891948535835039, Learning Rate - 0.000390625, magnitude of gradient - 0.04873417372789611\n",
      "Step - 7322, Loss - 0.2900945723751244, Learning Rate - 0.000390625, magnitude of gradient - 0.024799902097126438\n",
      "Step - 7323, Loss - 0.36004757926850306, Learning Rate - 0.000390625, magnitude of gradient - 0.09757080603747406\n",
      "Step - 7324, Loss - 0.32245936513179974, Learning Rate - 0.000390625, magnitude of gradient - 0.04804359058858793\n",
      "Step - 7325, Loss - 0.3315047603135713, Learning Rate - 0.000390625, magnitude of gradient - 0.03310814122563348\n",
      "Step - 7326, Loss - 0.31462167064372426, Learning Rate - 0.000390625, magnitude of gradient - 0.04088350504968168\n",
      "Step - 7327, Loss - 0.22645310219577586, Learning Rate - 0.000390625, magnitude of gradient - 0.06375884800700993\n",
      "Step - 7328, Loss - 0.3329276288267696, Learning Rate - 0.000390625, magnitude of gradient - 0.05971626944387752\n",
      "Step - 7329, Loss - 0.40381914329681345, Learning Rate - 0.000390625, magnitude of gradient - 0.04774503864947035\n",
      "Step - 7330, Loss - 0.3649026362870468, Learning Rate - 0.000390625, magnitude of gradient - 0.040596038027523866\n",
      "Step - 7331, Loss - 0.3402026952088036, Learning Rate - 0.000390625, magnitude of gradient - 0.05932407475871123\n",
      "Step - 7332, Loss - 0.26418346042088503, Learning Rate - 0.000390625, magnitude of gradient - 0.053388089253558285\n",
      "Step - 7333, Loss - 0.391180190709258, Learning Rate - 0.000390625, magnitude of gradient - 0.06338484017487694\n",
      "Step - 7334, Loss - 0.2973643593365455, Learning Rate - 0.000390625, magnitude of gradient - 0.01667501304677868\n",
      "Step - 7335, Loss - 0.3180939394046423, Learning Rate - 0.000390625, magnitude of gradient - 0.07511073110160203\n",
      "Step - 7336, Loss - 0.3175307363606228, Learning Rate - 0.000390625, magnitude of gradient - 0.05020340039474417\n",
      "Step - 7337, Loss - 0.3563365853017315, Learning Rate - 0.000390625, magnitude of gradient - 0.09125261499998139\n",
      "Step - 7338, Loss - 0.3079046043965026, Learning Rate - 0.000390625, magnitude of gradient - 0.05722332616544063\n",
      "Step - 7339, Loss - 0.26186388796426113, Learning Rate - 0.000390625, magnitude of gradient - 0.048280311218567774\n",
      "Step - 7340, Loss - 0.36530166984394663, Learning Rate - 0.000390625, magnitude of gradient - 0.030321263673966058\n",
      "Step - 7341, Loss - 0.31240490740031523, Learning Rate - 0.000390625, magnitude of gradient - 0.06982820430124853\n",
      "Step - 7342, Loss - 0.3576176333161798, Learning Rate - 0.000390625, magnitude of gradient - 0.060020485313076764\n",
      "Step - 7343, Loss - 0.30741665105640603, Learning Rate - 0.000390625, magnitude of gradient - 0.09129322149835224\n",
      "Step - 7344, Loss - 0.3117615226916668, Learning Rate - 0.000390625, magnitude of gradient - 0.07080994085896791\n",
      "Step - 7345, Loss - 0.3482550797522274, Learning Rate - 0.000390625, magnitude of gradient - 0.039137204822082966\n",
      "Step - 7346, Loss - 0.319743776831394, Learning Rate - 0.000390625, magnitude of gradient - 0.061220963187908276\n",
      "Step - 7347, Loss - 0.34746508507702273, Learning Rate - 0.000390625, magnitude of gradient - 0.06682686096115757\n",
      "Step - 7348, Loss - 0.27356898142220787, Learning Rate - 0.000390625, magnitude of gradient - 0.03273667058089099\n",
      "Step - 7349, Loss - 0.29517288778268513, Learning Rate - 0.000390625, magnitude of gradient - 0.027382738485433343\n",
      "Step - 7350, Loss - 0.27426869681885857, Learning Rate - 0.000390625, magnitude of gradient - 0.028054042354694345\n",
      "Step - 7351, Loss - 0.20711080664963444, Learning Rate - 0.000390625, magnitude of gradient - 0.061049087244547155\n",
      "Step - 7352, Loss - 0.3517589785538887, Learning Rate - 0.000390625, magnitude of gradient - 0.03903806103380394\n",
      "Step - 7353, Loss - 0.287873012058011, Learning Rate - 0.000390625, magnitude of gradient - 0.03213572611612137\n",
      "Step - 7354, Loss - 0.3743979107310097, Learning Rate - 0.000390625, magnitude of gradient - 0.07222400312701921\n",
      "Step - 7355, Loss - 0.2708843510352376, Learning Rate - 0.000390625, magnitude of gradient - 0.035708354240938184\n",
      "Step - 7356, Loss - 0.3416751374777666, Learning Rate - 0.000390625, magnitude of gradient - 0.08111472545434849\n",
      "Step - 7357, Loss - 0.3209630340354788, Learning Rate - 0.000390625, magnitude of gradient - 0.02123799087692554\n",
      "Step - 7358, Loss - 0.32424033656617535, Learning Rate - 0.000390625, magnitude of gradient - 0.0633493851329035\n",
      "Step - 7359, Loss - 0.3469864331298428, Learning Rate - 0.000390625, magnitude of gradient - 0.05403433442191525\n",
      "Step - 7360, Loss - 0.27761222827563015, Learning Rate - 0.000390625, magnitude of gradient - 0.031319309096420264\n",
      "Step - 7361, Loss - 0.25673888952548807, Learning Rate - 0.000390625, magnitude of gradient - 0.047948066035879505\n",
      "Step - 7362, Loss - 0.27337262623017444, Learning Rate - 0.000390625, magnitude of gradient - 0.07092291314430381\n",
      "Step - 7363, Loss - 0.39588281360414396, Learning Rate - 0.000390625, magnitude of gradient - 0.04220040683381561\n",
      "Step - 7364, Loss - 0.34693980080565356, Learning Rate - 0.000390625, magnitude of gradient - 0.057187532958309625\n",
      "Step - 7365, Loss - 0.3674235290202938, Learning Rate - 0.000390625, magnitude of gradient - 0.013608650331282281\n",
      "Step - 7366, Loss - 0.3026080551618532, Learning Rate - 0.000390625, magnitude of gradient - 0.019363945716623077\n",
      "Step - 7367, Loss - 0.3620550931818953, Learning Rate - 0.000390625, magnitude of gradient - 0.09138020331906593\n",
      "Step - 7368, Loss - 0.33992604559885276, Learning Rate - 0.000390625, magnitude of gradient - 0.06579795250945\n",
      "Step - 7369, Loss - 0.35260081836267926, Learning Rate - 0.000390625, magnitude of gradient - 0.004996707996354223\n",
      "Step - 7370, Loss - 0.30965518487353694, Learning Rate - 0.000390625, magnitude of gradient - 0.028152752164099733\n",
      "Step - 7371, Loss - 0.24765983037405473, Learning Rate - 0.000390625, magnitude of gradient - 0.040939405478865556\n",
      "Step - 7372, Loss - 0.39250078383179154, Learning Rate - 0.000390625, magnitude of gradient - 0.02021288901440674\n",
      "Step - 7373, Loss - 0.3208644468394245, Learning Rate - 0.000390625, magnitude of gradient - 0.04652706797586685\n",
      "Step - 7374, Loss - 0.30655769868032473, Learning Rate - 0.000390625, magnitude of gradient - 0.01811785654631153\n",
      "Step - 7375, Loss - 0.360709728965143, Learning Rate - 0.000390625, magnitude of gradient - 0.0784582553155093\n",
      "Step - 7376, Loss - 0.27896677875327924, Learning Rate - 0.000390625, magnitude of gradient - 0.03560175039519343\n",
      "Step - 7377, Loss - 0.3909839815665349, Learning Rate - 0.000390625, magnitude of gradient - 0.06577198807134098\n",
      "Step - 7378, Loss - 0.38734169623965853, Learning Rate - 0.000390625, magnitude of gradient - 0.09160087483712588\n",
      "Step - 7379, Loss - 0.33474226298727366, Learning Rate - 0.000390625, magnitude of gradient - 0.03595345214428725\n",
      "Step - 7380, Loss - 0.3153088079592588, Learning Rate - 0.000390625, magnitude of gradient - 0.05557021102995565\n",
      "Step - 7381, Loss - 0.2808437934719813, Learning Rate - 0.000390625, magnitude of gradient - 0.04738008299973071\n",
      "Step - 7382, Loss - 0.3368450984271274, Learning Rate - 0.000390625, magnitude of gradient - 0.07658450185778926\n",
      "Step - 7383, Loss - 0.27819972488821537, Learning Rate - 0.000390625, magnitude of gradient - 0.05228885504377161\n",
      "Step - 7384, Loss - 0.30719979681556575, Learning Rate - 0.000390625, magnitude of gradient - 0.06377323073729996\n",
      "Step - 7385, Loss - 0.2880916417261248, Learning Rate - 0.000390625, magnitude of gradient - 0.0247745238370583\n",
      "Step - 7386, Loss - 0.3577835188881322, Learning Rate - 0.000390625, magnitude of gradient - 0.009717915913217265\n",
      "Step - 7387, Loss - 0.30658216560848317, Learning Rate - 0.000390625, magnitude of gradient - 0.04090462101471441\n",
      "Step - 7388, Loss - 0.2883485380872092, Learning Rate - 0.000390625, magnitude of gradient - 0.05770102190009141\n",
      "Step - 7389, Loss - 0.42793170258239255, Learning Rate - 0.000390625, magnitude of gradient - 0.00973673828689653\n",
      "Step - 7390, Loss - 0.2546051165673513, Learning Rate - 0.000390625, magnitude of gradient - 0.05096521842762762\n",
      "Step - 7391, Loss - 0.3437437261704866, Learning Rate - 0.000390625, magnitude of gradient - 0.01954242441383107\n",
      "Step - 7392, Loss - 0.3322959614823292, Learning Rate - 0.000390625, magnitude of gradient - 0.06236633916775268\n",
      "Step - 7393, Loss - 0.38272019431095905, Learning Rate - 0.000390625, magnitude of gradient - 0.1235238460910266\n",
      "Step - 7394, Loss - 0.31631230710696107, Learning Rate - 0.000390625, magnitude of gradient - 0.06423776716744961\n",
      "Step - 7395, Loss - 0.3218205805240766, Learning Rate - 0.000390625, magnitude of gradient - 0.002551987582567026\n",
      "Step - 7396, Loss - 0.3262532513436352, Learning Rate - 0.000390625, magnitude of gradient - 0.03281419155272074\n",
      "Step - 7397, Loss - 0.3092446599134063, Learning Rate - 0.000390625, magnitude of gradient - 0.015676676733375326\n",
      "Step - 7398, Loss - 0.2968895906348997, Learning Rate - 0.000390625, magnitude of gradient - 0.024940392305631016\n",
      "Step - 7399, Loss - 0.3104069625752417, Learning Rate - 0.000390625, magnitude of gradient - 0.047382055590276875\n",
      "Step - 7400, Loss - 0.35370060169707795, Learning Rate - 0.000390625, magnitude of gradient - 0.09370653468488502\n",
      "Step - 7401, Loss - 0.27174086070967546, Learning Rate - 0.000390625, magnitude of gradient - 0.04585944358802689\n",
      "Step - 7402, Loss - 0.33389742082875273, Learning Rate - 0.000390625, magnitude of gradient - 0.1181621819181174\n",
      "Step - 7403, Loss - 0.3019751520348889, Learning Rate - 0.000390625, magnitude of gradient - 0.060767051585993354\n",
      "Step - 7404, Loss - 0.3208918692737097, Learning Rate - 0.000390625, magnitude of gradient - 0.09663323082642673\n",
      "Step - 7405, Loss - 0.3993555059899175, Learning Rate - 0.000390625, magnitude of gradient - 0.08615186067854493\n",
      "Step - 7406, Loss - 0.3676360033875211, Learning Rate - 0.000390625, magnitude of gradient - 0.025053830148442577\n",
      "Step - 7407, Loss - 0.3691803119461441, Learning Rate - 0.000390625, magnitude of gradient - 0.07053116321885784\n",
      "Step - 7408, Loss - 0.29501490799605495, Learning Rate - 0.000390625, magnitude of gradient - 0.0625921540992775\n",
      "Step - 7409, Loss - 0.31673278763712986, Learning Rate - 0.000390625, magnitude of gradient - 0.012781847728743904\n",
      "Step - 7410, Loss - 0.3536436573381999, Learning Rate - 0.000390625, magnitude of gradient - 0.08957475016508405\n",
      "Step - 7411, Loss - 0.20822604594706337, Learning Rate - 0.000390625, magnitude of gradient - 0.02646508544590562\n",
      "Step - 7412, Loss - 0.31172842453163485, Learning Rate - 0.000390625, magnitude of gradient - 0.020204628145402498\n",
      "Step - 7413, Loss - 0.37005846098553874, Learning Rate - 0.000390625, magnitude of gradient - 0.08357684053155048\n",
      "Step - 7414, Loss - 0.37981132788860283, Learning Rate - 0.000390625, magnitude of gradient - 0.08356496404057087\n",
      "Step - 7415, Loss - 0.35347075535680866, Learning Rate - 0.000390625, magnitude of gradient - 0.04095353884666129\n",
      "Step - 7416, Loss - 0.254705898097611, Learning Rate - 0.000390625, magnitude of gradient - 0.029623621380595277\n",
      "Step - 7417, Loss - 0.3444775950485684, Learning Rate - 0.000390625, magnitude of gradient - 0.056932033977519135\n",
      "Step - 7418, Loss - 0.2627646268314107, Learning Rate - 0.000390625, magnitude of gradient - 0.06607427245008929\n",
      "Step - 7419, Loss - 0.3280712831577723, Learning Rate - 0.000390625, magnitude of gradient - 0.034392670769195145\n",
      "Step - 7420, Loss - 0.34850755760809876, Learning Rate - 0.000390625, magnitude of gradient - 0.054936934685168255\n",
      "Step - 7421, Loss - 0.3521802732464828, Learning Rate - 0.000390625, magnitude of gradient - 0.0662124884189469\n",
      "Step - 7422, Loss - 0.30964092711723, Learning Rate - 0.000390625, magnitude of gradient - 0.06722259440278627\n",
      "Step - 7423, Loss - 0.35601958992251503, Learning Rate - 0.000390625, magnitude of gradient - 0.06396829538464881\n",
      "Step - 7424, Loss - 0.35962144513455363, Learning Rate - 0.000390625, magnitude of gradient - 0.048099225782344567\n",
      "Step - 7425, Loss - 0.2563962193731105, Learning Rate - 0.000390625, magnitude of gradient - 0.09561796634479876\n",
      "Step - 7426, Loss - 0.3571326825965192, Learning Rate - 0.000390625, magnitude of gradient - 0.028992164937995207\n",
      "Step - 7427, Loss - 0.35979626283845767, Learning Rate - 0.000390625, magnitude of gradient - 0.013314132059347795\n",
      "Step - 7428, Loss - 0.30895280845183387, Learning Rate - 0.000390625, magnitude of gradient - 0.07000505331853803\n",
      "Step - 7429, Loss - 0.3175315715821576, Learning Rate - 0.000390625, magnitude of gradient - 0.0247004878816337\n",
      "Step - 7430, Loss - 0.2754560580522522, Learning Rate - 0.000390625, magnitude of gradient - 0.016107167147090075\n",
      "Step - 7431, Loss - 0.32304457041505824, Learning Rate - 0.000390625, magnitude of gradient - 0.029946497322474976\n",
      "Step - 7432, Loss - 0.30670039434963636, Learning Rate - 0.000390625, magnitude of gradient - 0.08133215692584694\n",
      "Step - 7433, Loss - 0.38753981968394025, Learning Rate - 0.000390625, magnitude of gradient - 0.07796925738392949\n",
      "Step - 7434, Loss - 0.2249752731205656, Learning Rate - 0.000390625, magnitude of gradient - 0.0363023641719677\n",
      "Step - 7435, Loss - 0.2915629200483002, Learning Rate - 0.000390625, magnitude of gradient - 0.03168855618493562\n",
      "Step - 7436, Loss - 0.3873241686499914, Learning Rate - 0.000390625, magnitude of gradient - 0.049059054814063814\n",
      "Step - 7437, Loss - 0.3328821883768817, Learning Rate - 0.000390625, magnitude of gradient - 0.03734203597889736\n",
      "Step - 7438, Loss - 0.34442311394596553, Learning Rate - 0.000390625, magnitude of gradient - 0.028305894112018132\n",
      "Step - 7439, Loss - 0.37270971765935734, Learning Rate - 0.000390625, magnitude of gradient - 0.036142789543271645\n",
      "Step - 7440, Loss - 0.2851380812590006, Learning Rate - 0.000390625, magnitude of gradient - 0.04156365884045058\n",
      "Step - 7441, Loss - 0.2960766945535821, Learning Rate - 0.000390625, magnitude of gradient - 0.05962501622382948\n",
      "Step - 7442, Loss - 0.3243112491098844, Learning Rate - 0.000390625, magnitude of gradient - 0.02821262068299152\n",
      "Step - 7443, Loss - 0.25221030741901823, Learning Rate - 0.000390625, magnitude of gradient - 0.07094198148797742\n",
      "Step - 7444, Loss - 0.3910367883130461, Learning Rate - 0.000390625, magnitude of gradient - 0.03399338995980925\n",
      "Step - 7445, Loss - 0.3235906718790753, Learning Rate - 0.000390625, magnitude of gradient - 0.03489881634614513\n",
      "Step - 7446, Loss - 0.22104173354221382, Learning Rate - 0.000390625, magnitude of gradient - 0.003838110309313147\n",
      "Step - 7447, Loss - 0.3443902776394502, Learning Rate - 0.000390625, magnitude of gradient - 0.08070455135087157\n",
      "Step - 7448, Loss - 0.37448854864624126, Learning Rate - 0.000390625, magnitude of gradient - 0.12426805653137916\n",
      "Step - 7449, Loss - 0.3451922963257371, Learning Rate - 0.000390625, magnitude of gradient - 0.05758947286513819\n",
      "Step - 7450, Loss - 0.32482796063550484, Learning Rate - 0.000390625, magnitude of gradient - 0.06874888717496395\n",
      "Step - 7451, Loss - 0.26649746948366504, Learning Rate - 0.000390625, magnitude of gradient - 0.05730084538604394\n",
      "Step - 7452, Loss - 0.26306755191516484, Learning Rate - 0.000390625, magnitude of gradient - 0.05234525283234327\n",
      "Step - 7453, Loss - 0.3373290168283551, Learning Rate - 0.000390625, magnitude of gradient - 0.10228396685407373\n",
      "Step - 7454, Loss - 0.3506828442377167, Learning Rate - 0.000390625, magnitude of gradient - 0.03635936169846037\n",
      "Step - 7455, Loss - 0.32532329584689856, Learning Rate - 0.000390625, magnitude of gradient - 0.07330724899099225\n",
      "Step - 7456, Loss - 0.2819610984610265, Learning Rate - 0.000390625, magnitude of gradient - 0.06719285172756591\n",
      "Step - 7457, Loss - 0.29326428895652623, Learning Rate - 0.000390625, magnitude of gradient - 0.014057659306861616\n",
      "Step - 7458, Loss - 0.2848214840785517, Learning Rate - 0.000390625, magnitude of gradient - 0.023465820798512824\n",
      "Step - 7459, Loss - 0.3520505522260305, Learning Rate - 0.000390625, magnitude of gradient - 0.06578421240506459\n",
      "Step - 7460, Loss - 0.3718781641028882, Learning Rate - 0.000390625, magnitude of gradient - 0.02549279708652696\n",
      "Step - 7461, Loss - 0.2661449109192578, Learning Rate - 0.000390625, magnitude of gradient - 0.05966207733894239\n",
      "Step - 7462, Loss - 0.28395227132422474, Learning Rate - 0.000390625, magnitude of gradient - 0.03963758943084792\n",
      "Step - 7463, Loss - 0.32070620358098356, Learning Rate - 0.000390625, magnitude of gradient - 0.010038153406798069\n",
      "Step - 7464, Loss - 0.35510058648780485, Learning Rate - 0.000390625, magnitude of gradient - 0.06676609551097588\n",
      "Step - 7465, Loss - 0.3132829571237212, Learning Rate - 0.000390625, magnitude of gradient - 0.06899879413859203\n",
      "Step - 7466, Loss - 0.3207491332482808, Learning Rate - 0.000390625, magnitude of gradient - 0.074143009496109\n",
      "Step - 7467, Loss - 0.356097781952502, Learning Rate - 0.000390625, magnitude of gradient - 0.06793958707513917\n",
      "Step - 7468, Loss - 0.2512079730757867, Learning Rate - 0.000390625, magnitude of gradient - 0.014053309740242916\n",
      "Step - 7469, Loss - 0.28937880697899415, Learning Rate - 0.000390625, magnitude of gradient - 0.002009627649708452\n",
      "Step - 7470, Loss - 0.23922256547058335, Learning Rate - 0.000390625, magnitude of gradient - 0.07214487455509803\n",
      "Step - 7471, Loss - 0.3051290270226202, Learning Rate - 0.000390625, magnitude of gradient - 0.08490811703801897\n",
      "Step - 7472, Loss - 0.32783491445160184, Learning Rate - 0.000390625, magnitude of gradient - 0.0669624640003783\n",
      "Step - 7473, Loss - 0.2826526072732624, Learning Rate - 0.000390625, magnitude of gradient - 0.0565316049205193\n",
      "Step - 7474, Loss - 0.3964822029562332, Learning Rate - 0.000390625, magnitude of gradient - 0.03923514733148904\n",
      "Step - 7475, Loss - 0.31803947811761846, Learning Rate - 0.000390625, magnitude of gradient - 0.03230447988583092\n",
      "Step - 7476, Loss - 0.2926662728402913, Learning Rate - 0.000390625, magnitude of gradient - 0.0324279207454884\n",
      "Step - 7477, Loss - 0.3202813134139816, Learning Rate - 0.000390625, magnitude of gradient - 0.030731160219533402\n",
      "Step - 7478, Loss - 0.360349927906651, Learning Rate - 0.000390625, magnitude of gradient - 0.060563230688138785\n",
      "Step - 7479, Loss - 0.39133880548570954, Learning Rate - 0.000390625, magnitude of gradient - 0.08519739254531368\n",
      "Step - 7480, Loss - 0.2995313760951538, Learning Rate - 0.000390625, magnitude of gradient - 0.025778010193914083\n",
      "Step - 7481, Loss - 0.3933170851077821, Learning Rate - 0.000390625, magnitude of gradient - 0.0696388638115684\n",
      "Step - 7482, Loss - 0.33236773671460274, Learning Rate - 0.000390625, magnitude of gradient - 0.05581712069810164\n",
      "Step - 7483, Loss - 0.27614550364558593, Learning Rate - 0.000390625, magnitude of gradient - 0.05650631554455836\n",
      "Step - 7484, Loss - 0.2992018540197202, Learning Rate - 0.000390625, magnitude of gradient - 0.035552398507485664\n",
      "Step - 7485, Loss - 0.2726041764994371, Learning Rate - 0.000390625, magnitude of gradient - 0.03684759670258121\n",
      "Step - 7486, Loss - 0.3070926809487376, Learning Rate - 0.000390625, magnitude of gradient - 0.0416855999513732\n",
      "Step - 7487, Loss - 0.42623320588465957, Learning Rate - 0.000390625, magnitude of gradient - 0.02559584853041425\n",
      "Step - 7488, Loss - 0.3395791715864361, Learning Rate - 0.000390625, magnitude of gradient - 0.03600136898019918\n",
      "Step - 7489, Loss - 0.3089093643837897, Learning Rate - 0.000390625, magnitude of gradient - 0.025699543424865964\n",
      "Step - 7490, Loss - 0.3672306774364378, Learning Rate - 0.000390625, magnitude of gradient - 0.03797783252026367\n",
      "Step - 7491, Loss - 0.3760372161246423, Learning Rate - 0.000390625, magnitude of gradient - 0.10740971903030899\n",
      "Step - 7492, Loss - 0.3341725798086938, Learning Rate - 0.000390625, magnitude of gradient - 0.04199526971803202\n",
      "Step - 7493, Loss - 0.2728927202718374, Learning Rate - 0.000390625, magnitude of gradient - 0.03819943862617827\n",
      "Step - 7494, Loss - 0.2609310667511302, Learning Rate - 0.000390625, magnitude of gradient - 0.027967350749513162\n",
      "Step - 7495, Loss - 0.34919229168704957, Learning Rate - 0.000390625, magnitude of gradient - 0.04205239355522528\n",
      "Step - 7496, Loss - 0.25369310129327904, Learning Rate - 0.000390625, magnitude of gradient - 0.0546982582420101\n",
      "Step - 7497, Loss - 0.32010476686493217, Learning Rate - 0.000390625, magnitude of gradient - 0.04862997899441143\n",
      "Step - 7498, Loss - 0.2581400863283054, Learning Rate - 0.000390625, magnitude of gradient - 0.024475822641411227\n",
      "Step - 7499, Loss - 0.3791971767367479, Learning Rate - 0.000390625, magnitude of gradient - 0.08104654513271865\n",
      "Step - 7500, Loss - 0.2940862077361161, Learning Rate - 0.000390625, magnitude of gradient - 0.06148907540653304\n",
      "Step - 7501, Loss - 0.31248990539192045, Learning Rate - 0.000390625, magnitude of gradient - 0.011238678425360655\n",
      "Step - 7502, Loss - 0.2882541708218642, Learning Rate - 0.000390625, magnitude of gradient - 0.08037510636509704\n",
      "Step - 7503, Loss - 0.376272144778443, Learning Rate - 0.000390625, magnitude of gradient - 0.051494512373302395\n",
      "Step - 7504, Loss - 0.3174051650643581, Learning Rate - 0.000390625, magnitude of gradient - 0.02092296648667692\n",
      "Step - 7505, Loss - 0.30357563304224744, Learning Rate - 0.000390625, magnitude of gradient - 0.08570202662436997\n",
      "Step - 7506, Loss - 0.23333651135055, Learning Rate - 0.000390625, magnitude of gradient - 0.041217805035839085\n",
      "Step - 7507, Loss - 0.3690801929485765, Learning Rate - 0.000390625, magnitude of gradient - 0.03824929817311924\n",
      "Step - 7508, Loss - 0.32378127368519644, Learning Rate - 0.000390625, magnitude of gradient - 0.021882391671164216\n",
      "Step - 7509, Loss - 0.2424415837819547, Learning Rate - 0.000390625, magnitude of gradient - 0.06786120198577877\n",
      "Step - 7510, Loss - 0.35546628216610393, Learning Rate - 0.000390625, magnitude of gradient - 0.04583512909938418\n",
      "Step - 7511, Loss - 0.3861953639946517, Learning Rate - 0.000390625, magnitude of gradient - 0.05780690500268425\n",
      "Step - 7512, Loss - 0.30366678201038494, Learning Rate - 0.000390625, magnitude of gradient - 0.06362470325928107\n",
      "Step - 7513, Loss - 0.36340798580854583, Learning Rate - 0.000390625, magnitude of gradient - 0.03498197038596336\n",
      "Step - 7514, Loss - 0.38201947544809917, Learning Rate - 0.000390625, magnitude of gradient - 0.037181860576008925\n",
      "Step - 7515, Loss - 0.40114951877839683, Learning Rate - 0.000390625, magnitude of gradient - 0.07442420499312692\n",
      "Step - 7516, Loss - 0.2471175104248029, Learning Rate - 0.000390625, magnitude of gradient - 0.042594162467334246\n",
      "Step - 7517, Loss - 0.2869289178815865, Learning Rate - 0.000390625, magnitude of gradient - 0.08415735639076365\n",
      "Step - 7518, Loss - 0.357144743982842, Learning Rate - 0.000390625, magnitude of gradient - 0.03257141595712095\n",
      "Step - 7519, Loss - 0.34675110222858835, Learning Rate - 0.000390625, magnitude of gradient - 0.014496997887255796\n",
      "Step - 7520, Loss - 0.3278935600857329, Learning Rate - 0.000390625, magnitude of gradient - 0.015417720299679697\n",
      "Step - 7521, Loss - 0.31163897204968116, Learning Rate - 0.000390625, magnitude of gradient - 0.02411766653120337\n",
      "Step - 7522, Loss - 0.34153930384219133, Learning Rate - 0.000390625, magnitude of gradient - 0.024454159180128412\n",
      "Step - 7523, Loss - 0.37771312253125217, Learning Rate - 0.000390625, magnitude of gradient - 0.04391606877212345\n",
      "Step - 7524, Loss - 0.2544311417944404, Learning Rate - 0.000390625, magnitude of gradient - 0.025572010646364825\n",
      "Step - 7525, Loss - 0.3184289226877749, Learning Rate - 0.000390625, magnitude of gradient - 0.06565586173167537\n",
      "Step - 7526, Loss - 0.31901978537335063, Learning Rate - 0.000390625, magnitude of gradient - 0.02035910969788312\n",
      "Step - 7527, Loss - 0.4203385538090709, Learning Rate - 0.000390625, magnitude of gradient - 0.07527048602156834\n",
      "Step - 7528, Loss - 0.3679226458695747, Learning Rate - 0.000390625, magnitude of gradient - 0.023599763456969153\n",
      "Step - 7529, Loss - 0.36372272416873397, Learning Rate - 0.000390625, magnitude of gradient - 0.007581872525456424\n",
      "Step - 7530, Loss - 0.36276436659519823, Learning Rate - 0.000390625, magnitude of gradient - 0.04090068803517727\n",
      "Step - 7531, Loss - 0.27139183140310363, Learning Rate - 0.000390625, magnitude of gradient - 0.11006989793005235\n",
      "Step - 7532, Loss - 0.2588320403319127, Learning Rate - 0.000390625, magnitude of gradient - 0.005554970848382029\n",
      "Step - 7533, Loss - 0.2961284021527569, Learning Rate - 0.000390625, magnitude of gradient - 0.019462856166776534\n",
      "Step - 7534, Loss - 0.37698339701084793, Learning Rate - 0.000390625, magnitude of gradient - 0.08775517679951232\n",
      "Step - 7535, Loss - 0.25982716150392665, Learning Rate - 0.000390625, magnitude of gradient - 0.053010861065926854\n",
      "Step - 7536, Loss - 0.27252852371011593, Learning Rate - 0.000390625, magnitude of gradient - 0.004768399306407261\n",
      "Step - 7537, Loss - 0.44868594544667134, Learning Rate - 0.000390625, magnitude of gradient - 0.11335387084050667\n",
      "Step - 7538, Loss - 0.35243701613490586, Learning Rate - 0.000390625, magnitude of gradient - 0.04852050483873265\n",
      "Step - 7539, Loss - 0.2782689891316704, Learning Rate - 0.000390625, magnitude of gradient - 0.09005995041668736\n",
      "Step - 7540, Loss - 0.3026041601118193, Learning Rate - 0.000390625, magnitude of gradient - 0.053107100586631185\n",
      "Step - 7541, Loss - 0.32032250561131104, Learning Rate - 0.000390625, magnitude of gradient - 0.07923948426416356\n",
      "Step - 7542, Loss - 0.3134046695933492, Learning Rate - 0.000390625, magnitude of gradient - 0.06387467527141671\n",
      "Step - 7543, Loss - 0.28019269492322607, Learning Rate - 0.000390625, magnitude of gradient - 0.020383436934017098\n",
      "Step - 7544, Loss - 0.286347350542953, Learning Rate - 0.000390625, magnitude of gradient - 0.019335377241224173\n",
      "Step - 7545, Loss - 0.34850325153561357, Learning Rate - 0.000390625, magnitude of gradient - 0.02612130769500467\n",
      "Step - 7546, Loss - 0.36286756523986125, Learning Rate - 0.000390625, magnitude of gradient - 0.07400398006128868\n",
      "Step - 7547, Loss - 0.43498782704140687, Learning Rate - 0.000390625, magnitude of gradient - 0.11584544468686751\n",
      "Step - 7548, Loss - 0.3748451493557784, Learning Rate - 0.000390625, magnitude of gradient - 0.055732435753322984\n",
      "Step - 7549, Loss - 0.30312059882381825, Learning Rate - 0.000390625, magnitude of gradient - 0.033437481801246886\n",
      "Step - 7550, Loss - 0.33092024546627025, Learning Rate - 0.000390625, magnitude of gradient - 0.05543620714251525\n",
      "Step - 7551, Loss - 0.35075587402110775, Learning Rate - 0.000390625, magnitude of gradient - 0.03386686614410299\n",
      "Step - 7552, Loss - 0.3717558942099471, Learning Rate - 0.000390625, magnitude of gradient - 0.020948947772964233\n",
      "Step - 7553, Loss - 0.3101952592755806, Learning Rate - 0.000390625, magnitude of gradient - 0.033075888863295974\n",
      "Step - 7554, Loss - 0.3177286410866572, Learning Rate - 0.000390625, magnitude of gradient - 0.028683433239529342\n",
      "Step - 7555, Loss - 0.26541606273928764, Learning Rate - 0.000390625, magnitude of gradient - 0.05584279500986043\n",
      "Step - 7556, Loss - 0.30400338208242583, Learning Rate - 0.000390625, magnitude of gradient - 0.08248310849934412\n",
      "Step - 7557, Loss - 0.33817402776432626, Learning Rate - 0.000390625, magnitude of gradient - 0.04496593352284139\n",
      "Step - 7558, Loss - 0.3448469551378917, Learning Rate - 0.000390625, magnitude of gradient - 0.08308080076856791\n",
      "Step - 7559, Loss - 0.3170190642352192, Learning Rate - 0.000390625, magnitude of gradient - 0.069697301440011\n",
      "Step - 7560, Loss - 0.3210980270985867, Learning Rate - 0.000390625, magnitude of gradient - 0.08006653365138704\n",
      "Step - 7561, Loss - 0.3286056286434257, Learning Rate - 0.000390625, magnitude of gradient - 0.07900205926091468\n",
      "Step - 7562, Loss - 0.3363866674942911, Learning Rate - 0.000390625, magnitude of gradient - 0.06663975024725735\n",
      "Step - 7563, Loss - 0.25534209402826175, Learning Rate - 0.000390625, magnitude of gradient - 0.038136228638971585\n",
      "Step - 7564, Loss - 0.29959160284453423, Learning Rate - 0.000390625, magnitude of gradient - 0.07001289787358388\n",
      "Step - 7565, Loss - 0.36773836844217295, Learning Rate - 0.000390625, magnitude of gradient - 0.10238563639312324\n",
      "Step - 7566, Loss - 0.34789206712944243, Learning Rate - 0.000390625, magnitude of gradient - 0.05415501325812067\n",
      "Step - 7567, Loss - 0.3729776243110994, Learning Rate - 0.000390625, magnitude of gradient - 0.02302597781325002\n",
      "Step - 7568, Loss - 0.2917240681873522, Learning Rate - 0.000390625, magnitude of gradient - 0.035479564138290336\n",
      "Step - 7569, Loss - 0.2866122375046931, Learning Rate - 0.000390625, magnitude of gradient - 0.023479038021484336\n",
      "Step - 7570, Loss - 0.3348242547950974, Learning Rate - 0.000390625, magnitude of gradient - 0.026590762517939834\n",
      "Step - 7571, Loss - 0.3137437812632642, Learning Rate - 0.000390625, magnitude of gradient - 0.11751492989897692\n",
      "Step - 7572, Loss - 0.2742820180495925, Learning Rate - 0.000390625, magnitude of gradient - 0.030637172146751927\n",
      "Step - 7573, Loss - 0.3439156604366994, Learning Rate - 0.000390625, magnitude of gradient - 0.07980502455453352\n",
      "Step - 7574, Loss - 0.352714200685288, Learning Rate - 0.000390625, magnitude of gradient - 0.018187535638156015\n",
      "Step - 7575, Loss - 0.2999901075274172, Learning Rate - 0.000390625, magnitude of gradient - 0.07902787811760457\n",
      "Step - 7576, Loss - 0.2993836818208786, Learning Rate - 0.000390625, magnitude of gradient - 0.05161821368981329\n",
      "Step - 7577, Loss - 0.333334942404968, Learning Rate - 0.000390625, magnitude of gradient - 0.0961535370228671\n",
      "Step - 7578, Loss - 0.40322039087880857, Learning Rate - 0.000390625, magnitude of gradient - 0.12241791394510007\n",
      "Step - 7579, Loss - 0.27239500559234064, Learning Rate - 0.000390625, magnitude of gradient - 0.008135539035679802\n",
      "Step - 7580, Loss - 0.4013373861410846, Learning Rate - 0.000390625, magnitude of gradient - 0.026036769408371567\n",
      "Step - 7581, Loss - 0.2871601915524955, Learning Rate - 0.000390625, magnitude of gradient - 0.07678434012259208\n",
      "Step - 7582, Loss - 0.27106810231578815, Learning Rate - 0.000390625, magnitude of gradient - 0.030277405588476154\n",
      "Step - 7583, Loss - 0.36157254882222256, Learning Rate - 0.000390625, magnitude of gradient - 0.04943397497913324\n",
      "Step - 7584, Loss - 0.2979192267613635, Learning Rate - 0.000390625, magnitude of gradient - 0.045163336936133995\n",
      "Step - 7585, Loss - 0.33326195090021843, Learning Rate - 0.000390625, magnitude of gradient - 0.08120918776431478\n",
      "Step - 7586, Loss - 0.2680148879540457, Learning Rate - 0.000390625, magnitude of gradient - 0.018401078897580147\n",
      "Step - 7587, Loss - 0.3206121333094033, Learning Rate - 0.000390625, magnitude of gradient - 0.07828370665954507\n",
      "Step - 7588, Loss - 0.32434419219647687, Learning Rate - 0.000390625, magnitude of gradient - 0.08395504311979662\n",
      "Step - 7589, Loss - 0.28183010782012696, Learning Rate - 0.000390625, magnitude of gradient - 0.08038498717147807\n",
      "Step - 7590, Loss - 0.2730071167753254, Learning Rate - 0.000390625, magnitude of gradient - 0.07431136141549276\n",
      "Step - 7591, Loss - 0.34029390158826833, Learning Rate - 0.000390625, magnitude of gradient - 0.056186352652504636\n",
      "Step - 7592, Loss - 0.3475110877852087, Learning Rate - 0.000390625, magnitude of gradient - 0.06731273479332563\n",
      "Step - 7593, Loss - 0.3219879432734661, Learning Rate - 0.000390625, magnitude of gradient - 0.014170394868063724\n",
      "Step - 7594, Loss - 0.2679023939152706, Learning Rate - 0.000390625, magnitude of gradient - 0.09947124232393886\n",
      "Step - 7595, Loss - 0.299402405814223, Learning Rate - 0.000390625, magnitude of gradient - 0.0655809386408886\n",
      "Step - 7596, Loss - 0.3310506709584632, Learning Rate - 0.000390625, magnitude of gradient - 0.0525191056264569\n",
      "Step - 7597, Loss - 0.3430200490651597, Learning Rate - 0.000390625, magnitude of gradient - 0.0675889772092608\n",
      "Step - 7598, Loss - 0.307276244017328, Learning Rate - 0.000390625, magnitude of gradient - 0.0177675370389083\n",
      "Step - 7599, Loss - 0.2651149383046106, Learning Rate - 0.000390625, magnitude of gradient - 0.006421144348333903\n",
      "Step - 7600, Loss - 0.2641203658167725, Learning Rate - 0.000390625, magnitude of gradient - 0.0074791395908122\n",
      "Step - 7601, Loss - 0.284685885437256, Learning Rate - 0.000390625, magnitude of gradient - 0.0741376744155592\n",
      "Step - 7602, Loss - 0.2981803683816233, Learning Rate - 0.000390625, magnitude of gradient - 0.06670610558868895\n",
      "Step - 7603, Loss - 0.2943614512304047, Learning Rate - 0.000390625, magnitude of gradient - 0.06797192279749285\n",
      "Step - 7604, Loss - 0.3416711502053459, Learning Rate - 0.000390625, magnitude of gradient - 0.06150668405834464\n",
      "Step - 7605, Loss - 0.3325319039603776, Learning Rate - 0.000390625, magnitude of gradient - 0.07532624039780561\n",
      "Step - 7606, Loss - 0.342557959289154, Learning Rate - 0.000390625, magnitude of gradient - 0.02385227401578177\n",
      "Step - 7607, Loss - 0.32660975313510754, Learning Rate - 0.000390625, magnitude of gradient - 0.04463192162377652\n",
      "Step - 7608, Loss - 0.35935287338655153, Learning Rate - 0.000390625, magnitude of gradient - 0.06032392009691061\n",
      "Step - 7609, Loss - 0.2972723203312936, Learning Rate - 0.000390625, magnitude of gradient - 0.00813943950092164\n",
      "Step - 7610, Loss - 0.3873859743388946, Learning Rate - 0.000390625, magnitude of gradient - 0.0756461273935096\n",
      "Step - 7611, Loss - 0.4229810022847368, Learning Rate - 0.000390625, magnitude of gradient - 0.04313948142583454\n",
      "Step - 7612, Loss - 0.31855974464126785, Learning Rate - 0.000390625, magnitude of gradient - 0.043692690951749116\n",
      "Step - 7613, Loss - 0.3798966745804887, Learning Rate - 0.000390625, magnitude of gradient - 0.01092252527946062\n",
      "Step - 7614, Loss - 0.3183636615635717, Learning Rate - 0.000390625, magnitude of gradient - 0.055242511059440905\n",
      "Step - 7615, Loss - 0.33314046826257077, Learning Rate - 0.000390625, magnitude of gradient - 0.09025981544178456\n",
      "Step - 7616, Loss - 0.3342032151130217, Learning Rate - 0.000390625, magnitude of gradient - 0.05023701312641949\n",
      "Step - 7617, Loss - 0.302882851102385, Learning Rate - 0.000390625, magnitude of gradient - 0.06275637499050447\n",
      "Step - 7618, Loss - 0.3239891235685523, Learning Rate - 0.000390625, magnitude of gradient - 0.04010715482212457\n",
      "Step - 7619, Loss - 0.2836666028412009, Learning Rate - 0.000390625, magnitude of gradient - 0.025558753203042843\n",
      "Step - 7620, Loss - 0.2537667732642721, Learning Rate - 0.000390625, magnitude of gradient - 0.04752359311955031\n",
      "Step - 7621, Loss - 0.3401111481271415, Learning Rate - 0.000390625, magnitude of gradient - 0.008703037886656356\n",
      "Step - 7622, Loss - 0.36245006333077945, Learning Rate - 0.000390625, magnitude of gradient - 0.02920034384269555\n",
      "Step - 7623, Loss - 0.2893425295942292, Learning Rate - 0.000390625, magnitude of gradient - 0.07261854014304352\n",
      "Step - 7624, Loss - 0.349947320906231, Learning Rate - 0.000390625, magnitude of gradient - 0.10874967665197223\n",
      "Step - 7625, Loss - 0.3124718258774431, Learning Rate - 0.000390625, magnitude of gradient - 0.00975728968138139\n",
      "Step - 7626, Loss - 0.3674063481073605, Learning Rate - 0.000390625, magnitude of gradient - 0.03108860004602202\n",
      "Step - 7627, Loss - 0.31327102411994834, Learning Rate - 0.000390625, magnitude of gradient - 0.08547213015324043\n",
      "Step - 7628, Loss - 0.32230099223673725, Learning Rate - 0.000390625, magnitude of gradient - 0.04228210285270324\n",
      "Step - 7629, Loss - 0.35002098770971307, Learning Rate - 0.000390625, magnitude of gradient - 0.02466651124299411\n",
      "Step - 7630, Loss - 0.3214779520885038, Learning Rate - 0.000390625, magnitude of gradient - 0.06286314780423193\n",
      "Step - 7631, Loss - 0.3621389892350587, Learning Rate - 0.000390625, magnitude of gradient - 0.06677093100880714\n",
      "Step - 7632, Loss - 0.2742850401954028, Learning Rate - 0.000390625, magnitude of gradient - 0.051535008797407654\n",
      "Step - 7633, Loss - 0.26740019531186576, Learning Rate - 0.000390625, magnitude of gradient - 0.05580749211540408\n",
      "Step - 7634, Loss - 0.3516071814124606, Learning Rate - 0.000390625, magnitude of gradient - 0.04635028597203513\n",
      "Step - 7635, Loss - 0.3419485464500419, Learning Rate - 0.000390625, magnitude of gradient - 0.032079464297564905\n",
      "Step - 7636, Loss - 0.3393970344798959, Learning Rate - 0.000390625, magnitude of gradient - 0.022160941552084742\n",
      "Step - 7637, Loss - 0.30946444161476616, Learning Rate - 0.000390625, magnitude of gradient - 0.11355910580377394\n",
      "Step - 7638, Loss - 0.3203695154672338, Learning Rate - 0.000390625, magnitude of gradient - 0.03826721971576125\n",
      "Step - 7639, Loss - 0.36985467417955326, Learning Rate - 0.000390625, magnitude of gradient - 0.08238116662889779\n",
      "Step - 7640, Loss - 0.27190360546216186, Learning Rate - 0.000390625, magnitude of gradient - 0.014325354129504518\n",
      "Step - 7641, Loss - 0.2509579308391384, Learning Rate - 0.000390625, magnitude of gradient - 0.059699952280887536\n",
      "Step - 7642, Loss - 0.3359311280456912, Learning Rate - 0.000390625, magnitude of gradient - 0.04549260039236811\n",
      "Step - 7643, Loss - 0.32751939240517103, Learning Rate - 0.000390625, magnitude of gradient - 0.03189600416408199\n",
      "Step - 7644, Loss - 0.36618629722098806, Learning Rate - 0.000390625, magnitude of gradient - 0.09383632592815853\n",
      "Step - 7645, Loss - 0.3083040355911274, Learning Rate - 0.000390625, magnitude of gradient - 0.04868109494137081\n",
      "Step - 7646, Loss - 0.25677237396150315, Learning Rate - 0.000390625, magnitude of gradient - 0.09370446004525294\n",
      "Step - 7647, Loss - 0.32216127999933075, Learning Rate - 0.000390625, magnitude of gradient - 0.013813839494788514\n",
      "Step - 7648, Loss - 0.3653683587424255, Learning Rate - 0.000390625, magnitude of gradient - 0.11795456203642343\n",
      "Step - 7649, Loss - 0.2871984982093428, Learning Rate - 0.000390625, magnitude of gradient - 0.05544908651207763\n",
      "Step - 7650, Loss - 0.42141331994621606, Learning Rate - 0.000390625, magnitude of gradient - 0.0036777620893915794\n",
      "Step - 7651, Loss - 0.30537541459938783, Learning Rate - 0.000390625, magnitude of gradient - 0.08924899864511777\n",
      "Step - 7652, Loss - 0.34463119532179604, Learning Rate - 0.000390625, magnitude of gradient - 0.10932437981041108\n",
      "Step - 7653, Loss - 0.3202854522120324, Learning Rate - 0.000390625, magnitude of gradient - 0.04499092931267902\n",
      "Step - 7654, Loss - 0.3151456660815182, Learning Rate - 0.000390625, magnitude of gradient - 0.041231721912089396\n",
      "Step - 7655, Loss - 0.325353564327128, Learning Rate - 0.000390625, magnitude of gradient - 0.023439550096462033\n",
      "Step - 7656, Loss - 0.25079504967203403, Learning Rate - 0.000390625, magnitude of gradient - 0.06841396301280336\n",
      "Step - 7657, Loss - 0.2824004979632852, Learning Rate - 0.000390625, magnitude of gradient - 0.01588464329745731\n",
      "Step - 7658, Loss - 0.3726323348249987, Learning Rate - 0.000390625, magnitude of gradient - 0.05845538871795431\n",
      "Step - 7659, Loss - 0.3260729267120354, Learning Rate - 0.000390625, magnitude of gradient - 0.03793919260511603\n",
      "Step - 7660, Loss - 0.3138812884668841, Learning Rate - 0.000390625, magnitude of gradient - 0.06823284883902067\n",
      "Step - 7661, Loss - 0.2872936497739086, Learning Rate - 0.000390625, magnitude of gradient - 0.06146289256801673\n",
      "Step - 7662, Loss - 0.2789721626304897, Learning Rate - 0.000390625, magnitude of gradient - 0.056883455267700514\n",
      "Step - 7663, Loss - 0.34763664304885844, Learning Rate - 0.000390625, magnitude of gradient - 0.0682198186759973\n",
      "Step - 7664, Loss - 0.3536204134360371, Learning Rate - 0.000390625, magnitude of gradient - 0.017939174047937167\n",
      "Step - 7665, Loss - 0.34615299204822675, Learning Rate - 0.000390625, magnitude of gradient - 0.042617630662657995\n",
      "Step - 7666, Loss - 0.3450032116485918, Learning Rate - 0.000390625, magnitude of gradient - 0.031075786927503696\n",
      "Step - 7667, Loss - 0.286975482139405, Learning Rate - 0.000390625, magnitude of gradient - 0.07309173816602883\n",
      "Step - 7668, Loss - 0.3331130697451823, Learning Rate - 0.000390625, magnitude of gradient - 0.03848228724532638\n",
      "Step - 7669, Loss - 0.29202197474869174, Learning Rate - 0.000390625, magnitude of gradient - 0.05878571695988128\n",
      "Step - 7670, Loss - 0.31963418906117436, Learning Rate - 0.000390625, magnitude of gradient - 0.04456645460615729\n",
      "Step - 7671, Loss - 0.30675864042063306, Learning Rate - 0.000390625, magnitude of gradient - 0.04616607236959957\n",
      "Step - 7672, Loss - 0.3130361708894809, Learning Rate - 0.000390625, magnitude of gradient - 0.020900950624714327\n",
      "Step - 7673, Loss - 0.283327600751639, Learning Rate - 0.000390625, magnitude of gradient - 0.0483339024245034\n",
      "Step - 7674, Loss - 0.3849040059529753, Learning Rate - 0.000390625, magnitude of gradient - 0.028957676742903005\n",
      "Step - 7675, Loss - 0.35622611616632877, Learning Rate - 0.000390625, magnitude of gradient - 0.0729358387961112\n",
      "Step - 7676, Loss - 0.3102951599861403, Learning Rate - 0.000390625, magnitude of gradient - 0.06729797604673622\n",
      "Step - 7677, Loss - 0.28801036028569516, Learning Rate - 0.000390625, magnitude of gradient - 0.020663088362534474\n",
      "Step - 7678, Loss - 0.3567100423913251, Learning Rate - 0.000390625, magnitude of gradient - 0.09330097475180635\n",
      "Step - 7679, Loss - 0.4108072863861204, Learning Rate - 0.000390625, magnitude of gradient - 0.04389296491867136\n",
      "Step - 7680, Loss - 0.36048908741261243, Learning Rate - 0.000390625, magnitude of gradient - 0.10024320219299651\n",
      "Step - 7681, Loss - 0.398579643825387, Learning Rate - 0.000390625, magnitude of gradient - 0.06934533934765597\n",
      "Step - 7682, Loss - 0.27921014071225914, Learning Rate - 0.000390625, magnitude of gradient - 0.08098548315379062\n",
      "Step - 7683, Loss - 0.27654868098234847, Learning Rate - 0.000390625, magnitude of gradient - 0.008935491222805663\n",
      "Step - 7684, Loss - 0.3129760406430144, Learning Rate - 0.000390625, magnitude of gradient - 0.050367273239388756\n",
      "Step - 7685, Loss - 0.25965927998181265, Learning Rate - 0.000390625, magnitude of gradient - 0.009824757758023819\n",
      "Step - 7686, Loss - 0.3940800676512109, Learning Rate - 0.000390625, magnitude of gradient - 0.02976618147759543\n",
      "Step - 7687, Loss - 0.3428975052118238, Learning Rate - 0.000390625, magnitude of gradient - 0.01912337939926\n",
      "Step - 7688, Loss - 0.333646569090224, Learning Rate - 0.000390625, magnitude of gradient - 0.06976912936430586\n",
      "Step - 7689, Loss - 0.3944403296428769, Learning Rate - 0.000390625, magnitude of gradient - 0.034423474931966576\n",
      "Step - 7690, Loss - 0.32149513301154464, Learning Rate - 0.000390625, magnitude of gradient - 0.02451916305794038\n",
      "Step - 7691, Loss - 0.29722867186106083, Learning Rate - 0.000390625, magnitude of gradient - 0.027392905040113685\n",
      "Step - 7692, Loss - 0.2413206713616815, Learning Rate - 0.000390625, magnitude of gradient - 0.010107908803798646\n",
      "Step - 7693, Loss - 0.26193953127025693, Learning Rate - 0.000390625, magnitude of gradient - 0.06230945323681532\n",
      "Step - 7694, Loss - 0.3113481977078896, Learning Rate - 0.000390625, magnitude of gradient - 0.0751088278072823\n",
      "Step - 7695, Loss - 0.3207226804757218, Learning Rate - 0.000390625, magnitude of gradient - 0.12377373881481342\n",
      "Step - 7696, Loss - 0.35093783042203924, Learning Rate - 0.000390625, magnitude of gradient - 0.045377146448022834\n",
      "Step - 7697, Loss - 0.27327022873102214, Learning Rate - 0.000390625, magnitude of gradient - 0.01684724434322601\n",
      "Step - 7698, Loss - 0.27219869688292475, Learning Rate - 0.000390625, magnitude of gradient - 0.04211310725321776\n",
      "Step - 7699, Loss - 0.3246056887600384, Learning Rate - 0.000390625, magnitude of gradient - 0.04462233451763138\n",
      "Step - 7700, Loss - 0.27612496138143905, Learning Rate - 0.000390625, magnitude of gradient - 0.08514992244695631\n",
      "Step - 7701, Loss - 0.29726079323692817, Learning Rate - 0.000390625, magnitude of gradient - 0.04868536319539962\n",
      "Step - 7702, Loss - 0.2677838696003973, Learning Rate - 0.000390625, magnitude of gradient - 0.09159717776704801\n",
      "Step - 7703, Loss - 0.3336327725500906, Learning Rate - 0.000390625, magnitude of gradient - 0.037942932037629315\n",
      "Step - 7704, Loss - 0.3495609267129207, Learning Rate - 0.000390625, magnitude of gradient - 0.03041503488328373\n",
      "Step - 7705, Loss - 0.34662035538559877, Learning Rate - 0.000390625, magnitude of gradient - 0.08241047060748621\n",
      "Step - 7706, Loss - 0.36181653380189255, Learning Rate - 0.000390625, magnitude of gradient - 0.10298486982256772\n",
      "Step - 7707, Loss - 0.3573575989368368, Learning Rate - 0.000390625, magnitude of gradient - 0.06287894501045253\n",
      "Step - 7708, Loss - 0.2852119067989519, Learning Rate - 0.000390625, magnitude of gradient - 0.025995367316343436\n",
      "Step - 7709, Loss - 0.34129858558761933, Learning Rate - 0.000390625, magnitude of gradient - 0.03415978102703676\n",
      "Step - 7710, Loss - 0.3126822512391314, Learning Rate - 0.000390625, magnitude of gradient - 0.09770469677100037\n",
      "Step - 7711, Loss - 0.3248047042004041, Learning Rate - 0.000390625, magnitude of gradient - 0.029372154035382143\n",
      "Step - 7712, Loss - 0.4155622262075055, Learning Rate - 0.000390625, magnitude of gradient - 0.07492053291759417\n",
      "Step - 7713, Loss - 0.272944762718781, Learning Rate - 0.000390625, magnitude of gradient - 0.028269741820411365\n",
      "Step - 7714, Loss - 0.3532356484054588, Learning Rate - 0.000390625, magnitude of gradient - 0.0189582007651914\n",
      "Step - 7715, Loss - 0.3543093793951894, Learning Rate - 0.000390625, magnitude of gradient - 0.052961537979676496\n",
      "Step - 7716, Loss - 0.33663637173470995, Learning Rate - 0.000390625, magnitude of gradient - 0.021936726126698614\n",
      "Step - 7717, Loss - 0.2797536169669944, Learning Rate - 0.000390625, magnitude of gradient - 0.044386322942431534\n",
      "Step - 7718, Loss - 0.2915584575398302, Learning Rate - 0.000390625, magnitude of gradient - 0.027201097927090735\n",
      "Step - 7719, Loss - 0.30399186226983743, Learning Rate - 0.000390625, magnitude of gradient - 0.03328592342294319\n",
      "Step - 7720, Loss - 0.3176629744008836, Learning Rate - 0.000390625, magnitude of gradient - 0.026493281366968352\n",
      "Step - 7721, Loss - 0.3378996841098827, Learning Rate - 0.000390625, magnitude of gradient - 0.08531357098053033\n",
      "Step - 7722, Loss - 0.35151598160150893, Learning Rate - 0.000390625, magnitude of gradient - 0.07646892334689145\n",
      "Step - 7723, Loss - 0.334776493500166, Learning Rate - 0.000390625, magnitude of gradient - 0.05047339063157146\n",
      "Step - 7724, Loss - 0.24334444594853852, Learning Rate - 0.000390625, magnitude of gradient - 0.04393018297951046\n",
      "Step - 7725, Loss - 0.31994337698590475, Learning Rate - 0.000390625, magnitude of gradient - 0.03403124420450896\n",
      "Step - 7726, Loss - 0.32265595048943374, Learning Rate - 0.000390625, magnitude of gradient - 0.053010732853152776\n",
      "Step - 7727, Loss - 0.29940978467666945, Learning Rate - 0.000390625, magnitude of gradient - 0.021822665561808126\n",
      "Step - 7728, Loss - 0.33158909159471156, Learning Rate - 0.000390625, magnitude of gradient - 0.07769362341318276\n",
      "Step - 7729, Loss - 0.3453561693289232, Learning Rate - 0.000390625, magnitude of gradient - 0.09041925592307988\n",
      "Step - 7730, Loss - 0.4011009154226848, Learning Rate - 0.000390625, magnitude of gradient - 0.012493904803860463\n",
      "Step - 7731, Loss - 0.27449588451452356, Learning Rate - 0.000390625, magnitude of gradient - 0.06631952088615277\n",
      "Step - 7732, Loss - 0.2819002655604389, Learning Rate - 0.000390625, magnitude of gradient - 0.08372278337816924\n",
      "Step - 7733, Loss - 0.3594314738453646, Learning Rate - 0.000390625, magnitude of gradient - 0.016185563177276575\n",
      "Step - 7734, Loss - 0.27433680509595126, Learning Rate - 0.000390625, magnitude of gradient - 0.03552907405558688\n",
      "Step - 7735, Loss - 0.3349702269755894, Learning Rate - 0.000390625, magnitude of gradient - 0.040269676983830854\n",
      "Step - 7736, Loss - 0.3155514083591906, Learning Rate - 0.000390625, magnitude of gradient - 0.0400469957363488\n",
      "Step - 7737, Loss - 0.33402264304270324, Learning Rate - 0.000390625, magnitude of gradient - 0.04990550146867166\n",
      "Step - 7738, Loss - 0.31364762862287054, Learning Rate - 0.000390625, magnitude of gradient - 0.10630002001535449\n",
      "Step - 7739, Loss - 0.2665617073603718, Learning Rate - 0.000390625, magnitude of gradient - 0.04980054028932646\n",
      "Step - 7740, Loss - 0.30535617581386393, Learning Rate - 0.000390625, magnitude of gradient - 0.04301823258830346\n",
      "Step - 7741, Loss - 0.2879378381261092, Learning Rate - 0.000390625, magnitude of gradient - 0.04782404601596712\n",
      "Step - 7742, Loss - 0.2831781778643645, Learning Rate - 0.000390625, magnitude of gradient - 0.029502226689389312\n",
      "Step - 7743, Loss - 0.39375263813311406, Learning Rate - 0.000390625, magnitude of gradient - 0.03988617916558056\n",
      "Step - 7744, Loss - 0.281278857052476, Learning Rate - 0.000390625, magnitude of gradient - 0.013235011838978829\n",
      "Step - 7745, Loss - 0.2908193372790099, Learning Rate - 0.000390625, magnitude of gradient - 0.0626809017244996\n",
      "Step - 7746, Loss - 0.30753863615994603, Learning Rate - 0.000390625, magnitude of gradient - 0.03863113047441964\n",
      "Step - 7747, Loss - 0.24785687494070635, Learning Rate - 0.000390625, magnitude of gradient - 0.056572724391045645\n",
      "Step - 7748, Loss - 0.438785857798614, Learning Rate - 0.000390625, magnitude of gradient - 0.07745697403724272\n",
      "Step - 7749, Loss - 0.2814667886624164, Learning Rate - 0.000390625, magnitude of gradient - 0.024458966383816484\n",
      "Step - 7750, Loss - 0.3936340528550531, Learning Rate - 0.000390625, magnitude of gradient - 0.02224952344311252\n",
      "Step - 7751, Loss - 0.2503561852039316, Learning Rate - 0.000390625, magnitude of gradient - 0.03999122737663931\n",
      "Step - 7752, Loss - 0.3107871860070487, Learning Rate - 0.000390625, magnitude of gradient - 0.038350262789937084\n",
      "Step - 7753, Loss - 0.3199073810104709, Learning Rate - 0.000390625, magnitude of gradient - 0.06897859533034907\n",
      "Step - 7754, Loss - 0.24785185190566603, Learning Rate - 0.000390625, magnitude of gradient - 0.03837459222356684\n",
      "Step - 7755, Loss - 0.28039577742516714, Learning Rate - 0.000390625, magnitude of gradient - 0.027131435530404355\n",
      "Step - 7756, Loss - 0.3442638103938104, Learning Rate - 0.000390625, magnitude of gradient - 0.0396597056218683\n",
      "Step - 7757, Loss - 0.31498656604361364, Learning Rate - 0.000390625, magnitude of gradient - 0.06432112787665559\n",
      "Step - 7758, Loss - 0.303009845504983, Learning Rate - 0.000390625, magnitude of gradient - 0.08601716416202233\n",
      "Step - 7759, Loss - 0.26220138696610207, Learning Rate - 0.000390625, magnitude of gradient - 0.08741878720664795\n",
      "Step - 7760, Loss - 0.29121596227741764, Learning Rate - 0.000390625, magnitude of gradient - 0.029212233325465332\n",
      "Step - 7761, Loss - 0.35309242386013306, Learning Rate - 0.000390625, magnitude of gradient - 0.0730369076654991\n",
      "Step - 7762, Loss - 0.2737033259050774, Learning Rate - 0.000390625, magnitude of gradient - 0.026880244851317438\n",
      "Step - 7763, Loss - 0.3030376303332645, Learning Rate - 0.000390625, magnitude of gradient - 0.013065544722448643\n",
      "Step - 7764, Loss - 0.3365898321562675, Learning Rate - 0.000390625, magnitude of gradient - 0.030724549291133184\n",
      "Step - 7765, Loss - 0.29217922220564946, Learning Rate - 0.000390625, magnitude of gradient - 0.020551891136013547\n",
      "Step - 7766, Loss - 0.3005059893796162, Learning Rate - 0.000390625, magnitude of gradient - 0.05221153073524715\n",
      "Step - 7767, Loss - 0.3710337505907017, Learning Rate - 0.000390625, magnitude of gradient - 0.029573154146869212\n",
      "Step - 7768, Loss - 0.3752256544518883, Learning Rate - 0.000390625, magnitude of gradient - 0.0452073071611568\n",
      "Step - 7769, Loss - 0.31367042931364136, Learning Rate - 0.000390625, magnitude of gradient - 0.05657760427230253\n",
      "Step - 7770, Loss - 0.3504624134004494, Learning Rate - 0.000390625, magnitude of gradient - 0.05032451860510856\n",
      "Step - 7771, Loss - 0.21958668903439507, Learning Rate - 0.000390625, magnitude of gradient - 0.08463264612095255\n",
      "Step - 7772, Loss - 0.29838817868831957, Learning Rate - 0.000390625, magnitude of gradient - 0.02394955434384111\n",
      "Step - 7773, Loss - 0.38133930802186133, Learning Rate - 0.000390625, magnitude of gradient - 0.0468474602913536\n",
      "Step - 7774, Loss - 0.2901199995040621, Learning Rate - 0.000390625, magnitude of gradient - 0.056727605252018046\n",
      "Step - 7775, Loss - 0.3171751117337547, Learning Rate - 0.000390625, magnitude of gradient - 0.005674190690508203\n",
      "Step - 7776, Loss - 0.2939269827322056, Learning Rate - 0.000390625, magnitude of gradient - 0.0401587567791947\n",
      "Step - 7777, Loss - 0.3118401480120663, Learning Rate - 0.000390625, magnitude of gradient - 0.022273141162449155\n",
      "Step - 7778, Loss - 0.32437657439807677, Learning Rate - 0.000390625, magnitude of gradient - 0.06682407865766002\n",
      "Step - 7779, Loss - 0.2818194777596436, Learning Rate - 0.000390625, magnitude of gradient - 0.019931998380804238\n",
      "Step - 7780, Loss - 0.2633711165670391, Learning Rate - 0.000390625, magnitude of gradient - 0.024775632750226626\n",
      "Step - 7781, Loss - 0.36072705054836063, Learning Rate - 0.000390625, magnitude of gradient - 0.04871331212845688\n",
      "Step - 7782, Loss - 0.2980606493738566, Learning Rate - 0.000390625, magnitude of gradient - 0.0800484534862515\n",
      "Step - 7783, Loss - 0.32641545747491457, Learning Rate - 0.000390625, magnitude of gradient - 0.03779424616281704\n",
      "Step - 7784, Loss - 0.33028500864094346, Learning Rate - 0.000390625, magnitude of gradient - 0.04249107049751093\n",
      "Step - 7785, Loss - 0.3256918411336184, Learning Rate - 0.000390625, magnitude of gradient - 0.03300042713329977\n",
      "Step - 7786, Loss - 0.33832307454800226, Learning Rate - 0.000390625, magnitude of gradient - 0.02118991729057695\n",
      "Step - 7787, Loss - 0.3052701025943063, Learning Rate - 0.000390625, magnitude of gradient - 0.04165638223356269\n",
      "Step - 7788, Loss - 0.2836330125243893, Learning Rate - 0.000390625, magnitude of gradient - 0.044009221477399336\n",
      "Step - 7789, Loss - 0.2795965026475865, Learning Rate - 0.000390625, magnitude of gradient - 0.06147265669843555\n",
      "Step - 7790, Loss - 0.38782560061977067, Learning Rate - 0.000390625, magnitude of gradient - 0.05249909820933585\n",
      "Step - 7791, Loss - 0.3184809227718942, Learning Rate - 0.000390625, magnitude of gradient - 0.04414525514152369\n",
      "Step - 7792, Loss - 0.35770412608120594, Learning Rate - 0.000390625, magnitude of gradient - 0.030634884554430523\n",
      "Step - 7793, Loss - 0.3271362233225493, Learning Rate - 0.000390625, magnitude of gradient - 0.061258903422768185\n",
      "Step - 7794, Loss - 0.34638001604159174, Learning Rate - 0.000390625, magnitude of gradient - 0.10746699564529012\n",
      "Step - 7795, Loss - 0.3169079200681926, Learning Rate - 0.000390625, magnitude of gradient - 0.02682601862547815\n",
      "Step - 7796, Loss - 0.39742331693256694, Learning Rate - 0.000390625, magnitude of gradient - 0.049244713494734686\n",
      "Step - 7797, Loss - 0.33862186398341926, Learning Rate - 0.000390625, magnitude of gradient - 0.02155989913432313\n",
      "Step - 7798, Loss - 0.3927066558725677, Learning Rate - 0.000390625, magnitude of gradient - 0.05953050058005391\n",
      "Step - 7799, Loss - 0.35461127907394013, Learning Rate - 0.000390625, magnitude of gradient - 0.0291655683900247\n",
      "Step - 7800, Loss - 0.27131591255093096, Learning Rate - 0.000390625, magnitude of gradient - 0.013093927178192888\n",
      "Step - 7801, Loss - 0.3076986924865287, Learning Rate - 0.000390625, magnitude of gradient - 0.040627417230444614\n",
      "Step - 7802, Loss - 0.3500343728165222, Learning Rate - 0.000390625, magnitude of gradient - 0.060308637942842025\n",
      "Step - 7803, Loss - 0.3431496973589084, Learning Rate - 0.000390625, magnitude of gradient - 0.10207047834098021\n",
      "Step - 7804, Loss - 0.34683010320364305, Learning Rate - 0.000390625, magnitude of gradient - 0.020660030146517253\n",
      "Step - 7805, Loss - 0.35405038983406123, Learning Rate - 0.000390625, magnitude of gradient - 0.024240238161351613\n",
      "Step - 7806, Loss - 0.3425037399129589, Learning Rate - 0.000390625, magnitude of gradient - 0.07066050855860967\n",
      "Step - 7807, Loss - 0.3687165301375443, Learning Rate - 0.000390625, magnitude of gradient - 0.033488367640510996\n",
      "Step - 7808, Loss - 0.239033035038453, Learning Rate - 0.000390625, magnitude of gradient - 0.04853616920518156\n",
      "Step - 7809, Loss - 0.3449156042758501, Learning Rate - 0.000390625, magnitude of gradient - 0.030790631606153206\n",
      "Step - 7810, Loss - 0.325394880961579, Learning Rate - 0.000390625, magnitude of gradient - 0.017507313380555264\n",
      "Step - 7811, Loss - 0.28948229684671645, Learning Rate - 0.000390625, magnitude of gradient - 0.004422690099998541\n",
      "Step - 7812, Loss - 0.23702577774700773, Learning Rate - 0.000390625, magnitude of gradient - 0.020622538072932524\n",
      "Step - 7813, Loss - 0.28765928909727023, Learning Rate - 0.000390625, magnitude of gradient - 0.032046413922248274\n",
      "Step - 7814, Loss - 0.3980300171873638, Learning Rate - 0.000390625, magnitude of gradient - 0.03011573103996134\n",
      "Step - 7815, Loss - 0.35623119216489463, Learning Rate - 0.000390625, magnitude of gradient - 0.06035628602643323\n",
      "Step - 7816, Loss - 0.3032731202694844, Learning Rate - 0.000390625, magnitude of gradient - 0.09625413490758503\n",
      "Step - 7817, Loss - 0.3309132491695724, Learning Rate - 0.000390625, magnitude of gradient - 0.013402792828353181\n",
      "Step - 7818, Loss - 0.27594485693570836, Learning Rate - 0.000390625, magnitude of gradient - 0.0572180112281773\n",
      "Step - 7819, Loss - 0.33548803736031696, Learning Rate - 0.000390625, magnitude of gradient - 0.07451866922466172\n",
      "Step - 7820, Loss - 0.2575921862881763, Learning Rate - 0.000390625, magnitude of gradient - 0.02314735382689642\n",
      "Step - 7821, Loss - 0.3696215811990629, Learning Rate - 0.000390625, magnitude of gradient - 0.10043262277456105\n",
      "Step - 7822, Loss - 0.26095961386425837, Learning Rate - 0.000390625, magnitude of gradient - 0.004927597538583085\n",
      "Step - 7823, Loss - 0.3089549560693246, Learning Rate - 0.000390625, magnitude of gradient - 0.03692911185136876\n",
      "Step - 7824, Loss - 0.3519687594181254, Learning Rate - 0.000390625, magnitude of gradient - 0.12643837653917636\n",
      "Step - 7825, Loss - 0.26982542942543547, Learning Rate - 0.000390625, magnitude of gradient - 0.03815746461938535\n",
      "Step - 7826, Loss - 0.28446563247562534, Learning Rate - 0.000390625, magnitude of gradient - 0.03525052457078084\n",
      "Step - 7827, Loss - 0.3838609224151201, Learning Rate - 0.000390625, magnitude of gradient - 0.03708314154262402\n",
      "Step - 7828, Loss - 0.43430277485741314, Learning Rate - 0.000390625, magnitude of gradient - 0.06719121283621315\n",
      "Step - 7829, Loss - 0.3805629899282147, Learning Rate - 0.000390625, magnitude of gradient - 0.07352339136464518\n",
      "Step - 7830, Loss - 0.3226975013700957, Learning Rate - 0.000390625, magnitude of gradient - 0.017188830429757274\n",
      "Step - 7831, Loss - 0.2941776627999212, Learning Rate - 0.000390625, magnitude of gradient - 0.06974271358921745\n",
      "Step - 7832, Loss - 0.3502765092845226, Learning Rate - 0.000390625, magnitude of gradient - 0.07505053568085558\n",
      "Step - 7833, Loss - 0.29583393203084096, Learning Rate - 0.000390625, magnitude of gradient - 0.02897938682750731\n",
      "Step - 7834, Loss - 0.3078097804656017, Learning Rate - 0.000390625, magnitude of gradient - 0.025174507376128842\n",
      "Step - 7835, Loss - 0.3201343505261067, Learning Rate - 0.000390625, magnitude of gradient - 0.04448707706526966\n",
      "Step - 7836, Loss - 0.24944386938980184, Learning Rate - 0.000390625, magnitude of gradient - 0.0435330990567635\n",
      "Step - 7837, Loss - 0.32513832376536445, Learning Rate - 0.000390625, magnitude of gradient - 0.062066762649350134\n",
      "Step - 7838, Loss - 0.357612596052566, Learning Rate - 0.000390625, magnitude of gradient - 0.05360167262028243\n",
      "Step - 7839, Loss - 0.2886262552636776, Learning Rate - 0.000390625, magnitude of gradient - 0.09334883000662775\n",
      "Step - 7840, Loss - 0.2983898385932511, Learning Rate - 0.000390625, magnitude of gradient - 0.06859502180891183\n",
      "Step - 7841, Loss - 0.25937634611024274, Learning Rate - 0.000390625, magnitude of gradient - 0.07659361868663969\n",
      "Step - 7842, Loss - 0.3195983436249268, Learning Rate - 0.000390625, magnitude of gradient - 0.04087686334941688\n",
      "Step - 7843, Loss - 0.3456763394817573, Learning Rate - 0.000390625, magnitude of gradient - 0.07425329976771679\n",
      "Step - 7844, Loss - 0.2663416459047196, Learning Rate - 0.000390625, magnitude of gradient - 0.03958106321916319\n",
      "Step - 7845, Loss - 0.3606062989006923, Learning Rate - 0.000390625, magnitude of gradient - 0.07901441055120909\n",
      "Step - 7846, Loss - 0.3745393828378608, Learning Rate - 0.000390625, magnitude of gradient - 0.06177052739734672\n",
      "Step - 7847, Loss - 0.3048354956181304, Learning Rate - 0.000390625, magnitude of gradient - 0.016722161459510262\n",
      "Step - 7848, Loss - 0.27704990370933424, Learning Rate - 0.000390625, magnitude of gradient - 0.08107079199751241\n",
      "Step - 7849, Loss - 0.3945291688980277, Learning Rate - 0.000390625, magnitude of gradient - 0.10778122505202876\n",
      "Step - 7850, Loss - 0.2784468272624619, Learning Rate - 0.000390625, magnitude of gradient - 0.03373141685749651\n",
      "Step - 7851, Loss - 0.24922583650112623, Learning Rate - 0.000390625, magnitude of gradient - 0.018815183541545422\n",
      "Step - 7852, Loss - 0.3297091862788006, Learning Rate - 0.000390625, magnitude of gradient - 0.04828052901682141\n",
      "Step - 7853, Loss - 0.29211370261092084, Learning Rate - 0.000390625, magnitude of gradient - 0.021572963539846456\n",
      "Step - 7854, Loss - 0.2673976210795593, Learning Rate - 0.000390625, magnitude of gradient - 0.11609929887842171\n",
      "Step - 7855, Loss - 0.34925138629883423, Learning Rate - 0.000390625, magnitude of gradient - 0.048420114328552775\n",
      "Step - 7856, Loss - 0.3545388450398216, Learning Rate - 0.000390625, magnitude of gradient - 0.031112496946746226\n",
      "Step - 7857, Loss - 0.4440083095394217, Learning Rate - 0.000390625, magnitude of gradient - 0.08079458757578141\n",
      "Step - 7858, Loss - 0.2834413168543296, Learning Rate - 0.000390625, magnitude of gradient - 0.05400506779563999\n",
      "Step - 7859, Loss - 0.30268601247040294, Learning Rate - 0.000390625, magnitude of gradient - 0.09233740016258613\n",
      "Step - 7860, Loss - 0.31805709741197385, Learning Rate - 0.000390625, magnitude of gradient - 0.04124535575560726\n",
      "Step - 7861, Loss - 0.284384627672624, Learning Rate - 0.000390625, magnitude of gradient - 0.047961720722564605\n",
      "Step - 7862, Loss - 0.30894154728769485, Learning Rate - 0.000390625, magnitude of gradient - 0.019965097636909346\n",
      "Step - 7863, Loss - 0.33190901270204953, Learning Rate - 0.000390625, magnitude of gradient - 0.0241836282925046\n",
      "Step - 7864, Loss - 0.3710643077167465, Learning Rate - 0.000390625, magnitude of gradient - 0.04599171464998404\n",
      "Step - 7865, Loss - 0.29245738621010386, Learning Rate - 0.000390625, magnitude of gradient - 0.033244991455058916\n",
      "Step - 7866, Loss - 0.25786178068041143, Learning Rate - 0.000390625, magnitude of gradient - 0.03599747661448209\n",
      "Step - 7867, Loss - 0.2002452717227913, Learning Rate - 0.000390625, magnitude of gradient - 0.05657808585229663\n",
      "Step - 7868, Loss - 0.33700958823802946, Learning Rate - 0.000390625, magnitude of gradient - 0.038522764792791576\n",
      "Step - 7869, Loss - 0.33099606823906735, Learning Rate - 0.000390625, magnitude of gradient - 0.04744324423639885\n",
      "Step - 7870, Loss - 0.3076083952230263, Learning Rate - 0.000390625, magnitude of gradient - 0.06956005117042614\n",
      "Step - 7871, Loss - 0.3292234542644604, Learning Rate - 0.000390625, magnitude of gradient - 0.02178991283361832\n",
      "Step - 7872, Loss - 0.29233652641901153, Learning Rate - 0.000390625, magnitude of gradient - 0.024397991735631126\n",
      "Step - 7873, Loss - 0.26537497187907244, Learning Rate - 0.000390625, magnitude of gradient - 0.08095053502688815\n",
      "Step - 7874, Loss - 0.31322987108306977, Learning Rate - 0.000390625, magnitude of gradient - 0.08676834486343088\n",
      "Step - 7875, Loss - 0.36023738875556305, Learning Rate - 0.000390625, magnitude of gradient - 0.037022647093302365\n",
      "Step - 7876, Loss - 0.39744896752379516, Learning Rate - 0.000390625, magnitude of gradient - 0.09284092927974505\n",
      "Step - 7877, Loss - 0.32190099217182433, Learning Rate - 0.000390625, magnitude of gradient - 0.09918733249581976\n",
      "Step - 7878, Loss - 0.3161208355243201, Learning Rate - 0.000390625, magnitude of gradient - 0.02827072749066355\n",
      "Step - 7879, Loss - 0.31879109320819254, Learning Rate - 0.000390625, magnitude of gradient - 0.04719628503232627\n",
      "Step - 7880, Loss - 0.31442893217361917, Learning Rate - 0.000390625, magnitude of gradient - 0.111811642654629\n",
      "Step - 7881, Loss - 0.2895024084231255, Learning Rate - 0.000390625, magnitude of gradient - 0.04782904658559389\n",
      "Step - 7882, Loss - 0.3611072855212992, Learning Rate - 0.000390625, magnitude of gradient - 0.07780580017754567\n",
      "Step - 7883, Loss - 0.28363237014833576, Learning Rate - 0.000390625, magnitude of gradient - 0.026293360706858844\n",
      "Step - 7884, Loss - 0.2935317239520047, Learning Rate - 0.000390625, magnitude of gradient - 0.07235678882508079\n",
      "Step - 7885, Loss - 0.28924846436906887, Learning Rate - 0.000390625, magnitude of gradient - 0.0558961804826362\n",
      "Step - 7886, Loss - 0.2676868327882769, Learning Rate - 0.000390625, magnitude of gradient - 0.05284960912932092\n",
      "Step - 7887, Loss - 0.279779198566962, Learning Rate - 0.000390625, magnitude of gradient - 0.11264555539452135\n",
      "Step - 7888, Loss - 0.32624209796120884, Learning Rate - 0.000390625, magnitude of gradient - 0.05122952986206904\n",
      "Step - 7889, Loss - 0.3188238310298239, Learning Rate - 0.000390625, magnitude of gradient - 0.06693328215406154\n",
      "Step - 7890, Loss - 0.3222428766276306, Learning Rate - 0.000390625, magnitude of gradient - 0.08444965891418357\n",
      "Step - 7891, Loss - 0.2807619227474297, Learning Rate - 0.000390625, magnitude of gradient - 0.0588844090221713\n",
      "Step - 7892, Loss - 0.3251778817879161, Learning Rate - 0.000390625, magnitude of gradient - 0.028630600522565882\n",
      "Step - 7893, Loss - 0.32447381746013515, Learning Rate - 0.000390625, magnitude of gradient - 0.011268616532233782\n",
      "Step - 7894, Loss - 0.30009141036821935, Learning Rate - 0.000390625, magnitude of gradient - 0.06904975708508884\n",
      "Step - 7895, Loss - 0.2919216944520101, Learning Rate - 0.000390625, magnitude of gradient - 0.06195746586747961\n",
      "Step - 7896, Loss - 0.30029942777666563, Learning Rate - 0.000390625, magnitude of gradient - 0.07843110411647748\n",
      "Step - 7897, Loss - 0.3290111944729389, Learning Rate - 0.000390625, magnitude of gradient - 0.01869912772616224\n",
      "Step - 7898, Loss - 0.28137644314145965, Learning Rate - 0.000390625, magnitude of gradient - 0.03867816613272528\n",
      "Step - 7899, Loss - 0.2977351723031128, Learning Rate - 0.000390625, magnitude of gradient - 0.07761169998858822\n",
      "Step - 7900, Loss - 0.3494509093079504, Learning Rate - 0.000390625, magnitude of gradient - 0.037470266470618685\n",
      "Step - 7901, Loss - 0.33638707911120636, Learning Rate - 0.000390625, magnitude of gradient - 0.04849937572220051\n",
      "Step - 7902, Loss - 0.37726150474744885, Learning Rate - 0.000390625, magnitude of gradient - 0.07183473290758768\n",
      "Step - 7903, Loss - 0.4033466548105674, Learning Rate - 0.000390625, magnitude of gradient - 0.10728517797655358\n",
      "Step - 7904, Loss - 0.3438521352414346, Learning Rate - 0.000390625, magnitude of gradient - 0.018221463143884908\n",
      "Step - 7905, Loss - 0.2957011950622471, Learning Rate - 0.000390625, magnitude of gradient - 0.028090112191732998\n",
      "Step - 7906, Loss - 0.37203113311355623, Learning Rate - 0.000390625, magnitude of gradient - 0.022360637221796972\n",
      "Step - 7907, Loss - 0.2593326649129796, Learning Rate - 0.000390625, magnitude of gradient - 0.03557430784120702\n",
      "Step - 7908, Loss - 0.3154725991431887, Learning Rate - 0.000390625, magnitude of gradient - 0.09913358941310232\n",
      "Step - 7909, Loss - 0.3255327794276941, Learning Rate - 0.000390625, magnitude of gradient - 0.08664545312305513\n",
      "Step - 7910, Loss - 0.3631623633684427, Learning Rate - 0.000390625, magnitude of gradient - 0.01919462262193587\n",
      "Step - 7911, Loss - 0.29779271824377557, Learning Rate - 0.000390625, magnitude of gradient - 0.016525537853745057\n",
      "Step - 7912, Loss - 0.3563386485741735, Learning Rate - 0.000390625, magnitude of gradient - 0.02258095852264177\n",
      "Step - 7913, Loss - 0.3291139936100994, Learning Rate - 0.000390625, magnitude of gradient - 0.021407358585352684\n",
      "Step - 7914, Loss - 0.3582027827546891, Learning Rate - 0.000390625, magnitude of gradient - 0.053605812455323976\n",
      "Step - 7915, Loss - 0.2849582216267681, Learning Rate - 0.000390625, magnitude of gradient - 0.036826679743346594\n",
      "Step - 7916, Loss - 0.24309959363143574, Learning Rate - 0.000390625, magnitude of gradient - 0.0253946447047604\n",
      "Step - 7917, Loss - 0.22829028416904185, Learning Rate - 0.000390625, magnitude of gradient - 0.026693196369479415\n",
      "Step - 7918, Loss - 0.3439171988229808, Learning Rate - 0.000390625, magnitude of gradient - 0.02658781133700143\n",
      "Step - 7919, Loss - 0.3513439320996708, Learning Rate - 0.000390625, magnitude of gradient - 0.02259623263167753\n",
      "Step - 7920, Loss - 0.3332740164375264, Learning Rate - 0.000390625, magnitude of gradient - 0.025474240556831753\n",
      "Step - 7921, Loss - 0.2949636246768937, Learning Rate - 0.000390625, magnitude of gradient - 0.038546301212857084\n",
      "Step - 7922, Loss - 0.2745129277872509, Learning Rate - 0.000390625, magnitude of gradient - 0.037972022203677604\n",
      "Step - 7923, Loss - 0.25503961208100456, Learning Rate - 0.000390625, magnitude of gradient - 0.10308622348928116\n",
      "Step - 7924, Loss - 0.2919192781436968, Learning Rate - 0.000390625, magnitude of gradient - 0.028998679799917465\n",
      "Step - 7925, Loss - 0.3112980652843219, Learning Rate - 0.000390625, magnitude of gradient - 0.02320009087871634\n",
      "Step - 7926, Loss - 0.3285039706257643, Learning Rate - 0.000390625, magnitude of gradient - 0.07455555687164439\n",
      "Step - 7927, Loss - 0.324687137342226, Learning Rate - 0.000390625, magnitude of gradient - 0.06322029813484593\n",
      "Step - 7928, Loss - 0.3355349761385402, Learning Rate - 0.000390625, magnitude of gradient - 0.06816036555490296\n",
      "Step - 7929, Loss - 0.32488795700101003, Learning Rate - 0.000390625, magnitude of gradient - 0.0333514125420855\n",
      "Step - 7930, Loss - 0.3771680898656541, Learning Rate - 0.000390625, magnitude of gradient - 0.010176574725713396\n",
      "Step - 7931, Loss - 0.35487650330781817, Learning Rate - 0.000390625, magnitude of gradient - 0.03422945652447454\n",
      "Step - 7932, Loss - 0.3023480453336087, Learning Rate - 0.000390625, magnitude of gradient - 0.009869395576762292\n",
      "Step - 7933, Loss - 0.2450306827013311, Learning Rate - 0.000390625, magnitude of gradient - 0.06812307889467886\n",
      "Step - 7934, Loss - 0.27323624931179996, Learning Rate - 0.000390625, magnitude of gradient - 0.047978062350574474\n",
      "Step - 7935, Loss - 0.22476903800195552, Learning Rate - 0.000390625, magnitude of gradient - 0.06419179879685326\n",
      "Step - 7936, Loss - 0.35071839971604735, Learning Rate - 0.000390625, magnitude of gradient - 0.05308175849287007\n",
      "Step - 7937, Loss - 0.34114981603877487, Learning Rate - 0.000390625, magnitude of gradient - 0.028575499559189112\n",
      "Step - 7938, Loss - 0.3058769204479365, Learning Rate - 0.000390625, magnitude of gradient - 0.06903303865158325\n",
      "Step - 7939, Loss - 0.3172122558988574, Learning Rate - 0.000390625, magnitude of gradient - 0.026629742995023607\n",
      "Step - 7940, Loss - 0.33937170699907265, Learning Rate - 0.000390625, magnitude of gradient - 0.07203497891528511\n",
      "Step - 7941, Loss - 0.29556059844423166, Learning Rate - 0.000390625, magnitude of gradient - 0.020941051702838376\n",
      "Step - 7942, Loss - 0.27384962435883414, Learning Rate - 0.000390625, magnitude of gradient - 0.01982526609773273\n",
      "Step - 7943, Loss - 0.3638326935523299, Learning Rate - 0.000390625, magnitude of gradient - 0.04603732027425079\n",
      "Step - 7944, Loss - 0.21693407808777387, Learning Rate - 0.000390625, magnitude of gradient - 0.02027506242003901\n",
      "Step - 7945, Loss - 0.31001980393264994, Learning Rate - 0.000390625, magnitude of gradient - 0.07901652784756658\n",
      "Step - 7946, Loss - 0.31429083936175145, Learning Rate - 0.000390625, magnitude of gradient - 0.08092089090974904\n",
      "Step - 7947, Loss - 0.28441811604657985, Learning Rate - 0.000390625, magnitude of gradient - 0.06174014044881682\n",
      "Step - 7948, Loss - 0.25213451635267425, Learning Rate - 0.000390625, magnitude of gradient - 0.028997252363957244\n",
      "Step - 7949, Loss - 0.3385971898710039, Learning Rate - 0.000390625, magnitude of gradient - 0.027249848867590146\n",
      "Step - 7950, Loss - 0.310334334983991, Learning Rate - 0.000390625, magnitude of gradient - 0.041031590466509255\n",
      "Step - 7951, Loss - 0.27744597735370163, Learning Rate - 0.000390625, magnitude of gradient - 0.07232747986936709\n",
      "Step - 7952, Loss - 0.3618039290609522, Learning Rate - 0.000390625, magnitude of gradient - 0.05540270599272961\n",
      "Step - 7953, Loss - 0.27801818616801743, Learning Rate - 0.000390625, magnitude of gradient - 0.0461231332114575\n",
      "Step - 7954, Loss - 0.37976246862097246, Learning Rate - 0.000390625, magnitude of gradient - 0.006879960078031454\n",
      "Step - 7955, Loss - 0.2757989744451777, Learning Rate - 0.000390625, magnitude of gradient - 0.031147964784182268\n",
      "Step - 7956, Loss - 0.27093136905613807, Learning Rate - 0.000390625, magnitude of gradient - 0.06308375914042795\n",
      "Step - 7957, Loss - 0.3441260879528871, Learning Rate - 0.000390625, magnitude of gradient - 0.01073206475151812\n",
      "Step - 7958, Loss - 0.329041785678734, Learning Rate - 0.000390625, magnitude of gradient - 0.07054430562211608\n",
      "Step - 7959, Loss - 0.35855301415407254, Learning Rate - 0.000390625, magnitude of gradient - 0.03471864944104123\n",
      "Step - 7960, Loss - 0.3213574047478204, Learning Rate - 0.000390625, magnitude of gradient - 0.013716445102723395\n",
      "Step - 7961, Loss - 0.3537655903144439, Learning Rate - 0.000390625, magnitude of gradient - 0.05393296079603834\n",
      "Step - 7962, Loss - 0.280706446247086, Learning Rate - 0.000390625, magnitude of gradient - 0.10677631642554965\n",
      "Step - 7963, Loss - 0.3372509708209714, Learning Rate - 0.000390625, magnitude of gradient - 0.04153313767718769\n",
      "Step - 7964, Loss - 0.31756920488803625, Learning Rate - 0.000390625, magnitude of gradient - 0.11448694683288259\n",
      "Step - 7965, Loss - 0.26901390627188304, Learning Rate - 0.000390625, magnitude of gradient - 0.1104072063458227\n",
      "Step - 7966, Loss - 0.3220443465616012, Learning Rate - 0.000390625, magnitude of gradient - 0.033724883237753835\n",
      "Step - 7967, Loss - 0.3088884700921109, Learning Rate - 0.000390625, magnitude of gradient - 0.040329297608768876\n",
      "Step - 7968, Loss - 0.3161879108248773, Learning Rate - 0.000390625, magnitude of gradient - 0.02334405362938962\n",
      "Step - 7969, Loss - 0.335731088485731, Learning Rate - 0.000390625, magnitude of gradient - 0.09547692117583836\n",
      "Step - 7970, Loss - 0.2894250850888188, Learning Rate - 0.000390625, magnitude of gradient - 0.06422047855173452\n",
      "Step - 7971, Loss - 0.3443093247697664, Learning Rate - 0.000390625, magnitude of gradient - 0.04833278011328714\n",
      "Step - 7972, Loss - 0.30548623884296777, Learning Rate - 0.000390625, magnitude of gradient - 0.13114096898188538\n",
      "Step - 7973, Loss - 0.29487136770873257, Learning Rate - 0.000390625, magnitude of gradient - 0.035306729321069305\n",
      "Step - 7974, Loss - 0.38061573762593764, Learning Rate - 0.000390625, magnitude of gradient - 0.05724259684607026\n",
      "Step - 7975, Loss - 0.3287643437689164, Learning Rate - 0.000390625, magnitude of gradient - 0.039977193181524054\n",
      "Step - 7976, Loss - 0.3316027606745421, Learning Rate - 0.000390625, magnitude of gradient - 0.09881520451001952\n",
      "Step - 7977, Loss - 0.3198616820593255, Learning Rate - 0.000390625, magnitude of gradient - 0.021191180921566837\n",
      "Step - 7978, Loss - 0.2931726238628702, Learning Rate - 0.000390625, magnitude of gradient - 0.05168113613755512\n",
      "Step - 7979, Loss - 0.43060496462472253, Learning Rate - 0.000390625, magnitude of gradient - 0.06373373326933289\n",
      "Step - 7980, Loss - 0.2582610941254946, Learning Rate - 0.000390625, magnitude of gradient - 0.07661016981342667\n",
      "Step - 7981, Loss - 0.34015351730464155, Learning Rate - 0.000390625, magnitude of gradient - 0.04057106475382565\n",
      "Step - 7982, Loss - 0.2972479886119246, Learning Rate - 0.000390625, magnitude of gradient - 0.030025315483994678\n",
      "Step - 7983, Loss - 0.359712036516917, Learning Rate - 0.000390625, magnitude of gradient - 0.03441814593165842\n",
      "Step - 7984, Loss - 0.3215438342484521, Learning Rate - 0.000390625, magnitude of gradient - 0.036287727570111734\n",
      "Step - 7985, Loss - 0.3466884718485755, Learning Rate - 0.000390625, magnitude of gradient - 0.019242170216910912\n",
      "Step - 7986, Loss - 0.3634350680691286, Learning Rate - 0.000390625, magnitude of gradient - 0.09166367706351743\n",
      "Step - 7987, Loss - 0.3785177165676735, Learning Rate - 0.000390625, magnitude of gradient - 0.043314552169296625\n",
      "Step - 7988, Loss - 0.27915142061398773, Learning Rate - 0.000390625, magnitude of gradient - 0.051957924252074864\n",
      "Step - 7989, Loss - 0.26226361861365166, Learning Rate - 0.000390625, magnitude of gradient - 0.02778030269512989\n",
      "Step - 7990, Loss - 0.30202779482902786, Learning Rate - 0.000390625, magnitude of gradient - 0.029229321362508727\n",
      "Step - 7991, Loss - 0.3064013277051296, Learning Rate - 0.000390625, magnitude of gradient - 0.028182994571638326\n",
      "Step - 7992, Loss - 0.3466092341227651, Learning Rate - 0.000390625, magnitude of gradient - 0.03266363376520052\n",
      "Step - 7993, Loss - 0.33617858458091116, Learning Rate - 0.000390625, magnitude of gradient - 0.06273917574261184\n",
      "Step - 7994, Loss - 0.3244418804777104, Learning Rate - 0.000390625, magnitude of gradient - 0.028243202852075582\n",
      "Step - 7995, Loss - 0.3702930213856239, Learning Rate - 0.000390625, magnitude of gradient - 0.04578274662906638\n",
      "Step - 7996, Loss - 0.35563761494098006, Learning Rate - 0.000390625, magnitude of gradient - 0.027421447252578144\n",
      "Step - 7997, Loss - 0.2357898250545023, Learning Rate - 0.000390625, magnitude of gradient - 0.02123463072446035\n",
      "Step - 7998, Loss - 0.38418750148233505, Learning Rate - 0.000390625, magnitude of gradient - 0.019793280238506912\n",
      "Step - 7999, Loss - 0.3868296779307406, Learning Rate - 0.000390625, magnitude of gradient - 0.024072713454581278\n",
      "Step - 8000, Loss - 0.3295232607050881, Learning Rate - 0.000390625, magnitude of gradient - 0.014301431737127935\n",
      "Step - 8001, Loss - 0.31122754888985493, Learning Rate - 0.0001953125, magnitude of gradient - 0.04872013547103651\n",
      "Step - 8002, Loss - 0.3501801092253837, Learning Rate - 0.0001953125, magnitude of gradient - 0.023971550861776224\n",
      "Step - 8003, Loss - 0.27032160342839173, Learning Rate - 0.0001953125, magnitude of gradient - 0.07136235541730024\n",
      "Step - 8004, Loss - 0.3298041538104861, Learning Rate - 0.0001953125, magnitude of gradient - 0.03661797777043128\n",
      "Step - 8005, Loss - 0.3340646667003023, Learning Rate - 0.0001953125, magnitude of gradient - 0.0721573043044308\n",
      "Step - 8006, Loss - 0.312858022413915, Learning Rate - 0.0001953125, magnitude of gradient - 0.04602453909202955\n",
      "Step - 8007, Loss - 0.32467831251421997, Learning Rate - 0.0001953125, magnitude of gradient - 0.07068463484612574\n",
      "Step - 8008, Loss - 0.32799512903964045, Learning Rate - 0.0001953125, magnitude of gradient - 0.026073137464463284\n",
      "Step - 8009, Loss - 0.3098393006072038, Learning Rate - 0.0001953125, magnitude of gradient - 0.026005280859973057\n",
      "Step - 8010, Loss - 0.319191117024357, Learning Rate - 0.0001953125, magnitude of gradient - 0.023070178997440658\n",
      "Step - 8011, Loss - 0.36311433354240247, Learning Rate - 0.0001953125, magnitude of gradient - 0.049726126809460335\n",
      "Step - 8012, Loss - 0.21607437062053098, Learning Rate - 0.0001953125, magnitude of gradient - 0.00560144839217994\n",
      "Step - 8013, Loss - 0.36520261231294593, Learning Rate - 0.0001953125, magnitude of gradient - 0.043122031186600414\n",
      "Step - 8014, Loss - 0.2991134342246674, Learning Rate - 0.0001953125, magnitude of gradient - 0.03730433350788237\n",
      "Step - 8015, Loss - 0.3088773680735434, Learning Rate - 0.0001953125, magnitude of gradient - 0.05777144614226924\n",
      "Step - 8016, Loss - 0.31185136034548117, Learning Rate - 0.0001953125, magnitude of gradient - 0.0961890811463523\n",
      "Step - 8017, Loss - 0.357756503880362, Learning Rate - 0.0001953125, magnitude of gradient - 0.08664392806233918\n",
      "Step - 8018, Loss - 0.3946588863448728, Learning Rate - 0.0001953125, magnitude of gradient - 0.04892259260523777\n",
      "Step - 8019, Loss - 0.35990590440083736, Learning Rate - 0.0001953125, magnitude of gradient - 0.037988571231025094\n",
      "Step - 8020, Loss - 0.3076755797596623, Learning Rate - 0.0001953125, magnitude of gradient - 0.06735549159037081\n",
      "Step - 8021, Loss - 0.3055171717282962, Learning Rate - 0.0001953125, magnitude of gradient - 0.04176315837485603\n",
      "Step - 8022, Loss - 0.38532627231126276, Learning Rate - 0.0001953125, magnitude of gradient - 0.03578983977424636\n",
      "Step - 8023, Loss - 0.3635408988500353, Learning Rate - 0.0001953125, magnitude of gradient - 0.01989916833558525\n",
      "Step - 8024, Loss - 0.2524889088424223, Learning Rate - 0.0001953125, magnitude of gradient - 0.04840900844144584\n",
      "Step - 8025, Loss - 0.3892399690096704, Learning Rate - 0.0001953125, magnitude of gradient - 0.09690172965325966\n",
      "Step - 8026, Loss - 0.2641226216896051, Learning Rate - 0.0001953125, magnitude of gradient - 0.04240866698536536\n",
      "Step - 8027, Loss - 0.29013585857334545, Learning Rate - 0.0001953125, magnitude of gradient - 0.06999409598771981\n",
      "Step - 8028, Loss - 0.2891404447796835, Learning Rate - 0.0001953125, magnitude of gradient - 0.011832025045536479\n",
      "Step - 8029, Loss - 0.3154652146701289, Learning Rate - 0.0001953125, magnitude of gradient - 0.034938772801689846\n",
      "Step - 8030, Loss - 0.33728962568564397, Learning Rate - 0.0001953125, magnitude of gradient - 0.07229604878760128\n",
      "Step - 8031, Loss - 0.29340203409353005, Learning Rate - 0.0001953125, magnitude of gradient - 0.01681079435222625\n",
      "Step - 8032, Loss - 0.30574582312725307, Learning Rate - 0.0001953125, magnitude of gradient - 0.034906481956777306\n",
      "Step - 8033, Loss - 0.3801802427167584, Learning Rate - 0.0001953125, magnitude of gradient - 0.05174814183785812\n",
      "Step - 8034, Loss - 0.3375137477449468, Learning Rate - 0.0001953125, magnitude of gradient - 0.06485414471424751\n",
      "Step - 8035, Loss - 0.25839384612094973, Learning Rate - 0.0001953125, magnitude of gradient - 0.04477506785121697\n",
      "Step - 8036, Loss - 0.3094739858471252, Learning Rate - 0.0001953125, magnitude of gradient - 0.02859591802489172\n",
      "Step - 8037, Loss - 0.2998060639704849, Learning Rate - 0.0001953125, magnitude of gradient - 0.05917893024484323\n",
      "Step - 8038, Loss - 0.3404621153705424, Learning Rate - 0.0001953125, magnitude of gradient - 0.0711579100225592\n",
      "Step - 8039, Loss - 0.37630430997494085, Learning Rate - 0.0001953125, magnitude of gradient - 0.06229505681329946\n",
      "Step - 8040, Loss - 0.37269861584927966, Learning Rate - 0.0001953125, magnitude of gradient - 0.07174740739002146\n",
      "Step - 8041, Loss - 0.3997599271301572, Learning Rate - 0.0001953125, magnitude of gradient - 0.12308092470298293\n",
      "Step - 8042, Loss - 0.38775789723515186, Learning Rate - 0.0001953125, magnitude of gradient - 0.036175478945668534\n",
      "Step - 8043, Loss - 0.2904018204410225, Learning Rate - 0.0001953125, magnitude of gradient - 0.019845284875886465\n",
      "Step - 8044, Loss - 0.2986441628856972, Learning Rate - 0.0001953125, magnitude of gradient - 0.0754321256261638\n",
      "Step - 8045, Loss - 0.36955412118707326, Learning Rate - 0.0001953125, magnitude of gradient - 0.09321778397943348\n",
      "Step - 8046, Loss - 0.3377635886530664, Learning Rate - 0.0001953125, magnitude of gradient - 0.06120541319841702\n",
      "Step - 8047, Loss - 0.3195378150463012, Learning Rate - 0.0001953125, magnitude of gradient - 0.05652330564037018\n",
      "Step - 8048, Loss - 0.3388011247065491, Learning Rate - 0.0001953125, magnitude of gradient - 0.08310641285039098\n",
      "Step - 8049, Loss - 0.37820408010905393, Learning Rate - 0.0001953125, magnitude of gradient - 0.016311179724848995\n",
      "Step - 8050, Loss - 0.36458377985864593, Learning Rate - 0.0001953125, magnitude of gradient - 0.06635634651240999\n",
      "Step - 8051, Loss - 0.303531114952001, Learning Rate - 0.0001953125, magnitude of gradient - 0.00864601365808848\n",
      "Step - 8052, Loss - 0.29853722672656224, Learning Rate - 0.0001953125, magnitude of gradient - 0.059716103074685924\n",
      "Step - 8053, Loss - 0.3639899773043028, Learning Rate - 0.0001953125, magnitude of gradient - 0.13493135697404507\n",
      "Step - 8054, Loss - 0.37678931479723066, Learning Rate - 0.0001953125, magnitude of gradient - 0.05386409446244204\n",
      "Step - 8055, Loss - 0.38086774924452865, Learning Rate - 0.0001953125, magnitude of gradient - 0.0804174579582366\n",
      "Step - 8056, Loss - 0.32239335534997104, Learning Rate - 0.0001953125, magnitude of gradient - 0.03228905144998075\n",
      "Step - 8057, Loss - 0.329669931749632, Learning Rate - 0.0001953125, magnitude of gradient - 0.02850561409529502\n",
      "Step - 8058, Loss - 0.3177657352596673, Learning Rate - 0.0001953125, magnitude of gradient - 0.038287872421320285\n",
      "Step - 8059, Loss - 0.34689955738396805, Learning Rate - 0.0001953125, magnitude of gradient - 0.049948960950674165\n",
      "Step - 8060, Loss - 0.25948931852905033, Learning Rate - 0.0001953125, magnitude of gradient - 0.07128175359784028\n",
      "Step - 8061, Loss - 0.3318057377193303, Learning Rate - 0.0001953125, magnitude of gradient - 0.10382763014201317\n",
      "Step - 8062, Loss - 0.4185086103569558, Learning Rate - 0.0001953125, magnitude of gradient - 0.07243147386146653\n",
      "Step - 8063, Loss - 0.26217719340977186, Learning Rate - 0.0001953125, magnitude of gradient - 0.01622002760638059\n",
      "Step - 8064, Loss - 0.30441917670445945, Learning Rate - 0.0001953125, magnitude of gradient - 0.01087200570766322\n",
      "Step - 8065, Loss - 0.20794492637647416, Learning Rate - 0.0001953125, magnitude of gradient - 0.04456869457561637\n",
      "Step - 8066, Loss - 0.3918328914676751, Learning Rate - 0.0001953125, magnitude of gradient - 0.0803316450185641\n",
      "Step - 8067, Loss - 0.3020019290255762, Learning Rate - 0.0001953125, magnitude of gradient - 0.06151119606515872\n",
      "Step - 8068, Loss - 0.28644351215118447, Learning Rate - 0.0001953125, magnitude of gradient - 0.010667233293092309\n",
      "Step - 8069, Loss - 0.3564832961867987, Learning Rate - 0.0001953125, magnitude of gradient - 0.042760441873787816\n",
      "Step - 8070, Loss - 0.3050469234471683, Learning Rate - 0.0001953125, magnitude of gradient - 0.0907273402412542\n",
      "Step - 8071, Loss - 0.294438631332682, Learning Rate - 0.0001953125, magnitude of gradient - 0.02652000977682066\n",
      "Step - 8072, Loss - 0.3145010560506696, Learning Rate - 0.0001953125, magnitude of gradient - 0.048891360979936974\n",
      "Step - 8073, Loss - 0.34417278215570796, Learning Rate - 0.0001953125, magnitude of gradient - 0.07113142649624278\n",
      "Step - 8074, Loss - 0.30153900435608905, Learning Rate - 0.0001953125, magnitude of gradient - 0.04712063659844681\n",
      "Step - 8075, Loss - 0.3560224117264629, Learning Rate - 0.0001953125, magnitude of gradient - 0.03093517579867178\n",
      "Step - 8076, Loss - 0.38645610171503936, Learning Rate - 0.0001953125, magnitude of gradient - 0.03803529725311618\n",
      "Step - 8077, Loss - 0.3440146854289049, Learning Rate - 0.0001953125, magnitude of gradient - 0.04847544547497021\n",
      "Step - 8078, Loss - 0.34242834481561557, Learning Rate - 0.0001953125, magnitude of gradient - 0.033955664954565284\n",
      "Step - 8079, Loss - 0.2609553829585908, Learning Rate - 0.0001953125, magnitude of gradient - 0.09758522577918989\n",
      "Step - 8080, Loss - 0.35375066261088967, Learning Rate - 0.0001953125, magnitude of gradient - 0.04800398132718773\n",
      "Step - 8081, Loss - 0.2739284852382098, Learning Rate - 0.0001953125, magnitude of gradient - 0.0104218447371895\n",
      "Step - 8082, Loss - 0.2983784744539616, Learning Rate - 0.0001953125, magnitude of gradient - 0.03300727186987131\n",
      "Step - 8083, Loss - 0.2685221679579908, Learning Rate - 0.0001953125, magnitude of gradient - 0.014300806780865209\n",
      "Step - 8084, Loss - 0.2886528216010145, Learning Rate - 0.0001953125, magnitude of gradient - 0.0354287044852083\n",
      "Step - 8085, Loss - 0.31430058651806314, Learning Rate - 0.0001953125, magnitude of gradient - 0.030625502972201634\n",
      "Step - 8086, Loss - 0.330192635499305, Learning Rate - 0.0001953125, magnitude of gradient - 0.06823647462689797\n",
      "Step - 8087, Loss - 0.30017875745168926, Learning Rate - 0.0001953125, magnitude of gradient - 0.027820269058554847\n",
      "Step - 8088, Loss - 0.3851691443151861, Learning Rate - 0.0001953125, magnitude of gradient - 0.03637322001575473\n",
      "Step - 8089, Loss - 0.2931115084624048, Learning Rate - 0.0001953125, magnitude of gradient - 0.04958868257319932\n",
      "Step - 8090, Loss - 0.3703719709463879, Learning Rate - 0.0001953125, magnitude of gradient - 0.0519409750083401\n",
      "Step - 8091, Loss - 0.2941198554459077, Learning Rate - 0.0001953125, magnitude of gradient - 0.07227177220320227\n",
      "Step - 8092, Loss - 0.3642922613155063, Learning Rate - 0.0001953125, magnitude of gradient - 0.030157570671902813\n",
      "Step - 8093, Loss - 0.256780846770665, Learning Rate - 0.0001953125, magnitude of gradient - 0.07148303569562409\n",
      "Step - 8094, Loss - 0.34772519204582386, Learning Rate - 0.0001953125, magnitude of gradient - 0.010347230736442057\n",
      "Step - 8095, Loss - 0.3051520020791758, Learning Rate - 0.0001953125, magnitude of gradient - 0.07449089803929236\n",
      "Step - 8096, Loss - 0.29458980261005785, Learning Rate - 0.0001953125, magnitude of gradient - 0.04694566654394178\n",
      "Step - 8097, Loss - 0.3796561791019015, Learning Rate - 0.0001953125, magnitude of gradient - 0.029623043570965138\n",
      "Step - 8098, Loss - 0.2385111971102178, Learning Rate - 0.0001953125, magnitude of gradient - 0.03291603971120008\n",
      "Step - 8099, Loss - 0.3613518538713423, Learning Rate - 0.0001953125, magnitude of gradient - 0.028055799789760732\n",
      "Step - 8100, Loss - 0.34738185850826975, Learning Rate - 0.0001953125, magnitude of gradient - 0.03984541492850644\n",
      "Step - 8101, Loss - 0.3097892417412823, Learning Rate - 0.0001953125, magnitude of gradient - 0.0420288803332744\n",
      "Step - 8102, Loss - 0.30924764088809364, Learning Rate - 0.0001953125, magnitude of gradient - 0.05027588524347525\n",
      "Step - 8103, Loss - 0.3797825419079072, Learning Rate - 0.0001953125, magnitude of gradient - 0.04984741506779334\n",
      "Step - 8104, Loss - 0.26471357814146745, Learning Rate - 0.0001953125, magnitude of gradient - 0.048524451132832845\n",
      "Step - 8105, Loss - 0.3459439083610309, Learning Rate - 0.0001953125, magnitude of gradient - 0.06306156033263147\n",
      "Step - 8106, Loss - 0.3105892339352234, Learning Rate - 0.0001953125, magnitude of gradient - 0.046559609606347224\n",
      "Step - 8107, Loss - 0.41371375529475374, Learning Rate - 0.0001953125, magnitude of gradient - 0.03741972372955181\n",
      "Step - 8108, Loss - 0.29036856230016056, Learning Rate - 0.0001953125, magnitude of gradient - 0.07478177673823633\n",
      "Step - 8109, Loss - 0.3751392865421777, Learning Rate - 0.0001953125, magnitude of gradient - 0.03648547450269814\n",
      "Step - 8110, Loss - 0.3110507531770475, Learning Rate - 0.0001953125, magnitude of gradient - 0.005766539421323851\n",
      "Step - 8111, Loss - 0.37781508687134124, Learning Rate - 0.0001953125, magnitude of gradient - 0.08323473687043283\n",
      "Step - 8112, Loss - 0.3509438679869209, Learning Rate - 0.0001953125, magnitude of gradient - 0.07372662051703358\n",
      "Step - 8113, Loss - 0.2861599448416956, Learning Rate - 0.0001953125, magnitude of gradient - 0.02890001322713372\n",
      "Step - 8114, Loss - 0.3626999027885321, Learning Rate - 0.0001953125, magnitude of gradient - 0.06546658853905628\n",
      "Step - 8115, Loss - 0.3090012148871213, Learning Rate - 0.0001953125, magnitude of gradient - 0.04341022142594492\n",
      "Step - 8116, Loss - 0.29074447779379897, Learning Rate - 0.0001953125, magnitude of gradient - 0.023180752214901845\n",
      "Step - 8117, Loss - 0.2565613642011543, Learning Rate - 0.0001953125, magnitude of gradient - 0.046881377173916224\n",
      "Step - 8118, Loss - 0.320136057360845, Learning Rate - 0.0001953125, magnitude of gradient - 0.020079607790232414\n",
      "Step - 8119, Loss - 0.309106311067175, Learning Rate - 0.0001953125, magnitude of gradient - 0.03670446686898214\n",
      "Step - 8120, Loss - 0.3345963432980054, Learning Rate - 0.0001953125, magnitude of gradient - 0.03463047356391138\n",
      "Step - 8121, Loss - 0.18653743893635838, Learning Rate - 0.0001953125, magnitude of gradient - 0.03564294843902838\n",
      "Step - 8122, Loss - 0.3737698882903631, Learning Rate - 0.0001953125, magnitude of gradient - 0.059539178493762904\n",
      "Step - 8123, Loss - 0.31965079410048136, Learning Rate - 0.0001953125, magnitude of gradient - 0.014794673815100513\n",
      "Step - 8124, Loss - 0.3110590068983944, Learning Rate - 0.0001953125, magnitude of gradient - 0.02504905802102888\n",
      "Step - 8125, Loss - 0.31677550204802374, Learning Rate - 0.0001953125, magnitude of gradient - 0.11108425139633085\n",
      "Step - 8126, Loss - 0.28517176395714033, Learning Rate - 0.0001953125, magnitude of gradient - 0.040058864449087156\n",
      "Step - 8127, Loss - 0.3660795178859797, Learning Rate - 0.0001953125, magnitude of gradient - 0.06744313113890468\n",
      "Step - 8128, Loss - 0.3586149900206306, Learning Rate - 0.0001953125, magnitude of gradient - 0.07399426521380004\n",
      "Step - 8129, Loss - 0.2905278078679118, Learning Rate - 0.0001953125, magnitude of gradient - 0.011962117734135783\n",
      "Step - 8130, Loss - 0.2969488328883481, Learning Rate - 0.0001953125, magnitude of gradient - 0.03269535534979104\n",
      "Step - 8131, Loss - 0.31198926026308105, Learning Rate - 0.0001953125, magnitude of gradient - 0.029300575912222845\n",
      "Step - 8132, Loss - 0.2918344036413267, Learning Rate - 0.0001953125, magnitude of gradient - 0.040451565015439894\n",
      "Step - 8133, Loss - 0.3105581168654642, Learning Rate - 0.0001953125, magnitude of gradient - 0.040925115627194024\n",
      "Step - 8134, Loss - 0.34816661822080136, Learning Rate - 0.0001953125, magnitude of gradient - 0.06666812215773843\n",
      "Step - 8135, Loss - 0.2921931579680494, Learning Rate - 0.0001953125, magnitude of gradient - 0.05136392621399743\n",
      "Step - 8136, Loss - 0.33209694661105515, Learning Rate - 0.0001953125, magnitude of gradient - 0.044658993036685565\n",
      "Step - 8137, Loss - 0.32362329275610935, Learning Rate - 0.0001953125, magnitude of gradient - 0.034425777606172006\n",
      "Step - 8138, Loss - 0.3278814268569754, Learning Rate - 0.0001953125, magnitude of gradient - 0.0909320597050816\n",
      "Step - 8139, Loss - 0.37726928971438933, Learning Rate - 0.0001953125, magnitude of gradient - 0.04292934447165133\n",
      "Step - 8140, Loss - 0.2598285096771803, Learning Rate - 0.0001953125, magnitude of gradient - 0.02031881868441624\n",
      "Step - 8141, Loss - 0.3907500554200171, Learning Rate - 0.0001953125, magnitude of gradient - 0.05435612175892655\n",
      "Step - 8142, Loss - 0.37265547469277427, Learning Rate - 0.0001953125, magnitude of gradient - 0.02568282900426967\n",
      "Step - 8143, Loss - 0.38759540482464055, Learning Rate - 0.0001953125, magnitude of gradient - 0.07765669391315398\n",
      "Step - 8144, Loss - 0.31506715960731935, Learning Rate - 0.0001953125, magnitude of gradient - 0.06798965393240244\n",
      "Step - 8145, Loss - 0.38835710332505263, Learning Rate - 0.0001953125, magnitude of gradient - 0.0822227040057764\n",
      "Step - 8146, Loss - 0.30147089945917327, Learning Rate - 0.0001953125, magnitude of gradient - 0.024412543186848126\n",
      "Step - 8147, Loss - 0.2644044829613934, Learning Rate - 0.0001953125, magnitude of gradient - 0.0410246516093289\n",
      "Step - 8148, Loss - 0.27880737532090283, Learning Rate - 0.0001953125, magnitude of gradient - 0.00987361177005655\n",
      "Step - 8149, Loss - 0.31149986533387714, Learning Rate - 0.0001953125, magnitude of gradient - 0.03983818746363737\n",
      "Step - 8150, Loss - 0.3159286690224012, Learning Rate - 0.0001953125, magnitude of gradient - 0.07022421451107981\n",
      "Step - 8151, Loss - 0.33024678163548693, Learning Rate - 0.0001953125, magnitude of gradient - 0.028279295784814026\n",
      "Step - 8152, Loss - 0.3389867127783182, Learning Rate - 0.0001953125, magnitude of gradient - 0.0655738945394873\n",
      "Step - 8153, Loss - 0.298509393270638, Learning Rate - 0.0001953125, magnitude of gradient - 0.030250121912065983\n",
      "Step - 8154, Loss - 0.3455677810801868, Learning Rate - 0.0001953125, magnitude of gradient - 0.045436796474803975\n",
      "Step - 8155, Loss - 0.3244248847079851, Learning Rate - 0.0001953125, magnitude of gradient - 0.09330841684631683\n",
      "Step - 8156, Loss - 0.29351768082018026, Learning Rate - 0.0001953125, magnitude of gradient - 0.03949638635820161\n",
      "Step - 8157, Loss - 0.3051042683334954, Learning Rate - 0.0001953125, magnitude of gradient - 0.08554426893736612\n",
      "Step - 8158, Loss - 0.2226055682344738, Learning Rate - 0.0001953125, magnitude of gradient - 0.03366573390606209\n",
      "Step - 8159, Loss - 0.41779100593842505, Learning Rate - 0.0001953125, magnitude of gradient - 0.07892885393369402\n",
      "Step - 8160, Loss - 0.2999353273560538, Learning Rate - 0.0001953125, magnitude of gradient - 0.019186211568644147\n",
      "Step - 8161, Loss - 0.3304806473857989, Learning Rate - 0.0001953125, magnitude of gradient - 0.01686507366795258\n",
      "Step - 8162, Loss - 0.3783040385724129, Learning Rate - 0.0001953125, magnitude of gradient - 0.06771702939576536\n",
      "Step - 8163, Loss - 0.3477503122125333, Learning Rate - 0.0001953125, magnitude of gradient - 0.020316952506701992\n",
      "Step - 8164, Loss - 0.29548786837486735, Learning Rate - 0.0001953125, magnitude of gradient - 0.07410685050170486\n",
      "Step - 8165, Loss - 0.32249857566260565, Learning Rate - 0.0001953125, magnitude of gradient - 0.02398554390483449\n",
      "Step - 8166, Loss - 0.22308323408099828, Learning Rate - 0.0001953125, magnitude of gradient - 0.04483569705755256\n",
      "Step - 8167, Loss - 0.34362722190873196, Learning Rate - 0.0001953125, magnitude of gradient - 0.05489421949263033\n",
      "Step - 8168, Loss - 0.3032429442327561, Learning Rate - 0.0001953125, magnitude of gradient - 0.05795874833390879\n",
      "Step - 8169, Loss - 0.3202304662810755, Learning Rate - 0.0001953125, magnitude of gradient - 0.023761182442483493\n",
      "Step - 8170, Loss - 0.2935380107852383, Learning Rate - 0.0001953125, magnitude of gradient - 0.08983064993970472\n",
      "Step - 8171, Loss - 0.3836155340458556, Learning Rate - 0.0001953125, magnitude of gradient - 0.010861847683810731\n",
      "Step - 8172, Loss - 0.3048317206962716, Learning Rate - 0.0001953125, magnitude of gradient - 0.07298940080734279\n",
      "Step - 8173, Loss - 0.3268675989864227, Learning Rate - 0.0001953125, magnitude of gradient - 0.046038611371331015\n",
      "Step - 8174, Loss - 0.32477378687470193, Learning Rate - 0.0001953125, magnitude of gradient - 0.06141866145812444\n",
      "Step - 8175, Loss - 0.28810667071899554, Learning Rate - 0.0001953125, magnitude of gradient - 0.06182412596827309\n",
      "Step - 8176, Loss - 0.3098347730640486, Learning Rate - 0.0001953125, magnitude of gradient - 0.016580968665244153\n",
      "Step - 8177, Loss - 0.28948540639755205, Learning Rate - 0.0001953125, magnitude of gradient - 0.048490222153265934\n",
      "Step - 8178, Loss - 0.41338062358727157, Learning Rate - 0.0001953125, magnitude of gradient - 0.10501654946450399\n",
      "Step - 8179, Loss - 0.34258739938885413, Learning Rate - 0.0001953125, magnitude of gradient - 0.03686423606453805\n",
      "Step - 8180, Loss - 0.3462373601010193, Learning Rate - 0.0001953125, magnitude of gradient - 0.028885015680655025\n",
      "Step - 8181, Loss - 0.27451284564721673, Learning Rate - 0.0001953125, magnitude of gradient - 0.07804744936856452\n",
      "Step - 8182, Loss - 0.2932990546814618, Learning Rate - 0.0001953125, magnitude of gradient - 0.0576255313239712\n",
      "Step - 8183, Loss - 0.2633637513918034, Learning Rate - 0.0001953125, magnitude of gradient - 0.01871536376746221\n",
      "Step - 8184, Loss - 0.32923680577674974, Learning Rate - 0.0001953125, magnitude of gradient - 0.04866972537439781\n",
      "Step - 8185, Loss - 0.37811184569141004, Learning Rate - 0.0001953125, magnitude of gradient - 0.03608249075313874\n",
      "Step - 8186, Loss - 0.28369139121520043, Learning Rate - 0.0001953125, magnitude of gradient - 0.0409229547362763\n",
      "Step - 8187, Loss - 0.27855163011270456, Learning Rate - 0.0001953125, magnitude of gradient - 0.03780474774032177\n",
      "Step - 8188, Loss - 0.3602709876575052, Learning Rate - 0.0001953125, magnitude of gradient - 0.034487455794425435\n",
      "Step - 8189, Loss - 0.3711502162972909, Learning Rate - 0.0001953125, magnitude of gradient - 0.07095511872218666\n",
      "Step - 8190, Loss - 0.31252303569754125, Learning Rate - 0.0001953125, magnitude of gradient - 0.10380288303921874\n",
      "Step - 8191, Loss - 0.33460782112150894, Learning Rate - 0.0001953125, magnitude of gradient - 0.03814425192085895\n",
      "Step - 8192, Loss - 0.29642155392694397, Learning Rate - 0.0001953125, magnitude of gradient - 0.05212625186656366\n",
      "Step - 8193, Loss - 0.35983830556163154, Learning Rate - 0.0001953125, magnitude of gradient - 0.06638001768248329\n",
      "Step - 8194, Loss - 0.24895837399996854, Learning Rate - 0.0001953125, magnitude of gradient - 0.019934476971444404\n",
      "Step - 8195, Loss - 0.3540561677460695, Learning Rate - 0.0001953125, magnitude of gradient - 0.03902047113566888\n",
      "Step - 8196, Loss - 0.29289898601083253, Learning Rate - 0.0001953125, magnitude of gradient - 0.07783900831486616\n",
      "Step - 8197, Loss - 0.29156408257866123, Learning Rate - 0.0001953125, magnitude of gradient - 0.024076577013469765\n",
      "Step - 8198, Loss - 0.2873982404719756, Learning Rate - 0.0001953125, magnitude of gradient - 0.030394471476364267\n",
      "Step - 8199, Loss - 0.2975583293998777, Learning Rate - 0.0001953125, magnitude of gradient - 0.07598314663910474\n",
      "Step - 8200, Loss - 0.37031880821003454, Learning Rate - 0.0001953125, magnitude of gradient - 0.08158198826163283\n",
      "Step - 8201, Loss - 0.3015422215107334, Learning Rate - 0.0001953125, magnitude of gradient - 0.035225279299494996\n",
      "Step - 8202, Loss - 0.3064771590361036, Learning Rate - 0.0001953125, magnitude of gradient - 0.07977644915530549\n",
      "Step - 8203, Loss - 0.3498058058747207, Learning Rate - 0.0001953125, magnitude of gradient - 0.06985280242372048\n",
      "Step - 8204, Loss - 0.3323462878759714, Learning Rate - 0.0001953125, magnitude of gradient - 0.0473462134923781\n",
      "Step - 8205, Loss - 0.33224832700928314, Learning Rate - 0.0001953125, magnitude of gradient - 0.04804216260431689\n",
      "Step - 8206, Loss - 0.23257374718126794, Learning Rate - 0.0001953125, magnitude of gradient - 0.039275634043829624\n",
      "Step - 8207, Loss - 0.32587576697168397, Learning Rate - 0.0001953125, magnitude of gradient - 0.04718380700465986\n",
      "Step - 8208, Loss - 0.2454818724578301, Learning Rate - 0.0001953125, magnitude of gradient - 0.01286564441691753\n",
      "Step - 8209, Loss - 0.2716936183033629, Learning Rate - 0.0001953125, magnitude of gradient - 0.01123528975548211\n",
      "Step - 8210, Loss - 0.33030963473991515, Learning Rate - 0.0001953125, magnitude of gradient - 0.04324080583727046\n",
      "Step - 8211, Loss - 0.3687612253022024, Learning Rate - 0.0001953125, magnitude of gradient - 0.044438777624768215\n",
      "Step - 8212, Loss - 0.328432304733666, Learning Rate - 0.0001953125, magnitude of gradient - 0.044122069874356525\n",
      "Step - 8213, Loss - 0.3609564798615796, Learning Rate - 0.0001953125, magnitude of gradient - 0.01421123000411656\n",
      "Step - 8214, Loss - 0.30487284707460427, Learning Rate - 0.0001953125, magnitude of gradient - 0.06527287108113736\n",
      "Step - 8215, Loss - 0.2889293778218474, Learning Rate - 0.0001953125, magnitude of gradient - 0.04473878940193686\n",
      "Step - 8216, Loss - 0.3378251888114567, Learning Rate - 0.0001953125, magnitude of gradient - 0.009175590140332455\n",
      "Step - 8217, Loss - 0.23791630855031182, Learning Rate - 0.0001953125, magnitude of gradient - 0.08151642302445894\n",
      "Step - 8218, Loss - 0.32158688894969356, Learning Rate - 0.0001953125, magnitude of gradient - 0.056151638519429115\n",
      "Step - 8219, Loss - 0.27899088296855123, Learning Rate - 0.0001953125, magnitude of gradient - 0.08500436130796042\n",
      "Step - 8220, Loss - 0.3071674186502136, Learning Rate - 0.0001953125, magnitude of gradient - 0.030415916296864586\n",
      "Step - 8221, Loss - 0.2917865019286831, Learning Rate - 0.0001953125, magnitude of gradient - 0.08339545747413996\n",
      "Step - 8222, Loss - 0.2870035294984339, Learning Rate - 0.0001953125, magnitude of gradient - 0.01894937811052753\n",
      "Step - 8223, Loss - 0.2889678927018907, Learning Rate - 0.0001953125, magnitude of gradient - 0.03629686759362508\n",
      "Step - 8224, Loss - 0.243950564691413, Learning Rate - 0.0001953125, magnitude of gradient - 0.02918941570939317\n",
      "Step - 8225, Loss - 0.3033116832377617, Learning Rate - 0.0001953125, magnitude of gradient - 0.10015699258401883\n",
      "Step - 8226, Loss - 0.2735931444766598, Learning Rate - 0.0001953125, magnitude of gradient - 0.011319406482877974\n",
      "Step - 8227, Loss - 0.36474466396379945, Learning Rate - 0.0001953125, magnitude of gradient - 0.03205988315545249\n",
      "Step - 8228, Loss - 0.2661360854202951, Learning Rate - 0.0001953125, magnitude of gradient - 0.05281484453103861\n",
      "Step - 8229, Loss - 0.361394948128406, Learning Rate - 0.0001953125, magnitude of gradient - 0.0834299548095706\n",
      "Step - 8230, Loss - 0.29237230928756175, Learning Rate - 0.0001953125, magnitude of gradient - 0.027441844281932016\n",
      "Step - 8231, Loss - 0.2525150563225683, Learning Rate - 0.0001953125, magnitude of gradient - 0.05956424750579974\n",
      "Step - 8232, Loss - 0.34770123689146964, Learning Rate - 0.0001953125, magnitude of gradient - 0.07073269965248047\n",
      "Step - 8233, Loss - 0.41150907915310503, Learning Rate - 0.0001953125, magnitude of gradient - 0.03576073052322018\n",
      "Step - 8234, Loss - 0.30487935399685157, Learning Rate - 0.0001953125, magnitude of gradient - 0.043861145204078245\n",
      "Step - 8235, Loss - 0.29278080405813245, Learning Rate - 0.0001953125, magnitude of gradient - 0.03888078068358617\n",
      "Step - 8236, Loss - 0.26245192177989884, Learning Rate - 0.0001953125, magnitude of gradient - 0.033733672246734445\n",
      "Step - 8237, Loss - 0.2890151524788806, Learning Rate - 0.0001953125, magnitude of gradient - 0.055448076365890615\n",
      "Step - 8238, Loss - 0.28119293212328955, Learning Rate - 0.0001953125, magnitude of gradient - 0.024140994743146356\n",
      "Step - 8239, Loss - 0.21432804981284004, Learning Rate - 0.0001953125, magnitude of gradient - 0.03885096316352888\n",
      "Step - 8240, Loss - 0.3557624271901232, Learning Rate - 0.0001953125, magnitude of gradient - 0.12336813931693642\n",
      "Step - 8241, Loss - 0.3306141135465308, Learning Rate - 0.0001953125, magnitude of gradient - 0.023247847373099515\n",
      "Step - 8242, Loss - 0.3298876621453964, Learning Rate - 0.0001953125, magnitude of gradient - 0.022121641470626615\n",
      "Step - 8243, Loss - 0.338819275150291, Learning Rate - 0.0001953125, magnitude of gradient - 0.03621148795240218\n",
      "Step - 8244, Loss - 0.3114153600549567, Learning Rate - 0.0001953125, magnitude of gradient - 0.05663669198877749\n",
      "Step - 8245, Loss - 0.337068558574357, Learning Rate - 0.0001953125, magnitude of gradient - 0.058597450593874424\n",
      "Step - 8246, Loss - 0.32370912158510345, Learning Rate - 0.0001953125, magnitude of gradient - 0.117878807392483\n",
      "Step - 8247, Loss - 0.388467398099428, Learning Rate - 0.0001953125, magnitude of gradient - 0.08208719752837035\n",
      "Step - 8248, Loss - 0.28778221631025896, Learning Rate - 0.0001953125, magnitude of gradient - 0.022020264643118268\n",
      "Step - 8249, Loss - 0.325660373160923, Learning Rate - 0.0001953125, magnitude of gradient - 0.0503660970853376\n",
      "Step - 8250, Loss - 0.374910660085269, Learning Rate - 0.0001953125, magnitude of gradient - 0.005379565706743035\n",
      "Step - 8251, Loss - 0.3945910237100398, Learning Rate - 0.0001953125, magnitude of gradient - 0.09415754537505919\n",
      "Step - 8252, Loss - 0.3065962252486223, Learning Rate - 0.0001953125, magnitude of gradient - 0.04678428261183479\n",
      "Step - 8253, Loss - 0.3223417506016685, Learning Rate - 0.0001953125, magnitude of gradient - 0.04444963435359497\n",
      "Step - 8254, Loss - 0.2939789360342858, Learning Rate - 0.0001953125, magnitude of gradient - 0.05020944332362498\n",
      "Step - 8255, Loss - 0.3801627390674106, Learning Rate - 0.0001953125, magnitude of gradient - 0.03768040001613304\n",
      "Step - 8256, Loss - 0.34399267222781194, Learning Rate - 0.0001953125, magnitude of gradient - 0.0273340927200118\n",
      "Step - 8257, Loss - 0.2841423849641818, Learning Rate - 0.0001953125, magnitude of gradient - 0.06123400410512377\n",
      "Step - 8258, Loss - 0.37931678217371073, Learning Rate - 0.0001953125, magnitude of gradient - 0.05778076823145638\n",
      "Step - 8259, Loss - 0.33452607269907003, Learning Rate - 0.0001953125, magnitude of gradient - 0.042964616985994804\n",
      "Step - 8260, Loss - 0.3313471353151939, Learning Rate - 0.0001953125, magnitude of gradient - 0.06838690031199761\n",
      "Step - 8261, Loss - 0.33964667649924024, Learning Rate - 0.0001953125, magnitude of gradient - 0.05856516538908432\n",
      "Step - 8262, Loss - 0.340203150183614, Learning Rate - 0.0001953125, magnitude of gradient - 0.04308763903575114\n",
      "Step - 8263, Loss - 0.35021849168414265, Learning Rate - 0.0001953125, magnitude of gradient - 0.07840109991529734\n",
      "Step - 8264, Loss - 0.28333625768147597, Learning Rate - 0.0001953125, magnitude of gradient - 0.020675920955017493\n",
      "Step - 8265, Loss - 0.2845892243793239, Learning Rate - 0.0001953125, magnitude of gradient - 0.054390781853051115\n",
      "Step - 8266, Loss - 0.30225317889541814, Learning Rate - 0.0001953125, magnitude of gradient - 0.013480897347957273\n",
      "Step - 8267, Loss - 0.3698798147322965, Learning Rate - 0.0001953125, magnitude of gradient - 0.025993804213353802\n",
      "Step - 8268, Loss - 0.3114549198598279, Learning Rate - 0.0001953125, magnitude of gradient - 0.05031514031950933\n",
      "Step - 8269, Loss - 0.28792852128298474, Learning Rate - 0.0001953125, magnitude of gradient - 0.03591389444801682\n",
      "Step - 8270, Loss - 0.3209546058590472, Learning Rate - 0.0001953125, magnitude of gradient - 0.07648344041913752\n",
      "Step - 8271, Loss - 0.3193270405630715, Learning Rate - 0.0001953125, magnitude of gradient - 0.07084214561596267\n",
      "Step - 8272, Loss - 0.31252553589365234, Learning Rate - 0.0001953125, magnitude of gradient - 0.008970350632836603\n",
      "Step - 8273, Loss - 0.3812371604535192, Learning Rate - 0.0001953125, magnitude of gradient - 0.028052874908205218\n",
      "Step - 8274, Loss - 0.31128753236803675, Learning Rate - 0.0001953125, magnitude of gradient - 0.023001983686659518\n",
      "Step - 8275, Loss - 0.3086128706434256, Learning Rate - 0.0001953125, magnitude of gradient - 0.027364603635639924\n",
      "Step - 8276, Loss - 0.2870158342331001, Learning Rate - 0.0001953125, magnitude of gradient - 0.0735148621308751\n",
      "Step - 8277, Loss - 0.3547786464602777, Learning Rate - 0.0001953125, magnitude of gradient - 0.054994642103911355\n",
      "Step - 8278, Loss - 0.31795543262442394, Learning Rate - 0.0001953125, magnitude of gradient - 0.05921360948037605\n",
      "Step - 8279, Loss - 0.350174232132662, Learning Rate - 0.0001953125, magnitude of gradient - 0.02864563172452161\n",
      "Step - 8280, Loss - 0.3628643801076582, Learning Rate - 0.0001953125, magnitude of gradient - 0.07711099226459907\n",
      "Step - 8281, Loss - 0.339450150956695, Learning Rate - 0.0001953125, magnitude of gradient - 0.07953116068620705\n",
      "Step - 8282, Loss - 0.3297437350007029, Learning Rate - 0.0001953125, magnitude of gradient - 0.08399964159185445\n",
      "Step - 8283, Loss - 0.3329585414678312, Learning Rate - 0.0001953125, magnitude of gradient - 0.059999175909182106\n",
      "Step - 8284, Loss - 0.27214660959435993, Learning Rate - 0.0001953125, magnitude of gradient - 0.06132274419153872\n",
      "Step - 8285, Loss - 0.34996438134430874, Learning Rate - 0.0001953125, magnitude of gradient - 0.057468803667345296\n",
      "Step - 8286, Loss - 0.3442542361935102, Learning Rate - 0.0001953125, magnitude of gradient - 0.050806247318601054\n",
      "Step - 8287, Loss - 0.2511152668597919, Learning Rate - 0.0001953125, magnitude of gradient - 0.09261612840736032\n",
      "Step - 8288, Loss - 0.27666047747111133, Learning Rate - 0.0001953125, magnitude of gradient - 0.02284588924375496\n",
      "Step - 8289, Loss - 0.3127587570462076, Learning Rate - 0.0001953125, magnitude of gradient - 0.05916192458911726\n",
      "Step - 8290, Loss - 0.2656765406993174, Learning Rate - 0.0001953125, magnitude of gradient - 0.09551841070064312\n",
      "Step - 8291, Loss - 0.2971713033467869, Learning Rate - 0.0001953125, magnitude of gradient - 0.020222889144196107\n",
      "Step - 8292, Loss - 0.3645785659283441, Learning Rate - 0.0001953125, magnitude of gradient - 0.11948642467970413\n",
      "Step - 8293, Loss - 0.37909253159701567, Learning Rate - 0.0001953125, magnitude of gradient - 0.02302564280374778\n",
      "Step - 8294, Loss - 0.2581426047542554, Learning Rate - 0.0001953125, magnitude of gradient - 0.002896453455530066\n",
      "Step - 8295, Loss - 0.22274691817632358, Learning Rate - 0.0001953125, magnitude of gradient - 0.051289834124870134\n",
      "Step - 8296, Loss - 0.2909236458652552, Learning Rate - 0.0001953125, magnitude of gradient - 0.08885842271460857\n",
      "Step - 8297, Loss - 0.3679570393261821, Learning Rate - 0.0001953125, magnitude of gradient - 0.02784195144798258\n",
      "Step - 8298, Loss - 0.3350287941301318, Learning Rate - 0.0001953125, magnitude of gradient - 0.03724282241473051\n",
      "Step - 8299, Loss - 0.4008439277066945, Learning Rate - 0.0001953125, magnitude of gradient - 0.045305286870217815\n",
      "Step - 8300, Loss - 0.33860272427157, Learning Rate - 0.0001953125, magnitude of gradient - 0.03057342494194899\n",
      "Step - 8301, Loss - 0.32389860264773657, Learning Rate - 0.0001953125, magnitude of gradient - 0.032882761388292193\n",
      "Step - 8302, Loss - 0.3673197500942708, Learning Rate - 0.0001953125, magnitude of gradient - 0.04930671836998119\n",
      "Step - 8303, Loss - 0.30451146994629996, Learning Rate - 0.0001953125, magnitude of gradient - 0.04345945823953636\n",
      "Step - 8304, Loss - 0.3126719385322293, Learning Rate - 0.0001953125, magnitude of gradient - 0.11072271362898349\n",
      "Step - 8305, Loss - 0.36104720394218837, Learning Rate - 0.0001953125, magnitude of gradient - 0.07924760076938688\n",
      "Step - 8306, Loss - 0.22618362216617405, Learning Rate - 0.0001953125, magnitude of gradient - 0.06264853964448781\n",
      "Step - 8307, Loss - 0.33579575977394804, Learning Rate - 0.0001953125, magnitude of gradient - 0.03632513378262234\n",
      "Step - 8308, Loss - 0.3392532302523121, Learning Rate - 0.0001953125, magnitude of gradient - 0.016975365379386504\n",
      "Step - 8309, Loss - 0.33585761987902224, Learning Rate - 0.0001953125, magnitude of gradient - 0.04763593918639249\n",
      "Step - 8310, Loss - 0.3095908898870199, Learning Rate - 0.0001953125, magnitude of gradient - 0.014040978920556783\n",
      "Step - 8311, Loss - 0.2909216922540001, Learning Rate - 0.0001953125, magnitude of gradient - 0.059734297470498084\n",
      "Step - 8312, Loss - 0.33363703169852954, Learning Rate - 0.0001953125, magnitude of gradient - 0.06420288506017673\n",
      "Step - 8313, Loss - 0.23919536456442314, Learning Rate - 0.0001953125, magnitude of gradient - 0.048521751740706796\n",
      "Step - 8314, Loss - 0.29703365518190405, Learning Rate - 0.0001953125, magnitude of gradient - 0.04387073470012676\n",
      "Step - 8315, Loss - 0.31051660204678533, Learning Rate - 0.0001953125, magnitude of gradient - 0.027916515723712714\n",
      "Step - 8316, Loss - 0.34154086142883366, Learning Rate - 0.0001953125, magnitude of gradient - 0.0028947736777985757\n",
      "Step - 8317, Loss - 0.2858105500984582, Learning Rate - 0.0001953125, magnitude of gradient - 0.018865405705578764\n",
      "Step - 8318, Loss - 0.3611375354974582, Learning Rate - 0.0001953125, magnitude of gradient - 0.03765589157948317\n",
      "Step - 8319, Loss - 0.3066086271405025, Learning Rate - 0.0001953125, magnitude of gradient - 0.046172792239091806\n",
      "Step - 8320, Loss - 0.2740432545375329, Learning Rate - 0.0001953125, magnitude of gradient - 0.010772997077312236\n",
      "Step - 8321, Loss - 0.36058348128025663, Learning Rate - 0.0001953125, magnitude of gradient - 0.03540121615637374\n",
      "Step - 8322, Loss - 0.35826725297770307, Learning Rate - 0.0001953125, magnitude of gradient - 0.013773049725735856\n",
      "Step - 8323, Loss - 0.32153944009228475, Learning Rate - 0.0001953125, magnitude of gradient - 0.021479903463712702\n",
      "Step - 8324, Loss - 0.37625683657135733, Learning Rate - 0.0001953125, magnitude of gradient - 0.06790183931504781\n",
      "Step - 8325, Loss - 0.36598023892618864, Learning Rate - 0.0001953125, magnitude of gradient - 0.11908760461512784\n",
      "Step - 8326, Loss - 0.3484018437704205, Learning Rate - 0.0001953125, magnitude of gradient - 0.07344343892353751\n",
      "Step - 8327, Loss - 0.35123427326679285, Learning Rate - 0.0001953125, magnitude of gradient - 0.0564183744555445\n",
      "Step - 8328, Loss - 0.25172313587493134, Learning Rate - 0.0001953125, magnitude of gradient - 0.04874362014917722\n",
      "Step - 8329, Loss - 0.3317172774574729, Learning Rate - 0.0001953125, magnitude of gradient - 0.04724825822215491\n",
      "Step - 8330, Loss - 0.2635532696641447, Learning Rate - 0.0001953125, magnitude of gradient - 0.01635739809467437\n",
      "Step - 8331, Loss - 0.30782772366782385, Learning Rate - 0.0001953125, magnitude of gradient - 0.012784180292157403\n",
      "Step - 8332, Loss - 0.31614155536551314, Learning Rate - 0.0001953125, magnitude of gradient - 0.06681991446853375\n",
      "Step - 8333, Loss - 0.35830743814989297, Learning Rate - 0.0001953125, magnitude of gradient - 0.05282862564520065\n",
      "Step - 8334, Loss - 0.25399971914935315, Learning Rate - 0.0001953125, magnitude of gradient - 0.007626947904075237\n",
      "Step - 8335, Loss - 0.34417892582517273, Learning Rate - 0.0001953125, magnitude of gradient - 0.034083377009888206\n",
      "Step - 8336, Loss - 0.29504002753961905, Learning Rate - 0.0001953125, magnitude of gradient - 0.04710165777473731\n",
      "Step - 8337, Loss - 0.24919157465598815, Learning Rate - 0.0001953125, magnitude of gradient - 0.016402074663693453\n",
      "Step - 8338, Loss - 0.3703192903470122, Learning Rate - 0.0001953125, magnitude of gradient - 0.04466659169826945\n",
      "Step - 8339, Loss - 0.3124605564718795, Learning Rate - 0.0001953125, magnitude of gradient - 0.06543622514553189\n",
      "Step - 8340, Loss - 0.31498297220759514, Learning Rate - 0.0001953125, magnitude of gradient - 0.08034340282236571\n",
      "Step - 8341, Loss - 0.3702655273993451, Learning Rate - 0.0001953125, magnitude of gradient - 0.03632142553069366\n",
      "Step - 8342, Loss - 0.2824852956625234, Learning Rate - 0.0001953125, magnitude of gradient - 0.03742527570009269\n",
      "Step - 8343, Loss - 0.4059485559132875, Learning Rate - 0.0001953125, magnitude of gradient - 0.04111207853291322\n",
      "Step - 8344, Loss - 0.352064867301467, Learning Rate - 0.0001953125, magnitude of gradient - 0.09707533748244032\n",
      "Step - 8345, Loss - 0.3055030977931566, Learning Rate - 0.0001953125, magnitude of gradient - 0.011627631132978449\n",
      "Step - 8346, Loss - 0.321414723748689, Learning Rate - 0.0001953125, magnitude of gradient - 0.08327445215071265\n",
      "Step - 8347, Loss - 0.44343346802165373, Learning Rate - 0.0001953125, magnitude of gradient - 0.04286589108781217\n",
      "Step - 8348, Loss - 0.3473901243082049, Learning Rate - 0.0001953125, magnitude of gradient - 0.015546378429794068\n",
      "Step - 8349, Loss - 0.3745223207729845, Learning Rate - 0.0001953125, magnitude of gradient - 0.01567028390286579\n",
      "Step - 8350, Loss - 0.3504749042537696, Learning Rate - 0.0001953125, magnitude of gradient - 0.08990573960103537\n",
      "Step - 8351, Loss - 0.33361025864568095, Learning Rate - 0.0001953125, magnitude of gradient - 0.0543996754667694\n",
      "Step - 8352, Loss - 0.28994728670400205, Learning Rate - 0.0001953125, magnitude of gradient - 0.09465772729061149\n",
      "Step - 8353, Loss - 0.3256039930795822, Learning Rate - 0.0001953125, magnitude of gradient - 0.06054290951337499\n",
      "Step - 8354, Loss - 0.34614159360076946, Learning Rate - 0.0001953125, magnitude of gradient - 0.042812594790793974\n",
      "Step - 8355, Loss - 0.3255813298200004, Learning Rate - 0.0001953125, magnitude of gradient - 0.005983858933129158\n",
      "Step - 8356, Loss - 0.33467147354120796, Learning Rate - 0.0001953125, magnitude of gradient - 0.04012013817363612\n",
      "Step - 8357, Loss - 0.2939619152020817, Learning Rate - 0.0001953125, magnitude of gradient - 0.022095363880357658\n",
      "Step - 8358, Loss - 0.3210337045271219, Learning Rate - 0.0001953125, magnitude of gradient - 0.030056317122851128\n",
      "Step - 8359, Loss - 0.35401203241697227, Learning Rate - 0.0001953125, magnitude of gradient - 0.03068847184604503\n",
      "Step - 8360, Loss - 0.32988229590881646, Learning Rate - 0.0001953125, magnitude of gradient - 0.02388706742208636\n",
      "Step - 8361, Loss - 0.35751558292036884, Learning Rate - 0.0001953125, magnitude of gradient - 0.053401659111776766\n",
      "Step - 8362, Loss - 0.3423154904630933, Learning Rate - 0.0001953125, magnitude of gradient - 0.066363029487639\n",
      "Step - 8363, Loss - 0.24882188409581496, Learning Rate - 0.0001953125, magnitude of gradient - 0.02630875111908528\n",
      "Step - 8364, Loss - 0.3476029369782838, Learning Rate - 0.0001953125, magnitude of gradient - 0.02999868440418674\n",
      "Step - 8365, Loss - 0.29443413105608895, Learning Rate - 0.0001953125, magnitude of gradient - 0.03394996007664985\n",
      "Step - 8366, Loss - 0.2584585992563341, Learning Rate - 0.0001953125, magnitude of gradient - 0.027218965385265432\n",
      "Step - 8367, Loss - 0.2775139972436623, Learning Rate - 0.0001953125, magnitude of gradient - 0.08860539076815246\n",
      "Step - 8368, Loss - 0.29613025940368437, Learning Rate - 0.0001953125, magnitude of gradient - 0.04984633712382352\n",
      "Step - 8369, Loss - 0.3495326589282896, Learning Rate - 0.0001953125, magnitude of gradient - 0.013793315099699176\n",
      "Step - 8370, Loss - 0.31694267906537177, Learning Rate - 0.0001953125, magnitude of gradient - 0.019350013573348524\n",
      "Step - 8371, Loss - 0.3667590858400234, Learning Rate - 0.0001953125, magnitude of gradient - 0.011140903356896556\n",
      "Step - 8372, Loss - 0.38615814924594694, Learning Rate - 0.0001953125, magnitude of gradient - 0.039958722995067725\n",
      "Step - 8373, Loss - 0.3204062103514049, Learning Rate - 0.0001953125, magnitude of gradient - 0.019062352011342508\n",
      "Step - 8374, Loss - 0.37803390334721737, Learning Rate - 0.0001953125, magnitude of gradient - 0.03530390714839136\n",
      "Step - 8375, Loss - 0.2788004419240132, Learning Rate - 0.0001953125, magnitude of gradient - 0.05023479800989062\n",
      "Step - 8376, Loss - 0.3286949979255449, Learning Rate - 0.0001953125, magnitude of gradient - 0.038272238882808444\n",
      "Step - 8377, Loss - 0.28546834347636246, Learning Rate - 0.0001953125, magnitude of gradient - 0.12241071269808589\n",
      "Step - 8378, Loss - 0.33109258550092796, Learning Rate - 0.0001953125, magnitude of gradient - 0.0730484486325354\n",
      "Step - 8379, Loss - 0.3065823862557624, Learning Rate - 0.0001953125, magnitude of gradient - 0.040698362695537504\n",
      "Step - 8380, Loss - 0.2999796553001586, Learning Rate - 0.0001953125, magnitude of gradient - 0.06859015239642766\n",
      "Step - 8381, Loss - 0.368431945309308, Learning Rate - 0.0001953125, magnitude of gradient - 0.08906094766120028\n",
      "Step - 8382, Loss - 0.28383516932364156, Learning Rate - 0.0001953125, magnitude of gradient - 0.05508204404304645\n",
      "Step - 8383, Loss - 0.34395453745580273, Learning Rate - 0.0001953125, magnitude of gradient - 0.06182398718436067\n",
      "Step - 8384, Loss - 0.30636565105406693, Learning Rate - 0.0001953125, magnitude of gradient - 0.05437409971108344\n",
      "Step - 8385, Loss - 0.2596474008672491, Learning Rate - 0.0001953125, magnitude of gradient - 0.0655619460318561\n",
      "Step - 8386, Loss - 0.31270538341639, Learning Rate - 0.0001953125, magnitude of gradient - 0.013170439838925449\n",
      "Step - 8387, Loss - 0.29254958218414673, Learning Rate - 0.0001953125, magnitude of gradient - 0.0779510648894095\n",
      "Step - 8388, Loss - 0.39830679961727666, Learning Rate - 0.0001953125, magnitude of gradient - 0.05892525964120239\n",
      "Step - 8389, Loss - 0.3572654043683111, Learning Rate - 0.0001953125, magnitude of gradient - 0.01241108283969198\n",
      "Step - 8390, Loss - 0.29845755408892716, Learning Rate - 0.0001953125, magnitude of gradient - 0.07603707413564272\n",
      "Step - 8391, Loss - 0.2722682616471039, Learning Rate - 0.0001953125, magnitude of gradient - 0.02315761088125703\n",
      "Step - 8392, Loss - 0.3325906735120703, Learning Rate - 0.0001953125, magnitude of gradient - 0.06158662322514335\n",
      "Step - 8393, Loss - 0.35680419950163106, Learning Rate - 0.0001953125, magnitude of gradient - 0.1134878838581926\n",
      "Step - 8394, Loss - 0.32132589856070254, Learning Rate - 0.0001953125, magnitude of gradient - 0.06498991201701422\n",
      "Step - 8395, Loss - 0.27360665437528314, Learning Rate - 0.0001953125, magnitude of gradient - 0.048513313246794684\n",
      "Step - 8396, Loss - 0.3254833050497961, Learning Rate - 0.0001953125, magnitude of gradient - 0.033911191870230735\n",
      "Step - 8397, Loss - 0.3766382407988996, Learning Rate - 0.0001953125, magnitude of gradient - 0.02810188985586073\n",
      "Step - 8398, Loss - 0.28469734383998124, Learning Rate - 0.0001953125, magnitude of gradient - 0.024022574215436708\n",
      "Step - 8399, Loss - 0.3144190099763078, Learning Rate - 0.0001953125, magnitude of gradient - 0.05436139382104248\n",
      "Step - 8400, Loss - 0.339972858390362, Learning Rate - 0.0001953125, magnitude of gradient - 0.02291612656445582\n",
      "Step - 8401, Loss - 0.3770932892545079, Learning Rate - 0.0001953125, magnitude of gradient - 0.046138907116964695\n",
      "Step - 8402, Loss - 0.3643150763962377, Learning Rate - 0.0001953125, magnitude of gradient - 0.030694763400052397\n",
      "Step - 8403, Loss - 0.3751111929133709, Learning Rate - 0.0001953125, magnitude of gradient - 0.05121229792343466\n",
      "Step - 8404, Loss - 0.3157763157364085, Learning Rate - 0.0001953125, magnitude of gradient - 0.04312712954551036\n",
      "Step - 8405, Loss - 0.2857868783564154, Learning Rate - 0.0001953125, magnitude of gradient - 0.06857353046952386\n",
      "Step - 8406, Loss - 0.32352747425463846, Learning Rate - 0.0001953125, magnitude of gradient - 0.05059159992065135\n",
      "Step - 8407, Loss - 0.40647996462226654, Learning Rate - 0.0001953125, magnitude of gradient - 0.06950691286536226\n",
      "Step - 8408, Loss - 0.3450687990169605, Learning Rate - 0.0001953125, magnitude of gradient - 0.0650119754906016\n",
      "Step - 8409, Loss - 0.2795961969256128, Learning Rate - 0.0001953125, magnitude of gradient - 0.062235646899505774\n",
      "Step - 8410, Loss - 0.2715103215803253, Learning Rate - 0.0001953125, magnitude of gradient - 0.03587140450489636\n",
      "Step - 8411, Loss - 0.3766009968017595, Learning Rate - 0.0001953125, magnitude of gradient - 0.02332092388279287\n",
      "Step - 8412, Loss - 0.2715712772982399, Learning Rate - 0.0001953125, magnitude of gradient - 0.11121791684245559\n",
      "Step - 8413, Loss - 0.40402554151533343, Learning Rate - 0.0001953125, magnitude of gradient - 0.08360040562341484\n",
      "Step - 8414, Loss - 0.30680350595018996, Learning Rate - 0.0001953125, magnitude of gradient - 0.022860538102740195\n",
      "Step - 8415, Loss - 0.3239474036948318, Learning Rate - 0.0001953125, magnitude of gradient - 0.021990726075120905\n",
      "Step - 8416, Loss - 0.29749584724976197, Learning Rate - 0.0001953125, magnitude of gradient - 0.06495430691711093\n",
      "Step - 8417, Loss - 0.35157612493460205, Learning Rate - 0.0001953125, magnitude of gradient - 0.039806742114788955\n",
      "Step - 8418, Loss - 0.3223818923445404, Learning Rate - 0.0001953125, magnitude of gradient - 0.06290520667611282\n",
      "Step - 8419, Loss - 0.33785349485637955, Learning Rate - 0.0001953125, magnitude of gradient - 0.0383564200437291\n",
      "Step - 8420, Loss - 0.3309089713066551, Learning Rate - 0.0001953125, magnitude of gradient - 0.04907554939219397\n",
      "Step - 8421, Loss - 0.3376220841191865, Learning Rate - 0.0001953125, magnitude of gradient - 0.06271192652588806\n",
      "Step - 8422, Loss - 0.34789014777787275, Learning Rate - 0.0001953125, magnitude of gradient - 0.07664494690906987\n",
      "Step - 8423, Loss - 0.35070582086175306, Learning Rate - 0.0001953125, magnitude of gradient - 0.06275656087922662\n",
      "Step - 8424, Loss - 0.3365083733981039, Learning Rate - 0.0001953125, magnitude of gradient - 0.02366767412236417\n",
      "Step - 8425, Loss - 0.3615428644155901, Learning Rate - 0.0001953125, magnitude of gradient - 0.03339973001090131\n",
      "Step - 8426, Loss - 0.34471963778669557, Learning Rate - 0.0001953125, magnitude of gradient - 0.029071172491266693\n",
      "Step - 8427, Loss - 0.3281992370389327, Learning Rate - 0.0001953125, magnitude of gradient - 0.03719132398793375\n",
      "Step - 8428, Loss - 0.31143639865438083, Learning Rate - 0.0001953125, magnitude of gradient - 0.02112746322895626\n",
      "Step - 8429, Loss - 0.29566970326397934, Learning Rate - 0.0001953125, magnitude of gradient - 0.023282716057844245\n",
      "Step - 8430, Loss - 0.30893914808103234, Learning Rate - 0.0001953125, magnitude of gradient - 0.013455252185085923\n",
      "Step - 8431, Loss - 0.2637283406532554, Learning Rate - 0.0001953125, magnitude of gradient - 0.03297987421843218\n",
      "Step - 8432, Loss - 0.4120338796709542, Learning Rate - 0.0001953125, magnitude of gradient - 0.10380009498536542\n",
      "Step - 8433, Loss - 0.2831130869680446, Learning Rate - 0.0001953125, magnitude of gradient - 0.025624145605664193\n",
      "Step - 8434, Loss - 0.30047983968284697, Learning Rate - 0.0001953125, magnitude of gradient - 0.023103842186521544\n",
      "Step - 8435, Loss - 0.3595775832680167, Learning Rate - 0.0001953125, magnitude of gradient - 0.05445793128074693\n",
      "Step - 8436, Loss - 0.2625785919860799, Learning Rate - 0.0001953125, magnitude of gradient - 0.031974025390900634\n",
      "Step - 8437, Loss - 0.3593490294295034, Learning Rate - 0.0001953125, magnitude of gradient - 0.03710787801982137\n",
      "Step - 8438, Loss - 0.3934110770828622, Learning Rate - 0.0001953125, magnitude of gradient - 0.0337773439440157\n",
      "Step - 8439, Loss - 0.3492224454492538, Learning Rate - 0.0001953125, magnitude of gradient - 0.021872909333104602\n",
      "Step - 8440, Loss - 0.2897185219547055, Learning Rate - 0.0001953125, magnitude of gradient - 0.10148695280343677\n",
      "Step - 8441, Loss - 0.3127149336347993, Learning Rate - 0.0001953125, magnitude of gradient - 0.04229431740347855\n",
      "Step - 8442, Loss - 0.36601635658935133, Learning Rate - 0.0001953125, magnitude of gradient - 0.05505931137332725\n",
      "Step - 8443, Loss - 0.2968620436351823, Learning Rate - 0.0001953125, magnitude of gradient - 0.044216867542592665\n",
      "Step - 8444, Loss - 0.30537453992844743, Learning Rate - 0.0001953125, magnitude of gradient - 0.051843055004455\n",
      "Step - 8445, Loss - 0.300707527855108, Learning Rate - 0.0001953125, magnitude of gradient - 0.0449711853421426\n",
      "Step - 8446, Loss - 0.31176719618311455, Learning Rate - 0.0001953125, magnitude of gradient - 0.07030464669802436\n",
      "Step - 8447, Loss - 0.2445672093623358, Learning Rate - 0.0001953125, magnitude of gradient - 0.06684838407493102\n",
      "Step - 8448, Loss - 0.34935219897183073, Learning Rate - 0.0001953125, magnitude of gradient - 0.01454106072003851\n",
      "Step - 8449, Loss - 0.31849866409923305, Learning Rate - 0.0001953125, magnitude of gradient - 0.05385860642227198\n",
      "Step - 8450, Loss - 0.27924444906837326, Learning Rate - 0.0001953125, magnitude of gradient - 0.046345655872883376\n",
      "Step - 8451, Loss - 0.3466539007754767, Learning Rate - 0.0001953125, magnitude of gradient - 0.03752048745558694\n",
      "Step - 8452, Loss - 0.33587690348975163, Learning Rate - 0.0001953125, magnitude of gradient - 0.06621007748497532\n",
      "Step - 8453, Loss - 0.33537424765401097, Learning Rate - 0.0001953125, magnitude of gradient - 0.07044926711286975\n",
      "Step - 8454, Loss - 0.3004619029858128, Learning Rate - 0.0001953125, magnitude of gradient - 0.022641293742589445\n",
      "Step - 8455, Loss - 0.26261620396242563, Learning Rate - 0.0001953125, magnitude of gradient - 0.03585014223769218\n",
      "Step - 8456, Loss - 0.26232677444510466, Learning Rate - 0.0001953125, magnitude of gradient - 0.039119892703788475\n",
      "Step - 8457, Loss - 0.3108003330292221, Learning Rate - 0.0001953125, magnitude of gradient - 0.029769469021287718\n",
      "Step - 8458, Loss - 0.3127618180557704, Learning Rate - 0.0001953125, magnitude of gradient - 0.041751339719084284\n",
      "Step - 8459, Loss - 0.315684954722065, Learning Rate - 0.0001953125, magnitude of gradient - 0.016130359377497978\n",
      "Step - 8460, Loss - 0.3305096422363305, Learning Rate - 0.0001953125, magnitude of gradient - 0.021468323140856966\n",
      "Step - 8461, Loss - 0.3077543691349901, Learning Rate - 0.0001953125, magnitude of gradient - 0.06295062856645213\n",
      "Step - 8462, Loss - 0.2557194211897898, Learning Rate - 0.0001953125, magnitude of gradient - 0.0065150993285122485\n",
      "Step - 8463, Loss - 0.3253161483028426, Learning Rate - 0.0001953125, magnitude of gradient - 0.07066309185579117\n",
      "Step - 8464, Loss - 0.2987283434044785, Learning Rate - 0.0001953125, magnitude of gradient - 0.07468822523321357\n",
      "Step - 8465, Loss - 0.3225999017142711, Learning Rate - 0.0001953125, magnitude of gradient - 0.010938918041046538\n",
      "Step - 8466, Loss - 0.28708607533407055, Learning Rate - 0.0001953125, magnitude of gradient - 0.05567467399377478\n",
      "Step - 8467, Loss - 0.2571093643847935, Learning Rate - 0.0001953125, magnitude of gradient - 0.03744574964382601\n",
      "Step - 8468, Loss - 0.3419710752745041, Learning Rate - 0.0001953125, magnitude of gradient - 0.09177434437651628\n",
      "Step - 8469, Loss - 0.2981300283595909, Learning Rate - 0.0001953125, magnitude of gradient - 0.03342805583915099\n",
      "Step - 8470, Loss - 0.28897992261046024, Learning Rate - 0.0001953125, magnitude of gradient - 0.03854944468948071\n",
      "Step - 8471, Loss - 0.35926756287038064, Learning Rate - 0.0001953125, magnitude of gradient - 0.06807008366906979\n",
      "Step - 8472, Loss - 0.33965592514069154, Learning Rate - 0.0001953125, magnitude of gradient - 0.056878944564766866\n",
      "Step - 8473, Loss - 0.24040894624738657, Learning Rate - 0.0001953125, magnitude of gradient - 0.050141226007336386\n",
      "Step - 8474, Loss - 0.35316030182587077, Learning Rate - 0.0001953125, magnitude of gradient - 0.011079795289368633\n",
      "Step - 8475, Loss - 0.2813595836683056, Learning Rate - 0.0001953125, magnitude of gradient - 0.021845154141640417\n",
      "Step - 8476, Loss - 0.2949549328329693, Learning Rate - 0.0001953125, magnitude of gradient - 0.08842164065854759\n",
      "Step - 8477, Loss - 0.3079920764779475, Learning Rate - 0.0001953125, magnitude of gradient - 0.14063219767035373\n",
      "Step - 8478, Loss - 0.3631657995619426, Learning Rate - 0.0001953125, magnitude of gradient - 0.03193420814042348\n",
      "Step - 8479, Loss - 0.3440105575536641, Learning Rate - 0.0001953125, magnitude of gradient - 0.07857573530490995\n",
      "Step - 8480, Loss - 0.3050159907681969, Learning Rate - 0.0001953125, magnitude of gradient - 0.04351998237287222\n",
      "Step - 8481, Loss - 0.3987484812916901, Learning Rate - 0.0001953125, magnitude of gradient - 0.06311751926043864\n",
      "Step - 8482, Loss - 0.34622381526460416, Learning Rate - 0.0001953125, magnitude of gradient - 0.06656108822774452\n",
      "Step - 8483, Loss - 0.20196422317075038, Learning Rate - 0.0001953125, magnitude of gradient - 0.02303101876174682\n",
      "Step - 8484, Loss - 0.2737368805991059, Learning Rate - 0.0001953125, magnitude of gradient - 0.014126811854661534\n",
      "Step - 8485, Loss - 0.3073585074633329, Learning Rate - 0.0001953125, magnitude of gradient - 0.05352093684996471\n",
      "Step - 8486, Loss - 0.30508442927537877, Learning Rate - 0.0001953125, magnitude of gradient - 0.022582934569885797\n",
      "Step - 8487, Loss - 0.27600063711985157, Learning Rate - 0.0001953125, magnitude of gradient - 0.018584426863369242\n",
      "Step - 8488, Loss - 0.26284247814383366, Learning Rate - 0.0001953125, magnitude of gradient - 0.057070683092315964\n",
      "Step - 8489, Loss - 0.31028345741266267, Learning Rate - 0.0001953125, magnitude of gradient - 0.022137549397534876\n",
      "Step - 8490, Loss - 0.31668711415546424, Learning Rate - 0.0001953125, magnitude of gradient - 0.0608821072409351\n",
      "Step - 8491, Loss - 0.31470460362710406, Learning Rate - 0.0001953125, magnitude of gradient - 0.008701411300111483\n",
      "Step - 8492, Loss - 0.2558946562411154, Learning Rate - 0.0001953125, magnitude of gradient - 0.01606822156163831\n",
      "Step - 8493, Loss - 0.2924770736161837, Learning Rate - 0.0001953125, magnitude of gradient - 0.045703471414989455\n",
      "Step - 8494, Loss - 0.3696530908024904, Learning Rate - 0.0001953125, magnitude of gradient - 0.04089333529630129\n",
      "Step - 8495, Loss - 0.2706052590459804, Learning Rate - 0.0001953125, magnitude of gradient - 0.033638192711456016\n",
      "Step - 8496, Loss - 0.26697061409902745, Learning Rate - 0.0001953125, magnitude of gradient - 0.020728024603013304\n",
      "Step - 8497, Loss - 0.2456548776769484, Learning Rate - 0.0001953125, magnitude of gradient - 0.036677676994055594\n",
      "Step - 8498, Loss - 0.2991092787758508, Learning Rate - 0.0001953125, magnitude of gradient - 0.04436710753961109\n",
      "Step - 8499, Loss - 0.416706003617378, Learning Rate - 0.0001953125, magnitude of gradient - 0.007398171993154922\n",
      "Step - 8500, Loss - 0.2750988823001904, Learning Rate - 0.0001953125, magnitude of gradient - 0.05537244904463757\n",
      "Step - 8501, Loss - 0.267680503166202, Learning Rate - 0.0001953125, magnitude of gradient - 0.046772598971719946\n",
      "Step - 8502, Loss - 0.2677634622579398, Learning Rate - 0.0001953125, magnitude of gradient - 0.04014153693566118\n",
      "Step - 8503, Loss - 0.29450138902238443, Learning Rate - 0.0001953125, magnitude of gradient - 0.08955262665236155\n",
      "Step - 8504, Loss - 0.26687760002521754, Learning Rate - 0.0001953125, magnitude of gradient - 0.03018143369133509\n",
      "Step - 8505, Loss - 0.3925916898915388, Learning Rate - 0.0001953125, magnitude of gradient - 0.04513532245717932\n",
      "Step - 8506, Loss - 0.3466211160638498, Learning Rate - 0.0001953125, magnitude of gradient - 0.057854670557769224\n",
      "Step - 8507, Loss - 0.33682957269390523, Learning Rate - 0.0001953125, magnitude of gradient - 0.018821729402547317\n",
      "Step - 8508, Loss - 0.32818846137721774, Learning Rate - 0.0001953125, magnitude of gradient - 0.03616550328861069\n",
      "Step - 8509, Loss - 0.33629692347596485, Learning Rate - 0.0001953125, magnitude of gradient - 0.057817686144240564\n",
      "Step - 8510, Loss - 0.3070589878811498, Learning Rate - 0.0001953125, magnitude of gradient - 0.016517001777935244\n",
      "Step - 8511, Loss - 0.33616587574024515, Learning Rate - 0.0001953125, magnitude of gradient - 0.04026583264110681\n",
      "Step - 8512, Loss - 0.33989592449878164, Learning Rate - 0.0001953125, magnitude of gradient - 0.05705372504996993\n",
      "Step - 8513, Loss - 0.31351437752618044, Learning Rate - 0.0001953125, magnitude of gradient - 0.03627448984805995\n",
      "Step - 8514, Loss - 0.3086999227377809, Learning Rate - 0.0001953125, magnitude of gradient - 0.050918565163639594\n",
      "Step - 8515, Loss - 0.3380406770754891, Learning Rate - 0.0001953125, magnitude of gradient - 0.055455149675665054\n",
      "Step - 8516, Loss - 0.3616713962726144, Learning Rate - 0.0001953125, magnitude of gradient - 0.082052432129129\n",
      "Step - 8517, Loss - 0.333012942009583, Learning Rate - 0.0001953125, magnitude of gradient - 0.07485137002305646\n",
      "Step - 8518, Loss - 0.4039370065842499, Learning Rate - 0.0001953125, magnitude of gradient - 0.023231821705984073\n",
      "Step - 8519, Loss - 0.3649748440829836, Learning Rate - 0.0001953125, magnitude of gradient - 0.020928744093578245\n",
      "Step - 8520, Loss - 0.2571512563294019, Learning Rate - 0.0001953125, magnitude of gradient - 0.06784858425621645\n",
      "Step - 8521, Loss - 0.3960608856005103, Learning Rate - 0.0001953125, magnitude of gradient - 0.018204267633787928\n",
      "Step - 8522, Loss - 0.3041908136252934, Learning Rate - 0.0001953125, magnitude of gradient - 0.03793701107421543\n",
      "Step - 8523, Loss - 0.376486391057035, Learning Rate - 0.0001953125, magnitude of gradient - 0.05006225546706756\n",
      "Step - 8524, Loss - 0.3026029491109383, Learning Rate - 0.0001953125, magnitude of gradient - 0.08387280852605582\n",
      "Step - 8525, Loss - 0.3984368815876456, Learning Rate - 0.0001953125, magnitude of gradient - 0.05748419805445086\n",
      "Step - 8526, Loss - 0.3455464565548546, Learning Rate - 0.0001953125, magnitude of gradient - 0.03965925385842684\n",
      "Step - 8527, Loss - 0.3112826480853058, Learning Rate - 0.0001953125, magnitude of gradient - 0.01266079604323266\n",
      "Step - 8528, Loss - 0.37370828667752043, Learning Rate - 0.0001953125, magnitude of gradient - 0.014608093616595347\n",
      "Step - 8529, Loss - 0.38019828640071696, Learning Rate - 0.0001953125, magnitude of gradient - 0.047144031098900294\n",
      "Step - 8530, Loss - 0.26564453508637775, Learning Rate - 0.0001953125, magnitude of gradient - 0.1624865860789039\n",
      "Step - 8531, Loss - 0.3781212929332072, Learning Rate - 0.0001953125, magnitude of gradient - 0.022697488043372727\n",
      "Step - 8532, Loss - 0.3511090129141696, Learning Rate - 0.0001953125, magnitude of gradient - 0.09350146234744389\n",
      "Step - 8533, Loss - 0.30467935910769073, Learning Rate - 0.0001953125, magnitude of gradient - 0.029801192609281123\n",
      "Step - 8534, Loss - 0.33578610501046213, Learning Rate - 0.0001953125, magnitude of gradient - 0.12327518198270757\n",
      "Step - 8535, Loss - 0.2963934628356234, Learning Rate - 0.0001953125, magnitude of gradient - 0.07512348485193797\n",
      "Step - 8536, Loss - 0.27823582696956384, Learning Rate - 0.0001953125, magnitude of gradient - 0.01914816164130361\n",
      "Step - 8537, Loss - 0.24258819853697464, Learning Rate - 0.0001953125, magnitude of gradient - 0.025125566266125336\n",
      "Step - 8538, Loss - 0.24689007709934313, Learning Rate - 0.0001953125, magnitude of gradient - 0.09571859277371819\n",
      "Step - 8539, Loss - 0.2675592943361327, Learning Rate - 0.0001953125, magnitude of gradient - 0.059883587772825285\n",
      "Step - 8540, Loss - 0.2975080325444412, Learning Rate - 0.0001953125, magnitude of gradient - 0.030601864356435377\n",
      "Step - 8541, Loss - 0.2972768265386165, Learning Rate - 0.0001953125, magnitude of gradient - 0.041637029383608616\n",
      "Step - 8542, Loss - 0.2876294895356366, Learning Rate - 0.0001953125, magnitude of gradient - 0.029485933680412958\n",
      "Step - 8543, Loss - 0.36426588404986865, Learning Rate - 0.0001953125, magnitude of gradient - 0.06485740667544973\n",
      "Step - 8544, Loss - 0.3446145549833241, Learning Rate - 0.0001953125, magnitude of gradient - 0.012257713249152084\n",
      "Step - 8545, Loss - 0.2890604551476314, Learning Rate - 0.0001953125, magnitude of gradient - 0.03239537145177805\n",
      "Step - 8546, Loss - 0.359013101456267, Learning Rate - 0.0001953125, magnitude of gradient - 0.06652532432580756\n",
      "Step - 8547, Loss - 0.22848561472609696, Learning Rate - 0.0001953125, magnitude of gradient - 0.06463286972892111\n",
      "Step - 8548, Loss - 0.2759552693709463, Learning Rate - 0.0001953125, magnitude of gradient - 0.03853169293132268\n",
      "Step - 8549, Loss - 0.3989417080122842, Learning Rate - 0.0001953125, magnitude of gradient - 0.039484807954956076\n",
      "Step - 8550, Loss - 0.32622809111788453, Learning Rate - 0.0001953125, magnitude of gradient - 0.08654922600869054\n",
      "Step - 8551, Loss - 0.35188407021221674, Learning Rate - 0.0001953125, magnitude of gradient - 0.07735617211822995\n",
      "Step - 8552, Loss - 0.3083275877798245, Learning Rate - 0.0001953125, magnitude of gradient - 0.01594271491098602\n",
      "Step - 8553, Loss - 0.32331487798396663, Learning Rate - 0.0001953125, magnitude of gradient - 0.03176853849227934\n",
      "Step - 8554, Loss - 0.4211138664623494, Learning Rate - 0.0001953125, magnitude of gradient - 0.038006001427025295\n",
      "Step - 8555, Loss - 0.3301862835875368, Learning Rate - 0.0001953125, magnitude of gradient - 0.002509120093570853\n",
      "Step - 8556, Loss - 0.3149271884616983, Learning Rate - 0.0001953125, magnitude of gradient - 0.03500844333167005\n",
      "Step - 8557, Loss - 0.32981961974552665, Learning Rate - 0.0001953125, magnitude of gradient - 0.059384949660853635\n",
      "Step - 8558, Loss - 0.34455105543490966, Learning Rate - 0.0001953125, magnitude of gradient - 0.038111166129197564\n",
      "Step - 8559, Loss - 0.3118580856309013, Learning Rate - 0.0001953125, magnitude of gradient - 0.022283707580813144\n",
      "Step - 8560, Loss - 0.33467807299164287, Learning Rate - 0.0001953125, magnitude of gradient - 0.05454315520303138\n",
      "Step - 8561, Loss - 0.3592911034188144, Learning Rate - 0.0001953125, magnitude of gradient - 0.04423856605954508\n",
      "Step - 8562, Loss - 0.2814872108602072, Learning Rate - 0.0001953125, magnitude of gradient - 0.06913996378799116\n",
      "Step - 8563, Loss - 0.37732533363279974, Learning Rate - 0.0001953125, magnitude of gradient - 0.03809613660379911\n",
      "Step - 8564, Loss - 0.3869594774406633, Learning Rate - 0.0001953125, magnitude of gradient - 0.07159814121260902\n",
      "Step - 8565, Loss - 0.3224996179997607, Learning Rate - 0.0001953125, magnitude of gradient - 0.03785712266272346\n",
      "Step - 8566, Loss - 0.3687383830443543, Learning Rate - 0.0001953125, magnitude of gradient - 0.052146373274233014\n",
      "Step - 8567, Loss - 0.3360487236435147, Learning Rate - 0.0001953125, magnitude of gradient - 0.04179621719704401\n",
      "Step - 8568, Loss - 0.3923393894115535, Learning Rate - 0.0001953125, magnitude of gradient - 0.03342024240843182\n",
      "Step - 8569, Loss - 0.268719456520386, Learning Rate - 0.0001953125, magnitude of gradient - 0.032580037663695126\n",
      "Step - 8570, Loss - 0.37574080974067725, Learning Rate - 0.0001953125, magnitude of gradient - 0.041491552106531306\n",
      "Step - 8571, Loss - 0.21988129905126524, Learning Rate - 0.0001953125, magnitude of gradient - 0.10180941019630903\n",
      "Step - 8572, Loss - 0.37391678524575583, Learning Rate - 0.0001953125, magnitude of gradient - 0.029163677798327473\n",
      "Step - 8573, Loss - 0.3015411553095287, Learning Rate - 0.0001953125, magnitude of gradient - 0.032285612001450456\n",
      "Step - 8574, Loss - 0.2850329980779047, Learning Rate - 0.0001953125, magnitude of gradient - 0.03297776084825282\n",
      "Step - 8575, Loss - 0.3091749236312861, Learning Rate - 0.0001953125, magnitude of gradient - 0.01604980502700168\n",
      "Step - 8576, Loss - 0.30355875674551813, Learning Rate - 0.0001953125, magnitude of gradient - 0.06763061301096474\n",
      "Step - 8577, Loss - 0.2812095742479125, Learning Rate - 0.0001953125, magnitude of gradient - 0.09076362396826164\n",
      "Step - 8578, Loss - 0.30205049106051896, Learning Rate - 0.0001953125, magnitude of gradient - 0.012934383961814441\n",
      "Step - 8579, Loss - 0.3625229397869283, Learning Rate - 0.0001953125, magnitude of gradient - 0.0666630065671834\n",
      "Step - 8580, Loss - 0.34756001439975776, Learning Rate - 0.0001953125, magnitude of gradient - 0.14030949297113443\n",
      "Step - 8581, Loss - 0.24869334931085219, Learning Rate - 0.0001953125, magnitude of gradient - 0.07283299721095216\n",
      "Step - 8582, Loss - 0.33567890613977314, Learning Rate - 0.0001953125, magnitude of gradient - 0.02213128739076803\n",
      "Step - 8583, Loss - 0.3013878354448247, Learning Rate - 0.0001953125, magnitude of gradient - 0.0566058629521038\n",
      "Step - 8584, Loss - 0.2596573348237273, Learning Rate - 0.0001953125, magnitude of gradient - 0.046614574034256485\n",
      "Step - 8585, Loss - 0.2740055905679845, Learning Rate - 0.0001953125, magnitude of gradient - 0.00969067981476505\n",
      "Step - 8586, Loss - 0.2346564396097668, Learning Rate - 0.0001953125, magnitude of gradient - 0.06407928377708175\n",
      "Step - 8587, Loss - 0.3476691214129573, Learning Rate - 0.0001953125, magnitude of gradient - 0.026131490327620736\n",
      "Step - 8588, Loss - 0.4005300118149579, Learning Rate - 0.0001953125, magnitude of gradient - 0.060463774821622095\n",
      "Step - 8589, Loss - 0.32463666025269566, Learning Rate - 0.0001953125, magnitude of gradient - 0.0511866283089623\n",
      "Step - 8590, Loss - 0.3825699958986703, Learning Rate - 0.0001953125, magnitude of gradient - 0.06890522152543119\n",
      "Step - 8591, Loss - 0.3718869438404335, Learning Rate - 0.0001953125, magnitude of gradient - 0.047154145636200365\n",
      "Step - 8592, Loss - 0.3361140452431198, Learning Rate - 0.0001953125, magnitude of gradient - 0.03760568998906039\n",
      "Step - 8593, Loss - 0.27454572915974335, Learning Rate - 0.0001953125, magnitude of gradient - 0.055111880863670366\n",
      "Step - 8594, Loss - 0.34175264722702414, Learning Rate - 0.0001953125, magnitude of gradient - 0.03202336160495047\n",
      "Step - 8595, Loss - 0.33889262336702, Learning Rate - 0.0001953125, magnitude of gradient - 0.02803742732596994\n",
      "Step - 8596, Loss - 0.3608324095411218, Learning Rate - 0.0001953125, magnitude of gradient - 0.12988357133533696\n",
      "Step - 8597, Loss - 0.33587035365072304, Learning Rate - 0.0001953125, magnitude of gradient - 0.09821678853003686\n",
      "Step - 8598, Loss - 0.3602441435041458, Learning Rate - 0.0001953125, magnitude of gradient - 0.022402990512682762\n",
      "Step - 8599, Loss - 0.3928618088912955, Learning Rate - 0.0001953125, magnitude of gradient - 0.038947284948607705\n",
      "Step - 8600, Loss - 0.2583784300614774, Learning Rate - 0.0001953125, magnitude of gradient - 0.04313837034871656\n",
      "Step - 8601, Loss - 0.2846822591396373, Learning Rate - 0.0001953125, magnitude of gradient - 0.009981585289357475\n",
      "Step - 8602, Loss - 0.32556918482697234, Learning Rate - 0.0001953125, magnitude of gradient - 0.062223306068168296\n",
      "Step - 8603, Loss - 0.3022569802285705, Learning Rate - 0.0001953125, magnitude of gradient - 0.029699947355906624\n",
      "Step - 8604, Loss - 0.4221485990521102, Learning Rate - 0.0001953125, magnitude of gradient - 0.019912218542923178\n",
      "Step - 8605, Loss - 0.39774275487919747, Learning Rate - 0.0001953125, magnitude of gradient - 0.00308109530821637\n",
      "Step - 8606, Loss - 0.30664243398316826, Learning Rate - 0.0001953125, magnitude of gradient - 0.06582061608146951\n",
      "Step - 8607, Loss - 0.2694220470021431, Learning Rate - 0.0001953125, magnitude of gradient - 0.09067569341592173\n",
      "Step - 8608, Loss - 0.3037592905924752, Learning Rate - 0.0001953125, magnitude of gradient - 0.06550721959343407\n",
      "Step - 8609, Loss - 0.32595883129572156, Learning Rate - 0.0001953125, magnitude of gradient - 0.08102425627118377\n",
      "Step - 8610, Loss - 0.3059966295428674, Learning Rate - 0.0001953125, magnitude of gradient - 0.009536123780210176\n",
      "Step - 8611, Loss - 0.2743237414178556, Learning Rate - 0.0001953125, magnitude of gradient - 0.04980723683658883\n",
      "Step - 8612, Loss - 0.3000204139056951, Learning Rate - 0.0001953125, magnitude of gradient - 0.004599399340467192\n",
      "Step - 8613, Loss - 0.32923219874915105, Learning Rate - 0.0001953125, magnitude of gradient - 0.005232414693751984\n",
      "Step - 8614, Loss - 0.23189631452195586, Learning Rate - 0.0001953125, magnitude of gradient - 0.057325855740367124\n",
      "Step - 8615, Loss - 0.28989114939162475, Learning Rate - 0.0001953125, magnitude of gradient - 0.02341687478699615\n",
      "Step - 8616, Loss - 0.3149988839420104, Learning Rate - 0.0001953125, magnitude of gradient - 0.059010384688952144\n",
      "Step - 8617, Loss - 0.36407363286842126, Learning Rate - 0.0001953125, magnitude of gradient - 0.07145670012453392\n",
      "Step - 8618, Loss - 0.28648470545806826, Learning Rate - 0.0001953125, magnitude of gradient - 0.01824471986594498\n",
      "Step - 8619, Loss - 0.33858659394997603, Learning Rate - 0.0001953125, magnitude of gradient - 0.0491077652987162\n",
      "Step - 8620, Loss - 0.3021321761649994, Learning Rate - 0.0001953125, magnitude of gradient - 0.024839458522816275\n",
      "Step - 8621, Loss - 0.3189863665092437, Learning Rate - 0.0001953125, magnitude of gradient - 0.04217763857022424\n",
      "Step - 8622, Loss - 0.2949092546899438, Learning Rate - 0.0001953125, magnitude of gradient - 0.05795850419846013\n",
      "Step - 8623, Loss - 0.26938386472051534, Learning Rate - 0.0001953125, magnitude of gradient - 0.08703665513940817\n",
      "Step - 8624, Loss - 0.31407223077181406, Learning Rate - 0.0001953125, magnitude of gradient - 0.05331069045288182\n",
      "Step - 8625, Loss - 0.31700807259125774, Learning Rate - 0.0001953125, magnitude of gradient - 0.05777134472936627\n",
      "Step - 8626, Loss - 0.4215593668340626, Learning Rate - 0.0001953125, magnitude of gradient - 0.03195816120510118\n",
      "Step - 8627, Loss - 0.3633628755712341, Learning Rate - 0.0001953125, magnitude of gradient - 0.05267978198943962\n",
      "Step - 8628, Loss - 0.3964091528646364, Learning Rate - 0.0001953125, magnitude of gradient - 0.05071746843569653\n",
      "Step - 8629, Loss - 0.3155860854485627, Learning Rate - 0.0001953125, magnitude of gradient - 0.029277549869671952\n",
      "Step - 8630, Loss - 0.30063689372504276, Learning Rate - 0.0001953125, magnitude of gradient - 0.030898753002987587\n",
      "Step - 8631, Loss - 0.33459308116588804, Learning Rate - 0.0001953125, magnitude of gradient - 0.03631581298534197\n",
      "Step - 8632, Loss - 0.2900986458762055, Learning Rate - 0.0001953125, magnitude of gradient - 0.05523322789182865\n",
      "Step - 8633, Loss - 0.3885278265888392, Learning Rate - 0.0001953125, magnitude of gradient - 0.03763242768561296\n",
      "Step - 8634, Loss - 0.39668893346472134, Learning Rate - 0.0001953125, magnitude of gradient - 0.07635871782221813\n",
      "Step - 8635, Loss - 0.35129898747445937, Learning Rate - 0.0001953125, magnitude of gradient - 0.02188709626622955\n",
      "Step - 8636, Loss - 0.26920589248543036, Learning Rate - 0.0001953125, magnitude of gradient - 0.05507173457269173\n",
      "Step - 8637, Loss - 0.27049275283981133, Learning Rate - 0.0001953125, magnitude of gradient - 0.10568643789065076\n",
      "Step - 8638, Loss - 0.3419342741136158, Learning Rate - 0.0001953125, magnitude of gradient - 0.03670908673505039\n",
      "Step - 8639, Loss - 0.30983082686060165, Learning Rate - 0.0001953125, magnitude of gradient - 0.021830445047305237\n",
      "Step - 8640, Loss - 0.3241037256142487, Learning Rate - 0.0001953125, magnitude of gradient - 0.03879467107307641\n",
      "Step - 8641, Loss - 0.30161794379986784, Learning Rate - 0.0001953125, magnitude of gradient - 0.04484799369519614\n",
      "Step - 8642, Loss - 0.3488067683180206, Learning Rate - 0.0001953125, magnitude of gradient - 0.039935735565487414\n",
      "Step - 8643, Loss - 0.34732662963783384, Learning Rate - 0.0001953125, magnitude of gradient - 0.01829683873019856\n",
      "Step - 8644, Loss - 0.30181821899132083, Learning Rate - 0.0001953125, magnitude of gradient - 0.06036194187498782\n",
      "Step - 8645, Loss - 0.31472469369493006, Learning Rate - 0.0001953125, magnitude of gradient - 0.029429011242861007\n",
      "Step - 8646, Loss - 0.35015805492291346, Learning Rate - 0.0001953125, magnitude of gradient - 0.047278000197217826\n",
      "Step - 8647, Loss - 0.37656303003913055, Learning Rate - 0.0001953125, magnitude of gradient - 0.05702292394815141\n",
      "Step - 8648, Loss - 0.2933753516451293, Learning Rate - 0.0001953125, magnitude of gradient - 0.07994053988732301\n",
      "Step - 8649, Loss - 0.39863394582694583, Learning Rate - 0.0001953125, magnitude of gradient - 0.07213847771382201\n",
      "Step - 8650, Loss - 0.36693324197241106, Learning Rate - 0.0001953125, magnitude of gradient - 0.07132978727202083\n",
      "Step - 8651, Loss - 0.2875963662605381, Learning Rate - 0.0001953125, magnitude of gradient - 0.01932711223111144\n",
      "Step - 8652, Loss - 0.32425435494654137, Learning Rate - 0.0001953125, magnitude of gradient - 0.016003554777730838\n",
      "Step - 8653, Loss - 0.3185641709996617, Learning Rate - 0.0001953125, magnitude of gradient - 0.0646232148454228\n",
      "Step - 8654, Loss - 0.3057922866264582, Learning Rate - 0.0001953125, magnitude of gradient - 0.04340997336478174\n",
      "Step - 8655, Loss - 0.31393213411823084, Learning Rate - 0.0001953125, magnitude of gradient - 0.03955597330059176\n",
      "Step - 8656, Loss - 0.34596935967435494, Learning Rate - 0.0001953125, magnitude of gradient - 0.06333090997611612\n",
      "Step - 8657, Loss - 0.41254338678717484, Learning Rate - 0.0001953125, magnitude of gradient - 0.022027349750967828\n",
      "Step - 8658, Loss - 0.3560622186767644, Learning Rate - 0.0001953125, magnitude of gradient - 0.0948207913423114\n",
      "Step - 8659, Loss - 0.33217336877793846, Learning Rate - 0.0001953125, magnitude of gradient - 0.05462982189740511\n",
      "Step - 8660, Loss - 0.32855335391023177, Learning Rate - 0.0001953125, magnitude of gradient - 0.02175476529206639\n",
      "Step - 8661, Loss - 0.32855851297868954, Learning Rate - 0.0001953125, magnitude of gradient - 0.05912046835359507\n",
      "Step - 8662, Loss - 0.3720863159546036, Learning Rate - 0.0001953125, magnitude of gradient - 0.06943557123255359\n",
      "Step - 8663, Loss - 0.3879705946708455, Learning Rate - 0.0001953125, magnitude of gradient - 0.012087217603013569\n",
      "Step - 8664, Loss - 0.3248200966439845, Learning Rate - 0.0001953125, magnitude of gradient - 0.06003079012484918\n",
      "Step - 8665, Loss - 0.3214546427885392, Learning Rate - 0.0001953125, magnitude of gradient - 0.03773437676936583\n",
      "Step - 8666, Loss - 0.3186071265327864, Learning Rate - 0.0001953125, magnitude of gradient - 0.01831819274885524\n",
      "Step - 8667, Loss - 0.23070033591486241, Learning Rate - 0.0001953125, magnitude of gradient - 0.033919996159511225\n",
      "Step - 8668, Loss - 0.3163318646158804, Learning Rate - 0.0001953125, magnitude of gradient - 0.04702294554342309\n",
      "Step - 8669, Loss - 0.399006072975203, Learning Rate - 0.0001953125, magnitude of gradient - 0.05607984987978104\n",
      "Step - 8670, Loss - 0.29654995355682506, Learning Rate - 0.0001953125, magnitude of gradient - 0.04117426347418503\n",
      "Step - 8671, Loss - 0.35919666091170016, Learning Rate - 0.0001953125, magnitude of gradient - 0.058990242464112906\n",
      "Step - 8672, Loss - 0.36827708392546865, Learning Rate - 0.0001953125, magnitude of gradient - 0.03607207925531908\n",
      "Step - 8673, Loss - 0.252167236759665, Learning Rate - 0.0001953125, magnitude of gradient - 0.03823554268332826\n",
      "Step - 8674, Loss - 0.33217488689857566, Learning Rate - 0.0001953125, magnitude of gradient - 0.036093603250936405\n",
      "Step - 8675, Loss - 0.3062027681319253, Learning Rate - 0.0001953125, magnitude of gradient - 0.05455182087924389\n",
      "Step - 8676, Loss - 0.2813158855913688, Learning Rate - 0.0001953125, magnitude of gradient - 0.02115650892092728\n",
      "Step - 8677, Loss - 0.30517758814401724, Learning Rate - 0.0001953125, magnitude of gradient - 0.06519907492005649\n",
      "Step - 8678, Loss - 0.24350762280637567, Learning Rate - 0.0001953125, magnitude of gradient - 0.015596693102952013\n",
      "Step - 8679, Loss - 0.30641444775725835, Learning Rate - 0.0001953125, magnitude of gradient - 0.03728570623951701\n",
      "Step - 8680, Loss - 0.3044859034138408, Learning Rate - 0.0001953125, magnitude of gradient - 0.06309009535478047\n",
      "Step - 8681, Loss - 0.3203536060549896, Learning Rate - 0.0001953125, magnitude of gradient - 0.06425242889035535\n",
      "Step - 8682, Loss - 0.3769669715062303, Learning Rate - 0.0001953125, magnitude of gradient - 0.035982245592199105\n",
      "Step - 8683, Loss - 0.31348945038199566, Learning Rate - 0.0001953125, magnitude of gradient - 0.026934630216814432\n",
      "Step - 8684, Loss - 0.2709651887028329, Learning Rate - 0.0001953125, magnitude of gradient - 0.028233507052240998\n",
      "Step - 8685, Loss - 0.3732451222924176, Learning Rate - 0.0001953125, magnitude of gradient - 0.05685760599686358\n",
      "Step - 8686, Loss - 0.32983861126346076, Learning Rate - 0.0001953125, magnitude of gradient - 0.052758131013007144\n",
      "Step - 8687, Loss - 0.362042290439796, Learning Rate - 0.0001953125, magnitude of gradient - 0.040708616303370616\n",
      "Step - 8688, Loss - 0.3395249516776171, Learning Rate - 0.0001953125, magnitude of gradient - 0.028897317822513764\n",
      "Step - 8689, Loss - 0.38937323048687017, Learning Rate - 0.0001953125, magnitude of gradient - 0.06319750793829351\n",
      "Step - 8690, Loss - 0.35612307885192374, Learning Rate - 0.0001953125, magnitude of gradient - 0.05552570993319619\n",
      "Step - 8691, Loss - 0.29197270841232226, Learning Rate - 0.0001953125, magnitude of gradient - 0.03236336404431957\n",
      "Step - 8692, Loss - 0.3134323224503035, Learning Rate - 0.0001953125, magnitude of gradient - 0.05211976403666291\n",
      "Step - 8693, Loss - 0.3224203786921938, Learning Rate - 0.0001953125, magnitude of gradient - 0.031898762103938064\n",
      "Step - 8694, Loss - 0.3430437721085938, Learning Rate - 0.0001953125, magnitude of gradient - 0.07553000965960002\n",
      "Step - 8695, Loss - 0.40650186942994887, Learning Rate - 0.0001953125, magnitude of gradient - 0.006890525636551804\n",
      "Step - 8696, Loss - 0.3229179132738832, Learning Rate - 0.0001953125, magnitude of gradient - 0.03323460692295443\n",
      "Step - 8697, Loss - 0.38008088108286503, Learning Rate - 0.0001953125, magnitude of gradient - 0.0567151228906992\n",
      "Step - 8698, Loss - 0.27080391815908256, Learning Rate - 0.0001953125, magnitude of gradient - 0.012663601165055256\n",
      "Step - 8699, Loss - 0.21155662070914594, Learning Rate - 0.0001953125, magnitude of gradient - 0.030794779510084056\n",
      "Step - 8700, Loss - 0.3095621998253419, Learning Rate - 0.0001953125, magnitude of gradient - 0.018721917842058237\n",
      "Step - 8701, Loss - 0.3083214751556847, Learning Rate - 0.0001953125, magnitude of gradient - 0.07030438150937804\n",
      "Step - 8702, Loss - 0.35064382292995305, Learning Rate - 0.0001953125, magnitude of gradient - 0.04462792692448048\n",
      "Step - 8703, Loss - 0.22604959167593858, Learning Rate - 0.0001953125, magnitude of gradient - 0.10929370320685912\n",
      "Step - 8704, Loss - 0.31476077826994664, Learning Rate - 0.0001953125, magnitude of gradient - 0.06719127874625422\n",
      "Step - 8705, Loss - 0.3163617549800305, Learning Rate - 0.0001953125, magnitude of gradient - 0.08054126811312697\n",
      "Step - 8706, Loss - 0.3350777697542947, Learning Rate - 0.0001953125, magnitude of gradient - 0.04305621012787785\n",
      "Step - 8707, Loss - 0.318118080616915, Learning Rate - 0.0001953125, magnitude of gradient - 0.0578516305577485\n",
      "Step - 8708, Loss - 0.30339277235812884, Learning Rate - 0.0001953125, magnitude of gradient - 0.09813460026211396\n",
      "Step - 8709, Loss - 0.4083794392722727, Learning Rate - 0.0001953125, magnitude of gradient - 0.04752406290999987\n",
      "Step - 8710, Loss - 0.27853833498070857, Learning Rate - 0.0001953125, magnitude of gradient - 0.051397425696616796\n",
      "Step - 8711, Loss - 0.34320089890599254, Learning Rate - 0.0001953125, magnitude of gradient - 0.047751878322435\n",
      "Step - 8712, Loss - 0.3478372024030563, Learning Rate - 0.0001953125, magnitude of gradient - 0.06668393217598699\n",
      "Step - 8713, Loss - 0.4182530505866389, Learning Rate - 0.0001953125, magnitude of gradient - 0.11182518383208165\n",
      "Step - 8714, Loss - 0.3744343098014895, Learning Rate - 0.0001953125, magnitude of gradient - 0.06568351558683076\n",
      "Step - 8715, Loss - 0.3140466801240792, Learning Rate - 0.0001953125, magnitude of gradient - 0.02568956114880653\n",
      "Step - 8716, Loss - 0.3015711245124184, Learning Rate - 0.0001953125, magnitude of gradient - 0.03302242789405873\n",
      "Step - 8717, Loss - 0.3180904255302869, Learning Rate - 0.0001953125, magnitude of gradient - 0.07488421485540175\n",
      "Step - 8718, Loss - 0.2855627795996035, Learning Rate - 0.0001953125, magnitude of gradient - 0.013474751422096783\n",
      "Step - 8719, Loss - 0.3391438023033058, Learning Rate - 0.0001953125, magnitude of gradient - 0.11645158731436\n",
      "Step - 8720, Loss - 0.2676693480299447, Learning Rate - 0.0001953125, magnitude of gradient - 0.025542179253930215\n",
      "Step - 8721, Loss - 0.2970934734650076, Learning Rate - 0.0001953125, magnitude of gradient - 0.07269208538535966\n",
      "Step - 8722, Loss - 0.3764800718879283, Learning Rate - 0.0001953125, magnitude of gradient - 0.020780200397419988\n",
      "Step - 8723, Loss - 0.35831801885029785, Learning Rate - 0.0001953125, magnitude of gradient - 0.07803865911763794\n",
      "Step - 8724, Loss - 0.34607487115640223, Learning Rate - 0.0001953125, magnitude of gradient - 0.04890691613649908\n",
      "Step - 8725, Loss - 0.30757512063710923, Learning Rate - 0.0001953125, magnitude of gradient - 0.02780063886692303\n",
      "Step - 8726, Loss - 0.2544403305287701, Learning Rate - 0.0001953125, magnitude of gradient - 0.04573185927358073\n",
      "Step - 8727, Loss - 0.36249319249919715, Learning Rate - 0.0001953125, magnitude of gradient - 0.041701645756181786\n",
      "Step - 8728, Loss - 0.36816272092181035, Learning Rate - 0.0001953125, magnitude of gradient - 0.03949201912064447\n",
      "Step - 8729, Loss - 0.2540173703215024, Learning Rate - 0.0001953125, magnitude of gradient - 0.04377993528058298\n",
      "Step - 8730, Loss - 0.29470260298248274, Learning Rate - 0.0001953125, magnitude of gradient - 0.08422545231077151\n",
      "Step - 8731, Loss - 0.3259496594534703, Learning Rate - 0.0001953125, magnitude of gradient - 0.1413954010931456\n",
      "Step - 8732, Loss - 0.3098308356622978, Learning Rate - 0.0001953125, magnitude of gradient - 0.049703156006410544\n",
      "Step - 8733, Loss - 0.2847171277242365, Learning Rate - 0.0001953125, magnitude of gradient - 0.04103626767086966\n",
      "Step - 8734, Loss - 0.3316475153155892, Learning Rate - 0.0001953125, magnitude of gradient - 0.059720424127704896\n",
      "Step - 8735, Loss - 0.35592458986548037, Learning Rate - 0.0001953125, magnitude of gradient - 0.0329537580170221\n",
      "Step - 8736, Loss - 0.3002082607645668, Learning Rate - 0.0001953125, magnitude of gradient - 0.041564197695565845\n",
      "Step - 8737, Loss - 0.3528516326387044, Learning Rate - 0.0001953125, magnitude of gradient - 0.032508613707877734\n",
      "Step - 8738, Loss - 0.29631867313080834, Learning Rate - 0.0001953125, magnitude of gradient - 0.03972951992805563\n",
      "Step - 8739, Loss - 0.30582302469557665, Learning Rate - 0.0001953125, magnitude of gradient - 0.02036882625593642\n",
      "Step - 8740, Loss - 0.3022566839988945, Learning Rate - 0.0001953125, magnitude of gradient - 0.04799019817789309\n",
      "Step - 8741, Loss - 0.3461455528703504, Learning Rate - 0.0001953125, magnitude of gradient - 0.04366035754432792\n",
      "Step - 8742, Loss - 0.3746313940264346, Learning Rate - 0.0001953125, magnitude of gradient - 0.05546462185876091\n",
      "Step - 8743, Loss - 0.30532127915832047, Learning Rate - 0.0001953125, magnitude of gradient - 0.08275986388629518\n",
      "Step - 8744, Loss - 0.33943148313891774, Learning Rate - 0.0001953125, magnitude of gradient - 0.04374864123250271\n",
      "Step - 8745, Loss - 0.35975601888471875, Learning Rate - 0.0001953125, magnitude of gradient - 0.052535581809231666\n",
      "Step - 8746, Loss - 0.27363057804544866, Learning Rate - 0.0001953125, magnitude of gradient - 0.0995444439846353\n",
      "Step - 8747, Loss - 0.3444361630720228, Learning Rate - 0.0001953125, magnitude of gradient - 0.044350123005719624\n",
      "Step - 8748, Loss - 0.32173634578904053, Learning Rate - 0.0001953125, magnitude of gradient - 0.05388638054237357\n",
      "Step - 8749, Loss - 0.2904972700145107, Learning Rate - 0.0001953125, magnitude of gradient - 0.0926758850265727\n",
      "Step - 8750, Loss - 0.28578881258193967, Learning Rate - 0.0001953125, magnitude of gradient - 0.006963554707050963\n",
      "Step - 8751, Loss - 0.34899935377390184, Learning Rate - 0.0001953125, magnitude of gradient - 0.026755301123203557\n",
      "Step - 8752, Loss - 0.3093188001755933, Learning Rate - 0.0001953125, magnitude of gradient - 0.05551734867007833\n",
      "Step - 8753, Loss - 0.39019671457315824, Learning Rate - 0.0001953125, magnitude of gradient - 0.06518479325878868\n",
      "Step - 8754, Loss - 0.34295269098766645, Learning Rate - 0.0001953125, magnitude of gradient - 0.019617666717983612\n",
      "Step - 8755, Loss - 0.28926781616491815, Learning Rate - 0.0001953125, magnitude of gradient - 0.02117635482196046\n",
      "Step - 8756, Loss - 0.39618389319262387, Learning Rate - 0.0001953125, magnitude of gradient - 0.06945726200830933\n",
      "Step - 8757, Loss - 0.3321180176079116, Learning Rate - 0.0001953125, magnitude of gradient - 0.06979971937554989\n",
      "Step - 8758, Loss - 0.2790433428620889, Learning Rate - 0.0001953125, magnitude of gradient - 0.07597362873390065\n",
      "Step - 8759, Loss - 0.23756622766488988, Learning Rate - 0.0001953125, magnitude of gradient - 0.09645312767417735\n",
      "Step - 8760, Loss - 0.3974332303343986, Learning Rate - 0.0001953125, magnitude of gradient - 0.07660711634301327\n",
      "Step - 8761, Loss - 0.3136814602890026, Learning Rate - 0.0001953125, magnitude of gradient - 0.08007514114830146\n",
      "Step - 8762, Loss - 0.27853524939208096, Learning Rate - 0.0001953125, magnitude of gradient - 0.013936425473132999\n",
      "Step - 8763, Loss - 0.2801913576073235, Learning Rate - 0.0001953125, magnitude of gradient - 0.0532278943203507\n",
      "Step - 8764, Loss - 0.36526406893668817, Learning Rate - 0.0001953125, magnitude of gradient - 0.0313615796820168\n",
      "Step - 8765, Loss - 0.34857940600341825, Learning Rate - 0.0001953125, magnitude of gradient - 0.06854605197970487\n",
      "Step - 8766, Loss - 0.3484408917778134, Learning Rate - 0.0001953125, magnitude of gradient - 0.06170249849814566\n",
      "Step - 8767, Loss - 0.36344742732001784, Learning Rate - 0.0001953125, magnitude of gradient - 0.04540470353809582\n",
      "Step - 8768, Loss - 0.26829528432911287, Learning Rate - 0.0001953125, magnitude of gradient - 0.06354817416617818\n",
      "Step - 8769, Loss - 0.29808903512357393, Learning Rate - 0.0001953125, magnitude of gradient - 0.03139935056099612\n",
      "Step - 8770, Loss - 0.2508981729563441, Learning Rate - 0.0001953125, magnitude of gradient - 0.05317310729617955\n",
      "Step - 8771, Loss - 0.26535726915252994, Learning Rate - 0.0001953125, magnitude of gradient - 0.005738545398548541\n",
      "Step - 8772, Loss - 0.3225834565348418, Learning Rate - 0.0001953125, magnitude of gradient - 0.05416309827771247\n",
      "Step - 8773, Loss - 0.32287106513794195, Learning Rate - 0.0001953125, magnitude of gradient - 0.05510941049266088\n",
      "Step - 8774, Loss - 0.3321327539063274, Learning Rate - 0.0001953125, magnitude of gradient - 0.030995585138682984\n",
      "Step - 8775, Loss - 0.32645745021732175, Learning Rate - 0.0001953125, magnitude of gradient - 0.045664447120838954\n",
      "Step - 8776, Loss - 0.35479977373488186, Learning Rate - 0.0001953125, magnitude of gradient - 0.042033440211128904\n",
      "Step - 8777, Loss - 0.37871642707367326, Learning Rate - 0.0001953125, magnitude of gradient - 0.029061800674362922\n",
      "Step - 8778, Loss - 0.30779040832795634, Learning Rate - 0.0001953125, magnitude of gradient - 0.027700812258325224\n",
      "Step - 8779, Loss - 0.4433200596026655, Learning Rate - 0.0001953125, magnitude of gradient - 0.049190727684133566\n",
      "Step - 8780, Loss - 0.31020019643076013, Learning Rate - 0.0001953125, magnitude of gradient - 0.06837392419099796\n",
      "Step - 8781, Loss - 0.29499446978666743, Learning Rate - 0.0001953125, magnitude of gradient - 0.01657913618368979\n",
      "Step - 8782, Loss - 0.302951309861276, Learning Rate - 0.0001953125, magnitude of gradient - 0.030769238084068572\n",
      "Step - 8783, Loss - 0.36831161760080305, Learning Rate - 0.0001953125, magnitude of gradient - 0.05579141131768458\n",
      "Step - 8784, Loss - 0.2953452968849116, Learning Rate - 0.0001953125, magnitude of gradient - 0.04044525418274524\n",
      "Step - 8785, Loss - 0.39130146835714963, Learning Rate - 0.0001953125, magnitude of gradient - 0.061639475317145725\n",
      "Step - 8786, Loss - 0.26804691068728975, Learning Rate - 0.0001953125, magnitude of gradient - 0.06967913649554608\n",
      "Step - 8787, Loss - 0.29697646619005497, Learning Rate - 0.0001953125, magnitude of gradient - 0.08271407407043534\n",
      "Step - 8788, Loss - 0.380904798716197, Learning Rate - 0.0001953125, magnitude of gradient - 0.10291946901177078\n",
      "Step - 8789, Loss - 0.36509827969849057, Learning Rate - 0.0001953125, magnitude of gradient - 0.026266174244998025\n",
      "Step - 8790, Loss - 0.3440488865095793, Learning Rate - 0.0001953125, magnitude of gradient - 0.053442198927204705\n",
      "Step - 8791, Loss - 0.3099478784614417, Learning Rate - 0.0001953125, magnitude of gradient - 0.0625679776215601\n",
      "Step - 8792, Loss - 0.2526268217644815, Learning Rate - 0.0001953125, magnitude of gradient - 0.03473376719561259\n",
      "Step - 8793, Loss - 0.3678042926682912, Learning Rate - 0.0001953125, magnitude of gradient - 0.06894524408387909\n",
      "Step - 8794, Loss - 0.2235235868970577, Learning Rate - 0.0001953125, magnitude of gradient - 0.05732671172651645\n",
      "Step - 8795, Loss - 0.3532964507258951, Learning Rate - 0.0001953125, magnitude of gradient - 0.03861989018346187\n",
      "Step - 8796, Loss - 0.26555891429857337, Learning Rate - 0.0001953125, magnitude of gradient - 0.04575868777135621\n",
      "Step - 8797, Loss - 0.3791001907941675, Learning Rate - 0.0001953125, magnitude of gradient - 0.05504395951617935\n",
      "Step - 8798, Loss - 0.2765967308494236, Learning Rate - 0.0001953125, magnitude of gradient - 0.07436103051780234\n",
      "Step - 8799, Loss - 0.3825833776088811, Learning Rate - 0.0001953125, magnitude of gradient - 0.07690788018251062\n",
      "Step - 8800, Loss - 0.3425749472180785, Learning Rate - 0.0001953125, magnitude of gradient - 0.12728373701004161\n",
      "Step - 8801, Loss - 0.27487046795188036, Learning Rate - 0.0001953125, magnitude of gradient - 0.017810243238604025\n",
      "Step - 8802, Loss - 0.2796114580278147, Learning Rate - 0.0001953125, magnitude of gradient - 0.049848643471125595\n",
      "Step - 8803, Loss - 0.33234920215323704, Learning Rate - 0.0001953125, magnitude of gradient - 0.06291852658473704\n",
      "Step - 8804, Loss - 0.311018044381319, Learning Rate - 0.0001953125, magnitude of gradient - 0.018168034434197615\n",
      "Step - 8805, Loss - 0.26708136183893216, Learning Rate - 0.0001953125, magnitude of gradient - 0.016123708406895518\n",
      "Step - 8806, Loss - 0.29520800023386073, Learning Rate - 0.0001953125, magnitude of gradient - 0.010661668874513103\n",
      "Step - 8807, Loss - 0.3288637869637744, Learning Rate - 0.0001953125, magnitude of gradient - 0.005712134944460314\n",
      "Step - 8808, Loss - 0.29638507170016376, Learning Rate - 0.0001953125, magnitude of gradient - 0.03881946805769379\n",
      "Step - 8809, Loss - 0.35480038855872764, Learning Rate - 0.0001953125, magnitude of gradient - 0.046551375751262086\n",
      "Step - 8810, Loss - 0.30844838416778997, Learning Rate - 0.0001953125, magnitude of gradient - 0.05926035342011239\n",
      "Step - 8811, Loss - 0.3228261051672664, Learning Rate - 0.0001953125, magnitude of gradient - 0.01460086810688917\n",
      "Step - 8812, Loss - 0.3600415026780733, Learning Rate - 0.0001953125, magnitude of gradient - 0.08066886747741778\n",
      "Step - 8813, Loss - 0.3692624162628184, Learning Rate - 0.0001953125, magnitude of gradient - 0.050639039822480206\n",
      "Step - 8814, Loss - 0.38822527281037156, Learning Rate - 0.0001953125, magnitude of gradient - 0.04327888413974182\n",
      "Step - 8815, Loss - 0.33742122752368126, Learning Rate - 0.0001953125, magnitude of gradient - 0.03799062947375511\n",
      "Step - 8816, Loss - 0.32524816978953386, Learning Rate - 0.0001953125, magnitude of gradient - 0.04942897567180457\n",
      "Step - 8817, Loss - 0.25401583848147197, Learning Rate - 0.0001953125, magnitude of gradient - 0.03207648292086775\n",
      "Step - 8818, Loss - 0.33061587368428713, Learning Rate - 0.0001953125, magnitude of gradient - 0.029811282757907318\n",
      "Step - 8819, Loss - 0.2639906898474331, Learning Rate - 0.0001953125, magnitude of gradient - 0.00800450078242175\n",
      "Step - 8820, Loss - 0.38036496371976214, Learning Rate - 0.0001953125, magnitude of gradient - 0.03967942881534099\n",
      "Step - 8821, Loss - 0.35414407814977344, Learning Rate - 0.0001953125, magnitude of gradient - 0.10783565620385913\n",
      "Step - 8822, Loss - 0.29346509458231357, Learning Rate - 0.0001953125, magnitude of gradient - 0.04653385604875328\n",
      "Step - 8823, Loss - 0.3272367317260541, Learning Rate - 0.0001953125, magnitude of gradient - 0.07655330978695661\n",
      "Step - 8824, Loss - 0.34866267344078317, Learning Rate - 0.0001953125, magnitude of gradient - 0.010944360886031741\n",
      "Step - 8825, Loss - 0.38399273961224845, Learning Rate - 0.0001953125, magnitude of gradient - 0.0650353286408925\n",
      "Step - 8826, Loss - 0.3409889269622604, Learning Rate - 0.0001953125, magnitude of gradient - 0.056895748204232426\n",
      "Step - 8827, Loss - 0.2732341847705878, Learning Rate - 0.0001953125, magnitude of gradient - 0.06184043632530692\n",
      "Step - 8828, Loss - 0.3218184608626645, Learning Rate - 0.0001953125, magnitude of gradient - 0.05961352729504746\n",
      "Step - 8829, Loss - 0.32675556036845055, Learning Rate - 0.0001953125, magnitude of gradient - 0.07932079909263767\n",
      "Step - 8830, Loss - 0.36813249313910795, Learning Rate - 0.0001953125, magnitude of gradient - 0.01038617444318662\n",
      "Step - 8831, Loss - 0.3129257983005277, Learning Rate - 0.0001953125, magnitude of gradient - 0.01956244480866176\n",
      "Step - 8832, Loss - 0.36029215349273, Learning Rate - 0.0001953125, magnitude of gradient - 0.007274705961346641\n",
      "Step - 8833, Loss - 0.3018381605760019, Learning Rate - 0.0001953125, magnitude of gradient - 0.04156393926578354\n",
      "Step - 8834, Loss - 0.3722130243718552, Learning Rate - 0.0001953125, magnitude of gradient - 0.020647595454347786\n",
      "Step - 8835, Loss - 0.30783730572113543, Learning Rate - 0.0001953125, magnitude of gradient - 0.03655494036039491\n",
      "Step - 8836, Loss - 0.38361183868846604, Learning Rate - 0.0001953125, magnitude of gradient - 0.06290202136878043\n",
      "Step - 8837, Loss - 0.3669755588977685, Learning Rate - 0.0001953125, magnitude of gradient - 0.039246076944570225\n",
      "Step - 8838, Loss - 0.3369318570487585, Learning Rate - 0.0001953125, magnitude of gradient - 0.04327091099043526\n",
      "Step - 8839, Loss - 0.2801956885263013, Learning Rate - 0.0001953125, magnitude of gradient - 0.04511368686314751\n",
      "Step - 8840, Loss - 0.3692645261264925, Learning Rate - 0.0001953125, magnitude of gradient - 0.04293949298052737\n",
      "Step - 8841, Loss - 0.26735255373122746, Learning Rate - 0.0001953125, magnitude of gradient - 0.045449406548385786\n",
      "Step - 8842, Loss - 0.32203912981266963, Learning Rate - 0.0001953125, magnitude of gradient - 0.06743610412230536\n",
      "Step - 8843, Loss - 0.3627342545062207, Learning Rate - 0.0001953125, magnitude of gradient - 0.056815107729131925\n",
      "Step - 8844, Loss - 0.3333250225486196, Learning Rate - 0.0001953125, magnitude of gradient - 0.0588021859480548\n",
      "Step - 8845, Loss - 0.39374181681500375, Learning Rate - 0.0001953125, magnitude of gradient - 0.019882266595470502\n",
      "Step - 8846, Loss - 0.36333173417523135, Learning Rate - 0.0001953125, magnitude of gradient - 0.05085268673509123\n",
      "Step - 8847, Loss - 0.23322921518610606, Learning Rate - 0.0001953125, magnitude of gradient - 0.01620393907071571\n",
      "Step - 8848, Loss - 0.23566598510506004, Learning Rate - 0.0001953125, magnitude of gradient - 0.050899031454209716\n",
      "Step - 8849, Loss - 0.3457120299554199, Learning Rate - 0.0001953125, magnitude of gradient - 0.058185988123648565\n",
      "Step - 8850, Loss - 0.36570166114377556, Learning Rate - 0.0001953125, magnitude of gradient - 0.08379066426554117\n",
      "Step - 8851, Loss - 0.382691525064329, Learning Rate - 0.0001953125, magnitude of gradient - 0.015127130904589572\n",
      "Step - 8852, Loss - 0.2470758361028232, Learning Rate - 0.0001953125, magnitude of gradient - 0.10006874225196427\n",
      "Step - 8853, Loss - 0.27817386859954574, Learning Rate - 0.0001953125, magnitude of gradient - 0.1144447153333655\n",
      "Step - 8854, Loss - 0.3284049877390378, Learning Rate - 0.0001953125, magnitude of gradient - 0.05161950630526297\n",
      "Step - 8855, Loss - 0.28156061626375894, Learning Rate - 0.0001953125, magnitude of gradient - 0.09458343675269533\n",
      "Step - 8856, Loss - 0.2428066853153376, Learning Rate - 0.0001953125, magnitude of gradient - 0.07953521076396997\n",
      "Step - 8857, Loss - 0.2502185195402512, Learning Rate - 0.0001953125, magnitude of gradient - 0.012569143794415585\n",
      "Step - 8858, Loss - 0.2997124530813303, Learning Rate - 0.0001953125, magnitude of gradient - 0.06703361716161221\n",
      "Step - 8859, Loss - 0.35298150803470757, Learning Rate - 0.0001953125, magnitude of gradient - 0.04965543992977049\n",
      "Step - 8860, Loss - 0.3093821213600845, Learning Rate - 0.0001953125, magnitude of gradient - 0.035522980168082974\n",
      "Step - 8861, Loss - 0.30636895999035213, Learning Rate - 0.0001953125, magnitude of gradient - 0.08788931770326638\n",
      "Step - 8862, Loss - 0.26612667302292603, Learning Rate - 0.0001953125, magnitude of gradient - 0.03203634627263381\n",
      "Step - 8863, Loss - 0.3097377310277713, Learning Rate - 0.0001953125, magnitude of gradient - 0.08398758156295705\n",
      "Step - 8864, Loss - 0.2722905607358028, Learning Rate - 0.0001953125, magnitude of gradient - 0.03571683979778276\n",
      "Step - 8865, Loss - 0.37567023629865304, Learning Rate - 0.0001953125, magnitude of gradient - 0.14616572333280636\n",
      "Step - 8866, Loss - 0.32543255925175585, Learning Rate - 0.0001953125, magnitude of gradient - 0.06559820134148975\n",
      "Step - 8867, Loss - 0.3928680798393402, Learning Rate - 0.0001953125, magnitude of gradient - 0.02533754433263877\n",
      "Step - 8868, Loss - 0.35155738364195044, Learning Rate - 0.0001953125, magnitude of gradient - 0.049044520334167716\n",
      "Step - 8869, Loss - 0.3431571820972865, Learning Rate - 0.0001953125, magnitude of gradient - 0.04142676656537534\n",
      "Step - 8870, Loss - 0.3094721751330787, Learning Rate - 0.0001953125, magnitude of gradient - 0.08085925833315595\n",
      "Step - 8871, Loss - 0.28266340463094897, Learning Rate - 0.0001953125, magnitude of gradient - 0.06689831887552776\n",
      "Step - 8872, Loss - 0.3673453758040926, Learning Rate - 0.0001953125, magnitude of gradient - 0.03819098425951495\n",
      "Step - 8873, Loss - 0.29494821765942286, Learning Rate - 0.0001953125, magnitude of gradient - 0.043962675842361985\n",
      "Step - 8874, Loss - 0.33281362963781497, Learning Rate - 0.0001953125, magnitude of gradient - 0.02972890633989807\n",
      "Step - 8875, Loss - 0.33115844404012473, Learning Rate - 0.0001953125, magnitude of gradient - 0.03547494126501757\n",
      "Step - 8876, Loss - 0.26906101781368263, Learning Rate - 0.0001953125, magnitude of gradient - 0.021330988485087073\n",
      "Step - 8877, Loss - 0.4007393663466705, Learning Rate - 0.0001953125, magnitude of gradient - 0.06908461513913614\n",
      "Step - 8878, Loss - 0.3032325124440255, Learning Rate - 0.0001953125, magnitude of gradient - 0.07363308169949047\n",
      "Step - 8879, Loss - 0.3303870326714698, Learning Rate - 0.0001953125, magnitude of gradient - 0.07061193303616715\n",
      "Step - 8880, Loss - 0.2581235154917978, Learning Rate - 0.0001953125, magnitude of gradient - 0.07262414354627245\n",
      "Step - 8881, Loss - 0.3955440793204334, Learning Rate - 0.0001953125, magnitude of gradient - 0.02962171048898229\n",
      "Step - 8882, Loss - 0.31641420630643635, Learning Rate - 0.0001953125, magnitude of gradient - 0.03800934333557226\n",
      "Step - 8883, Loss - 0.3233294803256709, Learning Rate - 0.0001953125, magnitude of gradient - 0.06364649298399809\n",
      "Step - 8884, Loss - 0.33879114840907404, Learning Rate - 0.0001953125, magnitude of gradient - 0.0821293961510793\n",
      "Step - 8885, Loss - 0.20791658424633225, Learning Rate - 0.0001953125, magnitude of gradient - 0.059101812229317235\n",
      "Step - 8886, Loss - 0.3056978230348586, Learning Rate - 0.0001953125, magnitude of gradient - 0.0652632532789732\n",
      "Step - 8887, Loss - 0.32362221177076705, Learning Rate - 0.0001953125, magnitude of gradient - 0.04277376664146176\n",
      "Step - 8888, Loss - 0.2753129378430291, Learning Rate - 0.0001953125, magnitude of gradient - 0.056886089464108495\n",
      "Step - 8889, Loss - 0.3781894713722125, Learning Rate - 0.0001953125, magnitude of gradient - 0.036982819553480824\n",
      "Step - 8890, Loss - 0.4079683684538731, Learning Rate - 0.0001953125, magnitude of gradient - 0.0586100526806378\n",
      "Step - 8891, Loss - 0.3423609539662083, Learning Rate - 0.0001953125, magnitude of gradient - 0.08394309978807019\n",
      "Step - 8892, Loss - 0.29459966677037736, Learning Rate - 0.0001953125, magnitude of gradient - 0.05695590551420105\n",
      "Step - 8893, Loss - 0.36561935773098203, Learning Rate - 0.0001953125, magnitude of gradient - 0.010267706296012298\n",
      "Step - 8894, Loss - 0.27171817824582456, Learning Rate - 0.0001953125, magnitude of gradient - 0.05768889259477595\n",
      "Step - 8895, Loss - 0.3549199989830229, Learning Rate - 0.0001953125, magnitude of gradient - 0.0473517818354999\n",
      "Step - 8896, Loss - 0.315157766602072, Learning Rate - 0.0001953125, magnitude of gradient - 0.05257308437825301\n",
      "Step - 8897, Loss - 0.30342239742346166, Learning Rate - 0.0001953125, magnitude of gradient - 0.08197484002412503\n",
      "Step - 8898, Loss - 0.2919519951267991, Learning Rate - 0.0001953125, magnitude of gradient - 0.05061455490074571\n",
      "Step - 8899, Loss - 0.32960852800295887, Learning Rate - 0.0001953125, magnitude of gradient - 0.06114180887688204\n",
      "Step - 8900, Loss - 0.3129250608297791, Learning Rate - 0.0001953125, magnitude of gradient - 0.008173028229294851\n",
      "Step - 8901, Loss - 0.2820109320572198, Learning Rate - 0.0001953125, magnitude of gradient - 0.04585934462219712\n",
      "Step - 8902, Loss - 0.2800389537983449, Learning Rate - 0.0001953125, magnitude of gradient - 0.0468648742363065\n",
      "Step - 8903, Loss - 0.31053983300685833, Learning Rate - 0.0001953125, magnitude of gradient - 0.06700313296334559\n",
      "Step - 8904, Loss - 0.3156542853116029, Learning Rate - 0.0001953125, magnitude of gradient - 0.03258948634417181\n",
      "Step - 8905, Loss - 0.331907113773258, Learning Rate - 0.0001953125, magnitude of gradient - 0.05462284571209452\n",
      "Step - 8906, Loss - 0.2510368366202598, Learning Rate - 0.0001953125, magnitude of gradient - 0.04650301250013256\n",
      "Step - 8907, Loss - 0.33328182015789787, Learning Rate - 0.0001953125, magnitude of gradient - 0.03748820520724889\n",
      "Step - 8908, Loss - 0.31331436801024215, Learning Rate - 0.0001953125, magnitude of gradient - 0.05114283994048644\n",
      "Step - 8909, Loss - 0.39482042232180825, Learning Rate - 0.0001953125, magnitude of gradient - 0.032593324496879086\n",
      "Step - 8910, Loss - 0.33228678842626036, Learning Rate - 0.0001953125, magnitude of gradient - 0.07103455324976729\n",
      "Step - 8911, Loss - 0.3851108454037782, Learning Rate - 0.0001953125, magnitude of gradient - 0.03210728280987141\n",
      "Step - 8912, Loss - 0.32092901155228326, Learning Rate - 0.0001953125, magnitude of gradient - 0.03324573294542127\n",
      "Step - 8913, Loss - 0.2696725371815671, Learning Rate - 0.0001953125, magnitude of gradient - 0.08098338408013153\n",
      "Step - 8914, Loss - 0.35496421246476606, Learning Rate - 0.0001953125, magnitude of gradient - 0.04010167403757551\n",
      "Step - 8915, Loss - 0.37920499976255223, Learning Rate - 0.0001953125, magnitude of gradient - 0.04649515578510518\n",
      "Step - 8916, Loss - 0.33621153908848334, Learning Rate - 0.0001953125, magnitude of gradient - 0.04044938431286416\n",
      "Step - 8917, Loss - 0.3687885108289748, Learning Rate - 0.0001953125, magnitude of gradient - 0.01697701891491788\n",
      "Step - 8918, Loss - 0.4130846087664976, Learning Rate - 0.0001953125, magnitude of gradient - 0.05817261817995291\n",
      "Step - 8919, Loss - 0.3099800957992084, Learning Rate - 0.0001953125, magnitude of gradient - 0.044762122561854245\n",
      "Step - 8920, Loss - 0.3024122769253229, Learning Rate - 0.0001953125, magnitude of gradient - 0.021583276557270922\n",
      "Step - 8921, Loss - 0.3806107691100226, Learning Rate - 0.0001953125, magnitude of gradient - 0.07245469858415686\n",
      "Step - 8922, Loss - 0.3814487987842935, Learning Rate - 0.0001953125, magnitude of gradient - 0.0838800218744101\n",
      "Step - 8923, Loss - 0.34496929109711894, Learning Rate - 0.0001953125, magnitude of gradient - 0.07427112564789531\n",
      "Step - 8924, Loss - 0.3273502121673025, Learning Rate - 0.0001953125, magnitude of gradient - 0.06925653187278794\n",
      "Step - 8925, Loss - 0.3252060024920816, Learning Rate - 0.0001953125, magnitude of gradient - 0.04627128631136687\n",
      "Step - 8926, Loss - 0.26358581582589574, Learning Rate - 0.0001953125, magnitude of gradient - 0.029790821710432226\n",
      "Step - 8927, Loss - 0.26379556812956007, Learning Rate - 0.0001953125, magnitude of gradient - 0.0695231917118406\n",
      "Step - 8928, Loss - 0.3372153054183643, Learning Rate - 0.0001953125, magnitude of gradient - 0.0033100826786518914\n",
      "Step - 8929, Loss - 0.34729568386850246, Learning Rate - 0.0001953125, magnitude of gradient - 0.06137155691987688\n",
      "Step - 8930, Loss - 0.3353636026070437, Learning Rate - 0.0001953125, magnitude of gradient - 0.05395614205101129\n",
      "Step - 8931, Loss - 0.28789394848223016, Learning Rate - 0.0001953125, magnitude of gradient - 0.01603113700271651\n",
      "Step - 8932, Loss - 0.3181859801280103, Learning Rate - 0.0001953125, magnitude of gradient - 0.04953616286454296\n",
      "Step - 8933, Loss - 0.2806175072006116, Learning Rate - 0.0001953125, magnitude of gradient - 0.022341784371827815\n",
      "Step - 8934, Loss - 0.2850627343901574, Learning Rate - 0.0001953125, magnitude of gradient - 0.037913265254076955\n",
      "Step - 8935, Loss - 0.30345952438710777, Learning Rate - 0.0001953125, magnitude of gradient - 0.008253310554920922\n",
      "Step - 8936, Loss - 0.3531820272075702, Learning Rate - 0.0001953125, magnitude of gradient - 0.06076644255183296\n",
      "Step - 8937, Loss - 0.36763937556942217, Learning Rate - 0.0001953125, magnitude of gradient - 0.014401333037341547\n",
      "Step - 8938, Loss - 0.31811113216371006, Learning Rate - 0.0001953125, magnitude of gradient - 0.03262074237844013\n",
      "Step - 8939, Loss - 0.2811014437686278, Learning Rate - 0.0001953125, magnitude of gradient - 0.05070764481788361\n",
      "Step - 8940, Loss - 0.32807160725156737, Learning Rate - 0.0001953125, magnitude of gradient - 0.05824854963229481\n",
      "Step - 8941, Loss - 0.3099243616819609, Learning Rate - 0.0001953125, magnitude of gradient - 0.032866758433623466\n",
      "Step - 8942, Loss - 0.2912090664443443, Learning Rate - 0.0001953125, magnitude of gradient - 0.05470148927089231\n",
      "Step - 8943, Loss - 0.2293956003979608, Learning Rate - 0.0001953125, magnitude of gradient - 0.06802916297310906\n",
      "Step - 8944, Loss - 0.31264145119486053, Learning Rate - 0.0001953125, magnitude of gradient - 0.03933150920457619\n",
      "Step - 8945, Loss - 0.22936255942196607, Learning Rate - 0.0001953125, magnitude of gradient - 0.03527887812387026\n",
      "Step - 8946, Loss - 0.23730917377556426, Learning Rate - 0.0001953125, magnitude of gradient - 0.050805136936509794\n",
      "Step - 8947, Loss - 0.289069537881932, Learning Rate - 0.0001953125, magnitude of gradient - 0.07301981463697833\n",
      "Step - 8948, Loss - 0.3466569922920252, Learning Rate - 0.0001953125, magnitude of gradient - 0.08352262842360697\n",
      "Step - 8949, Loss - 0.23958035767111852, Learning Rate - 0.0001953125, magnitude of gradient - 0.0676790799408237\n",
      "Step - 8950, Loss - 0.35270883509909307, Learning Rate - 0.0001953125, magnitude of gradient - 0.053512587561738004\n",
      "Step - 8951, Loss - 0.25058761771060334, Learning Rate - 0.0001953125, magnitude of gradient - 0.08388207662245488\n",
      "Step - 8952, Loss - 0.3444086163912917, Learning Rate - 0.0001953125, magnitude of gradient - 0.05445797486718263\n",
      "Step - 8953, Loss - 0.37010366001879313, Learning Rate - 0.0001953125, magnitude of gradient - 0.026596161108821972\n",
      "Step - 8954, Loss - 0.3600142690383052, Learning Rate - 0.0001953125, magnitude of gradient - 0.019829655129026454\n",
      "Step - 8955, Loss - 0.28485169870559823, Learning Rate - 0.0001953125, magnitude of gradient - 0.022263780333810098\n",
      "Step - 8956, Loss - 0.2846606961673429, Learning Rate - 0.0001953125, magnitude of gradient - 0.06879869947987412\n",
      "Step - 8957, Loss - 0.32244199141739804, Learning Rate - 0.0001953125, magnitude of gradient - 0.059113724210144804\n",
      "Step - 8958, Loss - 0.331513716600552, Learning Rate - 0.0001953125, magnitude of gradient - 0.06222081925108235\n",
      "Step - 8959, Loss - 0.3232190519434284, Learning Rate - 0.0001953125, magnitude of gradient - 0.07477782490004933\n",
      "Step - 8960, Loss - 0.36595975711629025, Learning Rate - 0.0001953125, magnitude of gradient - 0.01146484397095314\n",
      "Step - 8961, Loss - 0.36111760079737504, Learning Rate - 0.0001953125, magnitude of gradient - 0.047841629528788045\n",
      "Step - 8962, Loss - 0.33038539097466074, Learning Rate - 0.0001953125, magnitude of gradient - 0.014625423897641102\n",
      "Step - 8963, Loss - 0.40580398354381014, Learning Rate - 0.0001953125, magnitude of gradient - 0.09093550628552333\n",
      "Step - 8964, Loss - 0.2764932732147808, Learning Rate - 0.0001953125, magnitude of gradient - 0.07512055765797386\n",
      "Step - 8965, Loss - 0.3053285621410259, Learning Rate - 0.0001953125, magnitude of gradient - 0.02828985902192089\n",
      "Step - 8966, Loss - 0.34780394562695993, Learning Rate - 0.0001953125, magnitude of gradient - 0.06169523490639699\n",
      "Step - 8967, Loss - 0.44337099331402996, Learning Rate - 0.0001953125, magnitude of gradient - 0.048889194284771786\n",
      "Step - 8968, Loss - 0.33093350484996137, Learning Rate - 0.0001953125, magnitude of gradient - 0.09234867493983842\n",
      "Step - 8969, Loss - 0.2976955658229698, Learning Rate - 0.0001953125, magnitude of gradient - 0.04216122932281152\n",
      "Step - 8970, Loss - 0.32250956316345275, Learning Rate - 0.0001953125, magnitude of gradient - 0.025161391996692446\n",
      "Step - 8971, Loss - 0.31551979862378743, Learning Rate - 0.0001953125, magnitude of gradient - 0.033939514225425106\n",
      "Step - 8972, Loss - 0.33844591178853367, Learning Rate - 0.0001953125, magnitude of gradient - 0.03543783340542691\n",
      "Step - 8973, Loss - 0.30819568951428716, Learning Rate - 0.0001953125, magnitude of gradient - 0.05023377447576699\n",
      "Step - 8974, Loss - 0.28582432705474703, Learning Rate - 0.0001953125, magnitude of gradient - 0.053100043226786436\n",
      "Step - 8975, Loss - 0.3545652020622098, Learning Rate - 0.0001953125, magnitude of gradient - 0.03667633654040458\n",
      "Step - 8976, Loss - 0.2931994524194292, Learning Rate - 0.0001953125, magnitude of gradient - 0.022348442296821973\n",
      "Step - 8977, Loss - 0.2833178328205209, Learning Rate - 0.0001953125, magnitude of gradient - 0.07531284272614916\n",
      "Step - 8978, Loss - 0.3255229626375249, Learning Rate - 0.0001953125, magnitude of gradient - 0.1202110395210641\n",
      "Step - 8979, Loss - 0.28940664446139136, Learning Rate - 0.0001953125, magnitude of gradient - 0.06158685360916936\n",
      "Step - 8980, Loss - 0.34285824268117604, Learning Rate - 0.0001953125, magnitude of gradient - 0.015824274768030708\n",
      "Step - 8981, Loss - 0.26483640266951003, Learning Rate - 0.0001953125, magnitude of gradient - 0.05264177849098402\n",
      "Step - 8982, Loss - 0.23822326868553267, Learning Rate - 0.0001953125, magnitude of gradient - 0.020116001828501796\n",
      "Step - 8983, Loss - 0.34851103091757907, Learning Rate - 0.0001953125, magnitude of gradient - 0.0434588682657265\n",
      "Step - 8984, Loss - 0.28952194075639015, Learning Rate - 0.0001953125, magnitude of gradient - 0.04864321298603616\n",
      "Step - 8985, Loss - 0.2987610785220953, Learning Rate - 0.0001953125, magnitude of gradient - 0.01422454657372728\n",
      "Step - 8986, Loss - 0.3309144721923083, Learning Rate - 0.0001953125, magnitude of gradient - 0.10069096369860277\n",
      "Step - 8987, Loss - 0.3560872897979354, Learning Rate - 0.0001953125, magnitude of gradient - 0.08305361279248073\n",
      "Step - 8988, Loss - 0.42155253317667973, Learning Rate - 0.0001953125, magnitude of gradient - 0.05420977685356875\n",
      "Step - 8989, Loss - 0.3830110952699302, Learning Rate - 0.0001953125, magnitude of gradient - 0.04479791945468801\n",
      "Step - 8990, Loss - 0.32062723732748505, Learning Rate - 0.0001953125, magnitude of gradient - 0.06929112541200425\n",
      "Step - 8991, Loss - 0.27747591823031825, Learning Rate - 0.0001953125, magnitude of gradient - 0.019666455324526513\n",
      "Step - 8992, Loss - 0.33878529988224554, Learning Rate - 0.0001953125, magnitude of gradient - 0.04497774018318927\n",
      "Step - 8993, Loss - 0.3458542842384292, Learning Rate - 0.0001953125, magnitude of gradient - 0.036856992087116985\n",
      "Step - 8994, Loss - 0.3746725175212701, Learning Rate - 0.0001953125, magnitude of gradient - 0.0612677827767752\n",
      "Step - 8995, Loss - 0.23207231563591166, Learning Rate - 0.0001953125, magnitude of gradient - 0.05065180695774432\n",
      "Step - 8996, Loss - 0.2927785109968238, Learning Rate - 0.0001953125, magnitude of gradient - 0.04939166086302816\n",
      "Step - 8997, Loss - 0.3085230512572956, Learning Rate - 0.0001953125, magnitude of gradient - 0.021106441491741106\n",
      "Step - 8998, Loss - 0.396742293413305, Learning Rate - 0.0001953125, magnitude of gradient - 0.025940797485346837\n",
      "Step - 8999, Loss - 0.3653531477256836, Learning Rate - 0.0001953125, magnitude of gradient - 0.06195441101523639\n",
      "Step - 9000, Loss - 0.2926614091692971, Learning Rate - 0.0001953125, magnitude of gradient - 0.010397610600345485\n",
      "Step - 9001, Loss - 0.2906268747807569, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07072179551961319\n",
      "Step - 9002, Loss - 0.2301775560648489, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07621637275453841\n",
      "Step - 9003, Loss - 0.3518382154081772, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02065968248650782\n",
      "Step - 9004, Loss - 0.33387113998469387, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09576558063067689\n",
      "Step - 9005, Loss - 0.4000005693404128, Learning Rate - 9.765625e-05, magnitude of gradient - 0.023797093948580362\n",
      "Step - 9006, Loss - 0.3445838387183114, Learning Rate - 9.765625e-05, magnitude of gradient - 0.010253561698494182\n",
      "Step - 9007, Loss - 0.3083900948237519, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08427261804864103\n",
      "Step - 9008, Loss - 0.27671708683995005, Learning Rate - 9.765625e-05, magnitude of gradient - 0.014743268668728486\n",
      "Step - 9009, Loss - 0.42240355997933254, Learning Rate - 9.765625e-05, magnitude of gradient - 0.10815769818040909\n",
      "Step - 9010, Loss - 0.306307609000117, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0343306979613932\n",
      "Step - 9011, Loss - 0.31363928014824904, Learning Rate - 9.765625e-05, magnitude of gradient - 0.033869642549524646\n",
      "Step - 9012, Loss - 0.431363594640228, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0777807834307571\n",
      "Step - 9013, Loss - 0.30113008397257324, Learning Rate - 9.765625e-05, magnitude of gradient - 0.044623471456583984\n",
      "Step - 9014, Loss - 0.3584205938798142, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06381154998951666\n",
      "Step - 9015, Loss - 0.3627470185069592, Learning Rate - 9.765625e-05, magnitude of gradient - 0.055529650933714876\n",
      "Step - 9016, Loss - 0.33861477296784614, Learning Rate - 9.765625e-05, magnitude of gradient - 0.030531766296566506\n",
      "Step - 9017, Loss - 0.3344766451069282, Learning Rate - 9.765625e-05, magnitude of gradient - 0.039151671140992314\n",
      "Step - 9018, Loss - 0.3035504713543897, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08172953557207178\n",
      "Step - 9019, Loss - 0.3319040635083478, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07377366199112004\n",
      "Step - 9020, Loss - 0.2676435960078381, Learning Rate - 9.765625e-05, magnitude of gradient - 0.008235729121056558\n",
      "Step - 9021, Loss - 0.260476684087028, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07004796401489714\n",
      "Step - 9022, Loss - 0.3074950502921831, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04081872232750501\n",
      "Step - 9023, Loss - 0.3176529615299153, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04357297228546576\n",
      "Step - 9024, Loss - 0.28184990901656937, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04619999961207807\n",
      "Step - 9025, Loss - 0.3759371739233035, Learning Rate - 9.765625e-05, magnitude of gradient - 0.024266747216731405\n",
      "Step - 9026, Loss - 0.3641269992952495, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04246098139118933\n",
      "Step - 9027, Loss - 0.3477171655921656, Learning Rate - 9.765625e-05, magnitude of gradient - 0.011197897547673398\n",
      "Step - 9028, Loss - 0.3353534498041566, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03935194311394832\n",
      "Step - 9029, Loss - 0.3406867953900268, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03286580785568134\n",
      "Step - 9030, Loss - 0.32001648530415217, Learning Rate - 9.765625e-05, magnitude of gradient - 0.009149932371497064\n",
      "Step - 9031, Loss - 0.3489823167293803, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04781795325209619\n",
      "Step - 9032, Loss - 0.47063644117810527, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04348894837191952\n",
      "Step - 9033, Loss - 0.34467691895772556, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04498151083466882\n",
      "Step - 9034, Loss - 0.2897531726346326, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05906538568663841\n",
      "Step - 9035, Loss - 0.29651493873241985, Learning Rate - 9.765625e-05, magnitude of gradient - 0.043048855782149495\n",
      "Step - 9036, Loss - 0.3071121590613309, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06282234650204037\n",
      "Step - 9037, Loss - 0.2515938324695358, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03525102989419097\n",
      "Step - 9038, Loss - 0.3295885548690416, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04472194278085713\n",
      "Step - 9039, Loss - 0.29399192482926234, Learning Rate - 9.765625e-05, magnitude of gradient - 0.024212551128748934\n",
      "Step - 9040, Loss - 0.33073380027199645, Learning Rate - 9.765625e-05, magnitude of gradient - 0.023616404479864635\n",
      "Step - 9041, Loss - 0.2946685764728355, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04002711095780209\n",
      "Step - 9042, Loss - 0.3559750997540314, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02585163592692284\n",
      "Step - 9043, Loss - 0.38453249222191593, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04496118009509493\n",
      "Step - 9044, Loss - 0.2842295783533257, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0533513211935532\n",
      "Step - 9045, Loss - 0.3039784996838336, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02579216331619263\n",
      "Step - 9046, Loss - 0.29550938507705193, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02724542801072951\n",
      "Step - 9047, Loss - 0.3220300576696994, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01437210439229167\n",
      "Step - 9048, Loss - 0.31484337552801445, Learning Rate - 9.765625e-05, magnitude of gradient - 0.009247876641588992\n",
      "Step - 9049, Loss - 0.3270388877849008, Learning Rate - 9.765625e-05, magnitude of gradient - 0.030934162377273543\n",
      "Step - 9050, Loss - 0.26600778944089293, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08821336188006108\n",
      "Step - 9051, Loss - 0.3699979481355253, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07290103122000717\n",
      "Step - 9052, Loss - 0.2959232361413999, Learning Rate - 9.765625e-05, magnitude of gradient - 0.030906569333660667\n",
      "Step - 9053, Loss - 0.30547677293036374, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08645599875206454\n",
      "Step - 9054, Loss - 0.25790435909273435, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07270357676914158\n",
      "Step - 9055, Loss - 0.31063824291929876, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08945264557185392\n",
      "Step - 9056, Loss - 0.31397210466001246, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05481547818033106\n",
      "Step - 9057, Loss - 0.28827867181173433, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08305894912658757\n",
      "Step - 9058, Loss - 0.2874933815579722, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06664105565362519\n",
      "Step - 9059, Loss - 0.32701156275247345, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05674141242621123\n",
      "Step - 9060, Loss - 0.38742080199424245, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08282184524785882\n",
      "Step - 9061, Loss - 0.2974470007410439, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05289916415235858\n",
      "Step - 9062, Loss - 0.3585585455283715, Learning Rate - 9.765625e-05, magnitude of gradient - 0.013339303974856975\n",
      "Step - 9063, Loss - 0.29338971089124943, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06228046407396228\n",
      "Step - 9064, Loss - 0.36417643744143957, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09615930254632181\n",
      "Step - 9065, Loss - 0.3782294434252481, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06885994090854132\n",
      "Step - 9066, Loss - 0.2772046466835247, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05769680396675924\n",
      "Step - 9067, Loss - 0.30639805613331733, Learning Rate - 9.765625e-05, magnitude of gradient - 0.018601619399327106\n",
      "Step - 9068, Loss - 0.27194499906976927, Learning Rate - 9.765625e-05, magnitude of gradient - 0.013701695713985313\n",
      "Step - 9069, Loss - 0.3363687063292252, Learning Rate - 9.765625e-05, magnitude of gradient - 0.006315480424705067\n",
      "Step - 9070, Loss - 0.3552881722498466, Learning Rate - 9.765625e-05, magnitude of gradient - 0.005335089821806198\n",
      "Step - 9071, Loss - 0.36241072557045634, Learning Rate - 9.765625e-05, magnitude of gradient - 0.028654252167650437\n",
      "Step - 9072, Loss - 0.3295650649453039, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04898118796419717\n",
      "Step - 9073, Loss - 0.3703326075093789, Learning Rate - 9.765625e-05, magnitude of gradient - 0.019312040689657466\n",
      "Step - 9074, Loss - 0.2861586561304207, Learning Rate - 9.765625e-05, magnitude of gradient - 0.013648348282197504\n",
      "Step - 9075, Loss - 0.31822755360659344, Learning Rate - 9.765625e-05, magnitude of gradient - 0.039767653560664376\n",
      "Step - 9076, Loss - 0.3536984364260005, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06291672799778708\n",
      "Step - 9077, Loss - 0.3685424049889723, Learning Rate - 9.765625e-05, magnitude of gradient - 0.015048628761916583\n",
      "Step - 9078, Loss - 0.3203141706441325, Learning Rate - 9.765625e-05, magnitude of gradient - 0.011876120435893424\n",
      "Step - 9079, Loss - 0.30120627423497526, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08058334220445454\n",
      "Step - 9080, Loss - 0.3019051004978809, Learning Rate - 9.765625e-05, magnitude of gradient - 0.022940725085208422\n",
      "Step - 9081, Loss - 0.25928452856054857, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03409848857959356\n",
      "Step - 9082, Loss - 0.3397972176924317, Learning Rate - 9.765625e-05, magnitude of gradient - 0.022312039565819678\n",
      "Step - 9083, Loss - 0.2833974218526067, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03631219583201041\n",
      "Step - 9084, Loss - 0.2839659622603987, Learning Rate - 9.765625e-05, magnitude of gradient - 0.015967629393624297\n",
      "Step - 9085, Loss - 0.3656632178968318, Learning Rate - 9.765625e-05, magnitude of gradient - 0.051931196109127754\n",
      "Step - 9086, Loss - 0.2758544851212729, Learning Rate - 9.765625e-05, magnitude of gradient - 0.038746051480772765\n",
      "Step - 9087, Loss - 0.34268866723286173, Learning Rate - 9.765625e-05, magnitude of gradient - 0.025660734909477415\n",
      "Step - 9088, Loss - 0.3355489803548415, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0077814607053772866\n",
      "Step - 9089, Loss - 0.309624652247405, Learning Rate - 9.765625e-05, magnitude of gradient - 0.022834305935928504\n",
      "Step - 9090, Loss - 0.28961718218049454, Learning Rate - 9.765625e-05, magnitude of gradient - 0.030596467679008407\n",
      "Step - 9091, Loss - 0.2580805231256408, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04758787133089997\n",
      "Step - 9092, Loss - 0.3842785552070472, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03975631754773482\n",
      "Step - 9093, Loss - 0.3089109797966261, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06949542556305131\n",
      "Step - 9094, Loss - 0.3501805423748938, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03767925641471006\n",
      "Step - 9095, Loss - 0.3862310435094178, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07324217102208073\n",
      "Step - 9096, Loss - 0.3036998600353911, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04066770463988819\n",
      "Step - 9097, Loss - 0.27346693794328186, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05177772210055856\n",
      "Step - 9098, Loss - 0.3395710825898483, Learning Rate - 9.765625e-05, magnitude of gradient - 0.056434598884690455\n",
      "Step - 9099, Loss - 0.35822627029634857, Learning Rate - 9.765625e-05, magnitude of gradient - 0.010930571333240833\n",
      "Step - 9100, Loss - 0.35839214734509195, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0014798217134636034\n",
      "Step - 9101, Loss - 0.3306498383225348, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06321159548308258\n",
      "Step - 9102, Loss - 0.3211576668397068, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06368017945320671\n",
      "Step - 9103, Loss - 0.3267826095726513, Learning Rate - 9.765625e-05, magnitude of gradient - 0.128383640373133\n",
      "Step - 9104, Loss - 0.315592759399413, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02766410237422257\n",
      "Step - 9105, Loss - 0.244084723708525, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09433732410098801\n",
      "Step - 9106, Loss - 0.3354944251375825, Learning Rate - 9.765625e-05, magnitude of gradient - 0.029333955453356743\n",
      "Step - 9107, Loss - 0.35450088354997683, Learning Rate - 9.765625e-05, magnitude of gradient - 0.016927919127096323\n",
      "Step - 9108, Loss - 0.30269070591911246, Learning Rate - 9.765625e-05, magnitude of gradient - 0.014714190998671326\n",
      "Step - 9109, Loss - 0.35910979684001665, Learning Rate - 9.765625e-05, magnitude of gradient - 0.020476961616201907\n",
      "Step - 9110, Loss - 0.311330776223408, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06281093826876695\n",
      "Step - 9111, Loss - 0.35629655160712376, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02670929562672368\n",
      "Step - 9112, Loss - 0.3858329489857331, Learning Rate - 9.765625e-05, magnitude of gradient - 0.041267089640956565\n",
      "Step - 9113, Loss - 0.28492938558411424, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01430839392188606\n",
      "Step - 9114, Loss - 0.3302282679756443, Learning Rate - 9.765625e-05, magnitude of gradient - 0.025019756104508212\n",
      "Step - 9115, Loss - 0.33550557492598926, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07962423666508683\n",
      "Step - 9116, Loss - 0.301153783068005, Learning Rate - 9.765625e-05, magnitude of gradient - 0.020798675167843454\n",
      "Step - 9117, Loss - 0.26187129313177726, Learning Rate - 9.765625e-05, magnitude of gradient - 0.024170572066485593\n",
      "Step - 9118, Loss - 0.3388509485981339, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03832560706607154\n",
      "Step - 9119, Loss - 0.32929437639181297, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03953677848797882\n",
      "Step - 9120, Loss - 0.3360876991765971, Learning Rate - 9.765625e-05, magnitude of gradient - 0.10211575595686508\n",
      "Step - 9121, Loss - 0.347568334476946, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03938717199193468\n",
      "Step - 9122, Loss - 0.3164255721816443, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04103919090502577\n",
      "Step - 9123, Loss - 0.32390513260608017, Learning Rate - 9.765625e-05, magnitude of gradient - 0.028962607908680695\n",
      "Step - 9124, Loss - 0.29271980658583774, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04108796075011011\n",
      "Step - 9125, Loss - 0.4017242756406861, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03475876200121019\n",
      "Step - 9126, Loss - 0.28196759364218127, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01861261074980083\n",
      "Step - 9127, Loss - 0.3148968913876993, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09516310157311435\n",
      "Step - 9128, Loss - 0.3653946233162098, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08482306816294563\n",
      "Step - 9129, Loss - 0.2934533524161324, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07461234057992218\n",
      "Step - 9130, Loss - 0.3136585458199723, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0832379553098584\n",
      "Step - 9131, Loss - 0.35580265601022765, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06564484858397256\n",
      "Step - 9132, Loss - 0.33710160698706193, Learning Rate - 9.765625e-05, magnitude of gradient - 0.021156200434093147\n",
      "Step - 9133, Loss - 0.3418927062786618, Learning Rate - 9.765625e-05, magnitude of gradient - 0.026124654020346236\n",
      "Step - 9134, Loss - 0.2982568377804168, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0463777062237359\n",
      "Step - 9135, Loss - 0.3379465678259165, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06934437446328066\n",
      "Step - 9136, Loss - 0.33201596797766164, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08472257231556378\n",
      "Step - 9137, Loss - 0.3272092349193451, Learning Rate - 9.765625e-05, magnitude of gradient - 0.021047828173509392\n",
      "Step - 9138, Loss - 0.37013174918816955, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03397786964180088\n",
      "Step - 9139, Loss - 0.3557533037628754, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05063356101833884\n",
      "Step - 9140, Loss - 0.3290137306288426, Learning Rate - 9.765625e-05, magnitude of gradient - 0.045354065132290906\n",
      "Step - 9141, Loss - 0.33455195691866396, Learning Rate - 9.765625e-05, magnitude of gradient - 0.10510039276289533\n",
      "Step - 9142, Loss - 0.2997559042356519, Learning Rate - 9.765625e-05, magnitude of gradient - 0.017308297201194676\n",
      "Step - 9143, Loss - 0.3576118331689116, Learning Rate - 9.765625e-05, magnitude of gradient - 0.059187672499293964\n",
      "Step - 9144, Loss - 0.35315067009707657, Learning Rate - 9.765625e-05, magnitude of gradient - 0.060593967517321504\n",
      "Step - 9145, Loss - 0.34093737881010133, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06561766025080187\n",
      "Step - 9146, Loss - 0.33327686844415505, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07181811434140363\n",
      "Step - 9147, Loss - 0.27427293707493694, Learning Rate - 9.765625e-05, magnitude of gradient - 0.023531084914106278\n",
      "Step - 9148, Loss - 0.3518150395726963, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06625765468813692\n",
      "Step - 9149, Loss - 0.34575181437551544, Learning Rate - 9.765625e-05, magnitude of gradient - 0.010355558522503939\n",
      "Step - 9150, Loss - 0.39191417212264107, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04051701936426397\n",
      "Step - 9151, Loss - 0.3493182250199318, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07859466043867332\n",
      "Step - 9152, Loss - 0.36970533117734583, Learning Rate - 9.765625e-05, magnitude of gradient - 0.056777601989118916\n",
      "Step - 9153, Loss - 0.32954955565875177, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08425761163159322\n",
      "Step - 9154, Loss - 0.30468091407305165, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05521383827164843\n",
      "Step - 9155, Loss - 0.42249131390388, Learning Rate - 9.765625e-05, magnitude of gradient - 0.10163550368094848\n",
      "Step - 9156, Loss - 0.30369868137398687, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03769139183923477\n",
      "Step - 9157, Loss - 0.3246855691555851, Learning Rate - 9.765625e-05, magnitude of gradient - 0.017055602323826097\n",
      "Step - 9158, Loss - 0.3509180191135247, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04463413938936636\n",
      "Step - 9159, Loss - 0.35543084787223805, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08465954709537225\n",
      "Step - 9160, Loss - 0.32323859163072266, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07450150186000663\n",
      "Step - 9161, Loss - 0.29658762359013496, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09735145140683013\n",
      "Step - 9162, Loss - 0.2860545933962138, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0714753573695464\n",
      "Step - 9163, Loss - 0.3151519531390399, Learning Rate - 9.765625e-05, magnitude of gradient - 0.016932989431606163\n",
      "Step - 9164, Loss - 0.33612530292656895, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02635702296924174\n",
      "Step - 9165, Loss - 0.28788738060446223, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08799276124198342\n",
      "Step - 9166, Loss - 0.2804214225130945, Learning Rate - 9.765625e-05, magnitude of gradient - 0.018213685780361585\n",
      "Step - 9167, Loss - 0.2999104316379473, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04373355971699326\n",
      "Step - 9168, Loss - 0.3672398022685341, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06818477933416908\n",
      "Step - 9169, Loss - 0.32741716580057356, Learning Rate - 9.765625e-05, magnitude of gradient - 0.014071087071662366\n",
      "Step - 9170, Loss - 0.2885524402056159, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04378851260234811\n",
      "Step - 9171, Loss - 0.27937622212438307, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03403607548923888\n",
      "Step - 9172, Loss - 0.2566690457977787, Learning Rate - 9.765625e-05, magnitude of gradient - 0.10051716385343701\n",
      "Step - 9173, Loss - 0.33060062036272836, Learning Rate - 9.765625e-05, magnitude of gradient - 0.041221452513843335\n",
      "Step - 9174, Loss - 0.2720448272166051, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0700459716889722\n",
      "Step - 9175, Loss - 0.35361166604111827, Learning Rate - 9.765625e-05, magnitude of gradient - 0.027441227885460103\n",
      "Step - 9176, Loss - 0.35317945630034264, Learning Rate - 9.765625e-05, magnitude of gradient - 0.024868364049763846\n",
      "Step - 9177, Loss - 0.34806566658221716, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04988330820488444\n",
      "Step - 9178, Loss - 0.3069911406876459, Learning Rate - 9.765625e-05, magnitude of gradient - 0.022769193313250707\n",
      "Step - 9179, Loss - 0.2688903062896013, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04041009534904947\n",
      "Step - 9180, Loss - 0.35165707854054495, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0766176687316041\n",
      "Step - 9181, Loss - 0.3249021983238164, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0661322228857172\n",
      "Step - 9182, Loss - 0.4237042740689292, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03781817345488804\n",
      "Step - 9183, Loss - 0.299940772005421, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07069339655389935\n",
      "Step - 9184, Loss - 0.3311596064068139, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03725237516407257\n",
      "Step - 9185, Loss - 0.29736490033973856, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04438680089003171\n",
      "Step - 9186, Loss - 0.35299307858158335, Learning Rate - 9.765625e-05, magnitude of gradient - 0.060498736433939555\n",
      "Step - 9187, Loss - 0.35176177297951017, Learning Rate - 9.765625e-05, magnitude of gradient - 0.049519533275615805\n",
      "Step - 9188, Loss - 0.2778427618311314, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02540557092951254\n",
      "Step - 9189, Loss - 0.32002107507317223, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08848298536704775\n",
      "Step - 9190, Loss - 0.2985672850975022, Learning Rate - 9.765625e-05, magnitude of gradient - 0.006855837498380999\n",
      "Step - 9191, Loss - 0.3168647515537667, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05945416353002727\n",
      "Step - 9192, Loss - 0.36657555051802637, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06063733841438872\n",
      "Step - 9193, Loss - 0.3102256642325, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04322140791257632\n",
      "Step - 9194, Loss - 0.34882977280888283, Learning Rate - 9.765625e-05, magnitude of gradient - 0.017406505466977317\n",
      "Step - 9195, Loss - 0.2875587432372644, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02913343001506101\n",
      "Step - 9196, Loss - 0.41228916211412026, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08381213274371857\n",
      "Step - 9197, Loss - 0.2998923650060733, Learning Rate - 9.765625e-05, magnitude of gradient - 0.1062473953900209\n",
      "Step - 9198, Loss - 0.3501499270652997, Learning Rate - 9.765625e-05, magnitude of gradient - 0.061066261660013096\n",
      "Step - 9199, Loss - 0.3173023354771337, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04156021471426767\n",
      "Step - 9200, Loss - 0.2855586655810476, Learning Rate - 9.765625e-05, magnitude of gradient - 0.061418841953937424\n",
      "Step - 9201, Loss - 0.36163923092106787, Learning Rate - 9.765625e-05, magnitude of gradient - 0.024585756127768373\n",
      "Step - 9202, Loss - 0.34397301029221217, Learning Rate - 9.765625e-05, magnitude of gradient - 0.018612926966623436\n",
      "Step - 9203, Loss - 0.31539125645684823, Learning Rate - 9.765625e-05, magnitude of gradient - 0.038307586946055766\n",
      "Step - 9204, Loss - 0.2667375157022125, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02245516729006343\n",
      "Step - 9205, Loss - 0.28616762465541246, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03279928196650696\n",
      "Step - 9206, Loss - 0.33659843380005283, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05434196363115961\n",
      "Step - 9207, Loss - 0.28871446882003804, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05417535150886769\n",
      "Step - 9208, Loss - 0.3085740705625194, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03848008529124858\n",
      "Step - 9209, Loss - 0.3412200792366388, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08488971791941166\n",
      "Step - 9210, Loss - 0.29124974197556797, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04495485818337069\n",
      "Step - 9211, Loss - 0.29460704150393535, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09052771436369958\n",
      "Step - 9212, Loss - 0.31327529904605056, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02933439128526361\n",
      "Step - 9213, Loss - 0.3998558188085377, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04264711855916494\n",
      "Step - 9214, Loss - 0.21701621815921568, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02669935568330788\n",
      "Step - 9215, Loss - 0.3356141376757434, Learning Rate - 9.765625e-05, magnitude of gradient - 0.052174648904030084\n",
      "Step - 9216, Loss - 0.2982781285888602, Learning Rate - 9.765625e-05, magnitude of gradient - 0.011993142529833226\n",
      "Step - 9217, Loss - 0.24323518088222934, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0527078601914294\n",
      "Step - 9218, Loss - 0.3547313021105642, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03123644732462876\n",
      "Step - 9219, Loss - 0.22841230476034985, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0486061858202464\n",
      "Step - 9220, Loss - 0.23396385821153742, Learning Rate - 9.765625e-05, magnitude of gradient - 0.021205770481129864\n",
      "Step - 9221, Loss - 0.34009561480868133, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06669658310053811\n",
      "Step - 9222, Loss - 0.3713016447428339, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08414047737910645\n",
      "Step - 9223, Loss - 0.30051524243290917, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09415027307198383\n",
      "Step - 9224, Loss - 0.27129098543731434, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06074450065777226\n",
      "Step - 9225, Loss - 0.26002402919645706, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06918747059676796\n",
      "Step - 9226, Loss - 0.34227363302115477, Learning Rate - 9.765625e-05, magnitude of gradient - 0.036365414864242544\n",
      "Step - 9227, Loss - 0.3594696389227646, Learning Rate - 9.765625e-05, magnitude of gradient - 0.019988865465663523\n",
      "Step - 9228, Loss - 0.29822897493665945, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03241415485837047\n",
      "Step - 9229, Loss - 0.27601977686131807, Learning Rate - 9.765625e-05, magnitude of gradient - 0.059779841244053555\n",
      "Step - 9230, Loss - 0.32657768908103135, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04837629109861118\n",
      "Step - 9231, Loss - 0.31975680539461865, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06926129021670153\n",
      "Step - 9232, Loss - 0.36033847744195163, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09683624151253058\n",
      "Step - 9233, Loss - 0.32784649258002957, Learning Rate - 9.765625e-05, magnitude of gradient - 0.016516407766672344\n",
      "Step - 9234, Loss - 0.33709772645313096, Learning Rate - 9.765625e-05, magnitude of gradient - 0.008360169333250862\n",
      "Step - 9235, Loss - 0.32061591971574543, Learning Rate - 9.765625e-05, magnitude of gradient - 0.10260730299085165\n",
      "Step - 9236, Loss - 0.3198493787990849, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08289687044803731\n",
      "Step - 9237, Loss - 0.33321298033433744, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08330121778907612\n",
      "Step - 9238, Loss - 0.33796193349557, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05942551103998342\n",
      "Step - 9239, Loss - 0.3665674715466234, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02038225191055626\n",
      "Step - 9240, Loss - 0.3909406954699349, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05550226089202454\n",
      "Step - 9241, Loss - 0.34735575952679343, Learning Rate - 9.765625e-05, magnitude of gradient - 0.013860179952239107\n",
      "Step - 9242, Loss - 0.32652968642692215, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05485160016300528\n",
      "Step - 9243, Loss - 0.3671072725217491, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05944059238031932\n",
      "Step - 9244, Loss - 0.3369083612817919, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06906459106974326\n",
      "Step - 9245, Loss - 0.2848509052126915, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06401608514709448\n",
      "Step - 9246, Loss - 0.30655788833194275, Learning Rate - 9.765625e-05, magnitude of gradient - 0.028677797328131287\n",
      "Step - 9247, Loss - 0.35123237327063317, Learning Rate - 9.765625e-05, magnitude of gradient - 0.040731657407798105\n",
      "Step - 9248, Loss - 0.3429969090814806, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03270113815684487\n",
      "Step - 9249, Loss - 0.2594008430876771, Learning Rate - 9.765625e-05, magnitude of gradient - 0.11650750187145142\n",
      "Step - 9250, Loss - 0.33989171428472387, Learning Rate - 9.765625e-05, magnitude of gradient - 0.013275707933485608\n",
      "Step - 9251, Loss - 0.30629446962862394, Learning Rate - 9.765625e-05, magnitude of gradient - 0.030155334685818644\n",
      "Step - 9252, Loss - 0.2865830410749033, Learning Rate - 9.765625e-05, magnitude of gradient - 0.11716605389237997\n",
      "Step - 9253, Loss - 0.304078521300435, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04188412906328244\n",
      "Step - 9254, Loss - 0.28729165811583424, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09728132135788937\n",
      "Step - 9255, Loss - 0.3931575927435778, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02379686829939653\n",
      "Step - 9256, Loss - 0.32211293234926863, Learning Rate - 9.765625e-05, magnitude of gradient - 0.10909933705099496\n",
      "Step - 9257, Loss - 0.31239388881225305, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0613704169996985\n",
      "Step - 9258, Loss - 0.3386497374075347, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04363552148648465\n",
      "Step - 9259, Loss - 0.37387136489919587, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05897494670685316\n",
      "Step - 9260, Loss - 0.3772980349158112, Learning Rate - 9.765625e-05, magnitude of gradient - 0.042908691635696834\n",
      "Step - 9261, Loss - 0.4437912561343004, Learning Rate - 9.765625e-05, magnitude of gradient - 0.028130801621166594\n",
      "Step - 9262, Loss - 0.27035495192417797, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05641052777247269\n",
      "Step - 9263, Loss - 0.2889311533980049, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0257080612314687\n",
      "Step - 9264, Loss - 0.29365404268512757, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0728266398278335\n",
      "Step - 9265, Loss - 0.33272853154025545, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06349271820675732\n",
      "Step - 9266, Loss - 0.29735383946106386, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07416882878664033\n",
      "Step - 9267, Loss - 0.3233082041574322, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04046988122312768\n",
      "Step - 9268, Loss - 0.3576035413799742, Learning Rate - 9.765625e-05, magnitude of gradient - 0.012808544807786797\n",
      "Step - 9269, Loss - 0.31693095754983935, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02526052701910456\n",
      "Step - 9270, Loss - 0.31182279241933486, Learning Rate - 9.765625e-05, magnitude of gradient - 0.024030152068351036\n",
      "Step - 9271, Loss - 0.32931936678964635, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06330252030915137\n",
      "Step - 9272, Loss - 0.30329355670615377, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09347624612385477\n",
      "Step - 9273, Loss - 0.2633344271081317, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05276013078446962\n",
      "Step - 9274, Loss - 0.3247078693060041, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06121676468320587\n",
      "Step - 9275, Loss - 0.3110643876636788, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04355538242165903\n",
      "Step - 9276, Loss - 0.31651097535837314, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06478756378789004\n",
      "Step - 9277, Loss - 0.30573296646756754, Learning Rate - 9.765625e-05, magnitude of gradient - 0.044177634701107214\n",
      "Step - 9278, Loss - 0.3084149459969808, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04798233030333699\n",
      "Step - 9279, Loss - 0.33304784895005224, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08266580318110348\n",
      "Step - 9280, Loss - 0.3785962736350164, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02220824757942059\n",
      "Step - 9281, Loss - 0.27713573806738157, Learning Rate - 9.765625e-05, magnitude of gradient - 0.016907742662803673\n",
      "Step - 9282, Loss - 0.3175030377208681, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07541670120614755\n",
      "Step - 9283, Loss - 0.2610455987799334, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03720490317846628\n",
      "Step - 9284, Loss - 0.27137633448253995, Learning Rate - 9.765625e-05, magnitude of gradient - 0.024544546348422472\n",
      "Step - 9285, Loss - 0.38078727078496943, Learning Rate - 9.765625e-05, magnitude of gradient - 0.003275422553941304\n",
      "Step - 9286, Loss - 0.2583167590771647, Learning Rate - 9.765625e-05, magnitude of gradient - 0.060750523587259526\n",
      "Step - 9287, Loss - 0.3734273971896488, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06452520288902107\n",
      "Step - 9288, Loss - 0.3584862360705185, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09043302651027206\n",
      "Step - 9289, Loss - 0.32640974332768724, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03362277289062273\n",
      "Step - 9290, Loss - 0.2945068436378658, Learning Rate - 9.765625e-05, magnitude of gradient - 0.027961135222831068\n",
      "Step - 9291, Loss - 0.30095261192526157, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05197774332512191\n",
      "Step - 9292, Loss - 0.2971654292599486, Learning Rate - 9.765625e-05, magnitude of gradient - 0.048904670943547564\n",
      "Step - 9293, Loss - 0.3035142131404218, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08152323245367472\n",
      "Step - 9294, Loss - 0.28981662098820404, Learning Rate - 9.765625e-05, magnitude of gradient - 0.058103762011744386\n",
      "Step - 9295, Loss - 0.3819062257260251, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03813212312543269\n",
      "Step - 9296, Loss - 0.3517689600134225, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05329180421318874\n",
      "Step - 9297, Loss - 0.370670094584574, Learning Rate - 9.765625e-05, magnitude of gradient - 0.010630908483853523\n",
      "Step - 9298, Loss - 0.3748379426879471, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03605849466297811\n",
      "Step - 9299, Loss - 0.34971857993738653, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04196382593923943\n",
      "Step - 9300, Loss - 0.30828473604912365, Learning Rate - 9.765625e-05, magnitude of gradient - 0.029994125736942382\n",
      "Step - 9301, Loss - 0.32889599006480663, Learning Rate - 9.765625e-05, magnitude of gradient - 0.12517321094695036\n",
      "Step - 9302, Loss - 0.32204701311572076, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03834947781543337\n",
      "Step - 9303, Loss - 0.3316732690888575, Learning Rate - 9.765625e-05, magnitude of gradient - 0.020800288095602592\n",
      "Step - 9304, Loss - 0.3268646662532168, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07403589055363713\n",
      "Step - 9305, Loss - 0.34623117923370805, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05351456062817473\n",
      "Step - 9306, Loss - 0.2821279770182298, Learning Rate - 9.765625e-05, magnitude of gradient - 0.056966672712167425\n",
      "Step - 9307, Loss - 0.3573515436090182, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07512361554924835\n",
      "Step - 9308, Loss - 0.35377978784634856, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03435535408891562\n",
      "Step - 9309, Loss - 0.30525398347451344, Learning Rate - 9.765625e-05, magnitude of gradient - 0.011495492535781643\n",
      "Step - 9310, Loss - 0.3621642332653403, Learning Rate - 9.765625e-05, magnitude of gradient - 0.060410412964156425\n",
      "Step - 9311, Loss - 0.24566185517831574, Learning Rate - 9.765625e-05, magnitude of gradient - 0.018465706493812024\n",
      "Step - 9312, Loss - 0.39096433504424455, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07052615465967073\n",
      "Step - 9313, Loss - 0.43651771555232166, Learning Rate - 9.765625e-05, magnitude of gradient - 0.11564273893679373\n",
      "Step - 9314, Loss - 0.26809748045241577, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06662571005410217\n",
      "Step - 9315, Loss - 0.32971083566816056, Learning Rate - 9.765625e-05, magnitude of gradient - 0.011206155532122498\n",
      "Step - 9316, Loss - 0.337929078712595, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07666899716164403\n",
      "Step - 9317, Loss - 0.3308765961328358, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04365046687898585\n",
      "Step - 9318, Loss - 0.28877148699116617, Learning Rate - 9.765625e-05, magnitude of gradient - 0.10131843038187183\n",
      "Step - 9319, Loss - 0.3153836788000947, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03999002328260746\n",
      "Step - 9320, Loss - 0.26294726740989927, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04702955982004787\n",
      "Step - 9321, Loss - 0.3109653870718729, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03694544484926224\n",
      "Step - 9322, Loss - 0.33984529045548784, Learning Rate - 9.765625e-05, magnitude of gradient - 0.040346099431136005\n",
      "Step - 9323, Loss - 0.34059503552549714, Learning Rate - 9.765625e-05, magnitude of gradient - 0.037837922549639566\n",
      "Step - 9324, Loss - 0.34086654711647546, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05281587739314629\n",
      "Step - 9325, Loss - 0.3197361191070539, Learning Rate - 9.765625e-05, magnitude of gradient - 0.033493570649096006\n",
      "Step - 9326, Loss - 0.3198288612871206, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03741894676578794\n",
      "Step - 9327, Loss - 0.32813157644364055, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0029070958194677963\n",
      "Step - 9328, Loss - 0.3096389454031688, Learning Rate - 9.765625e-05, magnitude of gradient - 0.025430170118061313\n",
      "Step - 9329, Loss - 0.31912578400962927, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03565750128314132\n",
      "Step - 9330, Loss - 0.3348552949591399, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04931335703541858\n",
      "Step - 9331, Loss - 0.22503258620670796, Learning Rate - 9.765625e-05, magnitude of gradient - 0.029442143851827952\n",
      "Step - 9332, Loss - 0.24122611292835305, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05220782263765395\n",
      "Step - 9333, Loss - 0.39984556775102936, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03363002408806421\n",
      "Step - 9334, Loss - 0.29418661863032713, Learning Rate - 9.765625e-05, magnitude of gradient - 0.059332712736571855\n",
      "Step - 9335, Loss - 0.27476213466296456, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08033681086818992\n",
      "Step - 9336, Loss - 0.29721471536853805, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04086991213448661\n",
      "Step - 9337, Loss - 0.31700632339576545, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03167631360937475\n",
      "Step - 9338, Loss - 0.3542728511600991, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03149365170068377\n",
      "Step - 9339, Loss - 0.2871538173086992, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04412098223387805\n",
      "Step - 9340, Loss - 0.33609802686000434, Learning Rate - 9.765625e-05, magnitude of gradient - 0.012586782679916203\n",
      "Step - 9341, Loss - 0.2943211753373376, Learning Rate - 9.765625e-05, magnitude of gradient - 0.026486362695882948\n",
      "Step - 9342, Loss - 0.24692113991903156, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06638743753298382\n",
      "Step - 9343, Loss - 0.27876755208345505, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07955065794552071\n",
      "Step - 9344, Loss - 0.3390687082362233, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0007343635056610049\n",
      "Step - 9345, Loss - 0.32032828193730645, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03461214417742092\n",
      "Step - 9346, Loss - 0.31165268062701845, Learning Rate - 9.765625e-05, magnitude of gradient - 0.10193440797348235\n",
      "Step - 9347, Loss - 0.3223200307412072, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01869053529558324\n",
      "Step - 9348, Loss - 0.3462160487858472, Learning Rate - 9.765625e-05, magnitude of gradient - 0.004460483185304003\n",
      "Step - 9349, Loss - 0.3054957205826924, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04636592065759407\n",
      "Step - 9350, Loss - 0.2616201646413482, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03681579182670705\n",
      "Step - 9351, Loss - 0.2704625169983645, Learning Rate - 9.765625e-05, magnitude of gradient - 0.026933148071707786\n",
      "Step - 9352, Loss - 0.29847104718937756, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06460186565111864\n",
      "Step - 9353, Loss - 0.3531825302723766, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07493842496435456\n",
      "Step - 9354, Loss - 0.34643791670299523, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0494350699809956\n",
      "Step - 9355, Loss - 0.3542718725018308, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07827514675477155\n",
      "Step - 9356, Loss - 0.28348057701432455, Learning Rate - 9.765625e-05, magnitude of gradient - 0.056214249289813535\n",
      "Step - 9357, Loss - 0.27371986231903844, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03916104094584175\n",
      "Step - 9358, Loss - 0.28482287639340503, Learning Rate - 9.765625e-05, magnitude of gradient - 0.009927447074475302\n",
      "Step - 9359, Loss - 0.35884557363740205, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03196167691462072\n",
      "Step - 9360, Loss - 0.25000507146204687, Learning Rate - 9.765625e-05, magnitude of gradient - 0.016176891746796075\n",
      "Step - 9361, Loss - 0.3143561861565769, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08755072706928305\n",
      "Step - 9362, Loss - 0.36121088046294414, Learning Rate - 9.765625e-05, magnitude of gradient - 0.043299466081336065\n",
      "Step - 9363, Loss - 0.3421808916862094, Learning Rate - 9.765625e-05, magnitude of gradient - 0.10028756726953943\n",
      "Step - 9364, Loss - 0.3486439552329432, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04373372751713918\n",
      "Step - 9365, Loss - 0.252383170536253, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06477787270537923\n",
      "Step - 9366, Loss - 0.2781334808989443, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04107769218500712\n",
      "Step - 9367, Loss - 0.33306464515362266, Learning Rate - 9.765625e-05, magnitude of gradient - 0.030768758408869577\n",
      "Step - 9368, Loss - 0.2401355355333406, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08347027724358835\n",
      "Step - 9369, Loss - 0.2884378939839798, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03109769701765795\n",
      "Step - 9370, Loss - 0.2958781184279251, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04316962459683045\n",
      "Step - 9371, Loss - 0.35160804602442053, Learning Rate - 9.765625e-05, magnitude of gradient - 0.045458751451993684\n",
      "Step - 9372, Loss - 0.24903046371842336, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07900513036068452\n",
      "Step - 9373, Loss - 0.2828905513330392, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05613633168291768\n",
      "Step - 9374, Loss - 0.3473061824905024, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06265052420355463\n",
      "Step - 9375, Loss - 0.3550012946457963, Learning Rate - 9.765625e-05, magnitude of gradient - 0.10340118347362444\n",
      "Step - 9376, Loss - 0.3388397683018891, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07828349617130112\n",
      "Step - 9377, Loss - 0.36172055594498603, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03792345113268714\n",
      "Step - 9378, Loss - 0.3089468218643948, Learning Rate - 9.765625e-05, magnitude of gradient - 0.039486911917278676\n",
      "Step - 9379, Loss - 0.3755412402647728, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06206456719744234\n",
      "Step - 9380, Loss - 0.2808708520292883, Learning Rate - 9.765625e-05, magnitude of gradient - 0.049481970009612776\n",
      "Step - 9381, Loss - 0.3176397569980271, Learning Rate - 9.765625e-05, magnitude of gradient - 0.017704800376502697\n",
      "Step - 9382, Loss - 0.343413585693261, Learning Rate - 9.765625e-05, magnitude of gradient - 0.004338814259406663\n",
      "Step - 9383, Loss - 0.3109684642214085, Learning Rate - 9.765625e-05, magnitude of gradient - 0.042990957133061024\n",
      "Step - 9384, Loss - 0.34717260702895253, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06453537383252855\n",
      "Step - 9385, Loss - 0.280422986379514, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09645844308095539\n",
      "Step - 9386, Loss - 0.2657827034503628, Learning Rate - 9.765625e-05, magnitude of gradient - 0.008334334457997515\n",
      "Step - 9387, Loss - 0.34476532679138633, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03755141031052758\n",
      "Step - 9388, Loss - 0.2673505384913067, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03271989236845781\n",
      "Step - 9389, Loss - 0.30676720511678013, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03850979405625312\n",
      "Step - 9390, Loss - 0.2992372384441736, Learning Rate - 9.765625e-05, magnitude of gradient - 0.059486064809675714\n",
      "Step - 9391, Loss - 0.301302392907649, Learning Rate - 9.765625e-05, magnitude of gradient - 0.039216188728685014\n",
      "Step - 9392, Loss - 0.3181108774890965, Learning Rate - 9.765625e-05, magnitude of gradient - 0.032269101591776476\n",
      "Step - 9393, Loss - 0.2511055962366099, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02467721485005344\n",
      "Step - 9394, Loss - 0.32075205153061126, Learning Rate - 9.765625e-05, magnitude of gradient - 0.057385717839349704\n",
      "Step - 9395, Loss - 0.29337742718173737, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0806705504290313\n",
      "Step - 9396, Loss - 0.3762719610705819, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02460709054938223\n",
      "Step - 9397, Loss - 0.39777784865100774, Learning Rate - 9.765625e-05, magnitude of gradient - 0.025300043800426116\n",
      "Step - 9398, Loss - 0.2510228565671225, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05091492748831608\n",
      "Step - 9399, Loss - 0.358339850179344, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0320801200964857\n",
      "Step - 9400, Loss - 0.3499953704608584, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06279364341045102\n",
      "Step - 9401, Loss - 0.3353140504475266, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06237187094182786\n",
      "Step - 9402, Loss - 0.32580742490838965, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07921571807183894\n",
      "Step - 9403, Loss - 0.22684232192164627, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0354122794388773\n",
      "Step - 9404, Loss - 0.31775411308243867, Learning Rate - 9.765625e-05, magnitude of gradient - 0.044571517223956296\n",
      "Step - 9405, Loss - 0.31025308945207775, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03744683250445223\n",
      "Step - 9406, Loss - 0.34294498410812696, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06412006496314333\n",
      "Step - 9407, Loss - 0.34100318399945856, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04342786216892886\n",
      "Step - 9408, Loss - 0.33326140245035163, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01600229645018721\n",
      "Step - 9409, Loss - 0.31720326433503887, Learning Rate - 9.765625e-05, magnitude of gradient - 0.028616853070491624\n",
      "Step - 9410, Loss - 0.2885492816072267, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03021072249506271\n",
      "Step - 9411, Loss - 0.2860232255442776, Learning Rate - 9.765625e-05, magnitude of gradient - 0.084920285717144\n",
      "Step - 9412, Loss - 0.3176820615987788, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0765046642148944\n",
      "Step - 9413, Loss - 0.33387253581721504, Learning Rate - 9.765625e-05, magnitude of gradient - 0.016590692027007328\n",
      "Step - 9414, Loss - 0.37923420197283275, Learning Rate - 9.765625e-05, magnitude of gradient - 0.061689434896918686\n",
      "Step - 9415, Loss - 0.2993418675998486, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05496742941898604\n",
      "Step - 9416, Loss - 0.387813386833925, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01056576134963416\n",
      "Step - 9417, Loss - 0.2895004584882401, Learning Rate - 9.765625e-05, magnitude of gradient - 0.023150324575447193\n",
      "Step - 9418, Loss - 0.28356885461827064, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05527793152387315\n",
      "Step - 9419, Loss - 0.30450652398599976, Learning Rate - 9.765625e-05, magnitude of gradient - 0.021497907816269722\n",
      "Step - 9420, Loss - 0.3300365381102391, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07429912213386364\n",
      "Step - 9421, Loss - 0.3083296042905381, Learning Rate - 9.765625e-05, magnitude of gradient - 0.026303698562154514\n",
      "Step - 9422, Loss - 0.3107534251473819, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04582315721117875\n",
      "Step - 9423, Loss - 0.3244242735026222, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08016187232181854\n",
      "Step - 9424, Loss - 0.33162006240931624, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07921516340770234\n",
      "Step - 9425, Loss - 0.279191755351919, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09001775776991108\n",
      "Step - 9426, Loss - 0.3368284687612444, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06748385639955468\n",
      "Step - 9427, Loss - 0.35367115426944973, Learning Rate - 9.765625e-05, magnitude of gradient - 0.031712403138893744\n",
      "Step - 9428, Loss - 0.32083102664063884, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03656193860460331\n",
      "Step - 9429, Loss - 0.31546463438733297, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05848657034133757\n",
      "Step - 9430, Loss - 0.40022007799970083, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0566445869928831\n",
      "Step - 9431, Loss - 0.32241238854262066, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05586369225139056\n",
      "Step - 9432, Loss - 0.26195878392178484, Learning Rate - 9.765625e-05, magnitude of gradient - 0.059860807331857654\n",
      "Step - 9433, Loss - 0.33593906041263827, Learning Rate - 9.765625e-05, magnitude of gradient - 0.057638865683406586\n",
      "Step - 9434, Loss - 0.3024331290197618, Learning Rate - 9.765625e-05, magnitude of gradient - 0.028319324672018607\n",
      "Step - 9435, Loss - 0.3717994965558536, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0693086317265856\n",
      "Step - 9436, Loss - 0.38126255777633467, Learning Rate - 9.765625e-05, magnitude of gradient - 0.046683014337835366\n",
      "Step - 9437, Loss - 0.2920729398496355, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07807774857304059\n",
      "Step - 9438, Loss - 0.3075654350435449, Learning Rate - 9.765625e-05, magnitude of gradient - 0.019766158145938798\n",
      "Step - 9439, Loss - 0.34847550677438577, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03390270558446895\n",
      "Step - 9440, Loss - 0.2832033375306585, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04056445967866772\n",
      "Step - 9441, Loss - 0.27909355409406067, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08654721845463025\n",
      "Step - 9442, Loss - 0.3331860274490589, Learning Rate - 9.765625e-05, magnitude of gradient - 0.043454417601285725\n",
      "Step - 9443, Loss - 0.328580482451204, Learning Rate - 9.765625e-05, magnitude of gradient - 0.014993606105108451\n",
      "Step - 9444, Loss - 0.34088219682398085, Learning Rate - 9.765625e-05, magnitude of gradient - 0.11997540482063625\n",
      "Step - 9445, Loss - 0.32170815017796556, Learning Rate - 9.765625e-05, magnitude of gradient - 0.026976347358703797\n",
      "Step - 9446, Loss - 0.37525094477110715, Learning Rate - 9.765625e-05, magnitude of gradient - 0.027872529147381114\n",
      "Step - 9447, Loss - 0.284095560336746, Learning Rate - 9.765625e-05, magnitude of gradient - 0.049626346672442485\n",
      "Step - 9448, Loss - 0.3069170496400726, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0086464422402918\n",
      "Step - 9449, Loss - 0.3766766579446693, Learning Rate - 9.765625e-05, magnitude of gradient - 0.032861366400917205\n",
      "Step - 9450, Loss - 0.3853081081801525, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0187797486437086\n",
      "Step - 9451, Loss - 0.28426195621371175, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07464656392506358\n",
      "Step - 9452, Loss - 0.226375256476062, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0494176921518782\n",
      "Step - 9453, Loss - 0.29262891686749176, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06768479913792523\n",
      "Step - 9454, Loss - 0.3273166818007011, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05280547936667233\n",
      "Step - 9455, Loss - 0.2772188239474477, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0699987201168731\n",
      "Step - 9456, Loss - 0.3054961217763685, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03819723453739152\n",
      "Step - 9457, Loss - 0.3351469861883276, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03442652521451989\n",
      "Step - 9458, Loss - 0.3394117832257916, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01546595755323521\n",
      "Step - 9459, Loss - 0.3436898630367282, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06665006688361765\n",
      "Step - 9460, Loss - 0.3552949185862098, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09483824742267931\n",
      "Step - 9461, Loss - 0.3541826981527922, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03927967280725687\n",
      "Step - 9462, Loss - 0.2869038572797983, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02455085830458903\n",
      "Step - 9463, Loss - 0.3112642850869567, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03371706339811164\n",
      "Step - 9464, Loss - 0.3606851023317108, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03671671254049493\n",
      "Step - 9465, Loss - 0.2849036600990193, Learning Rate - 9.765625e-05, magnitude of gradient - 0.048127917491080634\n",
      "Step - 9466, Loss - 0.3483232335993283, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07760057935503395\n",
      "Step - 9467, Loss - 0.3189974596837607, Learning Rate - 9.765625e-05, magnitude of gradient - 0.057994641915725335\n",
      "Step - 9468, Loss - 0.3344826766956779, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06590060423464382\n",
      "Step - 9469, Loss - 0.3350311046076962, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04329550902251287\n",
      "Step - 9470, Loss - 0.35069425516594993, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02244134394367182\n",
      "Step - 9471, Loss - 0.3100509598802381, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03998751928066006\n",
      "Step - 9472, Loss - 0.3868576374691074, Learning Rate - 9.765625e-05, magnitude of gradient - 0.024466526489836535\n",
      "Step - 9473, Loss - 0.4063006776637395, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01646602646646573\n",
      "Step - 9474, Loss - 0.31426302906500614, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04149745541947681\n",
      "Step - 9475, Loss - 0.34244205956361473, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04287787472091338\n",
      "Step - 9476, Loss - 0.30388538149823885, Learning Rate - 9.765625e-05, magnitude of gradient - 0.040138167931200446\n",
      "Step - 9477, Loss - 0.28533437772783804, Learning Rate - 9.765625e-05, magnitude of gradient - 0.035033505148384554\n",
      "Step - 9478, Loss - 0.3715560420131522, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06542791180587247\n",
      "Step - 9479, Loss - 0.3259953213697074, Learning Rate - 9.765625e-05, magnitude of gradient - 0.018900941137817963\n",
      "Step - 9480, Loss - 0.28730448695290434, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03509591642734644\n",
      "Step - 9481, Loss - 0.3041596936729075, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07269767619774281\n",
      "Step - 9482, Loss - 0.28757053397058135, Learning Rate - 9.765625e-05, magnitude of gradient - 0.008732950532233982\n",
      "Step - 9483, Loss - 0.3114479423775369, Learning Rate - 9.765625e-05, magnitude of gradient - 0.045049023984374714\n",
      "Step - 9484, Loss - 0.34095590840758006, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0350448215300945\n",
      "Step - 9485, Loss - 0.3892155619943394, Learning Rate - 9.765625e-05, magnitude of gradient - 0.036142176767962816\n",
      "Step - 9486, Loss - 0.2851331022787411, Learning Rate - 9.765625e-05, magnitude of gradient - 0.041365941165718005\n",
      "Step - 9487, Loss - 0.27192641202429446, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03280662187226085\n",
      "Step - 9488, Loss - 0.2859815792162596, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07356787662362416\n",
      "Step - 9489, Loss - 0.34335484268059313, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05647191121123509\n",
      "Step - 9490, Loss - 0.3813191220851277, Learning Rate - 9.765625e-05, magnitude of gradient - 0.013115177317798064\n",
      "Step - 9491, Loss - 0.26978006168848423, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04489111257720585\n",
      "Step - 9492, Loss - 0.32562137919746104, Learning Rate - 9.765625e-05, magnitude of gradient - 0.029973886913993778\n",
      "Step - 9493, Loss - 0.27961657556001657, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05563794285538332\n",
      "Step - 9494, Loss - 0.263314135698676, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03515517155205849\n",
      "Step - 9495, Loss - 0.3081632856374902, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0686744129980672\n",
      "Step - 9496, Loss - 0.3726717335749956, Learning Rate - 9.765625e-05, magnitude of gradient - 0.026422542313509764\n",
      "Step - 9497, Loss - 0.3041590004691107, Learning Rate - 9.765625e-05, magnitude of gradient - 0.059585918958749574\n",
      "Step - 9498, Loss - 0.3035480460753992, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09360115140207023\n",
      "Step - 9499, Loss - 0.30726148659579394, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08777543622382211\n",
      "Step - 9500, Loss - 0.30766727733220856, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0646083403483638\n",
      "Step - 9501, Loss - 0.36756835477674293, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04587322159379332\n",
      "Step - 9502, Loss - 0.28721911742315664, Learning Rate - 9.765625e-05, magnitude of gradient - 0.073980352529973\n",
      "Step - 9503, Loss - 0.30352366833789535, Learning Rate - 9.765625e-05, magnitude of gradient - 0.032030217692232396\n",
      "Step - 9504, Loss - 0.27293208083627696, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08016374346235518\n",
      "Step - 9505, Loss - 0.28575316066277456, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04737337131176514\n",
      "Step - 9506, Loss - 0.2930727676619108, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01619654092531075\n",
      "Step - 9507, Loss - 0.27548783981616004, Learning Rate - 9.765625e-05, magnitude of gradient - 0.023471783858737726\n",
      "Step - 9508, Loss - 0.3038446427723026, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04101754511407764\n",
      "Step - 9509, Loss - 0.3036660374790248, Learning Rate - 9.765625e-05, magnitude of gradient - 0.010746134378187607\n",
      "Step - 9510, Loss - 0.2755403310366793, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04473994966068963\n",
      "Step - 9511, Loss - 0.3144207202064166, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03005058082149138\n",
      "Step - 9512, Loss - 0.39629879419329217, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03836622259863181\n",
      "Step - 9513, Loss - 0.2911966241167735, Learning Rate - 9.765625e-05, magnitude of gradient - 0.042141830198638104\n",
      "Step - 9514, Loss - 0.2925069486884689, Learning Rate - 9.765625e-05, magnitude of gradient - 0.052873567570828416\n",
      "Step - 9515, Loss - 0.35536674680440294, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08462592874886304\n",
      "Step - 9516, Loss - 0.34329632732571597, Learning Rate - 9.765625e-05, magnitude of gradient - 0.030934283330742208\n",
      "Step - 9517, Loss - 0.2977311392763265, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01851796115725119\n",
      "Step - 9518, Loss - 0.3240280881162457, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05286916856118712\n",
      "Step - 9519, Loss - 0.3445726623725477, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04913874726000211\n",
      "Step - 9520, Loss - 0.2632770347144172, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03593671577911771\n",
      "Step - 9521, Loss - 0.3014511808531888, Learning Rate - 9.765625e-05, magnitude of gradient - 0.062327385206525095\n",
      "Step - 9522, Loss - 0.32004030518622123, Learning Rate - 9.765625e-05, magnitude of gradient - 0.028612299927437027\n",
      "Step - 9523, Loss - 0.3625337862262254, Learning Rate - 9.765625e-05, magnitude of gradient - 0.12442811745365401\n",
      "Step - 9524, Loss - 0.3615059513556145, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04256547137802023\n",
      "Step - 9525, Loss - 0.3618062662714688, Learning Rate - 9.765625e-05, magnitude of gradient - 0.034362217260275274\n",
      "Step - 9526, Loss - 0.35076255422503266, Learning Rate - 9.765625e-05, magnitude of gradient - 0.025928610473474466\n",
      "Step - 9527, Loss - 0.2522481321626491, Learning Rate - 9.765625e-05, magnitude of gradient - 0.027111642691719645\n",
      "Step - 9528, Loss - 0.3144910310731517, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02316713789951684\n",
      "Step - 9529, Loss - 0.4072179659430501, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0784968550161454\n",
      "Step - 9530, Loss - 0.32748054653324093, Learning Rate - 9.765625e-05, magnitude of gradient - 0.047382300035482064\n",
      "Step - 9531, Loss - 0.3099579040281325, Learning Rate - 9.765625e-05, magnitude of gradient - 0.034678758226057406\n",
      "Step - 9532, Loss - 0.33878317887416576, Learning Rate - 9.765625e-05, magnitude of gradient - 0.024226059782226195\n",
      "Step - 9533, Loss - 0.3146329020423328, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05403639309601774\n",
      "Step - 9534, Loss - 0.22777698747473485, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04245284543114812\n",
      "Step - 9535, Loss - 0.3017220346732206, Learning Rate - 9.765625e-05, magnitude of gradient - 0.023817033953782617\n",
      "Step - 9536, Loss - 0.37193538059864006, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04787897790061271\n",
      "Step - 9537, Loss - 0.3355831514026204, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01554231026724305\n",
      "Step - 9538, Loss - 0.3368440172309174, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07786507958799206\n",
      "Step - 9539, Loss - 0.3220633268863636, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07718818552109011\n",
      "Step - 9540, Loss - 0.4339428100022011, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08628762838500499\n",
      "Step - 9541, Loss - 0.3193614886367492, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02087888238753028\n",
      "Step - 9542, Loss - 0.35614598919809903, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06646441719225435\n",
      "Step - 9543, Loss - 0.2449392536825898, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03129505315527959\n",
      "Step - 9544, Loss - 0.27027269220671263, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05007073333494669\n",
      "Step - 9545, Loss - 0.2724240062942943, Learning Rate - 9.765625e-05, magnitude of gradient - 0.037001695602438954\n",
      "Step - 9546, Loss - 0.34351885317487885, Learning Rate - 9.765625e-05, magnitude of gradient - 0.048777087940678414\n",
      "Step - 9547, Loss - 0.37448286412212045, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03212126641177299\n",
      "Step - 9548, Loss - 0.2962950952915422, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05984456659733726\n",
      "Step - 9549, Loss - 0.29348857449559185, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06489235124003245\n",
      "Step - 9550, Loss - 0.35378046107152183, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07735027490347683\n",
      "Step - 9551, Loss - 0.24680499911063802, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02084507972667569\n",
      "Step - 9552, Loss - 0.2989182547618906, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07721680014672316\n",
      "Step - 9553, Loss - 0.21918174817007707, Learning Rate - 9.765625e-05, magnitude of gradient - 0.028586540897782283\n",
      "Step - 9554, Loss - 0.36356609467521694, Learning Rate - 9.765625e-05, magnitude of gradient - 0.11542853086721364\n",
      "Step - 9555, Loss - 0.32887739066987576, Learning Rate - 9.765625e-05, magnitude of gradient - 0.024489178598098217\n",
      "Step - 9556, Loss - 0.33328387193628367, Learning Rate - 9.765625e-05, magnitude of gradient - 0.052174320323106184\n",
      "Step - 9557, Loss - 0.3088345617009607, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05697759722375028\n",
      "Step - 9558, Loss - 0.2989227419939603, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09845782086743336\n",
      "Step - 9559, Loss - 0.30990361749562156, Learning Rate - 9.765625e-05, magnitude of gradient - 0.029551412630501525\n",
      "Step - 9560, Loss - 0.2633552617699476, Learning Rate - 9.765625e-05, magnitude of gradient - 0.043898203592939944\n",
      "Step - 9561, Loss - 0.26320721422157245, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05934087350458081\n",
      "Step - 9562, Loss - 0.28977031015028953, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03424785495908973\n",
      "Step - 9563, Loss - 0.2825245957719295, Learning Rate - 9.765625e-05, magnitude of gradient - 0.007472614856838877\n",
      "Step - 9564, Loss - 0.3261550872650051, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04199619461788671\n",
      "Step - 9565, Loss - 0.30791250773609213, Learning Rate - 9.765625e-05, magnitude of gradient - 0.013526427443351191\n",
      "Step - 9566, Loss - 0.2753477736916401, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07851834430709245\n",
      "Step - 9567, Loss - 0.32525597728772704, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07214081566211755\n",
      "Step - 9568, Loss - 0.39486724460396916, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05052477678400736\n",
      "Step - 9569, Loss - 0.3218537835256942, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06184766939983627\n",
      "Step - 9570, Loss - 0.3142303421369649, Learning Rate - 9.765625e-05, magnitude of gradient - 0.11220492137301616\n",
      "Step - 9571, Loss - 0.30773947375831073, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05568212657549854\n",
      "Step - 9572, Loss - 0.30121385750433904, Learning Rate - 9.765625e-05, magnitude of gradient - 0.031918494468413446\n",
      "Step - 9573, Loss - 0.2633196138779175, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0979204139439426\n",
      "Step - 9574, Loss - 0.2842506041080578, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09224111182012247\n",
      "Step - 9575, Loss - 0.3306047234285418, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06450795382085436\n",
      "Step - 9576, Loss - 0.29427286851282475, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03253250715585438\n",
      "Step - 9577, Loss - 0.27161460060650106, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05822600686708848\n",
      "Step - 9578, Loss - 0.22856146063957317, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01556221975411535\n",
      "Step - 9579, Loss - 0.2791407053343936, Learning Rate - 9.765625e-05, magnitude of gradient - 0.013262086459446197\n",
      "Step - 9580, Loss - 0.312446269144475, Learning Rate - 9.765625e-05, magnitude of gradient - 0.008267183585944027\n",
      "Step - 9581, Loss - 0.29903780026713145, Learning Rate - 9.765625e-05, magnitude of gradient - 0.049351163775502895\n",
      "Step - 9582, Loss - 0.37030963032596254, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04605010693533119\n",
      "Step - 9583, Loss - 0.29213181166961866, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06328519556425236\n",
      "Step - 9584, Loss - 0.33953874331995926, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05263378354479238\n",
      "Step - 9585, Loss - 0.3025078274349046, Learning Rate - 9.765625e-05, magnitude of gradient - 0.018543126182415444\n",
      "Step - 9586, Loss - 0.3136458609528351, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05144679936246787\n",
      "Step - 9587, Loss - 0.25336800221140454, Learning Rate - 9.765625e-05, magnitude of gradient - 0.041502579038313496\n",
      "Step - 9588, Loss - 0.3143600256278388, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06612071103978065\n",
      "Step - 9589, Loss - 0.2973029265692758, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05316533612320903\n",
      "Step - 9590, Loss - 0.24740176208556305, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09196056915297814\n",
      "Step - 9591, Loss - 0.33075334211697843, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09056665914201742\n",
      "Step - 9592, Loss - 0.38959894971537945, Learning Rate - 9.765625e-05, magnitude of gradient - 0.013582392264295449\n",
      "Step - 9593, Loss - 0.3133616295197242, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08877222594255547\n",
      "Step - 9594, Loss - 0.398011351716827, Learning Rate - 9.765625e-05, magnitude of gradient - 0.11030100362076334\n",
      "Step - 9595, Loss - 0.28647131366933054, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06104060320052095\n",
      "Step - 9596, Loss - 0.2748998092751581, Learning Rate - 9.765625e-05, magnitude of gradient - 0.017124747047711584\n",
      "Step - 9597, Loss - 0.3278161600370974, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08098793964606232\n",
      "Step - 9598, Loss - 0.30076906356417127, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04591332073193337\n",
      "Step - 9599, Loss - 0.32894390616359725, Learning Rate - 9.765625e-05, magnitude of gradient - 0.027139919944247397\n",
      "Step - 9600, Loss - 0.43015565011587686, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07918951931627588\n",
      "Step - 9601, Loss - 0.40416320133252626, Learning Rate - 9.765625e-05, magnitude of gradient - 0.026175541515434886\n",
      "Step - 9602, Loss - 0.2813009236130476, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07206008747252551\n",
      "Step - 9603, Loss - 0.29850948558191803, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07753269836308689\n",
      "Step - 9604, Loss - 0.30858397243405866, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07532408233614735\n",
      "Step - 9605, Loss - 0.34236878888090216, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03251138213097959\n",
      "Step - 9606, Loss - 0.34059795798548415, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04043860879560129\n",
      "Step - 9607, Loss - 0.28926488647478055, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03120312032142682\n",
      "Step - 9608, Loss - 0.29255590185354985, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04981792683864989\n",
      "Step - 9609, Loss - 0.2640698253789847, Learning Rate - 9.765625e-05, magnitude of gradient - 0.10523646373739003\n",
      "Step - 9610, Loss - 0.306125196151996, Learning Rate - 9.765625e-05, magnitude of gradient - 0.007110209886430409\n",
      "Step - 9611, Loss - 0.26485743894359115, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04272013702945618\n",
      "Step - 9612, Loss - 0.297904346078325, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07747184445629897\n",
      "Step - 9613, Loss - 0.3249035089248757, Learning Rate - 9.765625e-05, magnitude of gradient - 0.028575758905983583\n",
      "Step - 9614, Loss - 0.33231481809014174, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09806675467466847\n",
      "Step - 9615, Loss - 0.3448360632022339, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0440521853216137\n",
      "Step - 9616, Loss - 0.2934218815016435, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02164501086732081\n",
      "Step - 9617, Loss - 0.2752051912090896, Learning Rate - 9.765625e-05, magnitude of gradient - 0.016797109136437702\n",
      "Step - 9618, Loss - 0.3644893272750873, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09515642484423337\n",
      "Step - 9619, Loss - 0.36720882741282324, Learning Rate - 9.765625e-05, magnitude of gradient - 0.038149515702646505\n",
      "Step - 9620, Loss - 0.2930650403995647, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08833149536686245\n",
      "Step - 9621, Loss - 0.30763976221320094, Learning Rate - 9.765625e-05, magnitude of gradient - 0.029229657123333197\n",
      "Step - 9622, Loss - 0.2455389914958161, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03748996596143927\n",
      "Step - 9623, Loss - 0.31264837648033256, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03984133372391462\n",
      "Step - 9624, Loss - 0.33024283801708737, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0221313911873325\n",
      "Step - 9625, Loss - 0.30326276524652246, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03366750893470833\n",
      "Step - 9626, Loss - 0.368184276072495, Learning Rate - 9.765625e-05, magnitude of gradient - 0.054069407165951745\n",
      "Step - 9627, Loss - 0.284916273355824, Learning Rate - 9.765625e-05, magnitude of gradient - 0.061153302075841076\n",
      "Step - 9628, Loss - 0.26741471806467365, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03873667627090505\n",
      "Step - 9629, Loss - 0.31173903566036215, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07600820538352125\n",
      "Step - 9630, Loss - 0.2670747646870529, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08712339742419196\n",
      "Step - 9631, Loss - 0.29058411286111296, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04812062043547756\n",
      "Step - 9632, Loss - 0.3122611221475138, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0660912878478377\n",
      "Step - 9633, Loss - 0.38083327369133324, Learning Rate - 9.765625e-05, magnitude of gradient - 0.029625886561133578\n",
      "Step - 9634, Loss - 0.368387696438216, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05246304437364142\n",
      "Step - 9635, Loss - 0.2780245303069913, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03649405479824039\n",
      "Step - 9636, Loss - 0.2987676073830004, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08841518485094112\n",
      "Step - 9637, Loss - 0.33287209757567315, Learning Rate - 9.765625e-05, magnitude of gradient - 0.051550688007053806\n",
      "Step - 9638, Loss - 0.32622811428740034, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06056368898653479\n",
      "Step - 9639, Loss - 0.3019007786127741, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07252861090688921\n",
      "Step - 9640, Loss - 0.306486560878259, Learning Rate - 9.765625e-05, magnitude of gradient - 0.015203054431202569\n",
      "Step - 9641, Loss - 0.24756267210293914, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04591601641440174\n",
      "Step - 9642, Loss - 0.30916604831106187, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04651883110062223\n",
      "Step - 9643, Loss - 0.29899620037849295, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06266618261732394\n",
      "Step - 9644, Loss - 0.25867087160088853, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05884888770994988\n",
      "Step - 9645, Loss - 0.36441223956157404, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07092171008359692\n",
      "Step - 9646, Loss - 0.36427208783280574, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06828552351120742\n",
      "Step - 9647, Loss - 0.3212778957525522, Learning Rate - 9.765625e-05, magnitude of gradient - 0.1097712596147986\n",
      "Step - 9648, Loss - 0.37958930777507305, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04312315787879042\n",
      "Step - 9649, Loss - 0.2996247590545371, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07833358816254476\n",
      "Step - 9650, Loss - 0.3251245595312091, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03644129377728174\n",
      "Step - 9651, Loss - 0.2211098332996948, Learning Rate - 9.765625e-05, magnitude of gradient - 0.035450193185607395\n",
      "Step - 9652, Loss - 0.25866881828129035, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06943562672301214\n",
      "Step - 9653, Loss - 0.2746168928010426, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08319939954516137\n",
      "Step - 9654, Loss - 0.25264257444343063, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06142122992634135\n",
      "Step - 9655, Loss - 0.3453735107405859, Learning Rate - 9.765625e-05, magnitude of gradient - 0.029748493996460085\n",
      "Step - 9656, Loss - 0.3394866989238904, Learning Rate - 9.765625e-05, magnitude of gradient - 0.053073017088104406\n",
      "Step - 9657, Loss - 0.35127937859473884, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05974034143656031\n",
      "Step - 9658, Loss - 0.29473366258129696, Learning Rate - 9.765625e-05, magnitude of gradient - 0.027910071580512903\n",
      "Step - 9659, Loss - 0.3407681852661687, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03996117233948471\n",
      "Step - 9660, Loss - 0.33642528147453515, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06621428790068931\n",
      "Step - 9661, Loss - 0.26384691257587634, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05487622085387985\n",
      "Step - 9662, Loss - 0.305484483529715, Learning Rate - 9.765625e-05, magnitude of gradient - 0.036916616954997164\n",
      "Step - 9663, Loss - 0.29239016348857816, Learning Rate - 9.765625e-05, magnitude of gradient - 0.021561588845246067\n",
      "Step - 9664, Loss - 0.2702847071258838, Learning Rate - 9.765625e-05, magnitude of gradient - 0.024874511466444038\n",
      "Step - 9665, Loss - 0.2703661831534896, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04470801798704676\n",
      "Step - 9666, Loss - 0.34811358784173574, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07708155261290117\n",
      "Step - 9667, Loss - 0.3155771438366597, Learning Rate - 9.765625e-05, magnitude of gradient - 0.039288426947606044\n",
      "Step - 9668, Loss - 0.30360791324519476, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01784315027713821\n",
      "Step - 9669, Loss - 0.27856004242383636, Learning Rate - 9.765625e-05, magnitude of gradient - 0.047501322932621604\n",
      "Step - 9670, Loss - 0.34816832238959133, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08810631676558321\n",
      "Step - 9671, Loss - 0.35076893575517054, Learning Rate - 9.765625e-05, magnitude of gradient - 0.042501107099129014\n",
      "Step - 9672, Loss - 0.30164950879763364, Learning Rate - 9.765625e-05, magnitude of gradient - 0.037204563110257216\n",
      "Step - 9673, Loss - 0.28910648356540913, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04180316397718911\n",
      "Step - 9674, Loss - 0.28567814425760385, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0628635896362417\n",
      "Step - 9675, Loss - 0.27281692337955604, Learning Rate - 9.765625e-05, magnitude of gradient - 0.044120138084914926\n",
      "Step - 9676, Loss - 0.3254990348537705, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09972457012806327\n",
      "Step - 9677, Loss - 0.2945227737922476, Learning Rate - 9.765625e-05, magnitude of gradient - 0.010139109403381807\n",
      "Step - 9678, Loss - 0.3522810245240573, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03168880978880637\n",
      "Step - 9679, Loss - 0.40001900154275516, Learning Rate - 9.765625e-05, magnitude of gradient - 0.18499814730592568\n",
      "Step - 9680, Loss - 0.29101538587816694, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0022289924886189534\n",
      "Step - 9681, Loss - 0.33279683052866604, Learning Rate - 9.765625e-05, magnitude of gradient - 0.010575771002719723\n",
      "Step - 9682, Loss - 0.32394319619357637, Learning Rate - 9.765625e-05, magnitude of gradient - 0.041429451318891225\n",
      "Step - 9683, Loss - 0.3388326102923235, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04002105356283676\n",
      "Step - 9684, Loss - 0.3232628506558727, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01436061832487455\n",
      "Step - 9685, Loss - 0.2920343635525021, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0218990383104784\n",
      "Step - 9686, Loss - 0.32421043815196826, Learning Rate - 9.765625e-05, magnitude of gradient - 0.029129268925678162\n",
      "Step - 9687, Loss - 0.3361081623721702, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05038841573717924\n",
      "Step - 9688, Loss - 0.3528955375135887, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03621062911330867\n",
      "Step - 9689, Loss - 0.2624109972382853, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02162687003948816\n",
      "Step - 9690, Loss - 0.29837200051500634, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04354798366799624\n",
      "Step - 9691, Loss - 0.2871096631394696, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05859334020214788\n",
      "Step - 9692, Loss - 0.3356978804857407, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03789905806682332\n",
      "Step - 9693, Loss - 0.3514849437102839, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02647169609072522\n",
      "Step - 9694, Loss - 0.3298470751950059, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04451796562905211\n",
      "Step - 9695, Loss - 0.31065002551330667, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0590790044010802\n",
      "Step - 9696, Loss - 0.34182712827712824, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04372172153642057\n",
      "Step - 9697, Loss - 0.29737889830259184, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05836164492006346\n",
      "Step - 9698, Loss - 0.3706970579963374, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05703656527517444\n",
      "Step - 9699, Loss - 0.339294076605608, Learning Rate - 9.765625e-05, magnitude of gradient - 0.022074809437100943\n",
      "Step - 9700, Loss - 0.3323322912726049, Learning Rate - 9.765625e-05, magnitude of gradient - 0.046394459543568384\n",
      "Step - 9701, Loss - 0.36762287874851973, Learning Rate - 9.765625e-05, magnitude of gradient - 0.020836028919600375\n",
      "Step - 9702, Loss - 0.3097271544419756, Learning Rate - 9.765625e-05, magnitude of gradient - 0.029895421413941945\n",
      "Step - 9703, Loss - 0.3303048468084493, Learning Rate - 9.765625e-05, magnitude of gradient - 0.023113941078220967\n",
      "Step - 9704, Loss - 0.27121786811325, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0698952985172234\n",
      "Step - 9705, Loss - 0.3952801551696125, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05140270656722648\n",
      "Step - 9706, Loss - 0.27899147203832225, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03888130383502591\n",
      "Step - 9707, Loss - 0.3144189947558069, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05650098643653254\n",
      "Step - 9708, Loss - 0.2688299199789246, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04814560111597021\n",
      "Step - 9709, Loss - 0.42839976062737267, Learning Rate - 9.765625e-05, magnitude of gradient - 0.055709736542169375\n",
      "Step - 9710, Loss - 0.2847314450365753, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05655405945590035\n",
      "Step - 9711, Loss - 0.33496197255134913, Learning Rate - 9.765625e-05, magnitude of gradient - 0.10850489950651009\n",
      "Step - 9712, Loss - 0.342021870476746, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07178130953444298\n",
      "Step - 9713, Loss - 0.31630558362481315, Learning Rate - 9.765625e-05, magnitude of gradient - 0.016450085738435233\n",
      "Step - 9714, Loss - 0.22904820733501674, Learning Rate - 9.765625e-05, magnitude of gradient - 0.034878434848285955\n",
      "Step - 9715, Loss - 0.350267998608814, Learning Rate - 9.765625e-05, magnitude of gradient - 0.014911333206147584\n",
      "Step - 9716, Loss - 0.33629048547066076, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06131493584018397\n",
      "Step - 9717, Loss - 0.29793660563861135, Learning Rate - 9.765625e-05, magnitude of gradient - 0.049095074257715\n",
      "Step - 9718, Loss - 0.3116620121386066, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02454698425921226\n",
      "Step - 9719, Loss - 0.39922762059618017, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03296976810169968\n",
      "Step - 9720, Loss - 0.40189383849239746, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0551957053624239\n",
      "Step - 9721, Loss - 0.25339565664360986, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01584938212765122\n",
      "Step - 9722, Loss - 0.3547367731906827, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06680568419994766\n",
      "Step - 9723, Loss - 0.35171973361390085, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0628231418307472\n",
      "Step - 9724, Loss - 0.3274693101882279, Learning Rate - 9.765625e-05, magnitude of gradient - 0.039848854601435064\n",
      "Step - 9725, Loss - 0.30933568280022716, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04118518337426367\n",
      "Step - 9726, Loss - 0.30589427768786714, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05278815529890199\n",
      "Step - 9727, Loss - 0.31989366069561664, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07190397713333406\n",
      "Step - 9728, Loss - 0.27765073437580184, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05105709618004573\n",
      "Step - 9729, Loss - 0.36742516660920527, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04212464887859481\n",
      "Step - 9730, Loss - 0.3361294720162446, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06899734658756206\n",
      "Step - 9731, Loss - 0.331571113786037, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07366976859561199\n",
      "Step - 9732, Loss - 0.3576932096080509, Learning Rate - 9.765625e-05, magnitude of gradient - 0.048741445059686986\n",
      "Step - 9733, Loss - 0.32851464736737074, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0403863582485697\n",
      "Step - 9734, Loss - 0.32032117087166645, Learning Rate - 9.765625e-05, magnitude of gradient - 0.012424462098022969\n",
      "Step - 9735, Loss - 0.3611185479223315, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01641203184020108\n",
      "Step - 9736, Loss - 0.3343432140546025, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06256127690486475\n",
      "Step - 9737, Loss - 0.2686001917636434, Learning Rate - 9.765625e-05, magnitude of gradient - 0.043023833714190295\n",
      "Step - 9738, Loss - 0.3223600943635818, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06798067973141074\n",
      "Step - 9739, Loss - 0.34685757459629873, Learning Rate - 9.765625e-05, magnitude of gradient - 0.057514671044758596\n",
      "Step - 9740, Loss - 0.26190564735266486, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08059286708782709\n",
      "Step - 9741, Loss - 0.4136768447584876, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07197793331311131\n",
      "Step - 9742, Loss - 0.3641889153510508, Learning Rate - 9.765625e-05, magnitude of gradient - 0.026406661783838613\n",
      "Step - 9743, Loss - 0.34023196878223405, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01575133777977018\n",
      "Step - 9744, Loss - 0.3852357986049106, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03025973040116274\n",
      "Step - 9745, Loss - 0.2685090989287773, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05179057943204005\n",
      "Step - 9746, Loss - 0.29283524933867483, Learning Rate - 9.765625e-05, magnitude of gradient - 0.021157949494247087\n",
      "Step - 9747, Loss - 0.3381976399563313, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07934030233227875\n",
      "Step - 9748, Loss - 0.3110310045275362, Learning Rate - 9.765625e-05, magnitude of gradient - 0.030821063030546263\n",
      "Step - 9749, Loss - 0.3802589023487788, Learning Rate - 9.765625e-05, magnitude of gradient - 0.020874693358607987\n",
      "Step - 9750, Loss - 0.27286039847288507, Learning Rate - 9.765625e-05, magnitude of gradient - 0.026756748725665788\n",
      "Step - 9751, Loss - 0.3013114528504704, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07378967524820645\n",
      "Step - 9752, Loss - 0.2786884013905172, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01590075150803111\n",
      "Step - 9753, Loss - 0.32612013056238215, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06963376102954116\n",
      "Step - 9754, Loss - 0.3086539602322975, Learning Rate - 9.765625e-05, magnitude of gradient - 0.11449765215889882\n",
      "Step - 9755, Loss - 0.269309343711032, Learning Rate - 9.765625e-05, magnitude of gradient - 0.033052168483060106\n",
      "Step - 9756, Loss - 0.30690235247577563, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05148459934369298\n",
      "Step - 9757, Loss - 0.32070683539087586, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06436224071184735\n",
      "Step - 9758, Loss - 0.32437830208004803, Learning Rate - 9.765625e-05, magnitude of gradient - 0.016068137828321696\n",
      "Step - 9759, Loss - 0.28640697995945075, Learning Rate - 9.765625e-05, magnitude of gradient - 0.1008687289569902\n",
      "Step - 9760, Loss - 0.32598804666865855, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03982351802003344\n",
      "Step - 9761, Loss - 0.31480165589307013, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0397341937081252\n",
      "Step - 9762, Loss - 0.34792323918699497, Learning Rate - 9.765625e-05, magnitude of gradient - 0.053326268509119715\n",
      "Step - 9763, Loss - 0.2754944897052667, Learning Rate - 9.765625e-05, magnitude of gradient - 0.040992355409379067\n",
      "Step - 9764, Loss - 0.36318268277054144, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06100680276099923\n",
      "Step - 9765, Loss - 0.28349481370968693, Learning Rate - 9.765625e-05, magnitude of gradient - 0.047384135750657974\n",
      "Step - 9766, Loss - 0.2721020596090338, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09751114088795164\n",
      "Step - 9767, Loss - 0.30213125280714537, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01264389034155594\n",
      "Step - 9768, Loss - 0.3512848588469438, Learning Rate - 9.765625e-05, magnitude of gradient - 0.010513761459471613\n",
      "Step - 9769, Loss - 0.3063716315965533, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04523484787381422\n",
      "Step - 9770, Loss - 0.33368344831884544, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09058390694771234\n",
      "Step - 9771, Loss - 0.2834889477550747, Learning Rate - 9.765625e-05, magnitude of gradient - 0.010597984319597412\n",
      "Step - 9772, Loss - 0.3503064787705771, Learning Rate - 9.765625e-05, magnitude of gradient - 0.050831344894443194\n",
      "Step - 9773, Loss - 0.33911772663908896, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04616643279182752\n",
      "Step - 9774, Loss - 0.32338203392992954, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04853146250881485\n",
      "Step - 9775, Loss - 0.31666509952712774, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07149230938929853\n",
      "Step - 9776, Loss - 0.3307787861348054, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03561235009786723\n",
      "Step - 9777, Loss - 0.35988492552943563, Learning Rate - 9.765625e-05, magnitude of gradient - 0.057447859751059704\n",
      "Step - 9778, Loss - 0.29951000374763925, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06585957223145301\n",
      "Step - 9779, Loss - 0.32348131081527975, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05200333785742313\n",
      "Step - 9780, Loss - 0.33167117758974834, Learning Rate - 9.765625e-05, magnitude of gradient - 0.058199161796799535\n",
      "Step - 9781, Loss - 0.3066155242436881, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07554167919447262\n",
      "Step - 9782, Loss - 0.2865981926247883, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06401952760201254\n",
      "Step - 9783, Loss - 0.34686788486101006, Learning Rate - 9.765625e-05, magnitude of gradient - 0.10323604693164971\n",
      "Step - 9784, Loss - 0.383318740664984, Learning Rate - 9.765625e-05, magnitude of gradient - 0.011090577485750631\n",
      "Step - 9785, Loss - 0.3592060090747655, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08565866144926756\n",
      "Step - 9786, Loss - 0.32445320464399496, Learning Rate - 9.765625e-05, magnitude of gradient - 0.062267923217075596\n",
      "Step - 9787, Loss - 0.2739008168287805, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04613730807452836\n",
      "Step - 9788, Loss - 0.3144324665041133, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06981334773527365\n",
      "Step - 9789, Loss - 0.32181040205418043, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03745403074705555\n",
      "Step - 9790, Loss - 0.3569202509891097, Learning Rate - 9.765625e-05, magnitude of gradient - 0.025576703780495508\n",
      "Step - 9791, Loss - 0.33960626764705076, Learning Rate - 9.765625e-05, magnitude of gradient - 0.027399977203264512\n",
      "Step - 9792, Loss - 0.3955786226499995, Learning Rate - 9.765625e-05, magnitude of gradient - 0.014645493020540036\n",
      "Step - 9793, Loss - 0.3465963541943873, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07233039109282699\n",
      "Step - 9794, Loss - 0.3541381014444519, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04616829093019997\n",
      "Step - 9795, Loss - 0.3082970535767634, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05515338072347716\n",
      "Step - 9796, Loss - 0.3144054458372898, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0787854368203944\n",
      "Step - 9797, Loss - 0.3433030998578808, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0388338870809608\n",
      "Step - 9798, Loss - 0.3234450437284126, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03162864759354358\n",
      "Step - 9799, Loss - 0.3115335769422285, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05542389935660517\n",
      "Step - 9800, Loss - 0.30007402370688274, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03276577455657833\n",
      "Step - 9801, Loss - 0.3115300182654165, Learning Rate - 9.765625e-05, magnitude of gradient - 0.058032971272827236\n",
      "Step - 9802, Loss - 0.2577579466019428, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03563140180815114\n",
      "Step - 9803, Loss - 0.35378970741966936, Learning Rate - 9.765625e-05, magnitude of gradient - 0.036513571042541444\n",
      "Step - 9804, Loss - 0.2873053505272195, Learning Rate - 9.765625e-05, magnitude of gradient - 0.015985198184049546\n",
      "Step - 9805, Loss - 0.28904033382040906, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05394764491709843\n",
      "Step - 9806, Loss - 0.334695442005143, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07027858755438202\n",
      "Step - 9807, Loss - 0.3075934145660813, Learning Rate - 9.765625e-05, magnitude of gradient - 0.014241522845570249\n",
      "Step - 9808, Loss - 0.3407460731389975, Learning Rate - 9.765625e-05, magnitude of gradient - 0.021305639235510807\n",
      "Step - 9809, Loss - 0.3397930118525697, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06817677898696312\n",
      "Step - 9810, Loss - 0.2689615553198063, Learning Rate - 9.765625e-05, magnitude of gradient - 0.046162089381752106\n",
      "Step - 9811, Loss - 0.37080777149590644, Learning Rate - 9.765625e-05, magnitude of gradient - 0.021914333297165016\n",
      "Step - 9812, Loss - 0.2550735202764143, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02981797628889063\n",
      "Step - 9813, Loss - 0.35356626549721926, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04494806791386552\n",
      "Step - 9814, Loss - 0.3688731096722578, Learning Rate - 9.765625e-05, magnitude of gradient - 0.060718550339291404\n",
      "Step - 9815, Loss - 0.3571802074151714, Learning Rate - 9.765625e-05, magnitude of gradient - 0.008747152381404867\n",
      "Step - 9816, Loss - 0.3747804902318833, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04224674177894512\n",
      "Step - 9817, Loss - 0.3508089879207607, Learning Rate - 9.765625e-05, magnitude of gradient - 0.025067618875961074\n",
      "Step - 9818, Loss - 0.3218213686191517, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03381198916598949\n",
      "Step - 9819, Loss - 0.4322927818704463, Learning Rate - 9.765625e-05, magnitude of gradient - 0.047364671011926915\n",
      "Step - 9820, Loss - 0.31617445735455335, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04094435793999287\n",
      "Step - 9821, Loss - 0.2502331271933169, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04202293461072337\n",
      "Step - 9822, Loss - 0.31492927840076607, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05036113588444172\n",
      "Step - 9823, Loss - 0.3136261106716794, Learning Rate - 9.765625e-05, magnitude of gradient - 0.016637586536593047\n",
      "Step - 9824, Loss - 0.29224489944830995, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0331594352362461\n",
      "Step - 9825, Loss - 0.285452913499866, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09050467955455506\n",
      "Step - 9826, Loss - 0.3340421318323813, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0794806349555778\n",
      "Step - 9827, Loss - 0.37669134976097707, Learning Rate - 9.765625e-05, magnitude of gradient - 0.056164939402616215\n",
      "Step - 9828, Loss - 0.3060976438506635, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09967333025952387\n",
      "Step - 9829, Loss - 0.39129674507633877, Learning Rate - 9.765625e-05, magnitude of gradient - 0.048905290985163795\n",
      "Step - 9830, Loss - 0.36232790449033636, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07713040716763024\n",
      "Step - 9831, Loss - 0.33905323874419996, Learning Rate - 9.765625e-05, magnitude of gradient - 0.062167408667005555\n",
      "Step - 9832, Loss - 0.3501797396432261, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05389519447385773\n",
      "Step - 9833, Loss - 0.2610395289545069, Learning Rate - 9.765625e-05, magnitude of gradient - 0.052732542586200894\n",
      "Step - 9834, Loss - 0.3174643159578455, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01797367916187075\n",
      "Step - 9835, Loss - 0.29504143604270616, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05264182262709373\n",
      "Step - 9836, Loss - 0.29832779033000356, Learning Rate - 9.765625e-05, magnitude of gradient - 0.044752834317195574\n",
      "Step - 9837, Loss - 0.29582399438078155, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06228994655367179\n",
      "Step - 9838, Loss - 0.35076657921962456, Learning Rate - 9.765625e-05, magnitude of gradient - 0.028047490526874268\n",
      "Step - 9839, Loss - 0.3071595134868093, Learning Rate - 9.765625e-05, magnitude of gradient - 0.011478497923806388\n",
      "Step - 9840, Loss - 0.2944218033908635, Learning Rate - 9.765625e-05, magnitude of gradient - 0.073658007732339\n",
      "Step - 9841, Loss - 0.2765624276122272, Learning Rate - 9.765625e-05, magnitude of gradient - 0.010656551040711336\n",
      "Step - 9842, Loss - 0.2945172323718296, Learning Rate - 9.765625e-05, magnitude of gradient - 0.012977818162032207\n",
      "Step - 9843, Loss - 0.3205029966961264, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05380538015610012\n",
      "Step - 9844, Loss - 0.33259051587269534, Learning Rate - 9.765625e-05, magnitude of gradient - 0.022871303531981742\n",
      "Step - 9845, Loss - 0.3748142780109332, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03586819265461505\n",
      "Step - 9846, Loss - 0.3302327897358274, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03653521800010292\n",
      "Step - 9847, Loss - 0.29672058894823267, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05270188149968924\n",
      "Step - 9848, Loss - 0.3116544121602206, Learning Rate - 9.765625e-05, magnitude of gradient - 0.014301567554397177\n",
      "Step - 9849, Loss - 0.3811812197419643, Learning Rate - 9.765625e-05, magnitude of gradient - 0.057540974028570946\n",
      "Step - 9850, Loss - 0.29822314010559214, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06308888865908788\n",
      "Step - 9851, Loss - 0.26163623902428756, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04664563757840458\n",
      "Step - 9852, Loss - 0.3744781205516222, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04558119047780035\n",
      "Step - 9853, Loss - 0.3151395762731613, Learning Rate - 9.765625e-05, magnitude of gradient - 0.025860197899096726\n",
      "Step - 9854, Loss - 0.3261963731828698, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04267606445046633\n",
      "Step - 9855, Loss - 0.3428282732343576, Learning Rate - 9.765625e-05, magnitude of gradient - 0.026107825938141264\n",
      "Step - 9856, Loss - 0.3296940395279041, Learning Rate - 9.765625e-05, magnitude of gradient - 0.018631435745925302\n",
      "Step - 9857, Loss - 0.3359306101022038, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07948459198580445\n",
      "Step - 9858, Loss - 0.3813804359450061, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06444668483712108\n",
      "Step - 9859, Loss - 0.35243293194958636, Learning Rate - 9.765625e-05, magnitude of gradient - 0.090781112285697\n",
      "Step - 9860, Loss - 0.3620678121363844, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05951620153070853\n",
      "Step - 9861, Loss - 0.30011160849295215, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06589820517577416\n",
      "Step - 9862, Loss - 0.37890869750080036, Learning Rate - 9.765625e-05, magnitude of gradient - 0.10633012131998135\n",
      "Step - 9863, Loss - 0.30862206982595564, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04844287588956806\n",
      "Step - 9864, Loss - 0.36087368372246337, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01753967114209284\n",
      "Step - 9865, Loss - 0.2861916938895449, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03130624804133897\n",
      "Step - 9866, Loss - 0.373496879548602, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02222397740316157\n",
      "Step - 9867, Loss - 0.2874988391493571, Learning Rate - 9.765625e-05, magnitude of gradient - 0.1007483196471339\n",
      "Step - 9868, Loss - 0.28729104910898096, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05503370838590607\n",
      "Step - 9869, Loss - 0.35336609728363433, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04593196943642278\n",
      "Step - 9870, Loss - 0.36697069124059706, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07907440667634269\n",
      "Step - 9871, Loss - 0.34799623256639095, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09308757913788152\n",
      "Step - 9872, Loss - 0.3027346107589961, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07080910661316059\n",
      "Step - 9873, Loss - 0.32416288158846807, Learning Rate - 9.765625e-05, magnitude of gradient - 0.021214239944917595\n",
      "Step - 9874, Loss - 0.26463413872763275, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04706084367843828\n",
      "Step - 9875, Loss - 0.2619860248636245, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04750677544533109\n",
      "Step - 9876, Loss - 0.3538560993929003, Learning Rate - 9.765625e-05, magnitude of gradient - 0.007866748178686187\n",
      "Step - 9877, Loss - 0.27205053265911827, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04799868258485399\n",
      "Step - 9878, Loss - 0.3352196040722829, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06284938565025673\n",
      "Step - 9879, Loss - 0.3826759366412419, Learning Rate - 9.765625e-05, magnitude of gradient - 0.024615489532402998\n",
      "Step - 9880, Loss - 0.3429110998891078, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02795835393471201\n",
      "Step - 9881, Loss - 0.3581770369691539, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04668314390926661\n",
      "Step - 9882, Loss - 0.3501092004557463, Learning Rate - 9.765625e-05, magnitude of gradient - 0.033945866464779685\n",
      "Step - 9883, Loss - 0.3556131933323289, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07994577939983795\n",
      "Step - 9884, Loss - 0.31126784803735386, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07443844109972302\n",
      "Step - 9885, Loss - 0.3433612380108214, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07462482678555044\n",
      "Step - 9886, Loss - 0.356772871721189, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0553004649037486\n",
      "Step - 9887, Loss - 0.2562732367125933, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07164158084319258\n",
      "Step - 9888, Loss - 0.36910855073316423, Learning Rate - 9.765625e-05, magnitude of gradient - 0.045389116634361494\n",
      "Step - 9889, Loss - 0.29749447361717135, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0683506475523217\n",
      "Step - 9890, Loss - 0.3708872837762303, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0684316301964654\n",
      "Step - 9891, Loss - 0.37650425160926065, Learning Rate - 9.765625e-05, magnitude of gradient - 0.030679841864059604\n",
      "Step - 9892, Loss - 0.2839402261575811, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08212613978215054\n",
      "Step - 9893, Loss - 0.3186456958491346, Learning Rate - 9.765625e-05, magnitude of gradient - 0.053404019509183984\n",
      "Step - 9894, Loss - 0.33149404109482367, Learning Rate - 9.765625e-05, magnitude of gradient - 0.015286898260072814\n",
      "Step - 9895, Loss - 0.24302688800323927, Learning Rate - 9.765625e-05, magnitude of gradient - 0.020744059930267962\n",
      "Step - 9896, Loss - 0.3664965046220099, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03811812263128873\n",
      "Step - 9897, Loss - 0.3123417613951516, Learning Rate - 9.765625e-05, magnitude of gradient - 0.1613216921818859\n",
      "Step - 9898, Loss - 0.3476669973354047, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04812169637309224\n",
      "Step - 9899, Loss - 0.3043032828583524, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08159859965087592\n",
      "Step - 9900, Loss - 0.26960144583949613, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0695434903316563\n",
      "Step - 9901, Loss - 0.3197201122580063, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04665640254514196\n",
      "Step - 9902, Loss - 0.33691004258382184, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0377103333791325\n",
      "Step - 9903, Loss - 0.34508439346276726, Learning Rate - 9.765625e-05, magnitude of gradient - 0.013807447343274519\n",
      "Step - 9904, Loss - 0.3127117895445609, Learning Rate - 9.765625e-05, magnitude of gradient - 0.007768894403649718\n",
      "Step - 9905, Loss - 0.30869512949141353, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01717030923894087\n",
      "Step - 9906, Loss - 0.317140724281585, Learning Rate - 9.765625e-05, magnitude of gradient - 0.060563910578009424\n",
      "Step - 9907, Loss - 0.28952006799088625, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02253487714618933\n",
      "Step - 9908, Loss - 0.4015740563724188, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08335617508241278\n",
      "Step - 9909, Loss - 0.346266264051479, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0506482023383764\n",
      "Step - 9910, Loss - 0.2884032860615541, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03674712408108989\n",
      "Step - 9911, Loss - 0.33571780065965035, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05829077158140484\n",
      "Step - 9912, Loss - 0.2536646641867073, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08574482930984845\n",
      "Step - 9913, Loss - 0.26787271935972634, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0343587030202778\n",
      "Step - 9914, Loss - 0.3427347601907318, Learning Rate - 9.765625e-05, magnitude of gradient - 0.033543907484053094\n",
      "Step - 9915, Loss - 0.26615385888978854, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03449685983771709\n",
      "Step - 9916, Loss - 0.38535049859333353, Learning Rate - 9.765625e-05, magnitude of gradient - 0.052597749058580015\n",
      "Step - 9917, Loss - 0.2512744427832178, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0555356095246573\n",
      "Step - 9918, Loss - 0.2922931359908335, Learning Rate - 9.765625e-05, magnitude of gradient - 0.12714361178887307\n",
      "Step - 9919, Loss - 0.2567259826379275, Learning Rate - 9.765625e-05, magnitude of gradient - 0.039162070047947715\n",
      "Step - 9920, Loss - 0.3586571362590229, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0437450184208795\n",
      "Step - 9921, Loss - 0.4051801472613947, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0602775786589338\n",
      "Step - 9922, Loss - 0.3220368598975396, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05249503353408732\n",
      "Step - 9923, Loss - 0.3545910638184952, Learning Rate - 9.765625e-05, magnitude of gradient - 0.008915734873392558\n",
      "Step - 9924, Loss - 0.2622847878430612, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02346143798400428\n",
      "Step - 9925, Loss - 0.2430803846811826, Learning Rate - 9.765625e-05, magnitude of gradient - 0.059669027020161844\n",
      "Step - 9926, Loss - 0.2708039808179536, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02417867116686873\n",
      "Step - 9927, Loss - 0.34252678684714555, Learning Rate - 9.765625e-05, magnitude of gradient - 0.042115592921270956\n",
      "Step - 9928, Loss - 0.24823167437090965, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04375867510664281\n",
      "Step - 9929, Loss - 0.25598983930854335, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05089612000760662\n",
      "Step - 9930, Loss - 0.3088702690486699, Learning Rate - 9.765625e-05, magnitude of gradient - 0.021500508785941112\n",
      "Step - 9931, Loss - 0.33217854852889495, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01978444506459051\n",
      "Step - 9932, Loss - 0.3507366208656062, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03596792503727631\n",
      "Step - 9933, Loss - 0.29949244234053307, Learning Rate - 9.765625e-05, magnitude of gradient - 0.035164852507461895\n",
      "Step - 9934, Loss - 0.2863447870904075, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03089744027908834\n",
      "Step - 9935, Loss - 0.35795765649396033, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04129637700758366\n",
      "Step - 9936, Loss - 0.31032859763477916, Learning Rate - 9.765625e-05, magnitude of gradient - 0.050042364141927906\n",
      "Step - 9937, Loss - 0.2530378548092449, Learning Rate - 9.765625e-05, magnitude of gradient - 0.025974720383108055\n",
      "Step - 9938, Loss - 0.28703142605381565, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09869238249259119\n",
      "Step - 9939, Loss - 0.2924157602397339, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04951324866286288\n",
      "Step - 9940, Loss - 0.3934130545813628, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07575872245654404\n",
      "Step - 9941, Loss - 0.26319257954450115, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06268434009804888\n",
      "Step - 9942, Loss - 0.3646269314486525, Learning Rate - 9.765625e-05, magnitude of gradient - 0.019292383729557726\n",
      "Step - 9943, Loss - 0.29708769701313614, Learning Rate - 9.765625e-05, magnitude of gradient - 0.042589194695731886\n",
      "Step - 9944, Loss - 0.3372733811085308, Learning Rate - 9.765625e-05, magnitude of gradient - 0.036015347529119515\n",
      "Step - 9945, Loss - 0.3296727576909116, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06042758330566693\n",
      "Step - 9946, Loss - 0.33167557236221845, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02468648982162482\n",
      "Step - 9947, Loss - 0.27923230130332644, Learning Rate - 9.765625e-05, magnitude of gradient - 0.049422386444776915\n",
      "Step - 9948, Loss - 0.22637659404132593, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06852846427412784\n",
      "Step - 9949, Loss - 0.31448994039741546, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03364270711986192\n",
      "Step - 9950, Loss - 0.3699458848734995, Learning Rate - 9.765625e-05, magnitude of gradient - 0.047194106374688696\n",
      "Step - 9951, Loss - 0.31117414429762946, Learning Rate - 9.765625e-05, magnitude of gradient - 0.057556137995918265\n",
      "Step - 9952, Loss - 0.3306983141761343, Learning Rate - 9.765625e-05, magnitude of gradient - 0.039028444429641776\n",
      "Step - 9953, Loss - 0.22818004239431555, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09095103111986527\n",
      "Step - 9954, Loss - 0.25100481181346185, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0566478968190408\n",
      "Step - 9955, Loss - 0.39036620958874785, Learning Rate - 9.765625e-05, magnitude of gradient - 0.052072969995110525\n",
      "Step - 9956, Loss - 0.2977431861929636, Learning Rate - 9.765625e-05, magnitude of gradient - 0.006772228739777667\n",
      "Step - 9957, Loss - 0.29769496966941866, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07259292518651479\n",
      "Step - 9958, Loss - 0.35097989317493394, Learning Rate - 9.765625e-05, magnitude of gradient - 0.032568127062016596\n",
      "Step - 9959, Loss - 0.3773347251103746, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09283573010692309\n",
      "Step - 9960, Loss - 0.32224792397043933, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06924835342942008\n",
      "Step - 9961, Loss - 0.26155996430816164, Learning Rate - 9.765625e-05, magnitude of gradient - 0.013277108633613362\n",
      "Step - 9962, Loss - 0.3315496212665551, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07018112666698963\n",
      "Step - 9963, Loss - 0.2816748210501961, Learning Rate - 9.765625e-05, magnitude of gradient - 0.017330541751204145\n",
      "Step - 9964, Loss - 0.2889276827959939, Learning Rate - 9.765625e-05, magnitude of gradient - 0.052340422431686726\n",
      "Step - 9965, Loss - 0.2987407414775896, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08204161257218857\n",
      "Step - 9966, Loss - 0.31212167434761845, Learning Rate - 9.765625e-05, magnitude of gradient - 0.049280252840807774\n",
      "Step - 9967, Loss - 0.3302317747580209, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05507097826224133\n",
      "Step - 9968, Loss - 0.3179658199353331, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07198933153021844\n",
      "Step - 9969, Loss - 0.26990770395744607, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02583806579902408\n",
      "Step - 9970, Loss - 0.28554064006957314, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08365178419711104\n",
      "Step - 9971, Loss - 0.3752685222504412, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09413612967876699\n",
      "Step - 9972, Loss - 0.29088545364671525, Learning Rate - 9.765625e-05, magnitude of gradient - 0.0780857816524027\n",
      "Step - 9973, Loss - 0.23103643386856815, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08658268950328124\n",
      "Step - 9974, Loss - 0.2629416707936101, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04795778421769774\n",
      "Step - 9975, Loss - 0.3096761509437698, Learning Rate - 9.765625e-05, magnitude of gradient - 0.012786383701535875\n",
      "Step - 9976, Loss - 0.3454098560913315, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07541536340730068\n",
      "Step - 9977, Loss - 0.31948028999257566, Learning Rate - 9.765625e-05, magnitude of gradient - 0.049725574412211565\n",
      "Step - 9978, Loss - 0.3238437480847341, Learning Rate - 9.765625e-05, magnitude of gradient - 0.013459491710814215\n",
      "Step - 9979, Loss - 0.3777129851093057, Learning Rate - 9.765625e-05, magnitude of gradient - 0.059824537638197464\n",
      "Step - 9980, Loss - 0.25714422515837376, Learning Rate - 9.765625e-05, magnitude of gradient - 0.025509284606477677\n",
      "Step - 9981, Loss - 0.3108081792070595, Learning Rate - 9.765625e-05, magnitude of gradient - 0.03382839136284044\n",
      "Step - 9982, Loss - 0.3722338711422374, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07824224568642485\n",
      "Step - 9983, Loss - 0.36197127540454926, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02391556777374349\n",
      "Step - 9984, Loss - 0.3708132548621327, Learning Rate - 9.765625e-05, magnitude of gradient - 0.02101541024194821\n",
      "Step - 9985, Loss - 0.2717005497292857, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04425820621278897\n",
      "Step - 9986, Loss - 0.3220069195662959, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06077275504482709\n",
      "Step - 9987, Loss - 0.3318274631371846, Learning Rate - 9.765625e-05, magnitude of gradient - 0.056139498492169426\n",
      "Step - 9988, Loss - 0.305231228404964, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09170411421761299\n",
      "Step - 9989, Loss - 0.2716012133360376, Learning Rate - 9.765625e-05, magnitude of gradient - 0.05581789206613959\n",
      "Step - 9990, Loss - 0.4267270053419442, Learning Rate - 9.765625e-05, magnitude of gradient - 0.06928769315726689\n",
      "Step - 9991, Loss - 0.4002837333270357, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08489175664381225\n",
      "Step - 9992, Loss - 0.4090396015585104, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08022841826477595\n",
      "Step - 9993, Loss - 0.2531523769630789, Learning Rate - 9.765625e-05, magnitude of gradient - 0.07493858524058544\n",
      "Step - 9994, Loss - 0.35369637872820414, Learning Rate - 9.765625e-05, magnitude of gradient - 0.08221462998385914\n",
      "Step - 9995, Loss - 0.2944573191544836, Learning Rate - 9.765625e-05, magnitude of gradient - 0.030116961893716848\n",
      "Step - 9996, Loss - 0.2503701709494207, Learning Rate - 9.765625e-05, magnitude of gradient - 0.09678817259027736\n",
      "Step - 9997, Loss - 0.30676955210819423, Learning Rate - 9.765625e-05, magnitude of gradient - 0.04416831412125229\n",
      "Step - 9998, Loss - 0.3603649190028056, Learning Rate - 9.765625e-05, magnitude of gradient - 0.023182423556318076\n",
      "Step - 9999, Loss - 0.2754006698449513, Learning Rate - 9.765625e-05, magnitude of gradient - 0.01928073006730433\n",
      "Step - 10000, Loss - 0.25843764964320015, Learning Rate - 9.765625e-05, magnitude of gradient - 0.008299414631116227\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.random([num_labels,num_attri])\n",
    "print(weights)\n",
    "num_steps = 10000\n",
    "learning_rate = 0.1\n",
    "records = []\n",
    "for iind in range(num_steps):\n",
    "    selected_data = np.random.randint(0,num_rec,100)\n",
    "    loss,gradient=LossFunctionGradRegularization(attri[selected_data,:],label[selected_data],weights,1)\n",
    "    old_weights = weights.copy()\n",
    "    \n",
    "    grad_mag = np.linalg.norm(gradient)\n",
    "    \n",
    "    if(iind%1000 == 0):\n",
    "        learning_rate = learning_rate*0.5\n",
    "    \n",
    "    weights = weights-learning_rate*gradient/grad_mag\n",
    "    \n",
    "    records.append([iind+1,loss,learning_rate,grad_mag])\n",
    "\n",
    "    print(f'Step - {iind+1}, Loss - {loss}, Learning Rate - {learning_rate}, magnitude of gradient - {grad_mag}')\n",
    "    \n",
    "    if(grad_mag <1E-6):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a14ddc3850>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEvCAYAAABhSUTPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA7H0lEQVR4nO3de3wU1f3/8c9CpCoqlavAghgDCOEiEkRtvyhUwAuCRUUQaxFpQLDKo2qtWq0oAlrqDVB+Abwgt1pbhSoEFAFRRAhXBQXERJLgJeEmd0hyfn+cDju7OzN7mZ1sWF7PxyOPzc7O5ezO7Mx7zjkz61NKKQEAAEBcqiW7AAAAACczwhQAAIALhCkAAAAXCFMAAAAuEKYAAABcIEwBAAC4kJasBdetW1eaNWuWrMUDAABEraCgQEpLSy1fS1qYatasmeTl5SVr8QAAAFHLysqyfY1mPgAAABcIUwAAAC4QpgAAAFwgTAEAALhAmAIAAHCBMAUAAOACYQoAAMAFwhQAAIALhCkAAAAXUjdMlZaK5OSIFBQkuyQAACCFpW6YKioSGTpUZN26ZJcEAACksNQNUz6fflQqueUAAAApjTAFAADgQuqHKQAAAA+lbpgyUDMFAAA8lLphimY+AABQCQhTAAAALhCmAAAAXCBMAQAAuECYAgAAcIEwBQAA4AJhCgAAwIXUD1MAAAAeSt0wZaBmCgAAeCh1wxTNfAAAoBJEDFODBw+W+vXrS5s2bRzHW716tVSvXl3efvvthBXOFcIUAACoBBHD1KBBgyQ3N9dxnPLycnnooYekZ8+eCSuYa4QpAABQCSKGqS5dukjt2rUdx5kwYYLcdNNNUr9+/YQVzDXCFAAAqASu+0wVFxfLO++8I8OGDYs4bk5OjmRlZUlWVpaUlJS4XbQzwhQAAKgErsPUyJEj5ZlnnpHq1atHHDc7O1vy8vIkLy9P6tWr53bRzghTAACgEqS5nUFeXp70799fRERKS0tl/vz5kpaWJjfeeKPbWbtDmAIAAJXAdZjKz88/8f+gQYOkV69eyQ9SIty0EwAAVIqIYWrAgAGydOlSKS0tFb/fL6NGjZLjx4+LiETVTyrpqJkCAAAeihimZs+eHfXMXn/9dTdlSSya+QAAQCXgDugAAAAuEKYAAABcIEwBAAC4QJgCAABwgTAFAADgAmEKAADAhdQPUwAAAB5K3TBloGYKAAB4KHXDFM18AACgEhCmAAAAXCBMAQAAuECYAgAAcIEwBQAA4AJhCgAAwAXCFAAAgAupH6YAAAA8lLphykDNFAAA8FDqhima+QAAQCUgTAEAALhAmAIAAHCBMAUAAOACYQoAAMAFwhQAAIALhCkAAAAXUj9MAQAAeCh1w5SBmikAAOCh1A1TNPMBAIBKQJgCAABwIWKYGjx4sNSvX1/atGlj+frMmTOlXbt20q5dO7niiitkw4YNCS9kXAhTAACgEkQMU4MGDZLc3Fzb1y+44AJZtmyZbNy4UR577DHJzs5OaAHjRpgCAACVIC3SCF26dJGCggLb16+44ooT/1922WVSVFSUkIK5RpgCAACVIKF9pqZNmybXXnttImfpHmEKAAB4KGLNVLSWLFki06ZNk08++cR2nJycHMnJyRERkZKSkkQt2p7PR5gCAACeSkjN1MaNG2XIkCEyd+5cqVOnju142dnZkpeXJ3l5eVKvXr1ELNoZYQoAAHjMdZjasWOH9O3bV958801p0aJFIsqUONwFHQAAeCxiM9+AAQNk6dKlUlpaKn6/X0aNGiXHjx8XEZFhw4bJk08+Kbt27ZLhw4frGaalSV5enreljgU1UwAAwEMRw9Ts2bMdX586dapMnTo1YQVKKJr5AACAx1L3DugihCkAAOA5whQAAIALhCkAAAAXCFMAAAAuEKYAAABcIEwBAAC4kPphCgAAwEOpHaZEqJkCAACeSu0wRTMfAADwGGEKAADABcIUAACAC4QpAAAAFwhTAAAALhCmAAAAXCBMAQAAuJD6YQoAAMBDqR2mRKiZAgAAnkrtMEUzHwAA8BhhCgAAwAXCFAAAgAuEKQAAABcIUwAAAC4QpgAAAFwgTAEAALiQ+mEKAADAQ6kdpkSomQIAAJ5K7TBFMx8AAPAYYQoAAMAFwhQAAIALEcPU4MGDpX79+tKmTRvL15VScu+990pGRoa0a9dO1q5dm/BCxo0wBQAAPBYxTA0aNEhyc3NtX1+wYIFs27ZNtm3bJjk5OXL33XcntICuEKYAAIDHIoapLl26SO3atW1fnzt3rtxxxx3i8/nksssuk71798r333+f0ELGjTAFAAA85rrPVHFxsTRp0uTEc7/fL8XFxW5nmxiEKQAA4LE0tzNQFmHFZ3OzzJycHMnJyRERkZKSEreLjoybdgIAAI+5rpny+/1SWFh44nlRUZE0atTIctzs7GzJy8uTvLw8qVevnttFR4eaKQAA4CHXYap3794yffp0UUrJypUrpVatWtKwYcNElM09mvkAAIDHIjbzDRgwQJYuXSqlpaXi9/tl1KhRcvz4cRERGTZsmFx33XUyf/58ycjIkDPPPFNee+01zwsdNcIUAADwWMQwNXv2bMfXfT6fTJo0KWEFSijCFAAA8Bh3QAcAAHCBMAUAAOACYQoAAMAFwhQAAIALqR+mAAAAPJTaYUqEmikAAOCp1A5TNPMBAACPEaYAAABcIEwBAAC4QJgCAABwgTAFAADgAmEKAADABcIUAACAC6kfpgAAADyU2mFKhJopAADgqdQOUzTzAQAAjxGmAAAAXCBMAQAAuECYAgAAcIEwBQAA4AJhCgAAwAXCFAAAgAuEKQAAABdSP0wBAAB4KLXDlAg1UwAAwFOpHaZo5gMAAB4jTAEAALhAmAIAAHCBMAUAAOBCVGEqNzdXWrZsKRkZGTJu3Liw1/ft2yc33HCDtG/fXjIzM+W1115LeEHjQpgCAAAeiximysvLZcSIEbJgwQLZvHmzzJ49WzZv3hw0zqRJk6R169ayYcMGWbp0qdx///1y7NgxzwodNcIUAADwWMQwtWrVKsnIyJD09HSpUaOG9O/fX+bOnRs0js/nk/3794tSSg4cOCC1a9eWtLQ0zwodNcIUAADwWMQwVVxcLE2aNDnx3O/3S3FxcdA499xzj3z11VfSqFEjadu2rbz44otSrVoV6I7FTTsBAIDHIiYeZVGz4wsJKQsXLpSLL75Ydu7cKevXr5d77rlHfv7557DpcnJyJCsrS7KysqSkpMRFsWNAzRQAAPBQxDDl9/ulsLDwxPOioiJp1KhR0Divvfaa9O3bV3w+n2RkZMgFF1wgX3/9ddi8srOzJS8vT/Ly8qRevXoJKH4ENPMBAACPRQxTnTp1km3btkl+fr4cO3ZM5syZI7179w4ap2nTprJ48WIREfnxxx9ly5Ytkp6e7k2JY0GYAgAAHovYSzwtLU0mTpwoPXv2lPLychk8eLBkZmbK5MmTRURk2LBh8thjj8mgQYOkbdu2opSSZ555RurWret54SMiTAEAAI/5lFWnqEqQlZUleXl53i7k2mtFdu0SWbXK2+UAAICU5pRbqsAldx6iZgoAAHisCtwMykPffScScoNRAACARErtmimCFAAA8FhqhykAAACPEaYAAABcIEwBAAC4QJgCAABwgTAFAADgAmEKAADAhdQOUzfckOwSAACAFJfaYapBA5GGDZNdCgAAkMJSO0zxczIAAMBjqR+mAAAAPJTaYUqEmikAAOCp1A5TNPMBAACPEaYAAABcIEwBAAC4kPphCgAAwEOpHaZEqJkCAACeSu0wRTMfAADwGGEKAADABcIUAACAC4QpAAAAF1I/TAEAAHgotcOUCDVTAADAU6kdpmjmAwAAHiNMAQAAuECYAgAAcIEwBQAA4EJUYSo3N1datmwpGRkZMm7cOMtxli5dKhdffLFkZmbKlVdemdBCxo2r+QAAgMfSIo1QXl4uI0aMkA8++ED8fr906tRJevfuLa1btz4xzt69e2X48OGSm5srTZs2lZ9++snTQseEmikAAOChiDVTq1atkoyMDElPT5caNWpI//79Ze7cuUHjzJo1S/r27StNmzYVEZH69et7U9pY0cwHAAA8FjFMFRcXS5MmTU489/v9UlxcHDTO1q1bZc+ePXLVVVdJx44dZfr06ZbzysnJkaysLMnKypKSkhKXRY8CYQoAAHgsYjOfsggjvpC+SGVlZbJmzRpZvHixHD58WC6//HK57LLLpEWLFkHjZWdnS3Z2toiIZGVluSl3dAhTAADAYxHDlN/vl8LCwhPPi4qKpFGjRmHj1K1bV2rWrCk1a9aULl26yIYNG8LCVKUjTAEAAI9FbObr1KmTbNu2TfLz8+XYsWMyZ84c6d27d9A4ffr0keXLl0tZWZkcOnRIPv/8c2nVqpVnhY4aV/MBAACPRayZSktLk4kTJ0rPnj2lvLxcBg8eLJmZmTJ58mQRERk2bJi0atVKrrnmGmnXrp1Uq1ZNhgwZIm3atPG88FGhZgoAAHjIp6w6RVWCrKwsycvL83Yhjz4q8swzImVl3i4HAACkNKfcwh3QAQAAXCBMAQAAuECYAgAAcCH1wxQAAICHUjtMAQAAeCy1w5RRM0VTHwAA8AhhyslPP4ksWpS48gAAgJQT8aadJzW3YaprV5HNm0XKy0WqpXbuBAAA8UnthPDWW/rx0KH4pt+8WT/SkR0AANhI7TC1aZN+LC5ObjkAAEDKSu0wZaBmCQAAeCS1w1Ta/7qE1ajhbj5cDQgAAGykdpiaOFE/pqV2P3sAAJA8qR2mjBqp8vLklgMAAKSs1A5T1avrR8IUAADwCGEqGvSZQmUoLRVZsiTZpQAAxIgwdTIqKhKZNCnZpUCiXX21SLduqbe9oupp1EjkgguSXQogZaR2z2wjTFVUJLcciXbddSJffCHSt69Iw4bJLg0S5Ysvkl2CU8M334j84hciTZokuyTJ8/33yS4BkFJSu2bK+AkYr8/0v/1W5OOPvV2G2a5d+tHvF7nhhspbLuzt3y+yZ09i5kWzsreaNxdp2tSbeR87RlMtTh7l5SJ164q88UayS3LSOzXClPngtHevSEFBbPOJdHC78EKRK6+0n/bZZxN7JmiUp6JC5L33Yp8+N1dk1arElaeqmjxZ5OuvK2dZDRuK1K5dOctC1fXQQ7qpds2aZJcEiOzQIX1yfs89yS7JSe/UCFNGM9/KlSLnnlu5fQW+/FLvYG+9tfKWGcm114p07pzsUnjv7rtFLrkk9unefVdk69bYpjl4MPblIPUYv+dZUpLccgCoVKdWmLr88sBrSons3Bnb/Hbtiv2Ms6xMP+7fH9t0oX78Udd8bNjAz+PE4vDh2Kf57W9FWrZMfFmAVPTKK3qfdOCA/TiHD4vceSchEynr1ApTZlOmiDRuLLJ2bfTzu+wykaysxJQtVvPn6z45L7zgbZ+aESNE/vMf7+aP2FVUiEyYIHLkSPLKUFgoMn48/blOVuPGedeXa/x4/fjjj/bjTJ8u8vrrIo8+6k0ZzB55RGTWLO+XA5icGmHqs8/CXzN2LFu2WE87eXLgf+MA8s03iSublbFjRerUcT+frVtFli+Pb9qXXxa56ab4l11RIfLSS6dWs5fbWsdQoYFl1iyRe+8VefLJxC4nFk2bijz4oMicOfbj/Otfuobiu++im+eXX+rxv/02MWWsCqpirXF5ucjDD+u+XNFas0akuDi6cWMJ2NF8PkrprhG/+pXItm3Rz9swdqzIwIGxT2d44gmRHj3inz4a48bp/WSiPfmkyD//6TzOjh36wiWnmkQ7H38s0qZNck/sqqhTI0zdd5/9OIWFIm+9FT787rvtpzl4UGT9eldFs/TIIyK7d1u/Zt5hRdohtWwp0qVL8LCXX7YOlU6efjr2Zs1339Wf91/+Ett00frpJ91cG+2OvjKcc4638//5Z/24fLle99u3e7s8M6VEjh4NPP/hB/txjSuCNm6Mbt6vvqof33knvrKZ7dwp8stfVp3bSzz0kMhttyVn2du3i3zwQeD5Cy8E/o/2hDArS6RZM+vXuncXmTs3vrLl5ITvvz77LHi73rRJX7SzYoVIdnZ8y4nGTz+JnHGG7ktrNmpU8OfnhYcfdj4uxetvfxPp3995nL/8RV+4FM86vOcevX62bhXJyxNZty6+cqagUyNMOTE6h8dyL6qBA0U6dNB3rO7Y0Xqc8nJ9pm4OQQUFIn/+s+5HFe3tGowDqcHni6+pZcQIkSuuiG2av/41uFlz5077sGcwaqTc3iZg3jyRkSPDh0+bpnd+EyYEhv3hDyKLFgWeL18e/Dwax46JfPVVXEVNuND1azz/5BP9+O67lVeWhx8WOf30wHOnIG+U02mcMWOs182WLSINGsQfkv/7X5F9+wI/bp4oO3eKrF4deTyldPO4sR/ZuFFk9uzEliVaGRnBNSvmvqHNm0c/H6O/Z6gPPxS58cbAc7e1ca+/HpivSPC+2M28I11EsmyZrmH5xz/085IS5+UdPCjy+efxl6eq8fki12KFMtZNtWoinTpFf4HP+vV6eUYlxJdfRnccq6jQx+hoa7uT6NQJU6EhYO/e4OdOK3bKlOAD7aef6sevv7bvc/X88yL9+gW33d98s8jf/y5y2mnOl9EbG/jHH4vUqiXy73/rv2Rr3FikXj3ncRLVzNGnj8iLL0ae/9NPi0ydKtKzZ2BYly7Bz81efVXPw7z+P/1U5OyzRVq39v5mhocP66tJjQOHmd1nZxeuQu3erc/oo9lJ/fvf0V2AMXVq5HFChb6P1at108miRbrPjNW6ueYaXVPw9tuxL8/M6b2vWhV+oDbXZOXlhQegFi1ELr3Uen7vv69vi+LziTz2mG4ej7ZGY/t2PV0073fPHj1uvP2AFi+ObzoroZ9vcXF4M61dCIt23seOBYZZfSdWrtSh0NxMtWtX+Laanx/b8iM1N992m+43a9zn72S3Z48+EY2F8f2JdT9v1D7Pnav3fW3bhq+vu+4KP+Ffv17v0/r1s5/3jh36OJDk/pynTpgK7YuUmxv83GlFjBihV76htNR6PHMzWlGRfvzpp8Cw48cD/4fWOJkZzSTG/G6+WXdArwrsavAWLtR3lXa6em7vXn2GkZ8vsmBB8M7w+PHgnWgkSum/v/41+mlEAgHNfKbz618Hlm0OWUuX6kvdDx7UO49E9HHYulXXUA4ebN9kZmyLe/Y49xnZu1efXRuGDdNncUuXOpfh2DG9TXXtGrm8sRw47GqmLr1UN53YNf36fLHf+23nTuuz1SlTrE9wli7VtwMxaiEM5vDTqZM+YO7YITJggD6Zsev7t2+fSK9egQPwyy/HVn6j+co4cSovt68ZNZrmnnhC5KOPdHOmz6dryKdO1SdoTjXWGzZEV6biYpHbbw/vD2P0CezVK7y2P3TeO3bok8Vp06Jbpkj49tKpk/P4Dz2kPxNzF4SBA3UwMA/z+fR6qlFD72+eeMLd1YR5efqxMvsLHTki8swzOqBOnixyyy2xfbZOIt1b6sEHdW2xzxdoaTC2sWhafewYNYah39NXXw3vimIszymg33KLPg5U1j0FbUT1ieTm5krLli0lIyNDxo0bZzve6tWrpXr16vK227PLRIllhUdq5rNqlgvdeVk1o5l3FNEm53hrdw4erJxLj0NrNA4c0DULx47pL6Cd+vV1P4z0dP2TOEOGBF5r3VqHsUjMn02sQUokuD/Pjh3hNX7mbaZrV5HMTJGzztLPn38+9uWJBML3998HtpHCQn2jz82b9SXj330Xvo1dfLGuGQndbozPoHdvkauuChzw9+3Tj5FCqbGtm8PI0qXR1VQZy963T4c3c9iI1Mxn7l8xZYr151lWpmuvIm3HjRvb9+n53e/04w8/6LLMmBF4r19+GTyu1ff6P//RHe2vu85++aEH1Fiatffv16FIJPCZPfWU/g58+aX9wXrbNpHf/CawnmfN0gHi4Yf183nzAq/FY+RIkZkzdZOpYdUq3SfwnXd0TZzZBx/oZZoZF/Q4Xahgdvy4yP/7f/avW21LVvtR40rC0D5WX36pl3HddTrQG/uct9/W3383Pv9cl2/pUv25Hz+ua83c9musqBAZPlyXfdw4fRIyZYrux/v228H7zc2b9fK8uFJz/PhAZYDR/B5vzVQsfX737o3tV0uM/V08t8FJoIhpo7y8XEaMGCELFiyQzZs3y+zZs2WzcWO6kPEeeugh6WnXvJIMsYQppXQocPs7fjVrOi8jFlYb3SefhDdFGWdgl1yiA4vXGjcOfm7e+Ro7c6V07Zxx5l9eHlwzJxJc62LXMfb883WfiocfDu7orVSgr0W0QmsyLr1U19CYvfJKbPOMZOFC3TS6cKGuvTh0KPj1zEz9PqyCgbGzj1TjEPq5RuorYjW/rl11ePvtb3XthIgOfHbGjNEHQfPnZYz/88/67Nlpezcf9Mzb+auv6nkPH+78HpwYyzVqepzO5KNtCjJqJdxYtkw3KVvtY4wz8qFDdadocyhasMB5vsZvkN54Y/iVuNdeG3dxRSTQX6xv3/DXevQIDkLvv2+/zu1+cSHWe/1FElqTG7oPnT9f1wbecovIAw/oYdH09bNiBOKFC3Vt4W9/G/g+ZGToQLVokfMJpuG993Q/3PJyvZ965RV9xZ1RKxi63/D59H7DKMO//uU8/4oK3Tpz2WXRvrtgxn4yETVTBrtt5dxzdd/iaBkn4eYLZZIg4ieyatUqycjIkPT0dKlRo4b0799f5lpcBTBhwgS56aabpH5lHMyjFcsK37tX95t57LH4Lhk1HDoU3D9r+nT7cc1J2tzM4fSltmv2efjh6O7abddX64EH9C/JGxLxczPt2wc6wjpdHWkWGgx27NBt6ePG6R2L02czeLB1GHrqKV2LE3rGb3VfHKt+Wk7sQuDOnbpfx8yZ+nksV1KG7mTsrvqx+yzuvdd+3uaO0aHTl5Tozu0zZ+oDhNPv1xnV7uZ5bNqkH4cN02fP0XbWNX/mxona22/rfnNjx0Y3j3gZ68eKeT28955+r0b5oj0xMu8rr7pKNymbhdaMrlihH43azPvu01doOTHv58z9o1avDu/OIBL5hNFcgxjLz4zMmhU4eTp6VO9T33lHX6RjXLlpMHd/cOLziTz3nH4cOtR5vFAHDoSHkLIy+6vdQuehVPAJhd06N4aH1txt2qT7Bxr34bLzxBM6OK1fH9zNoKAgEBCslj1ypMgf/6j/d/o8V63SgXv3bv2djOXeigajxtHcAT2SmTMD2+5TTwWGG5+z03fIaP62G+ettwL7m5MlTBUXF0sT06+r+/1+KQ654qa4uFjeeecdGTZsWOJL6IZxxhYNo1lh1qzoD/x26tQJ3wi++y58mNGvSiT4J26mTIl9mXY1Krt3B/f3smqOqKjQfUnMNV6J+LkZc6i0ek9r1+ozKvNZeI0a4U0HVn7+Ofys9rXXrGs0Hn9c157FWpNlZ/LkwBfZ7uqomTN10HrzTf081lrJSZPsXzMuLzfXAoayqD0WER1w77pL/+8UTO12uMY0TtX9xgEh2mp3uyt15s3Ttwu56CL7/lbGspya05WyD3ZWfRetPs9Ro/TjhAmx9Z+78Ubd4TbezrHRLMfuwGbXcd68XzRq5ioqAmWM9tYWVozgsHy5rmEYMMB6W2rQINAMa9i1K7wDv88ncv/9+v+cnODvvFErY2a+59tNN+nbOESjoiK8L+yrr+oTCiPgmh0+HHsfygMHdJ88cxP27t2BbUtErwNzPzRjP2B1cmveb/7wg33T2A03BD+3u7eiYd063e8tVF6edZj6+GP9WFamy/TPf+pa2Ntvt743nnHCbO4P9fTTweM4fV9efFFfgd+mjfP7qGQRw5SyeFO+kB3oyJEj5ZlnnpHqEcJLTk6OZGVlSVZWlpRUtZ8VMDbWgoLgkBOv0J20XZ+KuXPDa5t+/FHXyCTiyri5c8P7iZgdORJd6HS6v5BVOc1nvyNG2E/br1/4TskplBsdrp36Wdhx6O8XtZdf1mHbyy+yUs41AnY1kOZtzLhpa16eDiVWzVSHD0fesYYyDiyJrO6PZMsW3QnXyrnn6lq0UF98oZtdRPQ2Y5xsOF34YTAOSqG1Gsa8RGKrMevePfo+RPGwKme00tN1f5vq1QO1ZKE1LG44HRRDa4b/+tfwm2WGhqsnnghs28aB+sgRd/c7OnBAf59D+8gZfZNCT0x27BBp0sR5nygS/N7nzdMne7NnBweM0AujyssD261ZNM14aWnBw3bt0kEn2lpAw8SJ1h2+O3UKvGfz9/7KK/Xj4MG6ubN/f10LazD69InoGmej5tyYx8yZ4f1fldIXE1gdO8y3zHF7C54ESos0gt/vl0JTVWdRUZE0MjcHiUheXp70/1/VaWlpqcyfP1/S0tLkRvO9SEQkOztbsv/XVyKrMn6WJZZqP/ON7aK9tNdpRxFNLYhSwfdrMTv/fH1JqBvVq4u0a+c8ToMGkedjFZa2btVfnGPHrD8H88Ej0pVOoQc4p9sTJOqqxniCakFB8Jfbah4+n36/oa/Fcvdyp/Br5Ztv9Hqwuh+S+cooqzu1X3SRdbOgXRn++U+9bs39kv70p8hNGYkwf77eAV9zTfDwjz/W3xeDUrpTv1VnbKt7dOXnB3/XnLa/eE+0jE7xVowrU83++U9dK1cZ4qkJj1YsV+hGI7Sst93m/gabVmHcLPS73LVr8LHFHPTtunX06RNdTVa8v3IQeuufFStEfv/76G/SeviwDl+RfoHDqG222vcZtfChzCex5v2KMQ+rkwGlRK6+OlDzbFdTXru27p9WBUQMU506dZJt27ZJfn6+NG7cWObMmSOzQu53km/qxDlo0CDp1atXWJBKiniv5jNujhiJ2x1FpL4LTh2Ao51/pDu1R3OmbsX8Q8Dmaup4uDmzjpeXByo3nadF7JtnYh0/9OaYVv1nRKx38pHuaWQc/HNy9KNV07hxJV2iXH998LINFRXh/WliuXP/wIHBfdqee85+3Hh/Osjp6qTDh8MDwaOPVl6YStbNRROhMsoeehyJ9iQ9Uh8rK/HewT+0hvlXv4pt+vJykbp1ox8/ESE5tMuAmV2tn9Xn4/XPvEUpYtpIS0uTiRMnSs+ePaVVq1bSr18/yczMlMmTJ8tk8+/XVUWxhKl4bsQWbXu8wehnY4h0+af5Lt9eSNQNNiN1kI0kUs2V2yssrSSyKaMq2rkz/OaYbsO5YdOm8HVid5dwpxqZeIX2sQrtmxVr/6TK7nIQejXsRRdV7vK9Es/dwc01ilVVYaFuuYj1hr6hzZhGDbBTy0dV/G1HKy1auJ/HK6/o92tXoxXK54vc0pJMKkk6duzo/UJWrDAq0Kvm38aNyS8Df/zF+tekiVIdO0Ye76OPKqc8Z5wR/LxFi+R/Ron4Ky1Nfhm8/Fu/PvllqGp/V16Z/DLE8zdzZvLLsGSJ55HCKbdEbOY7qVVG51g3BgxIdgmA2BUWRlfL1a2b92URCa+Z8qImMxliaXY5GYX+GDvi/ymeZIu3r1cKqeJpw6WqHqZCm/0AuFdF+lAggnj7a6Yy43dfTzaxXhXsBaWSuvgqnjZcquphCgAAuBfNneY9lNppgzAFAEDqi+UKXg+kdtogTAEAAI+ldtogTAEAAI+ldtogTAEAAI+ldtqI5YeOAQAA4pDaYYqaKQAA4LHUThsXXpjsEgAAgBSX2mGKZj4AAOCx1A5TAAAAHiNMAQAAuECYAgAAcIEwBQAA4AJhCgAAwAXCFAAAgAuEKQAAABdSP0w1aZLsEgBVR4MGyV1+RkZylw8kQ79+yS4BPJb6YapWrWSXAPBWhw7Rj/vrX3tXjmh89FFyl++1+vWTXQJURT5fsksQu7/9LdklOKmkfpgaODDZJUieO+4QOe20ZJcC8fjLX6If99prox93yJDYy5JIJ+NBJRZpaclbtlWo/uKLyi8Hwp2M2/0TTyR3+RUVyV1+jFI/TD30UHzTZWaK+P2JLUtlOvNMkTfeEFGqcpY3b15i5jNlSuWVOV69e3u/jObNrYfn54cPe/LJ6Ob54YeRf2Kpdu3o5nXkSHTjmb3xRtU9qJx/fuRxLr9cZO1a53ESFabmzo19mnr1wocl4rs0fLj7ecRiwQKRO+9MzLzWrIl/2kS2arRs6fz64sXRz+uBB9yVpTItXy4yenTk8e65J/j5u+9W3X2FjdQPUz6f9QEokpwc+wNaVffKK5F3+ol2ww2JmU+1Kr5JLl0qUqeO/etHjyZmOXYHwWbNwodF+xuU1auLNG4cPKxmzeDn5pqMLl2c5zV7tkjnzroG1OB04DWPJyLyq185l9eNV1+NbfyCgsjjVK8euUk1UWEqnsBuVbb09MD/Z58dX1nM72nvXpHvvotvPmbt2tm/ds01Iuee627+mZn68ZJL4p/HZZe5K4PZL35h/1phoUi3bolbVrxmzw78/+ab+rF58+CTtQULREpKRB55xH4+ffoE/v/1r0UefTTysidMCHxG554bPA9DFT/JruJHrgSxOgCZWZ2BXHGFyL//rc8Q9+8XKS7Wwxs0CF6pkQ5ky5ZFV8bnnhPZsUNk48boxncybFj4mdDnn4ssWhR8VnPppfEvo0eP8GFWNXnz54s8+KDzOFXB+++HDzMHu/37RaZO1QHD6Utdo0ZiyuPVjqN1a71tG0LP/ho1CvwfekAfNCj4ef/+IitX6honQ6Rtyvy+vOzPaNe8PW1abE2osXL74+pr1uizchFdVitz5lj3Z7GqAahZU2TsWJH33hPZtCn89ZkzI5fJHKZq1RJp2jTyNJF07ertwXHtWpHdu/X/mzbp7V5E5Kqrop9HaPlCA6DVPiPaeRmaNrXfJ553XvTzj6b2x87vf6+PGf37i9x2mx5m7Be2bhV57LHAuF27itStK/L00/bz++Uv4yuHsV0Z27+IyLFj+tGoqRw1yn768ePjW26CnBphKpI9e3SNwq23Bg8/91x9QDnrLPsqxy1bdFiw24nand2/8ELwc59PX3nYtq19OUN3BLFUYXfoINK9u96xGkJrCkINGqTPRBcuDB6+e7fI//1f+PgrVugdvdm114o8+2zg+ddfOy/TaqdTWKi/8ObahkhnnM8+G1vH7OuuCx9mbrM/6yyRu+4K3w7iOXv9/e+thxs1inl5gTNrQ58+gdqTM8+MfZkigZ22ed4XXxzdtBs2iLz2WuC53fchUh8982fq84ncf7/z+PfdF/g/mvBlVdv15pt6p3zwoMjgwfo74FRTEC2rA9iFF9qP/8Yb+rszZozIxx8HhuflBf6/5JLAWfngwcHT9+0b+P9vf9NN4mZpaeE1jyI6PF5/vfWVzcZ6DA1INWoEauaNg/qIEeHT9+unt1vzSWMsYdWoGe3RQ5/ozZqln7doYT9Nw4bBJ2ih/u//dPmN2q3WrXWgUko3dUfypz9ZDw/d/5pPPKLRpk3g/2XLRD75RGTVqsCw0G3yrrus52P13Qut/bnkEpFOnYKHHThgPb/XX9frUCSwP7ALzOZl24WmaE4onS6EMV9xfNpp+kTW2NbtuiGMHWu/3iqLSpKOHTtW7gL1V0mpatUC/993n1IPPBAYZ8iQwGuhdu7Uw887L3h+hrS0wDDzn3lc819BQfDz558PzOvAAaXefTf49RdfVKqkJHy5xvN9+6zLXqeOHnbsWPg0FRVK/eY31uUTUaqsTI+/cGHw8N27lRo3Lvx9GkaO1MP69w9fpt3nYfzNmBE+jtV67NbNfh7//ndgfGO8G24IHuf88wP/z51rXS7zNKHbQnq6UpMm6ed/+IMe55Zb9PNevZR67DH78tl9BqHGjAm8dv/9geGbNin1j384z+/sswOfxVdfKbVxY2D6u+7Sr/3iF0otXmxfNmMZI0eGf/7GthE6/Pjx8O3aPG/zdt+rl1KPPBJ4Pn58+PsoLw/eXs3rTUSpnj2Dn992m3588037z1Up/d7t3vdllyl19dVKde0avj0opdTf/qZUbq4uz5YtwePs3x/4v3dvpY4eVWr7dqXmzQsvQ/v2ery8PPuymufdr59+nDMn/PWaNfXzRo2ct6nQz/ebb/Tjf/8bvC+oUUOp7Gz9/w8/6O3Iah9iNW+r5Rjbm/H/f/8bmK6iIrycFRX235/XX1fq0UftX9++PXx+5vl2767U7NlK/ec/+j2FTp+bqx+7dw8eft99wc/Xr7cvQ+jf6NFKtWnjvE3u26fUK68Eb+9W83rwwfBhoZ/5JZco9atfBQ87eNB6fmbl5Xp7tNtuzNtA6DLPOEM/GvtD87xDlzl7dvBwpZRq3lz///XX9uuvrEypyZOVWrkyeH4vvGA/TQI55ZZTr2Zq0qTA//36ifz974HnRiq3as83qrqjvU9PjRqB2ptJk8JrbELPLi6/PPB/zZoivXoFv16njq5etXPOOdbDP/tM5KWXrGsMfL7AWadVLZVRRqWChyslcu+99pfOGs2qkS4Tt+p7FPq5dO5sPa3Pp99zZqZ184Xhggv046OP6poJw+ef68ezzrLvnxLan8jQsKHI9u2B/kFjxuhaBKO567//1f0MCguDa1ZERO6+Wz/OnRte4xfKXP1vXgetWzufhW3YILJvn56mb1+Riy4KrvE05jVxovVZ7ooVurm5b19d42p1hmxXM5WWJjJuXOB5zZr6ezBjRvj7CJ3H/feH18yam1p9PpH164NfP/NM3fwabfmcXjdqYh5/XOSDDwJ9SE47TdckGzVzTzwh0rOnnoe5BuW66/T2ZDSBjxql9wPp6dZ9Co33Fk1H2/btnV//zW/0Y+h3NZILL9TT9Oqlm1eM74WI7seyZYve5/XtG7wPmTAheNxQVjUIRtlKSoL3b1bv3+fT3xGriyuMQ6jZV1+J3Hyz/t+pBtPn090d+vcX+e1vg7evBx7Q+0un9WGuHTfGy8iI7oraSOvmnHN0c1vo/EOdcYb18PfeCy6f0zazdKn18GrVRDp2tJ/Ork+rUiLPP6//j2Z7jnU7NVSvLjJ0aPhxoQp0Vj/1wlRWlv1rw4frnfWGDeGv1aund6bz51tPG7pyjx4NVOUPH66bECsqAv1OzjpLNz8MHSpy6FD49NWri/zxj4HnxsZ36aXOfcBCdyTNmwfPJ9T48SK/+53I5MmBYaefrh/tNtBzz9VfaLtLZ43qamM+ofr31++hoED/7dgRCC7GMo0DUqtWwdMWFuqdzujROjB8+WWgP4TB/EV96SWRf/1Lf77m5jHjf6svdefOukn0uef080hNV3Xr6v4toTs5vz+4ObdrV5GXX9b/9+6t3+MPP9jP11yNbneZcMOG+tF89VO7ds47F+M92+0YL79ch5pmzUTKyoKbJwxW8zc6Oz/0UCBEnnGG/h4Ytyg5/3x9e4YGDYKbfw3GNjN+fPA9qYx1YHUgsWsOcWKcPFx0UXj5Q/3yl/r92F14MH26fjQ+kxkz9HYXKQCFbntO+6ZPP3WeVyKcdVbghLJJEx0E7Zrb7rnHuX+ccdJgJdqO8L176/46oVe6WX1nL7hAn8zk5TlfIBLKvB3//e+62d58EmmcjBmsrjI8/fTgfoiGhQsDfZCsAmA0jO+3iH5vjz+uv1+zZ4d3Tbj++uCKgPHj9YmR1XemQwfn7c2OU59AY59qdWIf2vxsdfPeAQP0o9VVqU7S0iJ3WakEp06Ymjcv/Aqf0IOkz+e8Axw0yL6d/P33A8ncjs+nQ8vWrfrM7fbb9XO7Mw0z44v4+ef2Vyfm54t8+23kee3erfuJieiaqenTdRl699YBYvVqkWeeCT9gdu+uy2Ee/vrrOiSYDR4s8te/6i++ldmzdVnPOksfXJs0CZypGvOeP19PH/qZ+v06RDn1VTKvozPPDJyxiugd3Dff2Ne6iegDyqJFeke2Y4fIzp32y4qFVV8Cp5rOXr10aDz77OAzVsPevSLbtun/Y7l6zQhm1aol7oxu69bgPnyjR+s/Ywdp8Pl0/4cfftBBJnSHbpTtyisD29X77+saEhEd1L/8MjC+cRIycGBwn65IXn5Zb0crVwbmHa/QviP16ukTmGg/W59Pn1CtWBH+mnHLBrtaUoPRMTeaA7bTRSA1auha9GgvnLETWo7PP4+/NqJbt8j3y1JKf9edalWs2NWKGfN0uiraaf2efro+WTKHMaMPk92FBVa+/jpQy1ejhl7PZ56pT0itOsAb4atXL71Pv/zy4M+9okKfIJ1zjt7Px7tODFdfHegjd9ttOpCOGqVryd56KzCe+XP85pvw/lwiuqVj377ob8+yerUOUcePx9/pPZGiaSdcsGCBatGihbrwwgvV2LFjw16fMWOGatu2rWrbtq26/PLL1fr16121PXqqqCi4TTpedm3fTm3isZozR88rLU2pn3+2HucPf1CqQ4fELM/O0qW6HLfeGv88In0ut94a3JYe7/xXrIhufKNvz4MPBoatXavUaacp9eOP8ZXBznPP6WXdcYf164naZqKdz+9+F+h7smSJff8JN8uIxfjxgT5hHTro+a9Z4zzN6NG671d5efBwo8/UjBmxl/Waa/T48+fr5z/8oJ/Xr+883bx5erxevaJfllJKTZmip9u1y36c/fsD26NTnynD738fGHbnneHzO3o0uJ9QIpnnOXOm/r9fv0D/KKNf6dGj8c1/0CA9/auvBvra3Xab7lvoRq1agf6PSin1wQd63t266efGe/n73/Vz431u3KgfMzP19iii++C+8YZeb0oF+nY9+aRShw8rtWpV5PKErptFi5Rq2lSpQ4cij6uU3l7M34vTT9fjWE0frURsL9nZehswW75cqWnT3M23EjnlloifTllZmUpPT1fbt29XR48eVe3atVObNm0KGufTTz9Vu3fvVkopNX/+fHXppZe6KpTn/vUvpSZMcDePyghTSilVWJi4ecWrokJ3gN+zJ/553H678+diHCjiDVO3367Un/8c2zRlZe4CdbQOHtTvr7jY+vVEbTMbN1p3dA71xz/q5b31lj6wGgepaMpgjOuVFSt0B/DDh+Ob/oUXdPk++yz2z/WLL/SyDxzQz6MNU3PnxhemYhVNmDp6NLr37UWY2r8/ECIqKpRaty749aFD9TJDL16IlrHtTZsWCFOjR7sqsqUPPwwOUxUV+phhlHv+fB24vvhCj9e6tVJPPaX/f+SR4Hlt3hy5U3WoWNaNcbLtJBFhauNGfbHFKc5VmFqxYoXq0aPHiedjxoxRYxzOBHbv3q0aNWrkqlAnhe+/11d1hfJiJ5UKKiqcd6JGTcLmzZVXpqpi6FClrryy8pZ34IC+Us989hrtdltWFjhgVkUVFfrqRaV0yDFf2Rmr3bv1Z9Kli/N4Rpgyrvbzyvr1uoaitDQwbMECHRzNkhWmInG77UyYoMu8ZIlSy5bFVhMdCyNMde3qPF5pqR7vqafsw1Q8Ylk3y5Yp9d13zuOMGKHnd/y4+7Kd4pxyS8Tb9RYXF0sT0/1J/H6/fO5wFce0adPk2lh+K+xkZXdDtT/+0fnuvqcqn8+58+LAgfqKoWj6j6Uac+f/ylCzZvjVgB99pO/nEkn16rqvW1Xl8wU6lbv92Z9zzxXJzY3tRqReat8+/O7j11xTOctOBLfbzogR+spQo1+rV5+7cRGE3RXShjp19L2bzjzT+SaW8Yj2HlZOv1JgeOklfbFHMn838hQQ8dNVFhusz6bj3ZIlS2TatGnyySefWL6ek5MjOTk5IiJSUlISSzlPHi+9lOwSnLxOxSBVVYReRACtZ8/I4xj7wypwebaIiPz4Y3y/nVjVRbpAKFGuuEKHj2h+HzDShQHxKCpK7AlLtWrx3+gXUYsYpvx+vxQWFp54XlRUJI0sUvPGjRtlyJAhsmDBAqljc2lqdna2ZGdni4hIVjyXZQJAVXPttfoWJ3ZXr1a2SPd3gzOfz/kO63bTJIrVXexR5UUMU506dZJt27ZJfn6+NG7cWObMmSOzjFv+/8+OHTukb9++8uabb0oLp58BAIBUc9ppld9U69ZXXyXuR7kRuEWH1f2mcEqIGKbS0tJk4sSJ0rNnTykvL5fBgwdLZmamTP7fzmPYsGHy5JNPyq5du2T4/+4InZaWJnnm35sCAFQd5puVwr2rr9Z3dnf6lQqkNJ+y6hRVCbKysghcAADgpOCUW06dO6ADAAB4gDAFAADgAmEKAADABcIUAACAC4QpAAAAFwhTAAAALhCmAAAAXCBMAQAAuECYAgAAcIEwBQAA4ELSfk6mbt260qxZM8+XU1JSIvXq1fN8OYge66TqYZ1UTayXqod1UjVVxnopKCiQ0tJSy9eSFqYqC78BWPWwTqoe1knVxHqpelgnVVOy1wvNfAAAAC4QpgAAAFxI+TCVnZ2d7CIgBOuk6mGdVE2sl6qHdVI1JXu9pHyfKQAAAC+lfM0UAACAl1I2TOXm5krLli0lIyNDxo0bl+zipLTCwkLp2rWrtGrVSjIzM+XFF18UEZHdu3dL9+7dpXnz5tK9e3fZs2fPiWnGjh0rGRkZ0rJlS1m4cOGJ4WvWrJG2bdtKRkaG3HvvvULFqTvl5eXSoUMH6dWrl4iwTqqCvXv3ys033ywXXXSRtGrVSj777DPWS5I9//zzkpmZKW3atJEBAwbIkSNHWCdJMHjwYKlfv760adPmxLBEroejR4/KrbfeKhkZGdK5c2cpKChIXOFVCiorK1Pp6elq+/bt6ujRo6pdu3Zq06ZNyS5Wytq5c6das2aNUkqpn3/+WTVv3lxt2rRJPfjgg2rs2LFKKaXGjh2r/vznPyullNq0aZNq166dOnLkiPr2229Venq6KisrU0op1alTJ7VixQpVUVGhrrnmGjV//vzkvKkU8Y9//EMNGDBAXX/99UopxTqpAu644w41ZcoUpZRSR48eVXv27GG9JFFRUZFq1qyZOnTokFJKqVtuuUW99tprrJMkWLZsmVqzZo3KzMw8MSyR62HSpElq6NChSimlZs+erfr165ewsqdkmFqxYoXq0aPHiedjxoxRY8aMSWKJTi29e/dWixYtUi1atFA7d+5USunA1aJFC6VU+Pro0aOHWrFihdq5c6dq2bLlieGzZs1S2dnZlVv4FFJYWKi6deumFi9efCJMsU6Sa9++fapZs2aqoqIiaDjrJXmKioqU3+9Xu3btUsePH1fXX3+9WrhwIeskSfLz84PCVCLXgzGOUkodP35c1alTJ+y7GK+UbOYrLi6WJk2anHju9/uluLg4iSU6dRQUFMi6deukc+fO8uOPP0rDhg1FRKRhw4by008/iYj9+ikuLha/3x82HPEZOXKkPPvss1KtWuBrzjpJrm+//Vbq1asnd955p3To0EGGDBkiBw8eZL0kUePGjeWBBx6Qpk2bSsOGDaVWrVrSo0cP1kkVkcj1YJ4mLS1NatWqJbt27UpIOVMyTCmLdmqfz5eEkpxaDhw4IDfddJO88MILcs4559iOZ7d+WG+J895770n9+vWlY8eOUY3POqkcZWVlsnbtWrn77rtl3bp1UrNmTcc+nawX7+3Zs0fmzp0r+fn5snPnTjl48KDMmDHDdnzWSdUQz3rwch2lZJjy+/1SWFh44nlRUZE0atQoiSVKfcePH5ebbrpJBg4cKH379hURkQYNGsj3338vIiLff/+91K9fX0Ts14/f75eioqKw4Yjdp59+KvPmzZNmzZpJ//795aOPPpLbb7+ddZJkfr9f/H6/dO7cWUREbr75Zlm7di3rJYk+/PBDueCCC6RevXpy2mmnSd++fWXFihWskyoikevBPE1ZWZns27dPateunZBypmSY6tSpk2zbtk3y8/Pl2LFjMmfOHOndu3eyi5WylFJy1113SatWreRPf/rTieG9e/eWN954Q0RE3njjDenTp8+J4XPmzJGjR49Kfn6+bNu2TS699FJp2LChnH322bJy5UpRSsn06dNPTIPYjB07VoqKiqSgoEDmzJkj3bp1kxkzZrBOkuy8886TJk2ayJYtW0REZPHixdK6dWvWSxI1bdpUVq5cKYcOHRKllCxevFhatWrFOqkiErkezPN6++23pVu3bomrPUxIz6sq6P3331fNmzdX6enpavTo0ckuTkpbvny5EhHVtm1b1b59e9W+fXv1/vvvq9LSUtWtWzeVkZGhunXrpnbt2nVimtGjR6v09HTVokWLoCteVq9erTIzM1V6eroaMWJEwjoHnsqWLFlyogM66yT51q1bpzp27Kjatm2r+vTpo3bv3s16SbLHH39ctWzZUmVmZqrbb79dHTlyhHWSBP3791fnnXeeSktLU40bN1ZTp05N6Ho4fPiwuvnmm9WFF16oOnXqpLZv356wsnMHdAAAABdSspkPAACgshCmAAAAXCBMAQAAuECYAgAAcIEwBQAA4AJhCgAAwAXCFAAAgAuEKQAAABf+P8Pgkla9gxLeAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dff=pd.DataFrame.from_records(records,columns=['Step','Loss','Learning Rate','Gradient_Magnitude'])\n",
    "fig = plt.figure(figsize=[10,5])\n",
    "fig.patch.set_facecolor(color=\"white\")\n",
    "plt.plot(dff['Step'],dff['Loss'],'-r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(weights,attri,num_rec):\n",
    "    p_score = weights @ attri.T\n",
    "    pred_score = []\n",
    "    for i in range(num_rec):\n",
    "        sep = list(p_score[:,i])\n",
    "        max_val = max(p_score[:,i])\n",
    "        pred_score.append(sep.index(max_val)+1)\n",
    "    pred_score = np.array(pred_score)\n",
    "    return np.array(pred_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = []\n",
    "y_val = []\n",
    "for ele in sc_x:\n",
    "    x_val.append(float(ele))\n",
    "for ele in sc_y:\n",
    "    y_val.append(float(ele))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.DataFrame({'x':x_val,'y':y_val,'label':df['diagnosis']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.097064</td>\n",
       "      <td>2.532475</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.829821</td>\n",
       "      <td>0.548144</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.579888</td>\n",
       "      <td>2.037231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.768909</td>\n",
       "      <td>1.451707</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.750297</td>\n",
       "      <td>1.428493</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         y  label\n",
       "0  1.097064  2.532475      1\n",
       "1  1.829821  0.548144      1\n",
       "2  1.579888  2.037231      1\n",
       "3 -0.768909  1.451707      1\n",
       "4  1.750297  1.428493      1"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf[\"label\"] = newdf[\"label\"].replace('B',0)\n",
    "newdf[\"label\"] = newdf[\"label\"].replace('M',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-122-e6e337c5fbc2>:5: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.\n",
      "  plt.pcolormesh(xx,yy,zz)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dg = pd.DataFrame({'x':xx_row,'y':yy_row})\n",
    "zz_row=prediction(weights,np.array(dg[['x','y']]),len(dg['x']))\n",
    "zz=zz_row.reshape(xx.shape)\n",
    "plt.figure(figsize=[5,5])\n",
    "plt.pcolormesh(xx,yy,zz)\n",
    "sns.scatterplot(data=newdf,x='x',y='y',hue='label')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
