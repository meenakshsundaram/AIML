{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from typing import List \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_clusters_at(centers:List[tuple],spreads:List,num_points_per_cluster:List[int])->pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Create random clusters on a x-y feature plane\n",
    "\n",
    "    Args:\n",
    "        centers (list[tuple]): list of tuples of center coordinate\n",
    "        spreads (list): list of floating point indicating spreads\n",
    "        num_points_per_cluster (list[int]): number of points in each cluster.\n",
    "    \"\"\"\n",
    "\n",
    "    num_clusters=len(centers)\n",
    "    points=[]\n",
    "    for iind in range(num_clusters):\n",
    "        point=np.random.randn(num_points_per_cluster[iind],3)\n",
    "        point[:,:2]=(point[:,:2]+centers[iind])*spreads[iind]\n",
    "        point[:,2]=iind+1\n",
    "        points.append(point)\n",
    "    points = np.concatenate(points,axis=0)\n",
    "    points=pd.DataFrame(points,columns=['x','y','label'])\n",
    "    points['label']=pd.Categorical(points['label'])\n",
    "    return points\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers=[(7,3),(3,7),(3,3),(7,7)]\n",
    "spreads=[1.8,1.5,1.1,1.3]\n",
    "num_points_per_cluster=[705,3801,915,750]\n",
    "points=generate_random_clusters_at(centers,spreads,num_points_per_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "sns.scatterplot(data=points,x='x',y='y',hue='label')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_train,attribute_test,label_train,label_test=train_test_split(points[['x','y']],points['label'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = np.min(points['x'])\n",
    "xmax = np.max(points['x'])\n",
    "ymin = np.min(points['y'])\n",
    "ymax = np.max(points['y'])\n",
    "h=0.1\n",
    "\n",
    "xx,yy = np.meshgrid(np.arange(xmin,xmax,h),np.arange(ymin,ymax,h))\n",
    "xx_row = xx.ravel()\n",
    "yy_row = yy.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TEST\\anaconda\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "<ipython-input-9-28c0d5e2ecf2>:7: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.\n",
      "  plt.pcolormesh(xx,yy,zz)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lcl = svm.SVC(kernel='linear')\n",
    "lcl.fit(attribute_train,label_train)\n",
    "zz_row=lcl.predict(np.c_[xx_row,yy_row])\n",
    "zz=zz_row.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.pcolormesh(xx,yy,zz)\n",
    "sns.scatterplot(data=points,x='x',y='y',hue='label')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "attri = np.array(points[['x','y']])\n",
    "label = points['label'].values.astype(int)-1\n",
    "num_labels = np.unique(label).shape[0]\n",
    "\n",
    "num_rec=attri.shape[0]\n",
    "num_attri = attri.shape[1]\n",
    "\n",
    "xmin = np.min(points['x'])\n",
    "xmax = np.max(points['x'])\n",
    "ymin = np.min(points['y'])\n",
    "ymax = np.max(points['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossFunction(data,label,weights,delta):\n",
    "    num_rec = data.shape[0]\n",
    "    scores = weights @ data.T \n",
    "    margins = np.maximum(0,scores-scores[label,np.arange(num_rec)]+delta)\n",
    "    margins[label,np.arange(num_rec)] = 0 \n",
    "    loss = np.sum(margins)/num_rec\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.425714650044269\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.random([num_labels,num_attri])\n",
    "print(LossFunction(attri,label,weights=weights,delta=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossFunctionGradient(data,label,weights,delta,eps=1E-3):\n",
    "    \n",
    "    num_rec = data.shape[0]\n",
    "    scores = weights @ data.T \n",
    "    margins = np.maximum(0,scores-scores[label,np.arange(num_rec)]+delta)\n",
    "    margins[label,np.arange(num_rec)] = 0 \n",
    "    loss = np.sum(margins)/num_rec\n",
    "    \n",
    "    num_pixels = weights.shape[1]  \n",
    "    num_classes = weights.shape[0]\n",
    "  \n",
    "    gradient = np.zeros(weights.shape)\n",
    "    for jind in range(num_pixels): \n",
    "        pert = eps*data.T[jind,:]\n",
    "        for iind in range(num_classes):    \n",
    "            newScores = scores.copy()\n",
    "            newScores[iind,:] = scores[iind,:]+pert\n",
    "            newmargins = np.maximum(0,newScores-newScores[label,np.arange(num_rec)]+delta)\n",
    "            newmargins[label,np.arange(num_rec)]=0\n",
    "            newloss = np.sum(newmargins)/num_rec\n",
    "            gradient[iind,jind]=(newloss - loss)/eps\n",
    "    \n",
    "    return loss,gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.043958469905221, array([[-0.40683722,  3.13239251],\n",
      "       [-4.46024485, -9.8890845 ],\n",
      "       [ 0.16903311, -0.74899244],\n",
      "       [ 4.70405317,  7.54490364]]))\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.random([num_labels,num_attri])\n",
    "print(LossFunctionGradient(attri,label,weights=weights,delta=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.random([num_labels,num_attri])\n",
    "num_steps = 2000\n",
    "learning_rate = 0.1\n",
    "records = []\n",
    "for iind in range(num_steps):\n",
    "    selected_data = np.random.randint(0,num_rec,100)\n",
    "    loss,gradient=LossFunctionGradient(attri[selected_data,:],label[selected_data],weights,1)\n",
    "    old_weights = weights.copy()\n",
    "    \n",
    "    grad_mag = np.linalg.norm(gradient)\n",
    "    \n",
    "    if(iind%100 == 0):\n",
    "        learning_rate = learning_rate*0.5\n",
    "    \n",
    "    weights = weights-learning_rate*gradient/grad_mag\n",
    "    \n",
    "    records.append([iind+1,loss,learning_rate,grad_mag])\n",
    "\n",
    "    # print(f'Step - {iind+1}, Loss - {loss}, Learning Rate - {learning_rate}, magnitude of gradient - {grad_mag}')\n",
    "    \n",
    "    if(grad_mag <1E-6):\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16494f787c0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=pd.DataFrame.from_records(records,columns=['Step','Loss','Learning Rate','Gradient_Magnitude'])\n",
    "fig = plt.figure(figsize=[10,5])\n",
    "fig.patch.set_facecolor(color=\"white\")\n",
    "plt.plot(df['Step'],df['Loss'],'-r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossFunctionGradRegularization(data,label,weights,delta,eps=1E-3,lam=0.1):\n",
    "    \n",
    "    num_rec = data.shape[0]\n",
    "    scores = weights @ data.T \n",
    "    margins = np.maximum(0,scores-scores[label,np.arange(num_rec)]+delta)\n",
    "    margins[label,np.arange(num_rec)] = 0 \n",
    "    loss = (np.sum(margins)/num_rec)\n",
    "    regularization = (lam*(np.sum(weights**2)))\n",
    "    \n",
    "    num_pixels = weights.shape[1]  \n",
    "    num_classes = weights.shape[0]\n",
    "  \n",
    "    gradient = np.zeros(weights.shape)\n",
    "    for jind in range(num_pixels): \n",
    "        pert = eps*data.T[jind,:]\n",
    "        for iind in range(num_classes):    \n",
    "            newScores = scores.copy()\n",
    "            newScores[iind,:] = scores[iind,:]+pert\n",
    "            newmargins = np.maximum(0,newScores-newScores[label,np.arange(num_rec)]+delta)\n",
    "            newmargins[label,np.arange(num_rec)]=0\n",
    "            newloss = np.sum(newmargins)/num_rec\n",
    "            gradient[iind,jind]=((newloss - loss)/eps)\n",
    "    gradient = gradient + (2*lam*weights)\n",
    "    loss = loss + regularization\n",
    "    \n",
    "    return loss,gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step - 1, Loss - 8.335074649643202, Learning Rate - 0.05, magnitude of gradient - 16.57548099244236\n",
      "Step - 2, Loss - 6.354662754364303, Learning Rate - 0.05, magnitude of gradient - 16.59689031097553\n",
      "Step - 3, Loss - 7.96024599927214, Learning Rate - 0.05, magnitude of gradient - 12.799362189488443\n",
      "Step - 4, Loss - 6.519063796563803, Learning Rate - 0.05, magnitude of gradient - 12.205463138264465\n",
      "Step - 5, Loss - 5.397138777758637, Learning Rate - 0.05, magnitude of gradient - 12.863969028732813\n",
      "Step - 6, Loss - 5.196431376597787, Learning Rate - 0.05, magnitude of gradient - 11.42057323485167\n",
      "Step - 7, Loss - 4.988184005835479, Learning Rate - 0.05, magnitude of gradient - 11.276594728246117\n",
      "Step - 8, Loss - 5.165980648996545, Learning Rate - 0.05, magnitude of gradient - 12.620870804072553\n",
      "Step - 9, Loss - 4.200078118094768, Learning Rate - 0.05, magnitude of gradient - 8.848242069330794\n",
      "Step - 10, Loss - 3.3233572006571226, Learning Rate - 0.05, magnitude of gradient - 5.843525381103314\n",
      "Step - 11, Loss - 2.4841531451001906, Learning Rate - 0.05, magnitude of gradient - 4.899367400870527\n",
      "Step - 12, Loss - 3.2256436746953803, Learning Rate - 0.05, magnitude of gradient - 7.825696520595943\n",
      "Step - 13, Loss - 2.4116050535029254, Learning Rate - 0.05, magnitude of gradient - 4.701545606483275\n",
      "Step - 14, Loss - 2.956612022320873, Learning Rate - 0.05, magnitude of gradient - 5.722949631580145\n",
      "Step - 15, Loss - 2.4518932056598377, Learning Rate - 0.05, magnitude of gradient - 4.563868216934482\n",
      "Step - 16, Loss - 1.7879838228581137, Learning Rate - 0.05, magnitude of gradient - 2.322854365077842\n",
      "Step - 17, Loss - 2.0985938906735897, Learning Rate - 0.05, magnitude of gradient - 5.3133330636490275\n",
      "Step - 18, Loss - 2.0427247562342243, Learning Rate - 0.05, magnitude of gradient - 5.670033154788036\n",
      "Step - 19, Loss - 1.7939543125946857, Learning Rate - 0.05, magnitude of gradient - 5.6020608707819015\n",
      "Step - 20, Loss - 1.8381584143680592, Learning Rate - 0.05, magnitude of gradient - 6.337472202591071\n",
      "Step - 21, Loss - 1.6662288269819903, Learning Rate - 0.05, magnitude of gradient - 4.236951126544162\n",
      "Step - 22, Loss - 1.3201440706305492, Learning Rate - 0.05, magnitude of gradient - 4.626459208429547\n",
      "Step - 23, Loss - 1.294022113527649, Learning Rate - 0.05, magnitude of gradient - 6.564864100060334\n",
      "Step - 24, Loss - 1.3696432408990955, Learning Rate - 0.05, magnitude of gradient - 5.292065384960173\n",
      "Step - 25, Loss - 1.396005101793464, Learning Rate - 0.05, magnitude of gradient - 6.449791671181031\n",
      "Step - 26, Loss - 1.0659081560689556, Learning Rate - 0.05, magnitude of gradient - 5.64595154695178\n",
      "Step - 27, Loss - 1.4505476116570002, Learning Rate - 0.05, magnitude of gradient - 6.517890523529147\n",
      "Step - 28, Loss - 1.1246703323569536, Learning Rate - 0.05, magnitude of gradient - 6.449864494546944\n",
      "Step - 29, Loss - 1.080544763538099, Learning Rate - 0.05, magnitude of gradient - 3.712452304503855\n",
      "Step - 30, Loss - 1.3601529989091505, Learning Rate - 0.05, magnitude of gradient - 2.161724987959998\n",
      "Step - 31, Loss - 0.9641531118875454, Learning Rate - 0.05, magnitude of gradient - 3.4812542601617844\n",
      "Step - 32, Loss - 0.9612302776101515, Learning Rate - 0.05, magnitude of gradient - 3.9704863982531884\n",
      "Step - 33, Loss - 1.2627814902850725, Learning Rate - 0.05, magnitude of gradient - 3.5697734434147295\n",
      "Step - 34, Loss - 1.134780987216544, Learning Rate - 0.05, magnitude of gradient - 2.982253752378196\n",
      "Step - 35, Loss - 1.072213696378642, Learning Rate - 0.05, magnitude of gradient - 5.257515882965134\n",
      "Step - 36, Loss - 1.1720801244474444, Learning Rate - 0.05, magnitude of gradient - 8.090092755210842\n",
      "Step - 37, Loss - 0.9209947905593721, Learning Rate - 0.05, magnitude of gradient - 4.561003000087233\n",
      "Step - 38, Loss - 1.2024806133270534, Learning Rate - 0.05, magnitude of gradient - 3.2685921101066824\n",
      "Step - 39, Loss - 1.1083144524234667, Learning Rate - 0.05, magnitude of gradient - 6.778282118578913\n",
      "Step - 40, Loss - 0.9262701481720472, Learning Rate - 0.05, magnitude of gradient - 2.4273313084019335\n",
      "Step - 41, Loss - 0.9338339603711585, Learning Rate - 0.05, magnitude of gradient - 3.3889468221853987\n",
      "Step - 42, Loss - 0.8343345360065033, Learning Rate - 0.05, magnitude of gradient - 3.283807030184254\n",
      "Step - 43, Loss - 0.7866011679363183, Learning Rate - 0.05, magnitude of gradient - 2.873017423706351\n",
      "Step - 44, Loss - 0.7645587185801538, Learning Rate - 0.05, magnitude of gradient - 1.5400594939048737\n",
      "Step - 45, Loss - 1.0241596711217968, Learning Rate - 0.05, magnitude of gradient - 2.8152893102072243\n",
      "Step - 46, Loss - 0.9753691453238232, Learning Rate - 0.05, magnitude of gradient - 1.0866668404198676\n",
      "Step - 47, Loss - 0.8488729777107652, Learning Rate - 0.05, magnitude of gradient - 3.941467752257655\n",
      "Step - 48, Loss - 0.965432267489148, Learning Rate - 0.05, magnitude of gradient - 2.403222324557604\n",
      "Step - 49, Loss - 0.8483699746330188, Learning Rate - 0.05, magnitude of gradient - 4.8606737363068975\n",
      "Step - 50, Loss - 0.9581646801806601, Learning Rate - 0.05, magnitude of gradient - 4.179572504578\n",
      "Step - 51, Loss - 0.8014179753954748, Learning Rate - 0.05, magnitude of gradient - 5.005875274376358\n",
      "Step - 52, Loss - 1.0863422414831665, Learning Rate - 0.05, magnitude of gradient - 4.473387375185539\n",
      "Step - 53, Loss - 0.7932625130610058, Learning Rate - 0.05, magnitude of gradient - 2.2811361533863375\n",
      "Step - 54, Loss - 0.9371153678137067, Learning Rate - 0.05, magnitude of gradient - 3.805157521324995\n",
      "Step - 55, Loss - 0.9827891199462286, Learning Rate - 0.05, magnitude of gradient - 3.88619211806015\n",
      "Step - 56, Loss - 0.986901566950491, Learning Rate - 0.05, magnitude of gradient - 0.9150429624293897\n",
      "Step - 57, Loss - 1.0344523181764544, Learning Rate - 0.05, magnitude of gradient - 7.524999696655689\n",
      "Step - 58, Loss - 0.8096587625503515, Learning Rate - 0.05, magnitude of gradient - 2.1178328989799824\n",
      "Step - 59, Loss - 0.7387560605624225, Learning Rate - 0.05, magnitude of gradient - 2.220218591177353\n",
      "Step - 60, Loss - 0.7731118423466329, Learning Rate - 0.05, magnitude of gradient - 2.801450663226557\n",
      "Step - 61, Loss - 1.1027154610089998, Learning Rate - 0.05, magnitude of gradient - 5.095396476024188\n",
      "Step - 62, Loss - 0.8278277588480768, Learning Rate - 0.05, magnitude of gradient - 4.762537028135236\n",
      "Step - 63, Loss - 0.7976958642887451, Learning Rate - 0.05, magnitude of gradient - 1.132993448775172\n",
      "Step - 64, Loss - 0.8663146071804808, Learning Rate - 0.05, magnitude of gradient - 0.8615886673762684\n",
      "Step - 65, Loss - 0.8121878774670995, Learning Rate - 0.05, magnitude of gradient - 4.242670318278497\n",
      "Step - 66, Loss - 0.89291297881672, Learning Rate - 0.05, magnitude of gradient - 3.7461292031335875\n",
      "Step - 67, Loss - 0.9574418222497274, Learning Rate - 0.05, magnitude of gradient - 4.965239217082067\n",
      "Step - 68, Loss - 0.740528061422977, Learning Rate - 0.05, magnitude of gradient - 3.2812087133115604\n",
      "Step - 69, Loss - 0.8845174884234188, Learning Rate - 0.05, magnitude of gradient - 3.597759723541159\n",
      "Step - 70, Loss - 0.8710244402231291, Learning Rate - 0.05, magnitude of gradient - 1.2372175314111105\n",
      "Step - 71, Loss - 0.9506404851014681, Learning Rate - 0.05, magnitude of gradient - 3.434087059694026\n",
      "Step - 72, Loss - 0.886881369765401, Learning Rate - 0.05, magnitude of gradient - 2.3909645186239166\n",
      "Step - 73, Loss - 0.968931966281434, Learning Rate - 0.05, magnitude of gradient - 5.923254195724698\n",
      "Step - 74, Loss - 0.7062165176249101, Learning Rate - 0.05, magnitude of gradient - 3.1390554305422538\n",
      "Step - 75, Loss - 0.7542615103847324, Learning Rate - 0.05, magnitude of gradient - 2.9833549399211363\n",
      "Step - 76, Loss - 0.8604158481378306, Learning Rate - 0.05, magnitude of gradient - 4.063278434602437\n",
      "Step - 77, Loss - 0.7355820045311298, Learning Rate - 0.05, magnitude of gradient - 3.0723538845298264\n",
      "Step - 78, Loss - 0.9094069414346596, Learning Rate - 0.05, magnitude of gradient - 2.5783341912459443\n",
      "Step - 79, Loss - 0.950127756193706, Learning Rate - 0.05, magnitude of gradient - 4.7200833332176595\n",
      "Step - 80, Loss - 0.6529647142954538, Learning Rate - 0.05, magnitude of gradient - 1.7150710563645957\n",
      "Step - 81, Loss - 0.765352640044031, Learning Rate - 0.05, magnitude of gradient - 3.670809273851097\n",
      "Step - 82, Loss - 0.6528146614310004, Learning Rate - 0.05, magnitude of gradient - 1.475262232918761\n",
      "Step - 83, Loss - 0.7787718339924479, Learning Rate - 0.05, magnitude of gradient - 3.434515623600842\n",
      "Step - 84, Loss - 0.9883757468319674, Learning Rate - 0.05, magnitude of gradient - 3.295471103384218\n",
      "Step - 85, Loss - 0.661286698067097, Learning Rate - 0.05, magnitude of gradient - 1.6236967426421431\n",
      "Step - 86, Loss - 0.8532227318819683, Learning Rate - 0.05, magnitude of gradient - 3.0423468140924155\n",
      "Step - 87, Loss - 0.9337956167218779, Learning Rate - 0.05, magnitude of gradient - 2.8223246500822596\n",
      "Step - 88, Loss - 0.7069919323796892, Learning Rate - 0.05, magnitude of gradient - 2.574233141560909\n",
      "Step - 89, Loss - 0.7791501550435057, Learning Rate - 0.05, magnitude of gradient - 2.507590224384156\n",
      "Step - 90, Loss - 0.71544279042808, Learning Rate - 0.05, magnitude of gradient - 1.4252937833871746\n",
      "Step - 91, Loss - 0.5943123509109225, Learning Rate - 0.05, magnitude of gradient - 1.502781548409261\n",
      "Step - 92, Loss - 0.8678908384505873, Learning Rate - 0.05, magnitude of gradient - 2.3341482306965875\n",
      "Step - 93, Loss - 0.7573468184974154, Learning Rate - 0.05, magnitude of gradient - 2.1425345627198227\n",
      "Step - 94, Loss - 0.660549629201992, Learning Rate - 0.05, magnitude of gradient - 2.5022731142470858\n",
      "Step - 95, Loss - 0.6271326214528125, Learning Rate - 0.05, magnitude of gradient - 3.57381606823063\n",
      "Step - 96, Loss - 0.9421023770988936, Learning Rate - 0.05, magnitude of gradient - 4.812984345829576\n",
      "Step - 97, Loss - 0.923285436240939, Learning Rate - 0.05, magnitude of gradient - 3.692008084309618\n",
      "Step - 98, Loss - 0.7711619491022763, Learning Rate - 0.05, magnitude of gradient - 1.4787741130569116\n",
      "Step - 99, Loss - 0.8342695733330915, Learning Rate - 0.05, magnitude of gradient - 2.7054292854059443\n",
      "Step - 100, Loss - 0.8304036262451838, Learning Rate - 0.05, magnitude of gradient - 1.6290953539986877\n",
      "Step - 101, Loss - 1.0412215695552314, Learning Rate - 0.05, magnitude of gradient - 2.2492831330488987\n",
      "Step - 102, Loss - 0.7081204268016911, Learning Rate - 0.05, magnitude of gradient - 0.5298352598221785\n",
      "Step - 103, Loss - 0.8182503332915081, Learning Rate - 0.05, magnitude of gradient - 3.3406138745090774\n",
      "Step - 104, Loss - 0.7581176816754419, Learning Rate - 0.05, magnitude of gradient - 2.1015025741470983\n",
      "Step - 105, Loss - 0.9329370944895863, Learning Rate - 0.05, magnitude of gradient - 1.789105417836483\n",
      "Step - 106, Loss - 0.7686079520287191, Learning Rate - 0.05, magnitude of gradient - 1.5997088899052212\n",
      "Step - 107, Loss - 0.7769237392723417, Learning Rate - 0.05, magnitude of gradient - 2.51509638293932\n",
      "Step - 108, Loss - 0.6093819113834389, Learning Rate - 0.05, magnitude of gradient - 1.11464325469722\n",
      "Step - 109, Loss - 0.7258792339928752, Learning Rate - 0.05, magnitude of gradient - 1.4750643248976307\n",
      "Step - 110, Loss - 0.748589028106806, Learning Rate - 0.05, magnitude of gradient - 1.082345703765232\n",
      "Step - 111, Loss - 0.9099344520219388, Learning Rate - 0.05, magnitude of gradient - 2.7527417314001785\n",
      "Step - 112, Loss - 0.8580027435145062, Learning Rate - 0.05, magnitude of gradient - 1.65291926153567\n",
      "Step - 113, Loss - 0.9086230567403948, Learning Rate - 0.05, magnitude of gradient - 1.8420568107197766\n",
      "Step - 114, Loss - 0.797328385859799, Learning Rate - 0.05, magnitude of gradient - 1.7356986289326246\n",
      "Step - 115, Loss - 0.8315468062098298, Learning Rate - 0.05, magnitude of gradient - 1.0109245108408207\n",
      "Step - 116, Loss - 0.7255735607623524, Learning Rate - 0.05, magnitude of gradient - 4.753034924949072\n",
      "Step - 117, Loss - 0.7348914279033325, Learning Rate - 0.05, magnitude of gradient - 1.9706357713976381\n",
      "Step - 118, Loss - 0.7420022780248507, Learning Rate - 0.05, magnitude of gradient - 3.9935481717616446\n",
      "Step - 119, Loss - 0.7074000580932813, Learning Rate - 0.05, magnitude of gradient - 0.9562691736832634\n",
      "Step - 120, Loss - 0.8358563523890676, Learning Rate - 0.05, magnitude of gradient - 4.012952124804862\n",
      "Step - 121, Loss - 0.7680276870387568, Learning Rate - 0.05, magnitude of gradient - 1.6898663192296888\n",
      "Step - 122, Loss - 0.7545984289678681, Learning Rate - 0.05, magnitude of gradient - 3.3546461469304076\n",
      "Step - 123, Loss - 0.8697236001511035, Learning Rate - 0.05, magnitude of gradient - 2.0255988058198726\n",
      "Step - 124, Loss - 0.7918349755664135, Learning Rate - 0.05, magnitude of gradient - 4.970002608131488\n",
      "Step - 125, Loss - 0.6380852189992614, Learning Rate - 0.05, magnitude of gradient - 0.7661254689407019\n",
      "Step - 126, Loss - 0.7157719159125805, Learning Rate - 0.05, magnitude of gradient - 3.5970953565639405\n",
      "Step - 127, Loss - 0.7254493084794322, Learning Rate - 0.05, magnitude of gradient - 0.43947570975629346\n",
      "Step - 128, Loss - 0.7896377825360327, Learning Rate - 0.05, magnitude of gradient - 2.0168463415403837\n",
      "Step - 129, Loss - 0.827064089070722, Learning Rate - 0.05, magnitude of gradient - 1.4352946277060417\n",
      "Step - 130, Loss - 0.8314643990795078, Learning Rate - 0.05, magnitude of gradient - 0.4372593140517249\n",
      "Step - 131, Loss - 0.6513242869730795, Learning Rate - 0.05, magnitude of gradient - 2.3679408387694783\n",
      "Step - 132, Loss - 0.7237777027575171, Learning Rate - 0.05, magnitude of gradient - 1.7384674164628677\n",
      "Step - 133, Loss - 0.8198668399252709, Learning Rate - 0.05, magnitude of gradient - 1.907459872636152\n",
      "Step - 134, Loss - 0.6976296740774282, Learning Rate - 0.05, magnitude of gradient - 2.1118463842356547\n",
      "Step - 135, Loss - 0.8719608890276677, Learning Rate - 0.05, magnitude of gradient - 2.0065712000053746\n",
      "Step - 136, Loss - 0.7848770914950038, Learning Rate - 0.05, magnitude of gradient - 1.4216646684892478\n",
      "Step - 137, Loss - 0.8268158857945356, Learning Rate - 0.05, magnitude of gradient - 3.0360270545326458\n",
      "Step - 138, Loss - 0.7644006976643601, Learning Rate - 0.05, magnitude of gradient - 0.6019415739415982\n",
      "Step - 139, Loss - 0.7606423068623466, Learning Rate - 0.05, magnitude of gradient - 3.9199723205043404\n",
      "Step - 140, Loss - 0.797894875720115, Learning Rate - 0.05, magnitude of gradient - 2.417270149904463\n",
      "Step - 141, Loss - 0.8311606223499484, Learning Rate - 0.05, magnitude of gradient - 3.8226308698517286\n",
      "Step - 142, Loss - 0.7526799276159293, Learning Rate - 0.05, magnitude of gradient - 1.6652286316150369\n",
      "Step - 143, Loss - 0.6151718010631937, Learning Rate - 0.05, magnitude of gradient - 2.3713813437123226\n",
      "Step - 144, Loss - 0.7536579835856497, Learning Rate - 0.05, magnitude of gradient - 0.6837794236377195\n",
      "Step - 145, Loss - 0.7665819021245579, Learning Rate - 0.05, magnitude of gradient - 1.0606568843129405\n",
      "Step - 146, Loss - 0.6381172570969194, Learning Rate - 0.05, magnitude of gradient - 3.6580231133844117\n",
      "Step - 147, Loss - 0.6536952152857158, Learning Rate - 0.05, magnitude of gradient - 3.0953072418259677\n",
      "Step - 148, Loss - 0.8762825759067522, Learning Rate - 0.05, magnitude of gradient - 1.825303165083629\n",
      "Step - 149, Loss - 0.6183244854032114, Learning Rate - 0.05, magnitude of gradient - 3.0383841513495393\n",
      "Step - 150, Loss - 0.7614898803690148, Learning Rate - 0.05, magnitude of gradient - 3.0910523701311448\n",
      "Step - 151, Loss - 0.7946050079940543, Learning Rate - 0.05, magnitude of gradient - 2.455485967882052\n",
      "Step - 152, Loss - 0.8515721789353126, Learning Rate - 0.05, magnitude of gradient - 3.830304975536016\n",
      "Step - 153, Loss - 0.7458684630845367, Learning Rate - 0.05, magnitude of gradient - 1.3709086010862577\n",
      "Step - 154, Loss - 0.5844551862179824, Learning Rate - 0.05, magnitude of gradient - 3.1117080409409414\n",
      "Step - 155, Loss - 0.8166752252465564, Learning Rate - 0.05, magnitude of gradient - 3.4894356096472046\n",
      "Step - 156, Loss - 0.7378419308231864, Learning Rate - 0.05, magnitude of gradient - 2.7293845861919945\n",
      "Step - 157, Loss - 0.7767163123811678, Learning Rate - 0.05, magnitude of gradient - 0.9362662423494376\n",
      "Step - 158, Loss - 0.7340935254580331, Learning Rate - 0.05, magnitude of gradient - 2.2039145827773257\n",
      "Step - 159, Loss - 0.859339742983427, Learning Rate - 0.05, magnitude of gradient - 2.459294359922624\n",
      "Step - 160, Loss - 0.6316220716114693, Learning Rate - 0.05, magnitude of gradient - 2.7206418693249037\n",
      "Step - 161, Loss - 0.7457534677026074, Learning Rate - 0.05, magnitude of gradient - 2.236768747953397\n",
      "Step - 162, Loss - 0.9018574710259047, Learning Rate - 0.05, magnitude of gradient - 2.296198104099341\n",
      "Step - 163, Loss - 0.865288205656578, Learning Rate - 0.05, magnitude of gradient - 1.9832866886944414\n",
      "Step - 164, Loss - 0.7288546068870964, Learning Rate - 0.05, magnitude of gradient - 1.6688564091134526\n",
      "Step - 165, Loss - 0.7889054434566944, Learning Rate - 0.05, magnitude of gradient - 2.080469874777495\n",
      "Step - 166, Loss - 0.7527595792119234, Learning Rate - 0.05, magnitude of gradient - 3.773800202105672\n",
      "Step - 167, Loss - 0.6459092748828215, Learning Rate - 0.05, magnitude of gradient - 1.560445222722504\n",
      "Step - 168, Loss - 0.680691955325063, Learning Rate - 0.05, magnitude of gradient - 2.820902901757115\n",
      "Step - 169, Loss - 0.5553576744417863, Learning Rate - 0.05, magnitude of gradient - 1.637614583133624\n",
      "Step - 170, Loss - 0.6831264885294029, Learning Rate - 0.05, magnitude of gradient - 2.67019290769555\n",
      "Step - 171, Loss - 0.6370528855126903, Learning Rate - 0.05, magnitude of gradient - 3.9403019857825132\n",
      "Step - 172, Loss - 0.7506704743218643, Learning Rate - 0.05, magnitude of gradient - 3.0307614818698583\n",
      "Step - 173, Loss - 0.7820276947603991, Learning Rate - 0.05, magnitude of gradient - 3.2568749024009196\n",
      "Step - 174, Loss - 0.7083734702885431, Learning Rate - 0.05, magnitude of gradient - 1.157164939971186\n",
      "Step - 175, Loss - 0.7117516104269396, Learning Rate - 0.05, magnitude of gradient - 1.4061169399968376\n",
      "Step - 176, Loss - 0.7756584568099835, Learning Rate - 0.05, magnitude of gradient - 3.2247363894878585\n",
      "Step - 177, Loss - 0.8784750563241, Learning Rate - 0.05, magnitude of gradient - 2.7953333521464647\n",
      "Step - 178, Loss - 0.6669559636546306, Learning Rate - 0.05, magnitude of gradient - 2.779464535852535\n",
      "Step - 179, Loss - 0.7136088528847853, Learning Rate - 0.05, magnitude of gradient - 0.34441428201024293\n",
      "Step - 180, Loss - 0.9109230565618157, Learning Rate - 0.05, magnitude of gradient - 3.334263412265022\n",
      "Step - 181, Loss - 0.6390818369732795, Learning Rate - 0.05, magnitude of gradient - 1.2615985550471611\n",
      "Step - 182, Loss - 0.831601695774369, Learning Rate - 0.05, magnitude of gradient - 1.5262037339831749\n",
      "Step - 183, Loss - 0.7752178695644258, Learning Rate - 0.05, magnitude of gradient - 2.533319631097057\n",
      "Step - 184, Loss - 0.6983014342150436, Learning Rate - 0.05, magnitude of gradient - 2.209931279454903\n",
      "Step - 185, Loss - 0.8149769131637922, Learning Rate - 0.05, magnitude of gradient - 4.866038156047313\n",
      "Step - 186, Loss - 0.6588018836900996, Learning Rate - 0.05, magnitude of gradient - 1.5931437308177316\n",
      "Step - 187, Loss - 0.5927476767906492, Learning Rate - 0.05, magnitude of gradient - 2.4593554534129107\n",
      "Step - 188, Loss - 0.4986242923660581, Learning Rate - 0.05, magnitude of gradient - 1.6805913119782903\n",
      "Step - 189, Loss - 0.7371002404796505, Learning Rate - 0.05, magnitude of gradient - 1.1701148416922769\n",
      "Step - 190, Loss - 0.5354982069947649, Learning Rate - 0.05, magnitude of gradient - 1.7629134910244673\n",
      "Step - 191, Loss - 0.7655547540564855, Learning Rate - 0.05, magnitude of gradient - 2.279421957290008\n",
      "Step - 192, Loss - 0.7058690768085073, Learning Rate - 0.05, magnitude of gradient - 2.016398335078151\n",
      "Step - 193, Loss - 0.7208082720808828, Learning Rate - 0.05, magnitude of gradient - 2.1185759789184897\n",
      "Step - 194, Loss - 0.6502477669072704, Learning Rate - 0.05, magnitude of gradient - 0.9592075597769982\n",
      "Step - 195, Loss - 0.6813037032836516, Learning Rate - 0.05, magnitude of gradient - 3.02136528504107\n",
      "Step - 196, Loss - 0.7632175660733611, Learning Rate - 0.05, magnitude of gradient - 1.470999007522328\n",
      "Step - 197, Loss - 0.7083491836374369, Learning Rate - 0.05, magnitude of gradient - 1.8977541472547954\n",
      "Step - 198, Loss - 0.5940274695755999, Learning Rate - 0.05, magnitude of gradient - 0.5445386842477297\n",
      "Step - 199, Loss - 0.8754475022399197, Learning Rate - 0.05, magnitude of gradient - 1.2765926096686386\n",
      "Step - 200, Loss - 0.7167688187613335, Learning Rate - 0.05, magnitude of gradient - 1.6533859628599674\n",
      "Step - 201, Loss - 0.6404961346272929, Learning Rate - 0.05, magnitude of gradient - 3.540883557465121\n",
      "Step - 202, Loss - 0.6034383663066574, Learning Rate - 0.05, magnitude of gradient - 1.7684926956721816\n",
      "Step - 203, Loss - 0.7967118942170952, Learning Rate - 0.05, magnitude of gradient - 3.4693644958247534\n",
      "Step - 204, Loss - 0.4428815516374658, Learning Rate - 0.05, magnitude of gradient - 1.4665471518526745\n",
      "Step - 205, Loss - 0.7952765219986898, Learning Rate - 0.05, magnitude of gradient - 4.085112125700746\n",
      "Step - 206, Loss - 0.7036889507945424, Learning Rate - 0.05, magnitude of gradient - 1.6649553600167113\n",
      "Step - 207, Loss - 0.825221205900313, Learning Rate - 0.05, magnitude of gradient - 2.621888024349054\n",
      "Step - 208, Loss - 0.8252057570601861, Learning Rate - 0.05, magnitude of gradient - 2.7547967334982704\n",
      "Step - 209, Loss - 0.6452961050216943, Learning Rate - 0.05, magnitude of gradient - 1.4270725247249647\n",
      "Step - 210, Loss - 0.9949290852380615, Learning Rate - 0.05, magnitude of gradient - 4.098967666397324\n",
      "Step - 211, Loss - 0.4981145645473589, Learning Rate - 0.05, magnitude of gradient - 2.5148043601640064\n",
      "Step - 212, Loss - 0.7310940947560239, Learning Rate - 0.05, magnitude of gradient - 2.736776978942266\n",
      "Step - 213, Loss - 0.7912059774129562, Learning Rate - 0.05, magnitude of gradient - 2.3442060217839025\n",
      "Step - 214, Loss - 0.7289441803262644, Learning Rate - 0.05, magnitude of gradient - 0.8305870206568055\n",
      "Step - 215, Loss - 0.8722728405864373, Learning Rate - 0.05, magnitude of gradient - 5.307530189594175\n",
      "Step - 216, Loss - 0.7027799338581702, Learning Rate - 0.05, magnitude of gradient - 1.9042056654536483\n",
      "Step - 217, Loss - 0.9711936692205457, Learning Rate - 0.05, magnitude of gradient - 3.049167701885448\n",
      "Step - 218, Loss - 0.6247744010007122, Learning Rate - 0.05, magnitude of gradient - 2.189268596261844\n",
      "Step - 219, Loss - 0.7626685533061366, Learning Rate - 0.05, magnitude of gradient - 2.6708190566518186\n",
      "Step - 220, Loss - 0.8744566863540081, Learning Rate - 0.05, magnitude of gradient - 1.2620393752602372\n",
      "Step - 221, Loss - 0.511840275386336, Learning Rate - 0.05, magnitude of gradient - 2.4330584528409758\n",
      "Step - 222, Loss - 0.791186792623168, Learning Rate - 0.05, magnitude of gradient - 1.0931699309971608\n",
      "Step - 223, Loss - 0.5833135080581758, Learning Rate - 0.05, magnitude of gradient - 1.7875996844314532\n",
      "Step - 224, Loss - 0.773075333248668, Learning Rate - 0.05, magnitude of gradient - 2.177439062560903\n",
      "Step - 225, Loss - 0.6770079373611796, Learning Rate - 0.05, magnitude of gradient - 2.8371505524671328\n",
      "Step - 226, Loss - 0.6807094590061051, Learning Rate - 0.05, magnitude of gradient - 1.790696993125877\n",
      "Step - 227, Loss - 0.7978839184464916, Learning Rate - 0.05, magnitude of gradient - 3.786350739389531\n",
      "Step - 228, Loss - 0.7462611697712274, Learning Rate - 0.05, magnitude of gradient - 0.8580306666520653\n",
      "Step - 229, Loss - 0.658761002359282, Learning Rate - 0.05, magnitude of gradient - 3.2796834547350135\n",
      "Step - 230, Loss - 0.867772829741136, Learning Rate - 0.05, magnitude of gradient - 1.6395719549778716\n",
      "Step - 231, Loss - 0.7140982302376038, Learning Rate - 0.05, magnitude of gradient - 1.1040592144733883\n",
      "Step - 232, Loss - 0.7803019357212103, Learning Rate - 0.05, magnitude of gradient - 3.5861981375098417\n",
      "Step - 233, Loss - 0.6108897280464499, Learning Rate - 0.05, magnitude of gradient - 1.8075721999834617\n",
      "Step - 234, Loss - 0.769316720751832, Learning Rate - 0.05, magnitude of gradient - 1.2750060104060814\n",
      "Step - 235, Loss - 0.5810803986752273, Learning Rate - 0.05, magnitude of gradient - 3.242015771969329\n",
      "Step - 236, Loss - 0.7890455518405558, Learning Rate - 0.05, magnitude of gradient - 4.932423046228476\n",
      "Step - 237, Loss - 0.6909987250516024, Learning Rate - 0.05, magnitude of gradient - 0.9402961622115975\n",
      "Step - 238, Loss - 0.8730464077427073, Learning Rate - 0.05, magnitude of gradient - 2.2203653646602257\n",
      "Step - 239, Loss - 0.651363153735268, Learning Rate - 0.05, magnitude of gradient - 2.2773900440155925\n",
      "Step - 240, Loss - 0.6186970879039814, Learning Rate - 0.05, magnitude of gradient - 0.4854409059691008\n",
      "Step - 241, Loss - 0.6858703579268873, Learning Rate - 0.05, magnitude of gradient - 3.2187737020593326\n",
      "Step - 242, Loss - 0.8039651089205244, Learning Rate - 0.05, magnitude of gradient - 1.956212875461831\n",
      "Step - 243, Loss - 0.8246635304616066, Learning Rate - 0.05, magnitude of gradient - 4.485009407353085\n",
      "Step - 244, Loss - 0.6873581230210821, Learning Rate - 0.05, magnitude of gradient - 2.084061068006652\n",
      "Step - 245, Loss - 0.7069524953607114, Learning Rate - 0.05, magnitude of gradient - 3.253317791333911\n",
      "Step - 246, Loss - 0.6071298102294291, Learning Rate - 0.05, magnitude of gradient - 2.2088090690698925\n",
      "Step - 247, Loss - 0.8471892500582545, Learning Rate - 0.05, magnitude of gradient - 3.2388668386106567\n",
      "Step - 248, Loss - 0.8357031611443693, Learning Rate - 0.05, magnitude of gradient - 1.5943362169823876\n",
      "Step - 249, Loss - 0.5679945188758189, Learning Rate - 0.05, magnitude of gradient - 4.082594513031045\n",
      "Step - 250, Loss - 0.6413936329492773, Learning Rate - 0.05, magnitude of gradient - 1.3325120679022895\n",
      "Step - 251, Loss - 0.634246464731868, Learning Rate - 0.05, magnitude of gradient - 1.7988234042855968\n",
      "Step - 252, Loss - 0.784308655175825, Learning Rate - 0.05, magnitude of gradient - 1.599378198931753\n",
      "Step - 253, Loss - 0.6557514057133143, Learning Rate - 0.05, magnitude of gradient - 1.9553676194297538\n",
      "Step - 254, Loss - 0.7388835490581187, Learning Rate - 0.05, magnitude of gradient - 2.192922382862254\n",
      "Step - 255, Loss - 0.8908152881557048, Learning Rate - 0.05, magnitude of gradient - 4.09932702862635\n",
      "Step - 256, Loss - 0.8224464879932492, Learning Rate - 0.05, magnitude of gradient - 1.7057098580777479\n",
      "Step - 257, Loss - 0.7763411523023231, Learning Rate - 0.05, magnitude of gradient - 2.624524918346601\n",
      "Step - 258, Loss - 0.5818527520011072, Learning Rate - 0.05, magnitude of gradient - 1.069718183076569\n",
      "Step - 259, Loss - 0.6767045599462842, Learning Rate - 0.05, magnitude of gradient - 0.9287480387866254\n",
      "Step - 260, Loss - 0.906341307769149, Learning Rate - 0.05, magnitude of gradient - 3.5242178468476064\n",
      "Step - 261, Loss - 0.7750192250169072, Learning Rate - 0.05, magnitude of gradient - 3.170088197209\n",
      "Step - 262, Loss - 0.6674085910011311, Learning Rate - 0.05, magnitude of gradient - 2.654093834814191\n",
      "Step - 263, Loss - 0.7368558300890915, Learning Rate - 0.05, magnitude of gradient - 2.0386265837813737\n",
      "Step - 264, Loss - 0.7747276649482228, Learning Rate - 0.05, magnitude of gradient - 1.4828634104738798\n",
      "Step - 265, Loss - 0.5334988928783921, Learning Rate - 0.05, magnitude of gradient - 1.303997017066506\n",
      "Step - 266, Loss - 0.8110577190010656, Learning Rate - 0.05, magnitude of gradient - 2.1136028928928874\n",
      "Step - 267, Loss - 0.7294125387345032, Learning Rate - 0.05, magnitude of gradient - 2.6071055444311644\n",
      "Step - 268, Loss - 0.6264612252897341, Learning Rate - 0.05, magnitude of gradient - 2.722892327515499\n",
      "Step - 269, Loss - 0.7583216694142956, Learning Rate - 0.05, magnitude of gradient - 0.9926029394913047\n",
      "Step - 270, Loss - 0.6354429706248182, Learning Rate - 0.05, magnitude of gradient - 2.089988610490031\n",
      "Step - 271, Loss - 0.8496783307013119, Learning Rate - 0.05, magnitude of gradient - 1.8175733186455263\n",
      "Step - 272, Loss - 0.855476520447608, Learning Rate - 0.05, magnitude of gradient - 2.386680819097072\n",
      "Step - 273, Loss - 0.7652235738789942, Learning Rate - 0.05, magnitude of gradient - 1.3869209326638727\n",
      "Step - 274, Loss - 1.0029189026425496, Learning Rate - 0.05, magnitude of gradient - 3.0553700413467806\n",
      "Step - 275, Loss - 0.729490784597927, Learning Rate - 0.05, magnitude of gradient - 2.1744890939561383\n",
      "Step - 276, Loss - 0.7419286074509512, Learning Rate - 0.05, magnitude of gradient - 2.194054591511248\n",
      "Step - 277, Loss - 0.5826679497188835, Learning Rate - 0.05, magnitude of gradient - 1.0328904415953803\n",
      "Step - 278, Loss - 0.76671172757339, Learning Rate - 0.05, magnitude of gradient - 2.4344133213562023\n",
      "Step - 279, Loss - 0.6017344460793923, Learning Rate - 0.05, magnitude of gradient - 2.8169963137473704\n",
      "Step - 280, Loss - 0.6700066732688778, Learning Rate - 0.05, magnitude of gradient - 1.4850231456471792\n",
      "Step - 281, Loss - 0.7139195901737304, Learning Rate - 0.05, magnitude of gradient - 2.3399258633010485\n",
      "Step - 282, Loss - 0.7325650876984418, Learning Rate - 0.05, magnitude of gradient - 3.3591968991178214\n",
      "Step - 283, Loss - 0.6672704377285578, Learning Rate - 0.05, magnitude of gradient - 2.1765069546442968\n",
      "Step - 284, Loss - 0.6232315887995196, Learning Rate - 0.05, magnitude of gradient - 1.3007525508429454\n",
      "Step - 285, Loss - 1.026329943016724, Learning Rate - 0.05, magnitude of gradient - 3.7296542555905434\n",
      "Step - 286, Loss - 0.5781480992866656, Learning Rate - 0.05, magnitude of gradient - 1.7002855103865027\n",
      "Step - 287, Loss - 0.7037253440868025, Learning Rate - 0.05, magnitude of gradient - 2.8004066500842937\n",
      "Step - 288, Loss - 0.7805758311310715, Learning Rate - 0.05, magnitude of gradient - 1.6801922294245333\n",
      "Step - 289, Loss - 0.6385484049400338, Learning Rate - 0.05, magnitude of gradient - 2.308611402956708\n",
      "Step - 290, Loss - 0.9712321096696419, Learning Rate - 0.05, magnitude of gradient - 2.303465124599222\n",
      "Step - 291, Loss - 0.6904064658899702, Learning Rate - 0.05, magnitude of gradient - 2.132369152393146\n",
      "Step - 292, Loss - 0.9443706440879064, Learning Rate - 0.05, magnitude of gradient - 3.7016013456982386\n",
      "Step - 293, Loss - 0.8516511443025605, Learning Rate - 0.05, magnitude of gradient - 3.7944397147377287\n",
      "Step - 294, Loss - 0.6705322422081584, Learning Rate - 0.05, magnitude of gradient - 1.3602946431863965\n",
      "Step - 295, Loss - 0.737484551094791, Learning Rate - 0.05, magnitude of gradient - 1.646669766014125\n",
      "Step - 296, Loss - 0.6981035046913721, Learning Rate - 0.05, magnitude of gradient - 1.4004253353850067\n",
      "Step - 297, Loss - 0.6592889543254048, Learning Rate - 0.05, magnitude of gradient - 2.5069711487774673\n",
      "Step - 298, Loss - 0.6361028221353626, Learning Rate - 0.05, magnitude of gradient - 2.27376588438029\n",
      "Step - 299, Loss - 0.7531424426668327, Learning Rate - 0.05, magnitude of gradient - 0.819683865266572\n",
      "Step - 300, Loss - 0.7938357708934458, Learning Rate - 0.05, magnitude of gradient - 2.6600775743959684\n",
      "Step - 301, Loss - 0.6662206786653091, Learning Rate - 0.05, magnitude of gradient - 1.1837071367727405\n",
      "Step - 302, Loss - 0.6257627623262222, Learning Rate - 0.05, magnitude of gradient - 1.079754953113896\n",
      "Step - 303, Loss - 0.6477323523840863, Learning Rate - 0.05, magnitude of gradient - 1.6997803761224008\n",
      "Step - 304, Loss - 0.6419421019561897, Learning Rate - 0.05, magnitude of gradient - 1.9907940251385836\n",
      "Step - 305, Loss - 0.8451650806186786, Learning Rate - 0.05, magnitude of gradient - 3.0777512989704565\n",
      "Step - 306, Loss - 0.8430603961656193, Learning Rate - 0.05, magnitude of gradient - 1.4885719851070718\n",
      "Step - 307, Loss - 0.6493839071084766, Learning Rate - 0.05, magnitude of gradient - 1.902759361112148\n",
      "Step - 308, Loss - 0.7850066435846822, Learning Rate - 0.05, magnitude of gradient - 2.742877393202359\n",
      "Step - 309, Loss - 0.8010727709962937, Learning Rate - 0.05, magnitude of gradient - 2.8466986642535033\n",
      "Step - 310, Loss - 0.6367599068963752, Learning Rate - 0.05, magnitude of gradient - 0.5238589298415725\n",
      "Step - 311, Loss - 0.6861269222991575, Learning Rate - 0.05, magnitude of gradient - 2.0373026615743295\n",
      "Step - 312, Loss - 0.8114925771936758, Learning Rate - 0.05, magnitude of gradient - 2.4991028455954107\n",
      "Step - 313, Loss - 0.8596087232540446, Learning Rate - 0.05, magnitude of gradient - 2.4670901756857755\n",
      "Step - 314, Loss - 0.8763111990829748, Learning Rate - 0.05, magnitude of gradient - 1.4687018950130764\n",
      "Step - 315, Loss - 0.6951969136053936, Learning Rate - 0.05, magnitude of gradient - 0.6901482113842325\n",
      "Step - 316, Loss - 0.627053081305966, Learning Rate - 0.05, magnitude of gradient - 1.3088210988708446\n",
      "Step - 317, Loss - 0.7371733779130969, Learning Rate - 0.05, magnitude of gradient - 1.264587398852214\n",
      "Step - 318, Loss - 0.7062598216490166, Learning Rate - 0.05, magnitude of gradient - 2.8474887464862575\n",
      "Step - 319, Loss - 0.7431468707897425, Learning Rate - 0.05, magnitude of gradient - 1.2987737364400878\n",
      "Step - 320, Loss - 0.5894395014755591, Learning Rate - 0.05, magnitude of gradient - 2.754078602877364\n",
      "Step - 321, Loss - 0.5678838811981325, Learning Rate - 0.05, magnitude of gradient - 4.161999543157851\n",
      "Step - 322, Loss - 0.7116632194379735, Learning Rate - 0.05, magnitude of gradient - 2.1008699882019384\n",
      "Step - 323, Loss - 0.8313088161079012, Learning Rate - 0.05, magnitude of gradient - 2.0380412434662336\n",
      "Step - 324, Loss - 0.6716890016587612, Learning Rate - 0.05, magnitude of gradient - 2.197505359234201\n",
      "Step - 325, Loss - 0.7224654345111616, Learning Rate - 0.05, magnitude of gradient - 2.2759586975158665\n",
      "Step - 326, Loss - 0.7897136144829933, Learning Rate - 0.05, magnitude of gradient - 0.6487640672588658\n",
      "Step - 327, Loss - 0.7490939273513739, Learning Rate - 0.05, magnitude of gradient - 2.3645387045462782\n",
      "Step - 328, Loss - 0.6384309776913335, Learning Rate - 0.05, magnitude of gradient - 1.944780964020737\n",
      "Step - 329, Loss - 0.6378335582691164, Learning Rate - 0.05, magnitude of gradient - 4.661112265698147\n",
      "Step - 330, Loss - 0.6944969148719405, Learning Rate - 0.05, magnitude of gradient - 1.6241719159696633\n",
      "Step - 331, Loss - 0.6455689576007072, Learning Rate - 0.05, magnitude of gradient - 3.324747582888531\n",
      "Step - 332, Loss - 0.6859768715803224, Learning Rate - 0.05, magnitude of gradient - 1.1232605931979114\n",
      "Step - 333, Loss - 0.6654129140352402, Learning Rate - 0.05, magnitude of gradient - 3.410409448956367\n",
      "Step - 334, Loss - 0.6823263242451775, Learning Rate - 0.05, magnitude of gradient - 2.559160452485955\n",
      "Step - 335, Loss - 0.6554868536307779, Learning Rate - 0.05, magnitude of gradient - 1.5467252762061268\n",
      "Step - 336, Loss - 0.9167711742751482, Learning Rate - 0.05, magnitude of gradient - 1.255707796243245\n",
      "Step - 337, Loss - 0.6177290551896393, Learning Rate - 0.05, magnitude of gradient - 3.1273603123533413\n",
      "Step - 338, Loss - 0.6134747692709012, Learning Rate - 0.05, magnitude of gradient - 0.653518629899931\n",
      "Step - 339, Loss - 0.8206579550371724, Learning Rate - 0.05, magnitude of gradient - 3.164772523764736\n",
      "Step - 340, Loss - 0.7812635028003763, Learning Rate - 0.05, magnitude of gradient - 0.6600492356650469\n",
      "Step - 341, Loss - 0.5958206752380597, Learning Rate - 0.05, magnitude of gradient - 2.9287792212064985\n",
      "Step - 342, Loss - 0.6563740577755418, Learning Rate - 0.05, magnitude of gradient - 2.7114612286745077\n",
      "Step - 343, Loss - 0.6881747986453718, Learning Rate - 0.05, magnitude of gradient - 1.284693980227868\n",
      "Step - 344, Loss - 0.8518916980453454, Learning Rate - 0.05, magnitude of gradient - 2.302085891267431\n",
      "Step - 345, Loss - 0.6671755242530762, Learning Rate - 0.05, magnitude of gradient - 1.659185966399743\n",
      "Step - 346, Loss - 0.651797530346837, Learning Rate - 0.05, magnitude of gradient - 3.0376891120116647\n",
      "Step - 347, Loss - 0.6550433208267045, Learning Rate - 0.05, magnitude of gradient - 1.6196954760053859\n",
      "Step - 348, Loss - 0.6329641671016111, Learning Rate - 0.05, magnitude of gradient - 1.691988553882155\n",
      "Step - 349, Loss - 0.8770383882180555, Learning Rate - 0.05, magnitude of gradient - 1.9895051515015918\n",
      "Step - 350, Loss - 0.764917892533314, Learning Rate - 0.05, magnitude of gradient - 2.5023388479424487\n",
      "Step - 351, Loss - 0.6563782082982398, Learning Rate - 0.05, magnitude of gradient - 2.54444408111811\n",
      "Step - 352, Loss - 0.8592008091315073, Learning Rate - 0.05, magnitude of gradient - 3.9194713548136666\n",
      "Step - 353, Loss - 0.8801170060691967, Learning Rate - 0.05, magnitude of gradient - 0.7471797980240804\n",
      "Step - 354, Loss - 0.8156325494259892, Learning Rate - 0.05, magnitude of gradient - 0.26537930488021816\n",
      "Step - 355, Loss - 0.8135392288568619, Learning Rate - 0.05, magnitude of gradient - 5.435619025238782\n",
      "Step - 356, Loss - 0.5223465635715133, Learning Rate - 0.05, magnitude of gradient - 1.0015182784677221\n",
      "Step - 357, Loss - 0.5570875378654112, Learning Rate - 0.05, magnitude of gradient - 2.961944015608891\n",
      "Step - 358, Loss - 0.7506328394517977, Learning Rate - 0.05, magnitude of gradient - 1.6360603517181544\n",
      "Step - 359, Loss - 0.5619086863514215, Learning Rate - 0.05, magnitude of gradient - 1.9392826365857931\n",
      "Step - 360, Loss - 0.6029335545091232, Learning Rate - 0.05, magnitude of gradient - 1.1924427601243641\n",
      "Step - 361, Loss - 0.6023854863092926, Learning Rate - 0.05, magnitude of gradient - 1.3460283064303078\n",
      "Step - 362, Loss - 0.6545021049080232, Learning Rate - 0.05, magnitude of gradient - 3.2681963459252215\n",
      "Step - 363, Loss - 0.7600835211534933, Learning Rate - 0.05, magnitude of gradient - 3.579007051200732\n",
      "Step - 364, Loss - 0.6600343392370656, Learning Rate - 0.05, magnitude of gradient - 0.5115911248203854\n",
      "Step - 365, Loss - 0.8888239209098995, Learning Rate - 0.05, magnitude of gradient - 2.6077096071178243\n",
      "Step - 366, Loss - 0.5978455719898069, Learning Rate - 0.05, magnitude of gradient - 1.3521451738991768\n",
      "Step - 367, Loss - 0.7454667246738265, Learning Rate - 0.05, magnitude of gradient - 2.8758690426517526\n",
      "Step - 368, Loss - 0.5963718480422114, Learning Rate - 0.05, magnitude of gradient - 2.195350012684771\n",
      "Step - 369, Loss - 0.6040424290001385, Learning Rate - 0.05, magnitude of gradient - 0.4577294727768204\n",
      "Step - 370, Loss - 0.7366881312391269, Learning Rate - 0.05, magnitude of gradient - 1.7768911090663966\n",
      "Step - 371, Loss - 0.740834434243598, Learning Rate - 0.05, magnitude of gradient - 0.6024323400069792\n",
      "Step - 372, Loss - 0.7288815738212397, Learning Rate - 0.05, magnitude of gradient - 1.3790787127147237\n",
      "Step - 373, Loss - 0.7883292473686998, Learning Rate - 0.05, magnitude of gradient - 0.5502407420055697\n",
      "Step - 374, Loss - 0.7956774844316473, Learning Rate - 0.05, magnitude of gradient - 2.3698095269327952\n",
      "Step - 375, Loss - 0.5781661718951139, Learning Rate - 0.05, magnitude of gradient - 1.561584116318882\n",
      "Step - 376, Loss - 0.7273571561304526, Learning Rate - 0.05, magnitude of gradient - 3.9199352386204622\n",
      "Step - 377, Loss - 0.6574498284995387, Learning Rate - 0.05, magnitude of gradient - 2.032163892908749\n",
      "Step - 378, Loss - 0.794755544030674, Learning Rate - 0.05, magnitude of gradient - 0.9495962938478241\n",
      "Step - 379, Loss - 0.7702338076328255, Learning Rate - 0.05, magnitude of gradient - 1.4970514985971155\n",
      "Step - 380, Loss - 0.7237554974884199, Learning Rate - 0.05, magnitude of gradient - 2.8324105018596626\n",
      "Step - 381, Loss - 0.4325124999467347, Learning Rate - 0.05, magnitude of gradient - 0.5363364211161082\n",
      "Step - 382, Loss - 0.6393321224196593, Learning Rate - 0.05, magnitude of gradient - 3.4130739131333874\n",
      "Step - 383, Loss - 0.8770071562815546, Learning Rate - 0.05, magnitude of gradient - 0.8183922397434833\n",
      "Step - 384, Loss - 0.8391734981733654, Learning Rate - 0.05, magnitude of gradient - 2.8906471561414566\n",
      "Step - 385, Loss - 0.5621950129352964, Learning Rate - 0.05, magnitude of gradient - 1.0606241192644366\n",
      "Step - 386, Loss - 0.634234413301155, Learning Rate - 0.05, magnitude of gradient - 2.8254048195608785\n",
      "Step - 387, Loss - 0.8666679160608309, Learning Rate - 0.05, magnitude of gradient - 3.051037212785342\n",
      "Step - 388, Loss - 0.5682175021151143, Learning Rate - 0.05, magnitude of gradient - 3.0031003703994523\n",
      "Step - 389, Loss - 0.6269174686837685, Learning Rate - 0.05, magnitude of gradient - 3.149561237260545\n",
      "Step - 390, Loss - 0.5964518189470784, Learning Rate - 0.05, magnitude of gradient - 3.5218667164866106\n",
      "Step - 391, Loss - 0.7798151565232972, Learning Rate - 0.05, magnitude of gradient - 3.4398803951881\n",
      "Step - 392, Loss - 0.7145408900008456, Learning Rate - 0.05, magnitude of gradient - 0.6874445126654932\n",
      "Step - 393, Loss - 0.9663608932450054, Learning Rate - 0.05, magnitude of gradient - 3.7974229713140417\n",
      "Step - 394, Loss - 0.6273042424741988, Learning Rate - 0.05, magnitude of gradient - 1.0064438698540046\n",
      "Step - 395, Loss - 1.023885948747031, Learning Rate - 0.05, magnitude of gradient - 4.648120566141158\n",
      "Step - 396, Loss - 0.6129353973928998, Learning Rate - 0.05, magnitude of gradient - 1.9239375144496302\n",
      "Step - 397, Loss - 0.7140061907496884, Learning Rate - 0.05, magnitude of gradient - 2.2481669540096942\n",
      "Step - 398, Loss - 0.8096816079389075, Learning Rate - 0.05, magnitude of gradient - 1.3687759506749115\n",
      "Step - 399, Loss - 0.726046559760958, Learning Rate - 0.05, magnitude of gradient - 4.7968683023625225\n",
      "Step - 400, Loss - 0.9815081617780519, Learning Rate - 0.05, magnitude of gradient - 2.5765411137763237\n",
      "Step - 401, Loss - 0.6221771690888875, Learning Rate - 0.05, magnitude of gradient - 1.9970901456891987\n",
      "Step - 402, Loss - 0.6864039515690055, Learning Rate - 0.05, magnitude of gradient - 2.416153140591232\n",
      "Step - 403, Loss - 0.6778240565090636, Learning Rate - 0.05, magnitude of gradient - 3.255999522965157\n",
      "Step - 404, Loss - 0.8088623272492242, Learning Rate - 0.05, magnitude of gradient - 1.5101376456350488\n",
      "Step - 405, Loss - 0.837501469714738, Learning Rate - 0.05, magnitude of gradient - 4.414274229385639\n",
      "Step - 406, Loss - 0.7670176027232262, Learning Rate - 0.05, magnitude of gradient - 1.2383741279958296\n",
      "Step - 407, Loss - 0.7207420658044236, Learning Rate - 0.05, magnitude of gradient - 3.913680621415791\n",
      "Step - 408, Loss - 0.7537656036604896, Learning Rate - 0.05, magnitude of gradient - 3.3984768962739653\n",
      "Step - 409, Loss - 0.5681472838866455, Learning Rate - 0.05, magnitude of gradient - 2.7290353902604556\n",
      "Step - 410, Loss - 0.7417064611387754, Learning Rate - 0.05, magnitude of gradient - 0.8076841193726655\n",
      "Step - 411, Loss - 0.645060632789491, Learning Rate - 0.05, magnitude of gradient - 2.1307813533561135\n",
      "Step - 412, Loss - 0.7922564619672351, Learning Rate - 0.05, magnitude of gradient - 0.4428902809965346\n",
      "Step - 413, Loss - 0.8143979244730503, Learning Rate - 0.05, magnitude of gradient - 2.152297221838355\n",
      "Step - 414, Loss - 0.5388392232613219, Learning Rate - 0.05, magnitude of gradient - 3.212832561533878\n",
      "Step - 415, Loss - 0.8401704642281691, Learning Rate - 0.05, magnitude of gradient - 3.457126251588939\n",
      "Step - 416, Loss - 0.6266095288114502, Learning Rate - 0.05, magnitude of gradient - 1.36022272483904\n",
      "Step - 417, Loss - 0.6810652564707886, Learning Rate - 0.05, magnitude of gradient - 3.1405413082880544\n",
      "Step - 418, Loss - 0.6124511338382979, Learning Rate - 0.05, magnitude of gradient - 1.5899859470867688\n",
      "Step - 419, Loss - 0.9598718262141859, Learning Rate - 0.05, magnitude of gradient - 1.2466263262350976\n",
      "Step - 420, Loss - 0.663332419192441, Learning Rate - 0.05, magnitude of gradient - 1.3229862590594528\n",
      "Step - 421, Loss - 0.748398116941928, Learning Rate - 0.05, magnitude of gradient - 0.8501608210896584\n",
      "Step - 422, Loss - 0.7608822941962327, Learning Rate - 0.05, magnitude of gradient - 1.9906183292581692\n",
      "Step - 423, Loss - 0.8763329824773293, Learning Rate - 0.05, magnitude of gradient - 2.505398533993801\n",
      "Step - 424, Loss - 0.7310987598162881, Learning Rate - 0.05, magnitude of gradient - 2.330543227752371\n",
      "Step - 425, Loss - 0.7451082173692974, Learning Rate - 0.05, magnitude of gradient - 3.0574659973800142\n",
      "Step - 426, Loss - 0.6474027567269369, Learning Rate - 0.05, magnitude of gradient - 2.4636749329638192\n",
      "Step - 427, Loss - 0.6154081404963107, Learning Rate - 0.05, magnitude of gradient - 3.9121279964406845\n",
      "Step - 428, Loss - 0.7625143274937566, Learning Rate - 0.05, magnitude of gradient - 1.1263016281102343\n",
      "Step - 429, Loss - 0.9975655869268667, Learning Rate - 0.05, magnitude of gradient - 4.084036993122894\n",
      "Step - 430, Loss - 0.6287085732933456, Learning Rate - 0.05, magnitude of gradient - 0.7269184917700072\n",
      "Step - 431, Loss - 0.9870854005403686, Learning Rate - 0.05, magnitude of gradient - 3.4612256192429776\n",
      "Step - 432, Loss - 0.937330084347519, Learning Rate - 0.05, magnitude of gradient - 1.2644614081129182\n",
      "Step - 433, Loss - 0.7284733361011368, Learning Rate - 0.05, magnitude of gradient - 2.9798798382199516\n",
      "Step - 434, Loss - 1.0722608814112684, Learning Rate - 0.05, magnitude of gradient - 2.70560773914469\n",
      "Step - 435, Loss - 0.5848774764542348, Learning Rate - 0.05, magnitude of gradient - 1.388459779686498\n",
      "Step - 436, Loss - 0.7230948964810028, Learning Rate - 0.05, magnitude of gradient - 1.564134097693976\n",
      "Step - 437, Loss - 0.8368185680716077, Learning Rate - 0.05, magnitude of gradient - 2.4289848247617534\n",
      "Step - 438, Loss - 0.707470345893393, Learning Rate - 0.05, magnitude of gradient - 2.6100643719611063\n",
      "Step - 439, Loss - 0.7863051583100796, Learning Rate - 0.05, magnitude of gradient - 2.0239074047018644\n",
      "Step - 440, Loss - 0.7097829396376174, Learning Rate - 0.05, magnitude of gradient - 2.098879309059776\n",
      "Step - 441, Loss - 0.5287154463203522, Learning Rate - 0.05, magnitude of gradient - 1.7096032275257533\n",
      "Step - 442, Loss - 0.7874530031593829, Learning Rate - 0.05, magnitude of gradient - 2.9206045403298946\n",
      "Step - 443, Loss - 0.8211437519368527, Learning Rate - 0.05, magnitude of gradient - 2.216165253550402\n",
      "Step - 444, Loss - 0.7289268684060426, Learning Rate - 0.05, magnitude of gradient - 2.3710258673737674\n",
      "Step - 445, Loss - 0.8017523484201499, Learning Rate - 0.05, magnitude of gradient - 3.658974414385526\n",
      "Step - 446, Loss - 0.5110543268668218, Learning Rate - 0.05, magnitude of gradient - 1.8637868849036463\n",
      "Step - 447, Loss - 0.600405262025782, Learning Rate - 0.05, magnitude of gradient - 2.3408141823363553\n",
      "Step - 448, Loss - 0.7302581779931956, Learning Rate - 0.05, magnitude of gradient - 1.1041637708079186\n",
      "Step - 449, Loss - 0.625853117459887, Learning Rate - 0.05, magnitude of gradient - 1.5742491700515504\n",
      "Step - 450, Loss - 0.5923605102220433, Learning Rate - 0.05, magnitude of gradient - 1.3519551966184826\n",
      "Step - 451, Loss - 0.7914332877130517, Learning Rate - 0.05, magnitude of gradient - 1.0563911865595668\n",
      "Step - 452, Loss - 0.7251447301712339, Learning Rate - 0.05, magnitude of gradient - 2.5513693695122477\n",
      "Step - 453, Loss - 0.5593530921082291, Learning Rate - 0.05, magnitude of gradient - 2.473020234440031\n",
      "Step - 454, Loss - 0.7101433273524134, Learning Rate - 0.05, magnitude of gradient - 2.605947780273635\n",
      "Step - 455, Loss - 0.6322305085605678, Learning Rate - 0.05, magnitude of gradient - 2.066553187402379\n",
      "Step - 456, Loss - 0.724242372996381, Learning Rate - 0.05, magnitude of gradient - 2.61745308977361\n",
      "Step - 457, Loss - 0.8953149601829336, Learning Rate - 0.05, magnitude of gradient - 1.9193830816718611\n",
      "Step - 458, Loss - 0.40358733257604906, Learning Rate - 0.05, magnitude of gradient - 3.3642513434863934\n",
      "Step - 459, Loss - 0.5326177797431195, Learning Rate - 0.05, magnitude of gradient - 1.0901729341961106\n",
      "Step - 460, Loss - 0.6861071849068817, Learning Rate - 0.05, magnitude of gradient - 3.138357625824717\n",
      "Step - 461, Loss - 0.8050722966181967, Learning Rate - 0.05, magnitude of gradient - 1.3809833232534579\n",
      "Step - 462, Loss - 0.7328640800986006, Learning Rate - 0.05, magnitude of gradient - 2.6618073786422367\n",
      "Step - 463, Loss - 0.6222325586061026, Learning Rate - 0.05, magnitude of gradient - 1.5164889906661607\n",
      "Step - 464, Loss - 0.6913430353628514, Learning Rate - 0.05, magnitude of gradient - 2.9181786578219304\n",
      "Step - 465, Loss - 0.9364662891678417, Learning Rate - 0.05, magnitude of gradient - 0.5084177050289856\n",
      "Step - 466, Loss - 0.703826445622927, Learning Rate - 0.05, magnitude of gradient - 3.6236379317009844\n",
      "Step - 467, Loss - 0.7408161617343652, Learning Rate - 0.05, magnitude of gradient - 1.6024217409135721\n",
      "Step - 468, Loss - 0.6482481939628821, Learning Rate - 0.05, magnitude of gradient - 2.355831549190218\n",
      "Step - 469, Loss - 0.5797174854596245, Learning Rate - 0.05, magnitude of gradient - 0.5918836471108683\n",
      "Step - 470, Loss - 0.6481411488037614, Learning Rate - 0.05, magnitude of gradient - 3.030192634675952\n",
      "Step - 471, Loss - 0.8892363897065818, Learning Rate - 0.05, magnitude of gradient - 1.8343348788696479\n",
      "Step - 472, Loss - 0.748938632703775, Learning Rate - 0.05, magnitude of gradient - 2.8823596918906356\n",
      "Step - 473, Loss - 0.7356660517998419, Learning Rate - 0.05, magnitude of gradient - 5.0483936426085725\n",
      "Step - 474, Loss - 0.6765011083516502, Learning Rate - 0.05, magnitude of gradient - 0.9716786666945403\n",
      "Step - 475, Loss - 0.6906510224088703, Learning Rate - 0.05, magnitude of gradient - 3.4833245711233\n",
      "Step - 476, Loss - 0.6682298433792863, Learning Rate - 0.05, magnitude of gradient - 1.0943762893106084\n",
      "Step - 477, Loss - 0.6973636365335357, Learning Rate - 0.05, magnitude of gradient - 2.0442016508653755\n",
      "Step - 478, Loss - 0.6687566137896073, Learning Rate - 0.05, magnitude of gradient - 1.1683910975528806\n",
      "Step - 479, Loss - 0.7270761585082589, Learning Rate - 0.05, magnitude of gradient - 0.8595671859499353\n",
      "Step - 480, Loss - 0.5713399462785049, Learning Rate - 0.05, magnitude of gradient - 1.9803749000665303\n",
      "Step - 481, Loss - 0.7566818173965726, Learning Rate - 0.05, magnitude of gradient - 1.1356076665240715\n",
      "Step - 482, Loss - 0.8285870984265454, Learning Rate - 0.05, magnitude of gradient - 3.4137228901744017\n",
      "Step - 483, Loss - 0.7876740797590986, Learning Rate - 0.05, magnitude of gradient - 1.211773603720232\n",
      "Step - 484, Loss - 0.8300365516507135, Learning Rate - 0.05, magnitude of gradient - 2.4103273634821045\n",
      "Step - 485, Loss - 0.6491079830169565, Learning Rate - 0.05, magnitude of gradient - 1.4878241347449068\n",
      "Step - 486, Loss - 0.37718567243274576, Learning Rate - 0.05, magnitude of gradient - 0.9951830435964584\n",
      "Step - 487, Loss - 0.7601140536437073, Learning Rate - 0.05, magnitude of gradient - 1.9729842859666402\n",
      "Step - 488, Loss - 0.7667340093739284, Learning Rate - 0.05, magnitude of gradient - 0.930730085769128\n",
      "Step - 489, Loss - 0.8580195159404925, Learning Rate - 0.05, magnitude of gradient - 3.582335281466531\n",
      "Step - 490, Loss - 0.59562962162038, Learning Rate - 0.05, magnitude of gradient - 1.4280327349626218\n",
      "Step - 491, Loss - 0.8534508025728804, Learning Rate - 0.05, magnitude of gradient - 1.307264137509429\n",
      "Step - 492, Loss - 0.6010726320852464, Learning Rate - 0.05, magnitude of gradient - 4.606318533791907\n",
      "Step - 493, Loss - 0.9408412253591522, Learning Rate - 0.05, magnitude of gradient - 3.865777349772298\n",
      "Step - 494, Loss - 0.7538516062080773, Learning Rate - 0.05, magnitude of gradient - 0.878392384898553\n",
      "Step - 495, Loss - 0.8580679400702449, Learning Rate - 0.05, magnitude of gradient - 0.9323598903039572\n",
      "Step - 496, Loss - 0.8050012975577304, Learning Rate - 0.05, magnitude of gradient - 1.5063126588735964\n",
      "Step - 497, Loss - 0.5744776972769049, Learning Rate - 0.05, magnitude of gradient - 1.3958132223914965\n",
      "Step - 498, Loss - 0.8339319810092733, Learning Rate - 0.05, magnitude of gradient - 1.7234716434369453\n",
      "Step - 499, Loss - 0.6922439992667487, Learning Rate - 0.05, magnitude of gradient - 2.160359733319265\n",
      "Step - 500, Loss - 0.7910483736284609, Learning Rate - 0.05, magnitude of gradient - 3.5191331197784477\n",
      "Step - 501, Loss - 0.6499134001103135, Learning Rate - 0.05, magnitude of gradient - 1.1821200367518174\n",
      "Step - 502, Loss - 0.799341971754861, Learning Rate - 0.05, magnitude of gradient - 1.2692051607096235\n",
      "Step - 503, Loss - 0.8161385409602223, Learning Rate - 0.05, magnitude of gradient - 3.038419049426765\n",
      "Step - 504, Loss - 0.6010645201098564, Learning Rate - 0.05, magnitude of gradient - 1.2564663147705875\n",
      "Step - 505, Loss - 0.6573610331027144, Learning Rate - 0.05, magnitude of gradient - 0.9289298375134489\n",
      "Step - 506, Loss - 0.8874171914716991, Learning Rate - 0.05, magnitude of gradient - 3.947215372450869\n",
      "Step - 507, Loss - 0.71309851869428, Learning Rate - 0.05, magnitude of gradient - 3.5040011021010136\n",
      "Step - 508, Loss - 0.7596632803831628, Learning Rate - 0.05, magnitude of gradient - 2.324522477692394\n",
      "Step - 509, Loss - 0.6465905152026687, Learning Rate - 0.05, magnitude of gradient - 1.288467637452184\n",
      "Step - 510, Loss - 0.8209822274250482, Learning Rate - 0.05, magnitude of gradient - 4.239342379694777\n",
      "Step - 511, Loss - 0.676619450261502, Learning Rate - 0.05, magnitude of gradient - 1.1673162760665676\n",
      "Step - 512, Loss - 0.7664245512320654, Learning Rate - 0.05, magnitude of gradient - 3.3131620819946765\n",
      "Step - 513, Loss - 0.7807573471533922, Learning Rate - 0.05, magnitude of gradient - 3.248784012955014\n",
      "Step - 514, Loss - 0.8544437018978635, Learning Rate - 0.05, magnitude of gradient - 5.652087438908393\n",
      "Step - 515, Loss - 0.927136274959825, Learning Rate - 0.05, magnitude of gradient - 1.1892284409237852\n",
      "Step - 516, Loss - 0.8840489555698727, Learning Rate - 0.05, magnitude of gradient - 2.020375178052859\n",
      "Step - 517, Loss - 0.7116359045374248, Learning Rate - 0.05, magnitude of gradient - 2.1910376717513333\n",
      "Step - 518, Loss - 0.5758814366816585, Learning Rate - 0.05, magnitude of gradient - 0.3346552060009571\n",
      "Step - 519, Loss - 0.5821310059469158, Learning Rate - 0.05, magnitude of gradient - 1.0996653495820825\n",
      "Step - 520, Loss - 0.7973314091049414, Learning Rate - 0.05, magnitude of gradient - 2.227655483102298\n",
      "Step - 521, Loss - 0.7243300128327828, Learning Rate - 0.05, magnitude of gradient - 2.228775380433649\n",
      "Step - 522, Loss - 0.5666554547388919, Learning Rate - 0.05, magnitude of gradient - 0.8751790462014917\n",
      "Step - 523, Loss - 0.8301959983350478, Learning Rate - 0.05, magnitude of gradient - 1.4626538718592428\n",
      "Step - 524, Loss - 0.8558038590803287, Learning Rate - 0.05, magnitude of gradient - 1.1571898961869296\n",
      "Step - 525, Loss - 0.7068455388632209, Learning Rate - 0.05, magnitude of gradient - 3.5485004128786377\n",
      "Step - 526, Loss - 0.7114037883579396, Learning Rate - 0.05, magnitude of gradient - 1.8029734627585718\n",
      "Step - 527, Loss - 0.6786319218509361, Learning Rate - 0.05, magnitude of gradient - 2.2919681004905224\n",
      "Step - 528, Loss - 0.6573319116339424, Learning Rate - 0.05, magnitude of gradient - 1.9738823185713192\n",
      "Step - 529, Loss - 0.8972030881588754, Learning Rate - 0.05, magnitude of gradient - 2.048201935840131\n",
      "Step - 530, Loss - 0.7388693757899242, Learning Rate - 0.05, magnitude of gradient - 3.8082479487067147\n",
      "Step - 531, Loss - 0.7359737033286586, Learning Rate - 0.05, magnitude of gradient - 1.907645852106452\n",
      "Step - 532, Loss - 0.6977977582585153, Learning Rate - 0.05, magnitude of gradient - 1.4786976881531069\n",
      "Step - 533, Loss - 0.7211546372763402, Learning Rate - 0.05, magnitude of gradient - 1.309193793379834\n",
      "Step - 534, Loss - 0.6361242901549993, Learning Rate - 0.05, magnitude of gradient - 1.8667550073051247\n",
      "Step - 535, Loss - 0.7064688330781884, Learning Rate - 0.05, magnitude of gradient - 0.8967850978500932\n",
      "Step - 536, Loss - 0.6723073886968629, Learning Rate - 0.05, magnitude of gradient - 5.843373949911394\n",
      "Step - 537, Loss - 0.8089933059849383, Learning Rate - 0.05, magnitude of gradient - 2.803352295851684\n",
      "Step - 538, Loss - 1.0023119482030873, Learning Rate - 0.05, magnitude of gradient - 2.648811579565817\n",
      "Step - 539, Loss - 0.6111879813162664, Learning Rate - 0.05, magnitude of gradient - 2.013112042154353\n",
      "Step - 540, Loss - 0.690390336848275, Learning Rate - 0.05, magnitude of gradient - 2.131331471273707\n",
      "Step - 541, Loss - 0.8758390188221724, Learning Rate - 0.05, magnitude of gradient - 1.5972055650524186\n",
      "Step - 542, Loss - 0.6934673253355144, Learning Rate - 0.05, magnitude of gradient - 0.9561754372252323\n",
      "Step - 543, Loss - 0.6619860929162725, Learning Rate - 0.05, magnitude of gradient - 4.174553706860698\n",
      "Step - 544, Loss - 0.7378394530253031, Learning Rate - 0.05, magnitude of gradient - 0.7452447096235147\n",
      "Step - 545, Loss - 0.8435009759271009, Learning Rate - 0.05, magnitude of gradient - 2.325299561099046\n",
      "Step - 546, Loss - 0.6683313418445823, Learning Rate - 0.05, magnitude of gradient - 0.9633331820619752\n",
      "Step - 547, Loss - 0.5361122388996862, Learning Rate - 0.05, magnitude of gradient - 1.8886272140586176\n",
      "Step - 548, Loss - 0.6422271224745223, Learning Rate - 0.05, magnitude of gradient - 1.5039850093294265\n",
      "Step - 549, Loss - 0.7732521096293835, Learning Rate - 0.05, magnitude of gradient - 3.8427844073891695\n",
      "Step - 550, Loss - 0.9504537011798992, Learning Rate - 0.05, magnitude of gradient - 3.657970122878079\n",
      "Step - 551, Loss - 0.6917718182054536, Learning Rate - 0.05, magnitude of gradient - 2.0067035050147797\n",
      "Step - 552, Loss - 0.8619749898433696, Learning Rate - 0.05, magnitude of gradient - 2.7774062777425987\n",
      "Step - 553, Loss - 0.6228593133720278, Learning Rate - 0.05, magnitude of gradient - 0.4045431801314547\n",
      "Step - 554, Loss - 0.8253042178899581, Learning Rate - 0.05, magnitude of gradient - 2.049897656846492\n",
      "Step - 555, Loss - 0.6523763714692514, Learning Rate - 0.05, magnitude of gradient - 1.138949704636615\n",
      "Step - 556, Loss - 0.8572653225307485, Learning Rate - 0.05, magnitude of gradient - 2.08824673347686\n",
      "Step - 557, Loss - 0.6226364585389966, Learning Rate - 0.05, magnitude of gradient - 1.2465993093924703\n",
      "Step - 558, Loss - 0.6440009144358012, Learning Rate - 0.05, magnitude of gradient - 2.0249949119623665\n",
      "Step - 559, Loss - 0.7144096325513326, Learning Rate - 0.05, magnitude of gradient - 2.23463543866169\n",
      "Step - 560, Loss - 0.7490016458711155, Learning Rate - 0.05, magnitude of gradient - 1.7583206792155481\n",
      "Step - 561, Loss - 0.7401070344909855, Learning Rate - 0.05, magnitude of gradient - 2.7814391835893084\n",
      "Step - 562, Loss - 0.6700840535925492, Learning Rate - 0.05, magnitude of gradient - 2.2926058769533584\n",
      "Step - 563, Loss - 0.6894278041796829, Learning Rate - 0.05, magnitude of gradient - 2.357072950885724\n",
      "Step - 564, Loss - 0.7455937024486673, Learning Rate - 0.05, magnitude of gradient - 1.2997598536695425\n",
      "Step - 565, Loss - 0.7740626622048236, Learning Rate - 0.05, magnitude of gradient - 1.9095501308126366\n",
      "Step - 566, Loss - 0.634155919969548, Learning Rate - 0.05, magnitude of gradient - 1.1897279007176893\n",
      "Step - 567, Loss - 0.964844103306685, Learning Rate - 0.05, magnitude of gradient - 3.515813914437068\n",
      "Step - 568, Loss - 0.7695503068599301, Learning Rate - 0.05, magnitude of gradient - 1.8851142211682068\n",
      "Step - 569, Loss - 0.8089708062938744, Learning Rate - 0.05, magnitude of gradient - 2.4163722184977345\n",
      "Step - 570, Loss - 0.6737510286429399, Learning Rate - 0.05, magnitude of gradient - 3.291675794458315\n",
      "Step - 571, Loss - 0.7235140788953718, Learning Rate - 0.05, magnitude of gradient - 2.6625912891663432\n",
      "Step - 572, Loss - 0.837232169445989, Learning Rate - 0.05, magnitude of gradient - 2.335859611397844\n",
      "Step - 573, Loss - 0.7881463479016553, Learning Rate - 0.05, magnitude of gradient - 2.4175912307042453\n",
      "Step - 574, Loss - 0.6202783718311975, Learning Rate - 0.05, magnitude of gradient - 3.069827360375125\n",
      "Step - 575, Loss - 0.6288968783321547, Learning Rate - 0.05, magnitude of gradient - 2.1754577446604495\n",
      "Step - 576, Loss - 0.7035434234841593, Learning Rate - 0.05, magnitude of gradient - 2.8247545799769718\n",
      "Step - 577, Loss - 0.6253341629730879, Learning Rate - 0.05, magnitude of gradient - 1.5834534532251865\n",
      "Step - 578, Loss - 0.7860796527802868, Learning Rate - 0.05, magnitude of gradient - 1.8643346294793959\n",
      "Step - 579, Loss - 0.7116415630800547, Learning Rate - 0.05, magnitude of gradient - 0.5307989428622071\n",
      "Step - 580, Loss - 0.8322045241379109, Learning Rate - 0.05, magnitude of gradient - 2.4820372458563726\n",
      "Step - 581, Loss - 0.7969588583924827, Learning Rate - 0.05, magnitude of gradient - 2.0526977883836026\n",
      "Step - 582, Loss - 0.7690180278783401, Learning Rate - 0.05, magnitude of gradient - 2.46168772409827\n",
      "Step - 583, Loss - 0.5320334293703192, Learning Rate - 0.05, magnitude of gradient - 1.0431572445730615\n",
      "Step - 584, Loss - 0.670658532329508, Learning Rate - 0.05, magnitude of gradient - 3.700927657055683\n",
      "Step - 585, Loss - 0.6538234850608272, Learning Rate - 0.05, magnitude of gradient - 3.820022096398984\n",
      "Step - 586, Loss - 0.7524837179013744, Learning Rate - 0.05, magnitude of gradient - 2.158781555138995\n",
      "Step - 587, Loss - 0.5549584936351887, Learning Rate - 0.05, magnitude of gradient - 1.25863271614904\n",
      "Step - 588, Loss - 0.741793206653159, Learning Rate - 0.05, magnitude of gradient - 0.8507048139552328\n",
      "Step - 589, Loss - 0.6407529882984115, Learning Rate - 0.05, magnitude of gradient - 1.1639739211470177\n",
      "Step - 590, Loss - 0.7610499387122788, Learning Rate - 0.05, magnitude of gradient - 2.003591165812869\n",
      "Step - 591, Loss - 0.7224551660893523, Learning Rate - 0.05, magnitude of gradient - 1.2618795698333751\n",
      "Step - 592, Loss - 0.5865397282401386, Learning Rate - 0.05, magnitude of gradient - 2.108535587398435\n",
      "Step - 593, Loss - 0.9220215827441597, Learning Rate - 0.05, magnitude of gradient - 3.642048709768058\n",
      "Step - 594, Loss - 0.7737357454043341, Learning Rate - 0.05, magnitude of gradient - 2.120535622911934\n",
      "Step - 595, Loss - 0.7014445961473579, Learning Rate - 0.05, magnitude of gradient - 0.8153052644597559\n",
      "Step - 596, Loss - 0.8646615016246317, Learning Rate - 0.05, magnitude of gradient - 2.0831448271082125\n",
      "Step - 597, Loss - 0.7058883180736979, Learning Rate - 0.05, magnitude of gradient - 1.3826102351654928\n",
      "Step - 598, Loss - 0.7023274692927168, Learning Rate - 0.05, magnitude of gradient - 1.1305612524695887\n",
      "Step - 599, Loss - 0.5846303695066651, Learning Rate - 0.05, magnitude of gradient - 1.3802755084059661\n",
      "Step - 600, Loss - 0.5438102491903197, Learning Rate - 0.05, magnitude of gradient - 4.824454087431829\n",
      "Step - 601, Loss - 0.941592788445597, Learning Rate - 0.05, magnitude of gradient - 2.942376231633626\n",
      "Step - 602, Loss - 0.7241874621067886, Learning Rate - 0.05, magnitude of gradient - 0.6607250234689156\n",
      "Step - 603, Loss - 0.894770926080656, Learning Rate - 0.05, magnitude of gradient - 4.834556695542304\n",
      "Step - 604, Loss - 0.7256233349536046, Learning Rate - 0.05, magnitude of gradient - 1.2784005339745212\n",
      "Step - 605, Loss - 0.7089449812725428, Learning Rate - 0.05, magnitude of gradient - 2.902690351209875\n",
      "Step - 606, Loss - 0.8149392037871348, Learning Rate - 0.05, magnitude of gradient - 2.7418115010888022\n",
      "Step - 607, Loss - 0.7570367043185349, Learning Rate - 0.05, magnitude of gradient - 3.5848628487156864\n",
      "Step - 608, Loss - 0.6128228090680394, Learning Rate - 0.05, magnitude of gradient - 0.5553444828699974\n",
      "Step - 609, Loss - 0.8640361746589498, Learning Rate - 0.05, magnitude of gradient - 3.1214866944865687\n",
      "Step - 610, Loss - 0.7819697530412886, Learning Rate - 0.05, magnitude of gradient - 2.166566500752161\n",
      "Step - 611, Loss - 0.7049844993341, Learning Rate - 0.05, magnitude of gradient - 2.158894246242419\n",
      "Step - 612, Loss - 0.5527683905987816, Learning Rate - 0.05, magnitude of gradient - 0.7224113708808135\n",
      "Step - 613, Loss - 0.8433567936334196, Learning Rate - 0.05, magnitude of gradient - 3.1830304151055353\n",
      "Step - 614, Loss - 0.9182360415505976, Learning Rate - 0.05, magnitude of gradient - 2.0736376696013528\n",
      "Step - 615, Loss - 0.8326088153861989, Learning Rate - 0.05, magnitude of gradient - 1.6069833083882588\n",
      "Step - 616, Loss - 0.7839624014758766, Learning Rate - 0.05, magnitude of gradient - 1.8123041674843967\n",
      "Step - 617, Loss - 0.5797533744600367, Learning Rate - 0.05, magnitude of gradient - 3.0124965378346302\n",
      "Step - 618, Loss - 0.5001722758004633, Learning Rate - 0.05, magnitude of gradient - 2.333217917540706\n",
      "Step - 619, Loss - 0.7006813714148656, Learning Rate - 0.05, magnitude of gradient - 3.137709676080683\n",
      "Step - 620, Loss - 0.6181847990900307, Learning Rate - 0.05, magnitude of gradient - 3.1819765310072814\n",
      "Step - 621, Loss - 0.8951004399587732, Learning Rate - 0.05, magnitude of gradient - 4.5293317384716065\n",
      "Step - 622, Loss - 0.7461549172687446, Learning Rate - 0.05, magnitude of gradient - 3.024695222228979\n",
      "Step - 623, Loss - 0.7600033141164841, Learning Rate - 0.05, magnitude of gradient - 4.049290990109182\n",
      "Step - 624, Loss - 0.6745218542838591, Learning Rate - 0.05, magnitude of gradient - 0.57498076090246\n",
      "Step - 625, Loss - 0.7930217621036331, Learning Rate - 0.05, magnitude of gradient - 2.497388990121053\n",
      "Step - 626, Loss - 0.7634878126552573, Learning Rate - 0.05, magnitude of gradient - 0.9386738832870792\n",
      "Step - 627, Loss - 0.9491916310299879, Learning Rate - 0.05, magnitude of gradient - 4.233962790666429\n",
      "Step - 628, Loss - 0.8243461462637682, Learning Rate - 0.05, magnitude of gradient - 1.3106411057255904\n",
      "Step - 629, Loss - 0.7910023122775027, Learning Rate - 0.05, magnitude of gradient - 4.6698410595490465\n",
      "Step - 630, Loss - 0.7414857869403696, Learning Rate - 0.05, magnitude of gradient - 1.3544459061984473\n",
      "Step - 631, Loss - 0.7606186961268544, Learning Rate - 0.05, magnitude of gradient - 2.4355525790933408\n",
      "Step - 632, Loss - 0.6777334935805367, Learning Rate - 0.05, magnitude of gradient - 2.5905309523992788\n",
      "Step - 633, Loss - 0.7845548791705186, Learning Rate - 0.05, magnitude of gradient - 4.163124529113186\n",
      "Step - 634, Loss - 0.6717566090627007, Learning Rate - 0.05, magnitude of gradient - 0.7762452074810444\n",
      "Step - 635, Loss - 0.7425662984504225, Learning Rate - 0.05, magnitude of gradient - 4.2694942976220025\n",
      "Step - 636, Loss - 0.6186173860114403, Learning Rate - 0.05, magnitude of gradient - 1.3083892654633764\n",
      "Step - 637, Loss - 0.6523078465097484, Learning Rate - 0.05, magnitude of gradient - 1.4837747672335548\n",
      "Step - 638, Loss - 0.6593321610625146, Learning Rate - 0.05, magnitude of gradient - 0.5762512857675018\n",
      "Step - 639, Loss - 0.780636989540399, Learning Rate - 0.05, magnitude of gradient - 3.1055202332056724\n",
      "Step - 640, Loss - 0.8046255830721624, Learning Rate - 0.05, magnitude of gradient - 0.6120652594335834\n",
      "Step - 641, Loss - 0.9804287377974936, Learning Rate - 0.05, magnitude of gradient - 1.530367814137706\n",
      "Step - 642, Loss - 0.8758542572382391, Learning Rate - 0.05, magnitude of gradient - 1.1245831654638607\n",
      "Step - 643, Loss - 0.6760383609548404, Learning Rate - 0.05, magnitude of gradient - 2.245279084259997\n",
      "Step - 644, Loss - 0.6618751384156304, Learning Rate - 0.05, magnitude of gradient - 2.5691213955557672\n",
      "Step - 645, Loss - 0.7704725692277031, Learning Rate - 0.05, magnitude of gradient - 0.951194058277482\n",
      "Step - 646, Loss - 0.9808390498467388, Learning Rate - 0.05, magnitude of gradient - 3.7935320453790897\n",
      "Step - 647, Loss - 0.6396532514736484, Learning Rate - 0.05, magnitude of gradient - 1.4030421962070658\n",
      "Step - 648, Loss - 0.8470456217227111, Learning Rate - 0.05, magnitude of gradient - 1.121463488961131\n",
      "Step - 649, Loss - 1.0699656182151105, Learning Rate - 0.05, magnitude of gradient - 2.9077040868770285\n",
      "Step - 650, Loss - 0.6420961895896411, Learning Rate - 0.05, magnitude of gradient - 1.1735354394901536\n",
      "Step - 651, Loss - 0.7541051738399984, Learning Rate - 0.05, magnitude of gradient - 3.766114471876403\n",
      "Step - 652, Loss - 0.846045267978287, Learning Rate - 0.05, magnitude of gradient - 1.2752399955264133\n",
      "Step - 653, Loss - 0.7404645340659181, Learning Rate - 0.05, magnitude of gradient - 4.532423852869308\n",
      "Step - 654, Loss - 0.80487430469995, Learning Rate - 0.05, magnitude of gradient - 1.3023635166830396\n",
      "Step - 655, Loss - 0.7231615213342186, Learning Rate - 0.05, magnitude of gradient - 1.8844066138180182\n",
      "Step - 656, Loss - 0.7537225832215113, Learning Rate - 0.05, magnitude of gradient - 2.136252838558355\n",
      "Step - 657, Loss - 0.9018560643546251, Learning Rate - 0.05, magnitude of gradient - 3.6336022839590782\n",
      "Step - 658, Loss - 0.8036635423473131, Learning Rate - 0.05, magnitude of gradient - 1.420676904462107\n",
      "Step - 659, Loss - 0.6262252598079983, Learning Rate - 0.05, magnitude of gradient - 2.619184415910238\n",
      "Step - 660, Loss - 0.653342792757, Learning Rate - 0.05, magnitude of gradient - 3.843702618845255\n",
      "Step - 661, Loss - 1.0003347746899398, Learning Rate - 0.05, magnitude of gradient - 2.1443189032765533\n",
      "Step - 662, Loss - 0.6335011001800994, Learning Rate - 0.05, magnitude of gradient - 1.487409239027004\n",
      "Step - 663, Loss - 0.7555564929434497, Learning Rate - 0.05, magnitude of gradient - 3.329496944568878\n",
      "Step - 664, Loss - 0.7979997793789142, Learning Rate - 0.05, magnitude of gradient - 2.727954309651357\n",
      "Step - 665, Loss - 0.7541350887279472, Learning Rate - 0.05, magnitude of gradient - 1.9372932169932389\n",
      "Step - 666, Loss - 0.6669662704338931, Learning Rate - 0.05, magnitude of gradient - 2.278896859928799\n",
      "Step - 667, Loss - 0.7133111904007027, Learning Rate - 0.05, magnitude of gradient - 0.9125485906713436\n",
      "Step - 668, Loss - 0.9321345964720433, Learning Rate - 0.05, magnitude of gradient - 1.9625359650320762\n",
      "Step - 669, Loss - 0.8459151272680738, Learning Rate - 0.05, magnitude of gradient - 1.721053866146831\n",
      "Step - 670, Loss - 0.7136797809894962, Learning Rate - 0.05, magnitude of gradient - 1.204758680450361\n",
      "Step - 671, Loss - 0.7184776199332925, Learning Rate - 0.05, magnitude of gradient - 1.3830921349601795\n",
      "Step - 672, Loss - 0.6496116987385624, Learning Rate - 0.05, magnitude of gradient - 1.862481528832153\n",
      "Step - 673, Loss - 0.6623674149495689, Learning Rate - 0.05, magnitude of gradient - 2.0525925090422077\n",
      "Step - 674, Loss - 0.6553848172935126, Learning Rate - 0.05, magnitude of gradient - 1.229022747031149\n",
      "Step - 675, Loss - 0.7309252317261079, Learning Rate - 0.05, magnitude of gradient - 1.8453627624337736\n",
      "Step - 676, Loss - 0.7579364189254885, Learning Rate - 0.05, magnitude of gradient - 1.1512392231257496\n",
      "Step - 677, Loss - 0.7624409159223133, Learning Rate - 0.05, magnitude of gradient - 3.682385804139687\n",
      "Step - 678, Loss - 0.9064200044156875, Learning Rate - 0.05, magnitude of gradient - 1.172510240319105\n",
      "Step - 679, Loss - 0.6215416658966479, Learning Rate - 0.05, magnitude of gradient - 2.915023036011086\n",
      "Step - 680, Loss - 0.8619923730258475, Learning Rate - 0.05, magnitude of gradient - 3.7320835673464705\n",
      "Step - 681, Loss - 0.8367140919110954, Learning Rate - 0.05, magnitude of gradient - 2.60522400663465\n",
      "Step - 682, Loss - 0.693014814336552, Learning Rate - 0.05, magnitude of gradient - 0.5769740747566962\n",
      "Step - 683, Loss - 0.6558143579647917, Learning Rate - 0.05, magnitude of gradient - 2.8885719624628945\n",
      "Step - 684, Loss - 0.6295509888110595, Learning Rate - 0.05, magnitude of gradient - 2.17809750166552\n",
      "Step - 685, Loss - 0.7754229197126198, Learning Rate - 0.05, magnitude of gradient - 3.9178997088926226\n",
      "Step - 686, Loss - 0.6634965798226393, Learning Rate - 0.05, magnitude of gradient - 0.25210297444765395\n",
      "Step - 687, Loss - 0.8418672072431328, Learning Rate - 0.05, magnitude of gradient - 5.623698745450067\n",
      "Step - 688, Loss - 0.7517243516197273, Learning Rate - 0.05, magnitude of gradient - 2.1018682454126227\n",
      "Step - 689, Loss - 0.7059209541682889, Learning Rate - 0.05, magnitude of gradient - 1.6414838469721016\n",
      "Step - 690, Loss - 0.7893012656729725, Learning Rate - 0.05, magnitude of gradient - 1.4729042624860746\n",
      "Step - 691, Loss - 0.6999448814733856, Learning Rate - 0.05, magnitude of gradient - 0.75897389889611\n",
      "Step - 692, Loss - 0.5820646524032403, Learning Rate - 0.05, magnitude of gradient - 2.8228209692019637\n",
      "Step - 693, Loss - 0.8016865070365025, Learning Rate - 0.05, magnitude of gradient - 1.3798145943467592\n",
      "Step - 694, Loss - 0.786144232477743, Learning Rate - 0.05, magnitude of gradient - 1.3793104698753873\n",
      "Step - 695, Loss - 0.7417342266757841, Learning Rate - 0.05, magnitude of gradient - 1.6707561809677525\n",
      "Step - 696, Loss - 0.719768689265259, Learning Rate - 0.05, magnitude of gradient - 0.8679422381726577\n",
      "Step - 697, Loss - 0.7109436329786035, Learning Rate - 0.05, magnitude of gradient - 2.252803183919629\n",
      "Step - 698, Loss - 0.7097172868802291, Learning Rate - 0.05, magnitude of gradient - 0.8418297681815586\n",
      "Step - 699, Loss - 0.7002962331609122, Learning Rate - 0.05, magnitude of gradient - 3.631132985160161\n",
      "Step - 700, Loss - 0.6361144847594828, Learning Rate - 0.05, magnitude of gradient - 0.989449072515474\n",
      "Step - 701, Loss - 0.8120066296006135, Learning Rate - 0.05, magnitude of gradient - 2.249513961572269\n",
      "Step - 702, Loss - 0.8232807136818187, Learning Rate - 0.05, magnitude of gradient - 3.3120280898131185\n",
      "Step - 703, Loss - 0.9669813285815386, Learning Rate - 0.05, magnitude of gradient - 3.088307867217356\n",
      "Step - 704, Loss - 0.9701710167537922, Learning Rate - 0.05, magnitude of gradient - 5.13683638667551\n",
      "Step - 705, Loss - 0.7587684758078939, Learning Rate - 0.05, magnitude of gradient - 2.1679587460180696\n",
      "Step - 706, Loss - 0.7898705532471094, Learning Rate - 0.05, magnitude of gradient - 3.0720526912494996\n",
      "Step - 707, Loss - 0.8365571136049289, Learning Rate - 0.05, magnitude of gradient - 2.9229907984098693\n",
      "Step - 708, Loss - 0.5699545876363555, Learning Rate - 0.05, magnitude of gradient - 1.7399245186051737\n",
      "Step - 709, Loss - 0.7384541000303724, Learning Rate - 0.05, magnitude of gradient - 2.5625340854989878\n",
      "Step - 710, Loss - 0.7829747230316684, Learning Rate - 0.05, magnitude of gradient - 1.9660613915444203\n",
      "Step - 711, Loss - 0.6033092317927703, Learning Rate - 0.05, magnitude of gradient - 1.8699106768053693\n",
      "Step - 712, Loss - 0.7129089539119053, Learning Rate - 0.05, magnitude of gradient - 2.97356760109884\n",
      "Step - 713, Loss - 0.6161792745641225, Learning Rate - 0.05, magnitude of gradient - 2.0995478923490722\n",
      "Step - 714, Loss - 0.6882271350432339, Learning Rate - 0.05, magnitude of gradient - 2.243498609804203\n",
      "Step - 715, Loss - 0.5743939927957454, Learning Rate - 0.05, magnitude of gradient - 2.823686852413915\n",
      "Step - 716, Loss - 0.8528561274470117, Learning Rate - 0.05, magnitude of gradient - 3.4904894530770667\n",
      "Step - 717, Loss - 0.6781552235574941, Learning Rate - 0.05, magnitude of gradient - 0.9534783627888974\n",
      "Step - 718, Loss - 0.7798419167857054, Learning Rate - 0.05, magnitude of gradient - 2.909752309710605\n",
      "Step - 719, Loss - 0.5443052869475035, Learning Rate - 0.05, magnitude of gradient - 1.9360687226249143\n",
      "Step - 720, Loss - 0.7941029942981186, Learning Rate - 0.05, magnitude of gradient - 2.756634389220844\n",
      "Step - 721, Loss - 0.675797551471138, Learning Rate - 0.05, magnitude of gradient - 0.5736045561884999\n",
      "Step - 722, Loss - 0.5174695810621621, Learning Rate - 0.05, magnitude of gradient - 0.5582432799963799\n",
      "Step - 723, Loss - 0.6378531036615529, Learning Rate - 0.05, magnitude of gradient - 0.7292388220302275\n",
      "Step - 724, Loss - 0.6116709082985334, Learning Rate - 0.05, magnitude of gradient - 1.5754515501996\n",
      "Step - 725, Loss - 0.7161844687672891, Learning Rate - 0.05, magnitude of gradient - 1.9125670847976233\n",
      "Step - 726, Loss - 0.9881211814550119, Learning Rate - 0.05, magnitude of gradient - 3.6492550338189984\n",
      "Step - 727, Loss - 0.5284911491433669, Learning Rate - 0.05, magnitude of gradient - 2.2032906750450865\n",
      "Step - 728, Loss - 0.6719297516465529, Learning Rate - 0.05, magnitude of gradient - 2.5195557233697166\n",
      "Step - 729, Loss - 0.6611663039828399, Learning Rate - 0.05, magnitude of gradient - 1.1630927401439375\n",
      "Step - 730, Loss - 0.7658115755634857, Learning Rate - 0.05, magnitude of gradient - 3.0391255586965227\n",
      "Step - 731, Loss - 0.7123733357765718, Learning Rate - 0.05, magnitude of gradient - 1.6477417045454326\n",
      "Step - 732, Loss - 0.6490132837377872, Learning Rate - 0.05, magnitude of gradient - 1.901489533535825\n",
      "Step - 733, Loss - 0.6854542691126155, Learning Rate - 0.05, magnitude of gradient - 1.1036496313187938\n",
      "Step - 734, Loss - 0.5573807522062658, Learning Rate - 0.05, magnitude of gradient - 1.4809455529883835\n",
      "Step - 735, Loss - 0.7253017747480415, Learning Rate - 0.05, magnitude of gradient - 1.440549083797611\n",
      "Step - 736, Loss - 0.663541753640715, Learning Rate - 0.05, magnitude of gradient - 2.721171768492446\n",
      "Step - 737, Loss - 0.5785932101764284, Learning Rate - 0.05, magnitude of gradient - 2.475347892401013\n",
      "Step - 738, Loss - 0.7100214534099908, Learning Rate - 0.05, magnitude of gradient - 2.9595349652801484\n",
      "Step - 739, Loss - 0.900436280477979, Learning Rate - 0.05, magnitude of gradient - 2.8435128772794473\n",
      "Step - 740, Loss - 0.5999737323814507, Learning Rate - 0.05, magnitude of gradient - 2.0503385395224316\n",
      "Step - 741, Loss - 0.7659187192749516, Learning Rate - 0.05, magnitude of gradient - 4.114353230194431\n",
      "Step - 742, Loss - 0.8247349271745589, Learning Rate - 0.05, magnitude of gradient - 0.6007564469505468\n",
      "Step - 743, Loss - 0.7782306807794214, Learning Rate - 0.05, magnitude of gradient - 4.63207065277527\n",
      "Step - 744, Loss - 0.610294775335291, Learning Rate - 0.05, magnitude of gradient - 3.482609473348927\n",
      "Step - 745, Loss - 0.5790563418824507, Learning Rate - 0.05, magnitude of gradient - 3.3310311073153676\n",
      "Step - 746, Loss - 0.537310167096094, Learning Rate - 0.05, magnitude of gradient - 1.2080724504152731\n",
      "Step - 747, Loss - 0.8673272378050847, Learning Rate - 0.05, magnitude of gradient - 1.8449106546751162\n",
      "Step - 748, Loss - 0.6530817478575733, Learning Rate - 0.05, magnitude of gradient - 0.6181755539914495\n",
      "Step - 749, Loss - 0.7255906058386505, Learning Rate - 0.05, magnitude of gradient - 1.8604089554159788\n",
      "Step - 750, Loss - 0.6538322553713978, Learning Rate - 0.05, magnitude of gradient - 3.48889380058769\n",
      "Step - 751, Loss - 0.6625176936898881, Learning Rate - 0.05, magnitude of gradient - 4.143116355402067\n",
      "Step - 752, Loss - 0.6474464688735964, Learning Rate - 0.05, magnitude of gradient - 0.7041537135365531\n",
      "Step - 753, Loss - 0.7339665253085581, Learning Rate - 0.05, magnitude of gradient - 4.743434889048924\n",
      "Step - 754, Loss - 0.624722859852243, Learning Rate - 0.05, magnitude of gradient - 1.7051897173641255\n",
      "Step - 755, Loss - 0.6446382930263629, Learning Rate - 0.05, magnitude of gradient - 3.4041892194139582\n",
      "Step - 756, Loss - 0.6024474119059211, Learning Rate - 0.05, magnitude of gradient - 0.7030322673756952\n",
      "Step - 757, Loss - 0.7499095370238236, Learning Rate - 0.05, magnitude of gradient - 1.94956778730459\n",
      "Step - 758, Loss - 0.7913145961836373, Learning Rate - 0.05, magnitude of gradient - 2.0863430678743744\n",
      "Step - 759, Loss - 0.6313342316591712, Learning Rate - 0.05, magnitude of gradient - 1.534104952373665\n",
      "Step - 760, Loss - 0.6401491184277978, Learning Rate - 0.05, magnitude of gradient - 2.4180799447376398\n",
      "Step - 761, Loss - 0.6939019293852181, Learning Rate - 0.05, magnitude of gradient - 0.7404663003973746\n",
      "Step - 762, Loss - 0.6023088114200869, Learning Rate - 0.05, magnitude of gradient - 2.793260286582771\n",
      "Step - 763, Loss - 0.6044867935018309, Learning Rate - 0.05, magnitude of gradient - 1.7574038785542536\n",
      "Step - 764, Loss - 0.7979141289131182, Learning Rate - 0.05, magnitude of gradient - 2.964284451560562\n",
      "Step - 765, Loss - 0.673279730890441, Learning Rate - 0.05, magnitude of gradient - 0.9921315780922565\n",
      "Step - 766, Loss - 0.7171735960568819, Learning Rate - 0.05, magnitude of gradient - 2.705658185648813\n",
      "Step - 767, Loss - 0.6872893830171796, Learning Rate - 0.05, magnitude of gradient - 1.0188426028523914\n",
      "Step - 768, Loss - 0.9748438524153047, Learning Rate - 0.05, magnitude of gradient - 4.752567247944802\n",
      "Step - 769, Loss - 0.6914133404947783, Learning Rate - 0.05, magnitude of gradient - 0.9444780395567021\n",
      "Step - 770, Loss - 0.8449408574478645, Learning Rate - 0.05, magnitude of gradient - 3.568264786824735\n",
      "Step - 771, Loss - 0.6416777778824536, Learning Rate - 0.05, magnitude of gradient - 1.3477218800921547\n",
      "Step - 772, Loss - 0.8966682468452842, Learning Rate - 0.05, magnitude of gradient - 4.509086652311532\n",
      "Step - 773, Loss - 0.7435221453713625, Learning Rate - 0.05, magnitude of gradient - 2.2656164981080757\n",
      "Step - 774, Loss - 0.5582213951894797, Learning Rate - 0.05, magnitude of gradient - 1.1954090817553176\n",
      "Step - 775, Loss - 0.7553968826840853, Learning Rate - 0.05, magnitude of gradient - 2.5343607340954137\n",
      "Step - 776, Loss - 0.7606638330576356, Learning Rate - 0.05, magnitude of gradient - 1.2784628459356029\n",
      "Step - 777, Loss - 0.8685443995307448, Learning Rate - 0.05, magnitude of gradient - 2.572676068752273\n",
      "Step - 778, Loss - 0.6312577407607709, Learning Rate - 0.05, magnitude of gradient - 2.2081186752375834\n",
      "Step - 779, Loss - 0.6452206832170718, Learning Rate - 0.05, magnitude of gradient - 1.2197673771520592\n",
      "Step - 780, Loss - 0.7085705960402224, Learning Rate - 0.05, magnitude of gradient - 3.242806898698364\n",
      "Step - 781, Loss - 0.7260508874247473, Learning Rate - 0.05, magnitude of gradient - 2.927809236170439\n",
      "Step - 782, Loss - 0.7097613127196789, Learning Rate - 0.05, magnitude of gradient - 1.8534579694288493\n",
      "Step - 783, Loss - 0.7950633532183216, Learning Rate - 0.05, magnitude of gradient - 2.2285492441026635\n",
      "Step - 784, Loss - 0.8448865561614075, Learning Rate - 0.05, magnitude of gradient - 4.774110117694744\n",
      "Step - 785, Loss - 0.6305773512957834, Learning Rate - 0.05, magnitude of gradient - 0.9077408746276928\n",
      "Step - 786, Loss - 0.7519283165292332, Learning Rate - 0.05, magnitude of gradient - 2.566516650819489\n",
      "Step - 787, Loss - 0.7609631475119997, Learning Rate - 0.05, magnitude of gradient - 1.4781943399652024\n",
      "Step - 788, Loss - 0.6851163276694455, Learning Rate - 0.05, magnitude of gradient - 2.827493905496863\n",
      "Step - 789, Loss - 0.5692408179441115, Learning Rate - 0.05, magnitude of gradient - 0.835128454611064\n",
      "Step - 790, Loss - 1.1304962103330563, Learning Rate - 0.05, magnitude of gradient - 3.791354538316328\n",
      "Step - 791, Loss - 0.7348159965983393, Learning Rate - 0.05, magnitude of gradient - 1.245062516016756\n",
      "Step - 792, Loss - 0.7564226978970635, Learning Rate - 0.05, magnitude of gradient - 3.7627026555490155\n",
      "Step - 793, Loss - 0.9574663407508961, Learning Rate - 0.05, magnitude of gradient - 2.1941886152319765\n",
      "Step - 794, Loss - 0.6364374967284478, Learning Rate - 0.05, magnitude of gradient - 3.595667640987787\n",
      "Step - 795, Loss - 0.8242128207314164, Learning Rate - 0.05, magnitude of gradient - 1.839837884037805\n",
      "Step - 796, Loss - 0.9663670349956817, Learning Rate - 0.05, magnitude of gradient - 4.57233022980284\n",
      "Step - 797, Loss - 0.5791973778846884, Learning Rate - 0.05, magnitude of gradient - 1.9513366213286192\n",
      "Step - 798, Loss - 0.8065245306732224, Learning Rate - 0.05, magnitude of gradient - 3.4643237818573827\n",
      "Step - 799, Loss - 0.7384461651978977, Learning Rate - 0.05, magnitude of gradient - 1.1870799215334156\n",
      "Step - 800, Loss - 0.9265879287987079, Learning Rate - 0.05, magnitude of gradient - 2.8961450856038597\n",
      "Step - 801, Loss - 0.7492298243868629, Learning Rate - 0.05, magnitude of gradient - 0.8625889461039254\n",
      "Step - 802, Loss - 0.8027464071488852, Learning Rate - 0.05, magnitude of gradient - 0.711612889636286\n",
      "Step - 803, Loss - 0.7791445427444741, Learning Rate - 0.05, magnitude of gradient - 2.519443041361073\n",
      "Step - 804, Loss - 0.5568160300448305, Learning Rate - 0.05, magnitude of gradient - 2.119767103317825\n",
      "Step - 805, Loss - 0.8301466477778778, Learning Rate - 0.05, magnitude of gradient - 1.643988704791429\n",
      "Step - 806, Loss - 0.7819359862672768, Learning Rate - 0.05, magnitude of gradient - 3.486944915154552\n",
      "Step - 807, Loss - 0.6854846396607839, Learning Rate - 0.05, magnitude of gradient - 1.8905913963427194\n",
      "Step - 808, Loss - 0.5998984831894676, Learning Rate - 0.05, magnitude of gradient - 0.7652084263463005\n",
      "Step - 809, Loss - 0.6613711276112495, Learning Rate - 0.05, magnitude of gradient - 3.2508861213043274\n",
      "Step - 810, Loss - 0.514859195915616, Learning Rate - 0.05, magnitude of gradient - 1.5583550625611529\n",
      "Step - 811, Loss - 0.7876030234141786, Learning Rate - 0.05, magnitude of gradient - 2.9464986529539137\n",
      "Step - 812, Loss - 0.7751480638737074, Learning Rate - 0.05, magnitude of gradient - 2.116308139791678\n",
      "Step - 813, Loss - 0.5492451432413351, Learning Rate - 0.05, magnitude of gradient - 2.9273504355727096\n",
      "Step - 814, Loss - 0.6289991097093788, Learning Rate - 0.05, magnitude of gradient - 3.4505978657221723\n",
      "Step - 815, Loss - 0.7184580969338423, Learning Rate - 0.05, magnitude of gradient - 2.209296123001817\n",
      "Step - 816, Loss - 0.7687186343697509, Learning Rate - 0.05, magnitude of gradient - 3.9121514594885647\n",
      "Step - 817, Loss - 0.7424344060398439, Learning Rate - 0.05, magnitude of gradient - 0.8178363501442062\n",
      "Step - 818, Loss - 0.7552920577648677, Learning Rate - 0.05, magnitude of gradient - 1.6858032697717433\n",
      "Step - 819, Loss - 0.65927955551685, Learning Rate - 0.05, magnitude of gradient - 2.4069118172704695\n",
      "Step - 820, Loss - 0.8545405196397994, Learning Rate - 0.05, magnitude of gradient - 5.773081663815232\n",
      "Step - 821, Loss - 0.7362605620621558, Learning Rate - 0.05, magnitude of gradient - 2.9161432247244914\n",
      "Step - 822, Loss - 0.8219551064333657, Learning Rate - 0.05, magnitude of gradient - 2.0918186287054352\n",
      "Step - 823, Loss - 0.6302964884235079, Learning Rate - 0.05, magnitude of gradient - 2.7005635943519075\n",
      "Step - 824, Loss - 0.7632422016319937, Learning Rate - 0.05, magnitude of gradient - 1.5269982100669481\n",
      "Step - 825, Loss - 0.6149925776238917, Learning Rate - 0.05, magnitude of gradient - 0.8657159666426842\n",
      "Step - 826, Loss - 0.7767718462040158, Learning Rate - 0.05, magnitude of gradient - 1.77061925562432\n",
      "Step - 827, Loss - 0.8420411832231628, Learning Rate - 0.05, magnitude of gradient - 3.0689306221763895\n",
      "Step - 828, Loss - 0.6431326033632265, Learning Rate - 0.05, magnitude of gradient - 2.0454143839579833\n",
      "Step - 829, Loss - 0.682529822026311, Learning Rate - 0.05, magnitude of gradient - 2.765229999902113\n",
      "Step - 830, Loss - 0.7598540396163266, Learning Rate - 0.05, magnitude of gradient - 1.5574683394052526\n",
      "Step - 831, Loss - 0.9320540034620322, Learning Rate - 0.05, magnitude of gradient - 4.243652773302092\n",
      "Step - 832, Loss - 0.778694322499566, Learning Rate - 0.05, magnitude of gradient - 0.5346512349093601\n",
      "Step - 833, Loss - 0.8057727427420861, Learning Rate - 0.05, magnitude of gradient - 2.8971449410639014\n",
      "Step - 834, Loss - 0.7239838881528945, Learning Rate - 0.05, magnitude of gradient - 1.5064326141636322\n",
      "Step - 835, Loss - 0.9149321396432694, Learning Rate - 0.05, magnitude of gradient - 3.6997941982568894\n",
      "Step - 836, Loss - 0.7653397824701282, Learning Rate - 0.05, magnitude of gradient - 1.047798381099007\n",
      "Step - 837, Loss - 0.7094867246140282, Learning Rate - 0.05, magnitude of gradient - 1.3912982075101537\n",
      "Step - 838, Loss - 0.5021407675793716, Learning Rate - 0.05, magnitude of gradient - 1.2805784208874527\n",
      "Step - 839, Loss - 0.9200313564693933, Learning Rate - 0.05, magnitude of gradient - 4.742708626827997\n",
      "Step - 840, Loss - 0.6820843350812072, Learning Rate - 0.05, magnitude of gradient - 0.7099344011578403\n",
      "Step - 841, Loss - 0.8754142715945479, Learning Rate - 0.05, magnitude of gradient - 1.528844404366863\n",
      "Step - 842, Loss - 0.6802885915807911, Learning Rate - 0.05, magnitude of gradient - 2.258392915510827\n",
      "Step - 843, Loss - 0.803276499134081, Learning Rate - 0.05, magnitude of gradient - 3.5110378208572204\n",
      "Step - 844, Loss - 0.6577830085634057, Learning Rate - 0.05, magnitude of gradient - 0.4915070457515514\n",
      "Step - 845, Loss - 0.8114206651296809, Learning Rate - 0.05, magnitude of gradient - 3.840369848144652\n",
      "Step - 846, Loss - 0.8547447553717522, Learning Rate - 0.05, magnitude of gradient - 1.2350692409466308\n",
      "Step - 847, Loss - 0.834465150907636, Learning Rate - 0.05, magnitude of gradient - 2.1654030240551934\n",
      "Step - 848, Loss - 0.6851581665620272, Learning Rate - 0.05, magnitude of gradient - 0.40577906660113433\n",
      "Step - 849, Loss - 0.7103169337462591, Learning Rate - 0.05, magnitude of gradient - 1.7035951145427943\n",
      "Step - 850, Loss - 0.7334700092677692, Learning Rate - 0.05, magnitude of gradient - 1.9873382874297447\n",
      "Step - 851, Loss - 0.822278011989496, Learning Rate - 0.05, magnitude of gradient - 5.1144640902821275\n",
      "Step - 852, Loss - 0.7547430123526755, Learning Rate - 0.05, magnitude of gradient - 1.8656547793305414\n",
      "Step - 853, Loss - 0.8116741492178166, Learning Rate - 0.05, magnitude of gradient - 4.448267351022743\n",
      "Step - 854, Loss - 0.7393829492391993, Learning Rate - 0.05, magnitude of gradient - 1.2351838656974372\n",
      "Step - 855, Loss - 0.6990944577738203, Learning Rate - 0.05, magnitude of gradient - 3.6131874566470112\n",
      "Step - 856, Loss - 0.6959598624057914, Learning Rate - 0.05, magnitude of gradient - 2.404000696163616\n",
      "Step - 857, Loss - 0.7919799636408136, Learning Rate - 0.05, magnitude of gradient - 3.030898881635534\n",
      "Step - 858, Loss - 0.8030926914998837, Learning Rate - 0.05, magnitude of gradient - 2.3935418564691058\n",
      "Step - 859, Loss - 0.771730655031035, Learning Rate - 0.05, magnitude of gradient - 1.8217503592207989\n",
      "Step - 860, Loss - 0.6194351421329212, Learning Rate - 0.05, magnitude of gradient - 0.8589097493832604\n",
      "Step - 861, Loss - 0.8680501238956826, Learning Rate - 0.05, magnitude of gradient - 3.4794331125838323\n",
      "Step - 862, Loss - 0.7308918557782993, Learning Rate - 0.05, magnitude of gradient - 1.9898592059772506\n",
      "Step - 863, Loss - 0.5136867158616725, Learning Rate - 0.05, magnitude of gradient - 0.5824003546531443\n",
      "Step - 864, Loss - 0.7940834462238242, Learning Rate - 0.05, magnitude of gradient - 3.234131073668784\n",
      "Step - 865, Loss - 0.7010172285005423, Learning Rate - 0.05, magnitude of gradient - 0.7711467351301572\n",
      "Step - 866, Loss - 0.7137597416330249, Learning Rate - 0.05, magnitude of gradient - 2.091904185899363\n",
      "Step - 867, Loss - 0.741669042863405, Learning Rate - 0.05, magnitude of gradient - 1.4297288716173842\n",
      "Step - 868, Loss - 0.9700453944388264, Learning Rate - 0.05, magnitude of gradient - 4.8388043428354806\n",
      "Step - 869, Loss - 0.7738897640859749, Learning Rate - 0.05, magnitude of gradient - 1.792499144240148\n",
      "Step - 870, Loss - 0.9180437474032344, Learning Rate - 0.05, magnitude of gradient - 4.7155229881371445\n",
      "Step - 871, Loss - 0.593580611284648, Learning Rate - 0.05, magnitude of gradient - 1.3750170511016766\n",
      "Step - 872, Loss - 0.8523847881150779, Learning Rate - 0.05, magnitude of gradient - 2.7376841509344705\n",
      "Step - 873, Loss - 0.8440690941355308, Learning Rate - 0.05, magnitude of gradient - 0.9936206550756356\n",
      "Step - 874, Loss - 0.6110111055038712, Learning Rate - 0.05, magnitude of gradient - 2.022823115321911\n",
      "Step - 875, Loss - 1.1226545504204464, Learning Rate - 0.05, magnitude of gradient - 4.402484620127808\n",
      "Step - 876, Loss - 0.6892460638344958, Learning Rate - 0.05, magnitude of gradient - 1.3974341797874357\n",
      "Step - 877, Loss - 0.7271853059677993, Learning Rate - 0.05, magnitude of gradient - 3.92724655477757\n",
      "Step - 878, Loss - 0.8527956213987962, Learning Rate - 0.05, magnitude of gradient - 2.6290379974902938\n",
      "Step - 879, Loss - 0.5149273381734668, Learning Rate - 0.05, magnitude of gradient - 2.3061648914773283\n",
      "Step - 880, Loss - 0.8155042883385722, Learning Rate - 0.05, magnitude of gradient - 3.8933620273565768\n",
      "Step - 881, Loss - 0.7647906361079692, Learning Rate - 0.05, magnitude of gradient - 2.103116138098537\n",
      "Step - 882, Loss - 0.7957870546028647, Learning Rate - 0.05, magnitude of gradient - 3.1745003959952203\n",
      "Step - 883, Loss - 0.8011286108049335, Learning Rate - 0.05, magnitude of gradient - 2.3712246137036312\n",
      "Step - 884, Loss - 0.7865534207551357, Learning Rate - 0.05, magnitude of gradient - 2.1335402692731686\n",
      "Step - 885, Loss - 0.5529010820869493, Learning Rate - 0.05, magnitude of gradient - 1.4156656022838576\n",
      "Step - 886, Loss - 0.8658705058478329, Learning Rate - 0.05, magnitude of gradient - 3.150542294487045\n",
      "Step - 887, Loss - 0.736733997910426, Learning Rate - 0.05, magnitude of gradient - 2.183954038555134\n",
      "Step - 888, Loss - 0.8039412207872897, Learning Rate - 0.05, magnitude of gradient - 3.3394026724635997\n",
      "Step - 889, Loss - 0.699800641845383, Learning Rate - 0.05, magnitude of gradient - 1.9282685005754512\n",
      "Step - 890, Loss - 0.7231765009831506, Learning Rate - 0.05, magnitude of gradient - 2.331541136889195\n",
      "Step - 891, Loss - 0.8401917713764362, Learning Rate - 0.05, magnitude of gradient - 2.0987629929456446\n",
      "Step - 892, Loss - 0.7065052818793421, Learning Rate - 0.05, magnitude of gradient - 1.967987013252073\n",
      "Step - 893, Loss - 0.7238258531533152, Learning Rate - 0.05, magnitude of gradient - 1.4943192399687146\n",
      "Step - 894, Loss - 0.7489748387112979, Learning Rate - 0.05, magnitude of gradient - 2.698518676882823\n",
      "Step - 895, Loss - 0.8266441512236626, Learning Rate - 0.05, magnitude of gradient - 0.9536268855381553\n",
      "Step - 896, Loss - 0.5975461781043169, Learning Rate - 0.05, magnitude of gradient - 1.774016057012385\n",
      "Step - 897, Loss - 0.7350140071085474, Learning Rate - 0.05, magnitude of gradient - 1.3187954571162583\n",
      "Step - 898, Loss - 0.7956750205183734, Learning Rate - 0.05, magnitude of gradient - 3.6303075293956786\n",
      "Step - 899, Loss - 0.7075633744480669, Learning Rate - 0.05, magnitude of gradient - 0.6316645660981528\n",
      "Step - 900, Loss - 0.7041620434758674, Learning Rate - 0.05, magnitude of gradient - 3.1825134549797944\n",
      "Step - 901, Loss - 0.7881915530062662, Learning Rate - 0.05, magnitude of gradient - 1.2050361709313553\n",
      "Step - 902, Loss - 0.9824435263590013, Learning Rate - 0.05, magnitude of gradient - 0.8910978961015756\n",
      "Step - 903, Loss - 0.9003369482015694, Learning Rate - 0.05, magnitude of gradient - 3.260413464182305\n",
      "Step - 904, Loss - 0.6742942875918043, Learning Rate - 0.05, magnitude of gradient - 2.101468973824644\n",
      "Step - 905, Loss - 0.8110425924487028, Learning Rate - 0.05, magnitude of gradient - 2.41025463053419\n",
      "Step - 906, Loss - 0.8598136941371392, Learning Rate - 0.05, magnitude of gradient - 1.53154652180549\n",
      "Step - 907, Loss - 0.7114577459023645, Learning Rate - 0.05, magnitude of gradient - 2.7720052965758954\n",
      "Step - 908, Loss - 0.8717426912089236, Learning Rate - 0.05, magnitude of gradient - 2.467541184725936\n",
      "Step - 909, Loss - 0.6764588191929125, Learning Rate - 0.05, magnitude of gradient - 2.610055515640635\n",
      "Step - 910, Loss - 0.6491892772560146, Learning Rate - 0.05, magnitude of gradient - 2.411850159421672\n",
      "Step - 911, Loss - 0.661703635849498, Learning Rate - 0.05, magnitude of gradient - 3.085179319737726\n",
      "Step - 912, Loss - 0.8225004757430042, Learning Rate - 0.05, magnitude of gradient - 1.2349976979583595\n",
      "Step - 913, Loss - 0.7130932031374091, Learning Rate - 0.05, magnitude of gradient - 1.1599451345190477\n",
      "Step - 914, Loss - 0.7872375468281734, Learning Rate - 0.05, magnitude of gradient - 4.659694428246234\n",
      "Step - 915, Loss - 0.7315502146535517, Learning Rate - 0.05, magnitude of gradient - 3.3757475444936587\n",
      "Step - 916, Loss - 0.8091261755888959, Learning Rate - 0.05, magnitude of gradient - 1.6994257748556194\n",
      "Step - 917, Loss - 0.7787647974892093, Learning Rate - 0.05, magnitude of gradient - 1.6563360818719113\n",
      "Step - 918, Loss - 0.8380001389232086, Learning Rate - 0.05, magnitude of gradient - 0.5113799593716494\n",
      "Step - 919, Loss - 0.9398568081320388, Learning Rate - 0.05, magnitude of gradient - 6.476306368612226\n",
      "Step - 920, Loss - 0.8202029018099877, Learning Rate - 0.05, magnitude of gradient - 0.6619448823164226\n",
      "Step - 921, Loss - 0.5672008932923411, Learning Rate - 0.05, magnitude of gradient - 3.405763241048395\n",
      "Step - 922, Loss - 0.5684910633843425, Learning Rate - 0.05, magnitude of gradient - 2.292417157934903\n",
      "Step - 923, Loss - 0.8053600587028056, Learning Rate - 0.05, magnitude of gradient - 2.790879078849513\n",
      "Step - 924, Loss - 0.7140294952450249, Learning Rate - 0.05, magnitude of gradient - 1.6368446015312266\n",
      "Step - 925, Loss - 0.8400531199155798, Learning Rate - 0.05, magnitude of gradient - 2.88613231666189\n",
      "Step - 926, Loss - 0.7205164093255797, Learning Rate - 0.05, magnitude of gradient - 2.7978486486335004\n",
      "Step - 927, Loss - 0.6403160255806875, Learning Rate - 0.05, magnitude of gradient - 3.657422000273542\n",
      "Step - 928, Loss - 0.6707968237918588, Learning Rate - 0.05, magnitude of gradient - 1.2379066575383113\n",
      "Step - 929, Loss - 0.5287991966242463, Learning Rate - 0.05, magnitude of gradient - 1.8319743154381036\n",
      "Step - 930, Loss - 0.6442228368925411, Learning Rate - 0.05, magnitude of gradient - 1.273679614753417\n",
      "Step - 931, Loss - 0.5456111221395219, Learning Rate - 0.05, magnitude of gradient - 2.4036740111073067\n",
      "Step - 932, Loss - 0.7761524092808578, Learning Rate - 0.05, magnitude of gradient - 2.50514443807361\n",
      "Step - 933, Loss - 0.7387537416664802, Learning Rate - 0.05, magnitude of gradient - 1.5801210436049589\n",
      "Step - 934, Loss - 0.6990903938086163, Learning Rate - 0.05, magnitude of gradient - 4.191829164366831\n",
      "Step - 935, Loss - 0.6367307353198937, Learning Rate - 0.05, magnitude of gradient - 2.056471542437011\n",
      "Step - 936, Loss - 0.7333894703639642, Learning Rate - 0.05, magnitude of gradient - 2.6819683132355907\n",
      "Step - 937, Loss - 0.6331060435232614, Learning Rate - 0.05, magnitude of gradient - 0.3609668605684524\n",
      "Step - 938, Loss - 0.9198974638098893, Learning Rate - 0.05, magnitude of gradient - 2.767477618557559\n",
      "Step - 939, Loss - 0.9123690723747121, Learning Rate - 0.05, magnitude of gradient - 1.294228002314697\n",
      "Step - 940, Loss - 0.815109527291024, Learning Rate - 0.05, magnitude of gradient - 0.669247904193213\n",
      "Step - 941, Loss - 0.6752013115961816, Learning Rate - 0.05, magnitude of gradient - 3.6561616599202598\n",
      "Step - 942, Loss - 0.7125384722354835, Learning Rate - 0.05, magnitude of gradient - 3.1276694887920886\n",
      "Step - 943, Loss - 0.7875378628915568, Learning Rate - 0.05, magnitude of gradient - 1.1268149654697994\n",
      "Step - 944, Loss - 0.9109547746952986, Learning Rate - 0.05, magnitude of gradient - 2.649291279891694\n",
      "Step - 945, Loss - 0.61950804843807, Learning Rate - 0.05, magnitude of gradient - 1.366612844596702\n",
      "Step - 946, Loss - 0.7337998351443975, Learning Rate - 0.05, magnitude of gradient - 2.772824901135539\n",
      "Step - 947, Loss - 0.7668581248934644, Learning Rate - 0.05, magnitude of gradient - 2.9115562595722575\n",
      "Step - 948, Loss - 0.7004262728263806, Learning Rate - 0.05, magnitude of gradient - 2.4868596542416284\n",
      "Step - 949, Loss - 0.5267595931673905, Learning Rate - 0.05, magnitude of gradient - 1.5629748729228923\n",
      "Step - 950, Loss - 0.7288256689167487, Learning Rate - 0.05, magnitude of gradient - 2.0035953076552753\n",
      "Step - 951, Loss - 0.9925760387444855, Learning Rate - 0.05, magnitude of gradient - 1.4882375173178972\n",
      "Step - 952, Loss - 0.7197689505320631, Learning Rate - 0.05, magnitude of gradient - 1.5075051267897646\n",
      "Step - 953, Loss - 0.7397446960386782, Learning Rate - 0.05, magnitude of gradient - 3.4070981015099315\n",
      "Step - 954, Loss - 0.9016191965841275, Learning Rate - 0.05, magnitude of gradient - 0.2624118239027356\n",
      "Step - 955, Loss - 0.6093589466014762, Learning Rate - 0.05, magnitude of gradient - 2.0853206288956203\n",
      "Step - 956, Loss - 0.5907825189725382, Learning Rate - 0.05, magnitude of gradient - 0.9919715770433307\n",
      "Step - 957, Loss - 1.0078849138100412, Learning Rate - 0.05, magnitude of gradient - 1.4740658951856667\n",
      "Step - 958, Loss - 0.8232368340579338, Learning Rate - 0.05, magnitude of gradient - 4.212575837022936\n",
      "Step - 959, Loss - 0.5416921224030172, Learning Rate - 0.05, magnitude of gradient - 1.675579682437089\n",
      "Step - 960, Loss - 0.5916375093183734, Learning Rate - 0.05, magnitude of gradient - 4.307598939849054\n",
      "Step - 961, Loss - 0.6226083769692983, Learning Rate - 0.05, magnitude of gradient - 0.7327805490408974\n",
      "Step - 962, Loss - 0.7730734100265284, Learning Rate - 0.05, magnitude of gradient - 0.5672392067923597\n",
      "Step - 963, Loss - 0.6703736267048325, Learning Rate - 0.05, magnitude of gradient - 3.792137512012987\n",
      "Step - 964, Loss - 0.5806745845105177, Learning Rate - 0.05, magnitude of gradient - 0.8793347303671246\n",
      "Step - 965, Loss - 0.8191531365437669, Learning Rate - 0.05, magnitude of gradient - 2.8282485505225283\n",
      "Step - 966, Loss - 0.5868329548951894, Learning Rate - 0.05, magnitude of gradient - 1.4945717646737993\n",
      "Step - 967, Loss - 0.7050410593603156, Learning Rate - 0.05, magnitude of gradient - 4.328252836740917\n",
      "Step - 968, Loss - 0.7910115708941605, Learning Rate - 0.05, magnitude of gradient - 2.2166674113596594\n",
      "Step - 969, Loss - 0.7945958764777928, Learning Rate - 0.05, magnitude of gradient - 0.34630108715258495\n",
      "Step - 970, Loss - 0.6745921353716422, Learning Rate - 0.05, magnitude of gradient - 5.641765922667124\n",
      "Step - 971, Loss - 0.6339986056008757, Learning Rate - 0.05, magnitude of gradient - 1.1248201827042206\n",
      "Step - 972, Loss - 0.7990058824727428, Learning Rate - 0.05, magnitude of gradient - 2.6817593736592635\n",
      "Step - 973, Loss - 0.5205125865837698, Learning Rate - 0.05, magnitude of gradient - 1.4327159770751652\n",
      "Step - 974, Loss - 0.7453217431320883, Learning Rate - 0.05, magnitude of gradient - 2.97772200845942\n",
      "Step - 975, Loss - 0.835960210257973, Learning Rate - 0.05, magnitude of gradient - 0.8859761019867904\n",
      "Step - 976, Loss - 0.7440477566825524, Learning Rate - 0.05, magnitude of gradient - 2.7680524282532497\n",
      "Step - 977, Loss - 0.6564924470974391, Learning Rate - 0.05, magnitude of gradient - 1.9737945833977968\n",
      "Step - 978, Loss - 0.8966222733522081, Learning Rate - 0.05, magnitude of gradient - 1.721962813264552\n",
      "Step - 979, Loss - 0.7807170298409362, Learning Rate - 0.05, magnitude of gradient - 0.8989808127778239\n",
      "Step - 980, Loss - 0.7345814160335116, Learning Rate - 0.05, magnitude of gradient - 1.4743404153992763\n",
      "Step - 981, Loss - 0.7630757641501762, Learning Rate - 0.05, magnitude of gradient - 1.833410913119345\n",
      "Step - 982, Loss - 0.9135245039967582, Learning Rate - 0.05, magnitude of gradient - 3.3270502762266094\n",
      "Step - 983, Loss - 0.6268888828297385, Learning Rate - 0.05, magnitude of gradient - 1.843458933694875\n",
      "Step - 984, Loss - 0.9191886947548058, Learning Rate - 0.05, magnitude of gradient - 1.4574204301004565\n",
      "Step - 985, Loss - 0.7425345733623672, Learning Rate - 0.05, magnitude of gradient - 1.6275550065873168\n",
      "Step - 986, Loss - 0.6660036402559385, Learning Rate - 0.05, magnitude of gradient - 1.5732054541852298\n",
      "Step - 987, Loss - 0.7157356100172043, Learning Rate - 0.05, magnitude of gradient - 2.550512829601777\n",
      "Step - 988, Loss - 0.9596033924043719, Learning Rate - 0.05, magnitude of gradient - 1.1023588966440552\n",
      "Step - 989, Loss - 0.841860477491349, Learning Rate - 0.05, magnitude of gradient - 2.6327878879655815\n",
      "Step - 990, Loss - 0.6956215231362629, Learning Rate - 0.05, magnitude of gradient - 2.663750874036175\n",
      "Step - 991, Loss - 0.6914424231345389, Learning Rate - 0.05, magnitude of gradient - 1.9663591744564268\n",
      "Step - 992, Loss - 0.6846721698497333, Learning Rate - 0.05, magnitude of gradient - 2.458016894087607\n",
      "Step - 993, Loss - 0.8021577930902183, Learning Rate - 0.05, magnitude of gradient - 2.186385654880723\n",
      "Step - 994, Loss - 0.7834835607958687, Learning Rate - 0.05, magnitude of gradient - 3.0010614753704834\n",
      "Step - 995, Loss - 0.6965525933439263, Learning Rate - 0.05, magnitude of gradient - 1.5287214487853342\n",
      "Step - 996, Loss - 0.6711087113736083, Learning Rate - 0.05, magnitude of gradient - 0.8016750051045195\n",
      "Step - 997, Loss - 1.0676554987827527, Learning Rate - 0.05, magnitude of gradient - 4.850265514747159\n",
      "Step - 998, Loss - 0.7695107472794569, Learning Rate - 0.05, magnitude of gradient - 1.7772343525929675\n",
      "Step - 999, Loss - 0.7070680507863123, Learning Rate - 0.05, magnitude of gradient - 3.161277006519816\n",
      "Step - 1000, Loss - 0.786769591828792, Learning Rate - 0.05, magnitude of gradient - 2.330008880159992\n",
      "Step - 1001, Loss - 0.7016738056126088, Learning Rate - 0.025, magnitude of gradient - 2.765894241032499\n",
      "Step - 1002, Loss - 0.6742086123095425, Learning Rate - 0.025, magnitude of gradient - 1.3627874374529854\n",
      "Step - 1003, Loss - 0.837121486888765, Learning Rate - 0.025, magnitude of gradient - 2.519785232461939\n",
      "Step - 1004, Loss - 0.6541931776015373, Learning Rate - 0.025, magnitude of gradient - 1.896114142439455\n",
      "Step - 1005, Loss - 0.5686863537127337, Learning Rate - 0.025, magnitude of gradient - 0.8731296287718203\n",
      "Step - 1006, Loss - 0.8388029492967176, Learning Rate - 0.025, magnitude of gradient - 1.5558987339005497\n",
      "Step - 1007, Loss - 1.0484554126362393, Learning Rate - 0.025, magnitude of gradient - 1.4389322460102503\n",
      "Step - 1008, Loss - 0.4891175906176619, Learning Rate - 0.025, magnitude of gradient - 2.2319692555116517\n",
      "Step - 1009, Loss - 0.7406449002430633, Learning Rate - 0.025, magnitude of gradient - 0.822831534743209\n",
      "Step - 1010, Loss - 0.5210575473339549, Learning Rate - 0.025, magnitude of gradient - 1.6182450919693745\n",
      "Step - 1011, Loss - 0.6220760075807235, Learning Rate - 0.025, magnitude of gradient - 1.0552701392213435\n",
      "Step - 1012, Loss - 0.6407694442442246, Learning Rate - 0.025, magnitude of gradient - 2.7254658599469543\n",
      "Step - 1013, Loss - 0.5895689955844781, Learning Rate - 0.025, magnitude of gradient - 1.6628927585847768\n",
      "Step - 1014, Loss - 0.6272313370399817, Learning Rate - 0.025, magnitude of gradient - 1.3883320580322225\n",
      "Step - 1015, Loss - 0.7212736336366891, Learning Rate - 0.025, magnitude of gradient - 1.5144225710812433\n",
      "Step - 1016, Loss - 0.6716442445109663, Learning Rate - 0.025, magnitude of gradient - 1.5464122441906696\n",
      "Step - 1017, Loss - 0.7536407407756199, Learning Rate - 0.025, magnitude of gradient - 1.3961405439730155\n",
      "Step - 1018, Loss - 0.6871214607861462, Learning Rate - 0.025, magnitude of gradient - 1.0259672446824977\n",
      "Step - 1019, Loss - 0.7370703407404092, Learning Rate - 0.025, magnitude of gradient - 1.9741482230848908\n",
      "Step - 1020, Loss - 0.5487738524458722, Learning Rate - 0.025, magnitude of gradient - 1.6304148964727776\n",
      "Step - 1021, Loss - 0.7988730228828507, Learning Rate - 0.025, magnitude of gradient - 1.7753888589731133\n",
      "Step - 1022, Loss - 0.6642527209592234, Learning Rate - 0.025, magnitude of gradient - 0.3151686491046881\n",
      "Step - 1023, Loss - 0.6915109981682936, Learning Rate - 0.025, magnitude of gradient - 1.4887664211306733\n",
      "Step - 1024, Loss - 0.7119212893596685, Learning Rate - 0.025, magnitude of gradient - 1.0498858112845781\n",
      "Step - 1025, Loss - 0.6449317150708997, Learning Rate - 0.025, magnitude of gradient - 2.548792899096667\n",
      "Step - 1026, Loss - 0.779516199043966, Learning Rate - 0.025, magnitude of gradient - 0.6907067033274187\n",
      "Step - 1027, Loss - 0.9071976137836543, Learning Rate - 0.025, magnitude of gradient - 1.5460496972456108\n",
      "Step - 1028, Loss - 0.5303001269517995, Learning Rate - 0.025, magnitude of gradient - 0.7125579842546932\n",
      "Step - 1029, Loss - 0.6215299440629671, Learning Rate - 0.025, magnitude of gradient - 4.283514588083299\n",
      "Step - 1030, Loss - 0.7113748070669775, Learning Rate - 0.025, magnitude of gradient - 0.5904360407819172\n",
      "Step - 1031, Loss - 0.6878728956733076, Learning Rate - 0.025, magnitude of gradient - 1.2859172365682532\n",
      "Step - 1032, Loss - 0.7408798583955964, Learning Rate - 0.025, magnitude of gradient - 1.5961670342592729\n",
      "Step - 1033, Loss - 0.8526512024076087, Learning Rate - 0.025, magnitude of gradient - 3.10078865254746\n",
      "Step - 1034, Loss - 0.6899615830897878, Learning Rate - 0.025, magnitude of gradient - 1.3048121005661613\n",
      "Step - 1035, Loss - 0.6547669601048982, Learning Rate - 0.025, magnitude of gradient - 2.210025861625458\n",
      "Step - 1036, Loss - 0.981426205505689, Learning Rate - 0.025, magnitude of gradient - 1.5267427201056119\n",
      "Step - 1037, Loss - 0.7023898756066123, Learning Rate - 0.025, magnitude of gradient - 1.8090665158559636\n",
      "Step - 1038, Loss - 0.7431534264278575, Learning Rate - 0.025, magnitude of gradient - 1.5374223504296023\n",
      "Step - 1039, Loss - 0.6729674293983442, Learning Rate - 0.025, magnitude of gradient - 3.2425727346061906\n",
      "Step - 1040, Loss - 0.45937292403029906, Learning Rate - 0.025, magnitude of gradient - 0.8884617055369376\n",
      "Step - 1041, Loss - 0.7965787825426601, Learning Rate - 0.025, magnitude of gradient - 2.4450364532042888\n",
      "Step - 1042, Loss - 0.7846173596407439, Learning Rate - 0.025, magnitude of gradient - 2.4018688170259246\n",
      "Step - 1043, Loss - 0.7340759500350854, Learning Rate - 0.025, magnitude of gradient - 1.7846803845917527\n",
      "Step - 1044, Loss - 0.97883548567988, Learning Rate - 0.025, magnitude of gradient - 1.3656982221331437\n",
      "Step - 1045, Loss - 0.5757656252664991, Learning Rate - 0.025, magnitude of gradient - 1.0283573441860183\n",
      "Step - 1046, Loss - 0.9064693530380388, Learning Rate - 0.025, magnitude of gradient - 3.0185895270121996\n",
      "Step - 1047, Loss - 0.7187614709466685, Learning Rate - 0.025, magnitude of gradient - 1.6690007121530026\n",
      "Step - 1048, Loss - 0.8405971333862516, Learning Rate - 0.025, magnitude of gradient - 3.6523207746153217\n",
      "Step - 1049, Loss - 0.6239393602791719, Learning Rate - 0.025, magnitude of gradient - 2.2599446460245485\n",
      "Step - 1050, Loss - 0.5518341312471825, Learning Rate - 0.025, magnitude of gradient - 0.737773070741755\n",
      "Step - 1051, Loss - 0.5627266513165354, Learning Rate - 0.025, magnitude of gradient - 0.6514220106225553\n",
      "Step - 1052, Loss - 0.7202973431943734, Learning Rate - 0.025, magnitude of gradient - 1.2585317020759579\n",
      "Step - 1053, Loss - 0.7034221902635376, Learning Rate - 0.025, magnitude of gradient - 1.5664749318227955\n",
      "Step - 1054, Loss - 0.8191220275326296, Learning Rate - 0.025, magnitude of gradient - 1.8066588441658809\n",
      "Step - 1055, Loss - 0.4762508944778829, Learning Rate - 0.025, magnitude of gradient - 1.1963825839435456\n",
      "Step - 1056, Loss - 0.7499378669684245, Learning Rate - 0.025, magnitude of gradient - 1.070815126274109\n",
      "Step - 1057, Loss - 0.7199593771336658, Learning Rate - 0.025, magnitude of gradient - 2.381209053021929\n",
      "Step - 1058, Loss - 0.46903879705956164, Learning Rate - 0.025, magnitude of gradient - 0.3824863111134392\n",
      "Step - 1059, Loss - 0.70290210642353, Learning Rate - 0.025, magnitude of gradient - 2.1957782335178817\n",
      "Step - 1060, Loss - 0.7272755245045575, Learning Rate - 0.025, magnitude of gradient - 2.0863416351709962\n",
      "Step - 1061, Loss - 0.5322956204525302, Learning Rate - 0.025, magnitude of gradient - 1.4037861732209176\n",
      "Step - 1062, Loss - 0.7279606492032646, Learning Rate - 0.025, magnitude of gradient - 2.7549253908672937\n",
      "Step - 1063, Loss - 0.8128556038736011, Learning Rate - 0.025, magnitude of gradient - 1.2376818033256478\n",
      "Step - 1064, Loss - 0.8349270310438209, Learning Rate - 0.025, magnitude of gradient - 4.779311830607727\n",
      "Step - 1065, Loss - 0.5741702986013462, Learning Rate - 0.025, magnitude of gradient - 1.0809950849302727\n",
      "Step - 1066, Loss - 0.6657435940688641, Learning Rate - 0.025, magnitude of gradient - 0.8120562816224405\n",
      "Step - 1067, Loss - 0.7220778122651044, Learning Rate - 0.025, magnitude of gradient - 2.1843542185387532\n",
      "Step - 1068, Loss - 0.6419955675131317, Learning Rate - 0.025, magnitude of gradient - 0.8242242199924914\n",
      "Step - 1069, Loss - 0.6063614259923258, Learning Rate - 0.025, magnitude of gradient - 1.7964890595675853\n",
      "Step - 1070, Loss - 0.6235279403153305, Learning Rate - 0.025, magnitude of gradient - 1.9982311332560245\n",
      "Step - 1071, Loss - 0.5655280513523466, Learning Rate - 0.025, magnitude of gradient - 0.9908742243666668\n",
      "Step - 1072, Loss - 0.5519665198612094, Learning Rate - 0.025, magnitude of gradient - 2.6961218460821454\n",
      "Step - 1073, Loss - 0.7411851452455916, Learning Rate - 0.025, magnitude of gradient - 2.2287183589789157\n",
      "Step - 1074, Loss - 0.6397799214011922, Learning Rate - 0.025, magnitude of gradient - 0.8854714568344892\n",
      "Step - 1075, Loss - 0.7663445393241817, Learning Rate - 0.025, magnitude of gradient - 1.280019319116691\n",
      "Step - 1076, Loss - 0.7149048876686928, Learning Rate - 0.025, magnitude of gradient - 0.8508519281011242\n",
      "Step - 1077, Loss - 0.7502936865453329, Learning Rate - 0.025, magnitude of gradient - 1.6938363343298173\n",
      "Step - 1078, Loss - 0.6174904088658514, Learning Rate - 0.025, magnitude of gradient - 0.5376457737630881\n",
      "Step - 1079, Loss - 0.6838941369129173, Learning Rate - 0.025, magnitude of gradient - 1.0990498901606343\n",
      "Step - 1080, Loss - 0.7420671543047351, Learning Rate - 0.025, magnitude of gradient - 1.5472931597747264\n",
      "Step - 1081, Loss - 0.7621664314514323, Learning Rate - 0.025, magnitude of gradient - 0.9308376693075258\n",
      "Step - 1082, Loss - 0.7364469201922056, Learning Rate - 0.025, magnitude of gradient - 0.7960362730217742\n",
      "Step - 1083, Loss - 0.7794807359665376, Learning Rate - 0.025, magnitude of gradient - 1.7088845610187864\n",
      "Step - 1084, Loss - 0.793050439398681, Learning Rate - 0.025, magnitude of gradient - 1.2700843097410657\n",
      "Step - 1085, Loss - 0.6910498574864432, Learning Rate - 0.025, magnitude of gradient - 3.099202805641147\n",
      "Step - 1086, Loss - 0.6889563729890147, Learning Rate - 0.025, magnitude of gradient - 3.128634869201734\n",
      "Step - 1087, Loss - 0.793102829145713, Learning Rate - 0.025, magnitude of gradient - 1.8565151578042665\n",
      "Step - 1088, Loss - 0.7201663110155149, Learning Rate - 0.025, magnitude of gradient - 0.8496666546278313\n",
      "Step - 1089, Loss - 0.6909267695110946, Learning Rate - 0.025, magnitude of gradient - 0.9509508459394513\n",
      "Step - 1090, Loss - 0.8539148694781733, Learning Rate - 0.025, magnitude of gradient - 1.3993370990361618\n",
      "Step - 1091, Loss - 0.5649835922161175, Learning Rate - 0.025, magnitude of gradient - 1.3077378529408321\n",
      "Step - 1092, Loss - 0.8214882318518161, Learning Rate - 0.025, magnitude of gradient - 3.12006272093025\n",
      "Step - 1093, Loss - 0.6732169739570097, Learning Rate - 0.025, magnitude of gradient - 2.243121640784182\n",
      "Step - 1094, Loss - 0.6847773689898145, Learning Rate - 0.025, magnitude of gradient - 2.7613001156572548\n",
      "Step - 1095, Loss - 0.6752643401496622, Learning Rate - 0.025, magnitude of gradient - 1.9933308122042235\n",
      "Step - 1096, Loss - 0.7434022347569749, Learning Rate - 0.025, magnitude of gradient - 1.7801520115374811\n",
      "Step - 1097, Loss - 1.0527004446781791, Learning Rate - 0.025, magnitude of gradient - 2.524272821405894\n",
      "Step - 1098, Loss - 0.7832882388773028, Learning Rate - 0.025, magnitude of gradient - 1.2077101065847864\n",
      "Step - 1099, Loss - 0.7462124313982481, Learning Rate - 0.025, magnitude of gradient - 3.1096725569678605\n",
      "Step - 1100, Loss - 0.7111879003388341, Learning Rate - 0.025, magnitude of gradient - 2.6177359039981907\n",
      "Step - 1101, Loss - 0.8290555860563569, Learning Rate - 0.025, magnitude of gradient - 2.6616171990911317\n",
      "Step - 1102, Loss - 0.636142191122486, Learning Rate - 0.025, magnitude of gradient - 1.4681169939723169\n",
      "Step - 1103, Loss - 0.7138537127339423, Learning Rate - 0.025, magnitude of gradient - 2.083615275428273\n",
      "Step - 1104, Loss - 0.5939386185896348, Learning Rate - 0.025, magnitude of gradient - 0.7782635365432677\n",
      "Step - 1105, Loss - 0.652646502247501, Learning Rate - 0.025, magnitude of gradient - 3.016316839378243\n",
      "Step - 1106, Loss - 0.8191530684215815, Learning Rate - 0.025, magnitude of gradient - 0.8128144909508216\n",
      "Step - 1107, Loss - 0.8395818911273307, Learning Rate - 0.025, magnitude of gradient - 3.4026969869063013\n",
      "Step - 1108, Loss - 0.7166785038558995, Learning Rate - 0.025, magnitude of gradient - 0.3369102954054502\n",
      "Step - 1109, Loss - 0.5940118980389258, Learning Rate - 0.025, magnitude of gradient - 1.2317827146270237\n",
      "Step - 1110, Loss - 0.8205360047317599, Learning Rate - 0.025, magnitude of gradient - 2.4187717326348577\n",
      "Step - 1111, Loss - 0.6914819312652501, Learning Rate - 0.025, magnitude of gradient - 2.447217306852303\n",
      "Step - 1112, Loss - 0.8390155809860723, Learning Rate - 0.025, magnitude of gradient - 1.786716721626727\n",
      "Step - 1113, Loss - 0.9672398854497979, Learning Rate - 0.025, magnitude of gradient - 2.844492678611437\n",
      "Step - 1114, Loss - 0.8038018172478933, Learning Rate - 0.025, magnitude of gradient - 0.8320889264011591\n",
      "Step - 1115, Loss - 0.6930771804985185, Learning Rate - 0.025, magnitude of gradient - 1.249243742455941\n",
      "Step - 1116, Loss - 0.7448119838685912, Learning Rate - 0.025, magnitude of gradient - 2.297711160411277\n",
      "Step - 1117, Loss - 0.7715026729010516, Learning Rate - 0.025, magnitude of gradient - 2.096982291273186\n",
      "Step - 1118, Loss - 0.7126535728758858, Learning Rate - 0.025, magnitude of gradient - 1.6868082364175654\n",
      "Step - 1119, Loss - 0.9973854780249263, Learning Rate - 0.025, magnitude of gradient - 2.4679982899216775\n",
      "Step - 1120, Loss - 0.7148535212718508, Learning Rate - 0.025, magnitude of gradient - 3.1190013508766103\n",
      "Step - 1121, Loss - 0.6695110360403075, Learning Rate - 0.025, magnitude of gradient - 1.1778738847342831\n",
      "Step - 1122, Loss - 0.8614883770646238, Learning Rate - 0.025, magnitude of gradient - 2.532418615874156\n",
      "Step - 1123, Loss - 0.5504180657438272, Learning Rate - 0.025, magnitude of gradient - 2.044844565544887\n",
      "Step - 1124, Loss - 0.6979575212136484, Learning Rate - 0.025, magnitude of gradient - 2.817221312023443\n",
      "Step - 1125, Loss - 0.5547104135682696, Learning Rate - 0.025, magnitude of gradient - 1.765904072614765\n",
      "Step - 1126, Loss - 0.6990905370240004, Learning Rate - 0.025, magnitude of gradient - 1.7967782854000292\n",
      "Step - 1127, Loss - 0.6058394094119842, Learning Rate - 0.025, magnitude of gradient - 1.4789967391362728\n",
      "Step - 1128, Loss - 0.8024587478640436, Learning Rate - 0.025, magnitude of gradient - 0.9899470867771658\n",
      "Step - 1129, Loss - 0.7904439818171072, Learning Rate - 0.025, magnitude of gradient - 2.885687096771855\n",
      "Step - 1130, Loss - 0.7309176794227906, Learning Rate - 0.025, magnitude of gradient - 0.9951664672055099\n",
      "Step - 1131, Loss - 0.7893667213905555, Learning Rate - 0.025, magnitude of gradient - 0.7476663012878841\n",
      "Step - 1132, Loss - 0.8917610797682032, Learning Rate - 0.025, magnitude of gradient - 1.4235981344451283\n",
      "Step - 1133, Loss - 0.7210862304383467, Learning Rate - 0.025, magnitude of gradient - 1.0811136584183603\n",
      "Step - 1134, Loss - 0.8341482301798374, Learning Rate - 0.025, magnitude of gradient - 1.480932009722589\n",
      "Step - 1135, Loss - 0.6942775223948501, Learning Rate - 0.025, magnitude of gradient - 0.9613176959310962\n",
      "Step - 1136, Loss - 0.863243699752044, Learning Rate - 0.025, magnitude of gradient - 0.8187022388582791\n",
      "Step - 1137, Loss - 0.70961150293983, Learning Rate - 0.025, magnitude of gradient - 1.9144645786612335\n",
      "Step - 1138, Loss - 0.8019058520291883, Learning Rate - 0.025, magnitude of gradient - 1.4785208887893926\n",
      "Step - 1139, Loss - 0.8372227125224148, Learning Rate - 0.025, magnitude of gradient - 3.048360967748203\n",
      "Step - 1140, Loss - 0.7919284871242999, Learning Rate - 0.025, magnitude of gradient - 1.0530030426097785\n",
      "Step - 1141, Loss - 0.6417164446996291, Learning Rate - 0.025, magnitude of gradient - 1.8514326099570586\n",
      "Step - 1142, Loss - 0.8027603401348444, Learning Rate - 0.025, magnitude of gradient - 2.299623153928435\n",
      "Step - 1143, Loss - 0.8030261498823352, Learning Rate - 0.025, magnitude of gradient - 0.715954354036829\n",
      "Step - 1144, Loss - 0.8361800760370348, Learning Rate - 0.025, magnitude of gradient - 2.3824180600319904\n",
      "Step - 1145, Loss - 0.6943409771074598, Learning Rate - 0.025, magnitude of gradient - 1.7309109383479322\n",
      "Step - 1146, Loss - 0.6870839788848522, Learning Rate - 0.025, magnitude of gradient - 0.6300035150916907\n",
      "Step - 1147, Loss - 0.7096035021480512, Learning Rate - 0.025, magnitude of gradient - 1.9154832774770105\n",
      "Step - 1148, Loss - 0.7823175681444425, Learning Rate - 0.025, magnitude of gradient - 1.3204550221486213\n",
      "Step - 1149, Loss - 0.7386470517823903, Learning Rate - 0.025, magnitude of gradient - 0.9919799328583686\n",
      "Step - 1150, Loss - 0.6020435681715939, Learning Rate - 0.025, magnitude of gradient - 4.025310494718538\n",
      "Step - 1151, Loss - 0.564717513460399, Learning Rate - 0.025, magnitude of gradient - 1.9680077997635987\n",
      "Step - 1152, Loss - 0.6388985643735015, Learning Rate - 0.025, magnitude of gradient - 1.3464396688903906\n",
      "Step - 1153, Loss - 0.7815280219075225, Learning Rate - 0.025, magnitude of gradient - 1.0945296371291664\n",
      "Step - 1154, Loss - 0.886912993592854, Learning Rate - 0.025, magnitude of gradient - 0.9803771040071969\n",
      "Step - 1155, Loss - 0.8144763514899792, Learning Rate - 0.025, magnitude of gradient - 2.085196340270365\n",
      "Step - 1156, Loss - 0.8116714268008054, Learning Rate - 0.025, magnitude of gradient - 1.3846071927891481\n",
      "Step - 1157, Loss - 0.470146321677151, Learning Rate - 0.025, magnitude of gradient - 1.867264970150998\n",
      "Step - 1158, Loss - 0.7293765683877921, Learning Rate - 0.025, magnitude of gradient - 0.8546736775979478\n",
      "Step - 1159, Loss - 0.5528735701616376, Learning Rate - 0.025, magnitude of gradient - 1.6988501572797488\n",
      "Step - 1160, Loss - 0.6539648315558692, Learning Rate - 0.025, magnitude of gradient - 2.003882758058462\n",
      "Step - 1161, Loss - 0.7965730376700036, Learning Rate - 0.025, magnitude of gradient - 1.0475686710288834\n",
      "Step - 1162, Loss - 0.8528284832801674, Learning Rate - 0.025, magnitude of gradient - 1.8618793965347462\n",
      "Step - 1163, Loss - 0.6251088093177426, Learning Rate - 0.025, magnitude of gradient - 2.268274330335469\n",
      "Step - 1164, Loss - 0.9164129310221735, Learning Rate - 0.025, magnitude of gradient - 1.9414727140218366\n",
      "Step - 1165, Loss - 0.7095132718096212, Learning Rate - 0.025, magnitude of gradient - 1.0385098204744512\n",
      "Step - 1166, Loss - 0.7207669003810513, Learning Rate - 0.025, magnitude of gradient - 0.7114206432711917\n",
      "Step - 1167, Loss - 0.668687701730672, Learning Rate - 0.025, magnitude of gradient - 0.9564633006976838\n",
      "Step - 1168, Loss - 0.6906577008190005, Learning Rate - 0.025, magnitude of gradient - 1.5069939006140673\n",
      "Step - 1169, Loss - 0.6959382786965881, Learning Rate - 0.025, magnitude of gradient - 2.5838666597255493\n",
      "Step - 1170, Loss - 0.6301080258719344, Learning Rate - 0.025, magnitude of gradient - 1.976234345066597\n",
      "Step - 1171, Loss - 0.771454525443376, Learning Rate - 0.025, magnitude of gradient - 3.4029788128438674\n",
      "Step - 1172, Loss - 0.6334648750185655, Learning Rate - 0.025, magnitude of gradient - 0.3975300118562725\n",
      "Step - 1173, Loss - 0.5749595231677331, Learning Rate - 0.025, magnitude of gradient - 2.456000195126348\n",
      "Step - 1174, Loss - 0.7297085431202841, Learning Rate - 0.025, magnitude of gradient - 2.164272260032439\n",
      "Step - 1175, Loss - 0.7527069866771969, Learning Rate - 0.025, magnitude of gradient - 3.881911568685211\n",
      "Step - 1176, Loss - 0.7167755149258973, Learning Rate - 0.025, magnitude of gradient - 1.691923292643621\n",
      "Step - 1177, Loss - 0.7431527885357394, Learning Rate - 0.025, magnitude of gradient - 0.3073367098750463\n",
      "Step - 1178, Loss - 0.8254964682853084, Learning Rate - 0.025, magnitude of gradient - 3.7053250935736526\n",
      "Step - 1179, Loss - 0.9124354529458135, Learning Rate - 0.025, magnitude of gradient - 2.170174295019928\n",
      "Step - 1180, Loss - 0.6345508581575814, Learning Rate - 0.025, magnitude of gradient - 1.443033118580321\n",
      "Step - 1181, Loss - 0.5334425458614241, Learning Rate - 0.025, magnitude of gradient - 2.2680979279550524\n",
      "Step - 1182, Loss - 0.6158893025029353, Learning Rate - 0.025, magnitude of gradient - 1.03861784993745\n",
      "Step - 1183, Loss - 0.8828924114473925, Learning Rate - 0.025, magnitude of gradient - 4.08764765701526\n",
      "Step - 1184, Loss - 0.555193882105711, Learning Rate - 0.025, magnitude of gradient - 0.8441328529257631\n",
      "Step - 1185, Loss - 0.8966653543187139, Learning Rate - 0.025, magnitude of gradient - 2.4755926424700676\n",
      "Step - 1186, Loss - 0.8554577952280386, Learning Rate - 0.025, magnitude of gradient - 2.1204057969226326\n",
      "Step - 1187, Loss - 0.7953491275158993, Learning Rate - 0.025, magnitude of gradient - 1.7086855297491836\n",
      "Step - 1188, Loss - 0.8528440281441098, Learning Rate - 0.025, magnitude of gradient - 1.812514224067999\n",
      "Step - 1189, Loss - 0.521273708388801, Learning Rate - 0.025, magnitude of gradient - 0.5260280733107291\n",
      "Step - 1190, Loss - 0.5788081438545261, Learning Rate - 0.025, magnitude of gradient - 1.9283467929010292\n",
      "Step - 1191, Loss - 0.7767765574871885, Learning Rate - 0.025, magnitude of gradient - 2.0134755449421253\n",
      "Step - 1192, Loss - 0.6713482778250704, Learning Rate - 0.025, magnitude of gradient - 1.2528411626324387\n",
      "Step - 1193, Loss - 0.7552446278064011, Learning Rate - 0.025, magnitude of gradient - 3.2975218589880324\n",
      "Step - 1194, Loss - 0.7461032615672757, Learning Rate - 0.025, magnitude of gradient - 0.5219630084139536\n",
      "Step - 1195, Loss - 0.8299437668156389, Learning Rate - 0.025, magnitude of gradient - 1.582832916578074\n",
      "Step - 1196, Loss - 0.7494954250785828, Learning Rate - 0.025, magnitude of gradient - 1.469462475309891\n",
      "Step - 1197, Loss - 0.532402479904093, Learning Rate - 0.025, magnitude of gradient - 1.6070576892395132\n",
      "Step - 1198, Loss - 0.7756737304619977, Learning Rate - 0.025, magnitude of gradient - 1.589905729288002\n",
      "Step - 1199, Loss - 0.6490049631412145, Learning Rate - 0.025, magnitude of gradient - 2.6602503596097598\n",
      "Step - 1200, Loss - 0.8285948375784811, Learning Rate - 0.025, magnitude of gradient - 2.443504355210753\n",
      "Step - 1201, Loss - 0.6881279117381712, Learning Rate - 0.025, magnitude of gradient - 1.5811475274702018\n",
      "Step - 1202, Loss - 0.7099355459177619, Learning Rate - 0.025, magnitude of gradient - 0.8401932035772716\n",
      "Step - 1203, Loss - 0.6593595703163556, Learning Rate - 0.025, magnitude of gradient - 1.6434957099242762\n",
      "Step - 1204, Loss - 0.8920173369254222, Learning Rate - 0.025, magnitude of gradient - 3.9864561743600846\n",
      "Step - 1205, Loss - 0.8064831825669817, Learning Rate - 0.025, magnitude of gradient - 1.187028966294968\n",
      "Step - 1206, Loss - 0.5673138710589246, Learning Rate - 0.025, magnitude of gradient - 0.5108959703230435\n",
      "Step - 1207, Loss - 0.6536248472580144, Learning Rate - 0.025, magnitude of gradient - 2.355889100370819\n",
      "Step - 1208, Loss - 0.6852319813372139, Learning Rate - 0.025, magnitude of gradient - 0.7601770386072687\n",
      "Step - 1209, Loss - 1.0138083455217468, Learning Rate - 0.025, magnitude of gradient - 3.4621190553213608\n",
      "Step - 1210, Loss - 0.68455695359497, Learning Rate - 0.025, magnitude of gradient - 2.686120963082448\n",
      "Step - 1211, Loss - 0.7653312951182131, Learning Rate - 0.025, magnitude of gradient - 1.721246749070727\n",
      "Step - 1212, Loss - 0.7387283489529992, Learning Rate - 0.025, magnitude of gradient - 1.613102661544376\n",
      "Step - 1213, Loss - 0.7531417404945144, Learning Rate - 0.025, magnitude of gradient - 0.9304922884369909\n",
      "Step - 1214, Loss - 1.0118750716128089, Learning Rate - 0.025, magnitude of gradient - 1.5198952771504406\n",
      "Step - 1215, Loss - 0.8551786612312648, Learning Rate - 0.025, magnitude of gradient - 1.443257400192566\n",
      "Step - 1216, Loss - 0.5729176992127873, Learning Rate - 0.025, magnitude of gradient - 1.7796950267791805\n",
      "Step - 1217, Loss - 0.5679156371406883, Learning Rate - 0.025, magnitude of gradient - 1.201490810727343\n",
      "Step - 1218, Loss - 0.7334408339756442, Learning Rate - 0.025, magnitude of gradient - 1.9789651580789052\n",
      "Step - 1219, Loss - 1.04541244125289, Learning Rate - 0.025, magnitude of gradient - 3.355248816321442\n",
      "Step - 1220, Loss - 0.6790551283554832, Learning Rate - 0.025, magnitude of gradient - 1.3498010103989828\n",
      "Step - 1221, Loss - 0.7753643498388181, Learning Rate - 0.025, magnitude of gradient - 0.7741595345411652\n",
      "Step - 1222, Loss - 0.6846725851976417, Learning Rate - 0.025, magnitude of gradient - 1.1746224325274588\n",
      "Step - 1223, Loss - 0.8709650532791529, Learning Rate - 0.025, magnitude of gradient - 0.7445796721334508\n",
      "Step - 1224, Loss - 0.685787471468968, Learning Rate - 0.025, magnitude of gradient - 1.8216022020329865\n",
      "Step - 1225, Loss - 0.7212142360199569, Learning Rate - 0.025, magnitude of gradient - 2.4985710904907004\n",
      "Step - 1226, Loss - 0.7400153342843039, Learning Rate - 0.025, magnitude of gradient - 2.400638972380399\n",
      "Step - 1227, Loss - 0.8843754239806965, Learning Rate - 0.025, magnitude of gradient - 2.8993036844543436\n",
      "Step - 1228, Loss - 0.6458204535607318, Learning Rate - 0.025, magnitude of gradient - 0.9604976023962266\n",
      "Step - 1229, Loss - 0.7037654646120759, Learning Rate - 0.025, magnitude of gradient - 1.9298585212555863\n",
      "Step - 1230, Loss - 0.7838837716771333, Learning Rate - 0.025, magnitude of gradient - 2.8220009774939436\n",
      "Step - 1231, Loss - 0.8888417736206177, Learning Rate - 0.025, magnitude of gradient - 1.4487693943256448\n",
      "Step - 1232, Loss - 0.6629323428924522, Learning Rate - 0.025, magnitude of gradient - 0.6621369557046114\n",
      "Step - 1233, Loss - 0.5624666076817251, Learning Rate - 0.025, magnitude of gradient - 1.6855237711349262\n",
      "Step - 1234, Loss - 0.691341144990185, Learning Rate - 0.025, magnitude of gradient - 1.261205683811101\n",
      "Step - 1235, Loss - 0.8036096979610905, Learning Rate - 0.025, magnitude of gradient - 2.054364488274792\n",
      "Step - 1236, Loss - 0.7819079308527269, Learning Rate - 0.025, magnitude of gradient - 0.7084959415105471\n",
      "Step - 1237, Loss - 0.5596486894243499, Learning Rate - 0.025, magnitude of gradient - 2.1614833277544174\n",
      "Step - 1238, Loss - 0.6640709703232646, Learning Rate - 0.025, magnitude of gradient - 1.0922405903117112\n",
      "Step - 1239, Loss - 0.5126432014846847, Learning Rate - 0.025, magnitude of gradient - 1.1253289166343914\n",
      "Step - 1240, Loss - 0.6465411334280879, Learning Rate - 0.025, magnitude of gradient - 1.705049428772661\n",
      "Step - 1241, Loss - 0.7829884711766424, Learning Rate - 0.025, magnitude of gradient - 2.4529405868552807\n",
      "Step - 1242, Loss - 0.6936175013048316, Learning Rate - 0.025, magnitude of gradient - 1.047486264304857\n",
      "Step - 1243, Loss - 0.7756930394625308, Learning Rate - 0.025, magnitude of gradient - 0.8685108238479291\n",
      "Step - 1244, Loss - 0.5810179909593316, Learning Rate - 0.025, magnitude of gradient - 0.4146681660594361\n",
      "Step - 1245, Loss - 0.799610819629978, Learning Rate - 0.025, magnitude of gradient - 1.3712230680015727\n",
      "Step - 1246, Loss - 0.575409907013049, Learning Rate - 0.025, magnitude of gradient - 1.4409805893434942\n",
      "Step - 1247, Loss - 0.7168027644932444, Learning Rate - 0.025, magnitude of gradient - 1.1275771874694729\n",
      "Step - 1248, Loss - 0.6594970006608721, Learning Rate - 0.025, magnitude of gradient - 1.3445656970535447\n",
      "Step - 1249, Loss - 0.7645636442410526, Learning Rate - 0.025, magnitude of gradient - 1.488757398924053\n",
      "Step - 1250, Loss - 0.6368708568207381, Learning Rate - 0.025, magnitude of gradient - 1.3748733492984753\n",
      "Step - 1251, Loss - 0.6747324454140715, Learning Rate - 0.025, magnitude of gradient - 1.765932822753819\n",
      "Step - 1252, Loss - 0.503993282179249, Learning Rate - 0.025, magnitude of gradient - 2.7819662388949102\n",
      "Step - 1253, Loss - 0.7165372480380137, Learning Rate - 0.025, magnitude of gradient - 0.6304153401703032\n",
      "Step - 1254, Loss - 0.8209975934494398, Learning Rate - 0.025, magnitude of gradient - 3.080138125396105\n",
      "Step - 1255, Loss - 0.6907934848912984, Learning Rate - 0.025, magnitude of gradient - 1.3561824616247067\n",
      "Step - 1256, Loss - 0.7264630061553896, Learning Rate - 0.025, magnitude of gradient - 3.040504174172825\n",
      "Step - 1257, Loss - 0.616100030884287, Learning Rate - 0.025, magnitude of gradient - 0.6726738736250554\n",
      "Step - 1258, Loss - 0.581780018616535, Learning Rate - 0.025, magnitude of gradient - 2.2078876848730418\n",
      "Step - 1259, Loss - 0.8644997413271974, Learning Rate - 0.025, magnitude of gradient - 0.7940431185276893\n",
      "Step - 1260, Loss - 0.7172079490183341, Learning Rate - 0.025, magnitude of gradient - 0.8501066559311006\n",
      "Step - 1261, Loss - 0.6479409875996597, Learning Rate - 0.025, magnitude of gradient - 2.860199128514929\n",
      "Step - 1262, Loss - 0.7547530701620859, Learning Rate - 0.025, magnitude of gradient - 1.0771107972616125\n",
      "Step - 1263, Loss - 0.6322858975488237, Learning Rate - 0.025, magnitude of gradient - 0.8736658976457097\n",
      "Step - 1264, Loss - 0.7077499048605606, Learning Rate - 0.025, magnitude of gradient - 0.8173662835887294\n",
      "Step - 1265, Loss - 0.6758102133787034, Learning Rate - 0.025, magnitude of gradient - 2.8407199075715335\n",
      "Step - 1266, Loss - 0.6940980330503501, Learning Rate - 0.025, magnitude of gradient - 1.2524966457024442\n",
      "Step - 1267, Loss - 0.7084026332577749, Learning Rate - 0.025, magnitude of gradient - 1.627097668138213\n",
      "Step - 1268, Loss - 0.6970059511689591, Learning Rate - 0.025, magnitude of gradient - 0.5847928114754815\n",
      "Step - 1269, Loss - 0.8466237301139603, Learning Rate - 0.025, magnitude of gradient - 2.867036280170403\n",
      "Step - 1270, Loss - 0.7068068959504837, Learning Rate - 0.025, magnitude of gradient - 0.7767192723107658\n",
      "Step - 1271, Loss - 0.7298734538143401, Learning Rate - 0.025, magnitude of gradient - 1.490785881374414\n",
      "Step - 1272, Loss - 0.7100389663674728, Learning Rate - 0.025, magnitude of gradient - 2.5897476759626405\n",
      "Step - 1273, Loss - 0.705573794762297, Learning Rate - 0.025, magnitude of gradient - 0.45748035519674785\n",
      "Step - 1274, Loss - 0.6670318840825697, Learning Rate - 0.025, magnitude of gradient - 3.4159590509790663\n",
      "Step - 1275, Loss - 0.730479634113716, Learning Rate - 0.025, magnitude of gradient - 0.6076990341518231\n",
      "Step - 1276, Loss - 0.7638959614042967, Learning Rate - 0.025, magnitude of gradient - 1.9462558338845117\n",
      "Step - 1277, Loss - 0.5494976146579226, Learning Rate - 0.025, magnitude of gradient - 0.7239129998296885\n",
      "Step - 1278, Loss - 0.6341767878421631, Learning Rate - 0.025, magnitude of gradient - 1.5195144575259403\n",
      "Step - 1279, Loss - 0.8822809140623381, Learning Rate - 0.025, magnitude of gradient - 2.7005762964486153\n",
      "Step - 1280, Loss - 0.695018001036487, Learning Rate - 0.025, magnitude of gradient - 0.5231504830096327\n",
      "Step - 1281, Loss - 0.6769640307428677, Learning Rate - 0.025, magnitude of gradient - 1.647398412959912\n",
      "Step - 1282, Loss - 0.6903605700261228, Learning Rate - 0.025, magnitude of gradient - 1.491645154367285\n",
      "Step - 1283, Loss - 0.8551607867511775, Learning Rate - 0.025, magnitude of gradient - 1.816835929194802\n",
      "Step - 1284, Loss - 0.728684006882584, Learning Rate - 0.025, magnitude of gradient - 1.725345577199652\n",
      "Step - 1285, Loss - 0.7086270689988023, Learning Rate - 0.025, magnitude of gradient - 0.45307832478560817\n",
      "Step - 1286, Loss - 0.6428920028682538, Learning Rate - 0.025, magnitude of gradient - 2.1596492967095835\n",
      "Step - 1287, Loss - 0.8779029586571645, Learning Rate - 0.025, magnitude of gradient - 1.9960628808871304\n",
      "Step - 1288, Loss - 0.7742783537494192, Learning Rate - 0.025, magnitude of gradient - 1.4376558557351191\n",
      "Step - 1289, Loss - 0.8325597203020758, Learning Rate - 0.025, magnitude of gradient - 1.3170886761442777\n",
      "Step - 1290, Loss - 0.6817154834295549, Learning Rate - 0.025, magnitude of gradient - 2.0783548907819362\n",
      "Step - 1291, Loss - 0.676668302116326, Learning Rate - 0.025, magnitude of gradient - 0.9124271741311988\n",
      "Step - 1292, Loss - 0.6754662282956381, Learning Rate - 0.025, magnitude of gradient - 0.9978717338981417\n",
      "Step - 1293, Loss - 0.5844408286101271, Learning Rate - 0.025, magnitude of gradient - 1.1126092447766749\n",
      "Step - 1294, Loss - 0.834640592870751, Learning Rate - 0.025, magnitude of gradient - 2.375744690372473\n",
      "Step - 1295, Loss - 0.5915719538014153, Learning Rate - 0.025, magnitude of gradient - 1.492079678172694\n",
      "Step - 1296, Loss - 0.7065706789311603, Learning Rate - 0.025, magnitude of gradient - 2.795644256820098\n",
      "Step - 1297, Loss - 0.5502111922902188, Learning Rate - 0.025, magnitude of gradient - 0.4220958705076681\n",
      "Step - 1298, Loss - 0.8894245842559981, Learning Rate - 0.025, magnitude of gradient - 1.2063607716943414\n",
      "Step - 1299, Loss - 0.5360510818439204, Learning Rate - 0.025, magnitude of gradient - 1.7608719556154462\n",
      "Step - 1300, Loss - 0.7226660237616163, Learning Rate - 0.025, magnitude of gradient - 2.452604337595817\n",
      "Step - 1301, Loss - 0.532870209055274, Learning Rate - 0.025, magnitude of gradient - 2.2908248901733197\n",
      "Step - 1302, Loss - 0.7266098607855889, Learning Rate - 0.025, magnitude of gradient - 0.8254599667748101\n",
      "Step - 1303, Loss - 0.6006756018896304, Learning Rate - 0.025, magnitude of gradient - 2.316390575708064\n",
      "Step - 1304, Loss - 0.6666014323614443, Learning Rate - 0.025, magnitude of gradient - 0.6831169548330769\n",
      "Step - 1305, Loss - 0.8123930273441773, Learning Rate - 0.025, magnitude of gradient - 2.2388768972226853\n",
      "Step - 1306, Loss - 0.7520170892966117, Learning Rate - 0.025, magnitude of gradient - 1.2335500265651183\n",
      "Step - 1307, Loss - 0.6744759372503232, Learning Rate - 0.025, magnitude of gradient - 1.82521712803391\n",
      "Step - 1308, Loss - 0.9180351279268768, Learning Rate - 0.025, magnitude of gradient - 1.5887391710284278\n",
      "Step - 1309, Loss - 0.9055530060970655, Learning Rate - 0.025, magnitude of gradient - 0.6403230058527927\n",
      "Step - 1310, Loss - 0.6197633166072287, Learning Rate - 0.025, magnitude of gradient - 0.7714609637814566\n",
      "Step - 1311, Loss - 0.654348800930483, Learning Rate - 0.025, magnitude of gradient - 0.4590398248515417\n",
      "Step - 1312, Loss - 0.6411796893310988, Learning Rate - 0.025, magnitude of gradient - 2.0052712433522615\n",
      "Step - 1313, Loss - 0.7504229827263873, Learning Rate - 0.025, magnitude of gradient - 2.621505407893263\n",
      "Step - 1314, Loss - 0.7253076614301622, Learning Rate - 0.025, magnitude of gradient - 1.0209353408729462\n",
      "Step - 1315, Loss - 0.46066374413426764, Learning Rate - 0.025, magnitude of gradient - 2.856028443651386\n",
      "Step - 1316, Loss - 0.5487965965324634, Learning Rate - 0.025, magnitude of gradient - 2.266240845359466\n",
      "Step - 1317, Loss - 0.8934585060112568, Learning Rate - 0.025, magnitude of gradient - 1.619010132315838\n",
      "Step - 1318, Loss - 0.5474903551375802, Learning Rate - 0.025, magnitude of gradient - 1.12357524235049\n",
      "Step - 1319, Loss - 0.6841897755237951, Learning Rate - 0.025, magnitude of gradient - 2.317001601527273\n",
      "Step - 1320, Loss - 0.6481487617221363, Learning Rate - 0.025, magnitude of gradient - 2.271276594177183\n",
      "Step - 1321, Loss - 0.6477879911230522, Learning Rate - 0.025, magnitude of gradient - 2.9255062985950664\n",
      "Step - 1322, Loss - 0.5306963807955575, Learning Rate - 0.025, magnitude of gradient - 1.0212205588193652\n",
      "Step - 1323, Loss - 0.7163190787444242, Learning Rate - 0.025, magnitude of gradient - 2.1406953237738717\n",
      "Step - 1324, Loss - 0.5729331011181122, Learning Rate - 0.025, magnitude of gradient - 0.6768121762009941\n",
      "Step - 1325, Loss - 0.7165878251452957, Learning Rate - 0.025, magnitude of gradient - 3.252884157676555\n",
      "Step - 1326, Loss - 0.7612015236249714, Learning Rate - 0.025, magnitude of gradient - 0.6373505909472711\n",
      "Step - 1327, Loss - 0.9590179757273325, Learning Rate - 0.025, magnitude of gradient - 0.6777113178282711\n",
      "Step - 1328, Loss - 0.8062990576256674, Learning Rate - 0.025, magnitude of gradient - 0.7133377654298159\n",
      "Step - 1329, Loss - 0.9576539674222597, Learning Rate - 0.025, magnitude of gradient - 3.506038120894039\n",
      "Step - 1330, Loss - 0.8063418138944567, Learning Rate - 0.025, magnitude of gradient - 1.2224145056849485\n",
      "Step - 1331, Loss - 0.8816454849637506, Learning Rate - 0.025, magnitude of gradient - 1.0775085826701822\n",
      "Step - 1332, Loss - 0.7048470033228125, Learning Rate - 0.025, magnitude of gradient - 1.4881364533428711\n",
      "Step - 1333, Loss - 0.7889780213825541, Learning Rate - 0.025, magnitude of gradient - 1.354864327802754\n",
      "Step - 1334, Loss - 0.8764013188624851, Learning Rate - 0.025, magnitude of gradient - 1.0286579656112187\n",
      "Step - 1335, Loss - 0.7917540769126542, Learning Rate - 0.025, magnitude of gradient - 2.075269782618125\n",
      "Step - 1336, Loss - 0.5563400051277383, Learning Rate - 0.025, magnitude of gradient - 1.4501378956704913\n",
      "Step - 1337, Loss - 0.5621587051788236, Learning Rate - 0.025, magnitude of gradient - 1.6111056165549285\n",
      "Step - 1338, Loss - 0.7312722290817288, Learning Rate - 0.025, magnitude of gradient - 1.6234027144083423\n",
      "Step - 1339, Loss - 0.7195357676528946, Learning Rate - 0.025, magnitude of gradient - 0.8574326740617129\n",
      "Step - 1340, Loss - 0.6678830547029766, Learning Rate - 0.025, magnitude of gradient - 3.039515127760914\n",
      "Step - 1341, Loss - 0.6348081209757762, Learning Rate - 0.025, magnitude of gradient - 2.5655354298944717\n",
      "Step - 1342, Loss - 0.7710512269770833, Learning Rate - 0.025, magnitude of gradient - 3.2348149000064614\n",
      "Step - 1343, Loss - 0.8212457020125178, Learning Rate - 0.025, magnitude of gradient - 1.867649768892534\n",
      "Step - 1344, Loss - 0.6861188525373538, Learning Rate - 0.025, magnitude of gradient - 1.2654793036560164\n",
      "Step - 1345, Loss - 0.5749370484070959, Learning Rate - 0.025, magnitude of gradient - 0.8720972631948383\n",
      "Step - 1346, Loss - 0.844546514232088, Learning Rate - 0.025, magnitude of gradient - 1.4294366270339651\n",
      "Step - 1347, Loss - 0.7017729792673327, Learning Rate - 0.025, magnitude of gradient - 2.7447348550657584\n",
      "Step - 1348, Loss - 0.6372760472331952, Learning Rate - 0.025, magnitude of gradient - 1.4209850145899803\n",
      "Step - 1349, Loss - 0.7061695920360981, Learning Rate - 0.025, magnitude of gradient - 1.7847710530533176\n",
      "Step - 1350, Loss - 0.9951740842121781, Learning Rate - 0.025, magnitude of gradient - 2.4222279061562553\n",
      "Step - 1351, Loss - 0.8510300981884238, Learning Rate - 0.025, magnitude of gradient - 1.6033181866545299\n",
      "Step - 1352, Loss - 0.5868985642859731, Learning Rate - 0.025, magnitude of gradient - 0.7858414867780429\n",
      "Step - 1353, Loss - 0.6584104079345874, Learning Rate - 0.025, magnitude of gradient - 1.2629185475539102\n",
      "Step - 1354, Loss - 0.7346685181744784, Learning Rate - 0.025, magnitude of gradient - 1.8452433501570338\n",
      "Step - 1355, Loss - 0.8859342751494304, Learning Rate - 0.025, magnitude of gradient - 3.319729081929976\n",
      "Step - 1356, Loss - 0.6756859952711498, Learning Rate - 0.025, magnitude of gradient - 3.028344475329723\n",
      "Step - 1357, Loss - 0.6512487710423551, Learning Rate - 0.025, magnitude of gradient - 1.7232256755222934\n",
      "Step - 1358, Loss - 0.7703314142714281, Learning Rate - 0.025, magnitude of gradient - 1.504097384479222\n",
      "Step - 1359, Loss - 0.6937404595428255, Learning Rate - 0.025, magnitude of gradient - 1.6197093666300286\n",
      "Step - 1360, Loss - 0.7249110521573273, Learning Rate - 0.025, magnitude of gradient - 1.8259317354641271\n",
      "Step - 1361, Loss - 0.7213862359522427, Learning Rate - 0.025, magnitude of gradient - 1.318809406184206\n",
      "Step - 1362, Loss - 0.7991840223410004, Learning Rate - 0.025, magnitude of gradient - 2.0777755648235128\n",
      "Step - 1363, Loss - 0.8424102894621238, Learning Rate - 0.025, magnitude of gradient - 2.0386992515700353\n",
      "Step - 1364, Loss - 0.696307181533619, Learning Rate - 0.025, magnitude of gradient - 0.8064784197485028\n",
      "Step - 1365, Loss - 0.6279411832507243, Learning Rate - 0.025, magnitude of gradient - 1.367029072634988\n",
      "Step - 1366, Loss - 0.7801143211595132, Learning Rate - 0.025, magnitude of gradient - 3.29994560060893\n",
      "Step - 1367, Loss - 0.8250719663889292, Learning Rate - 0.025, magnitude of gradient - 0.9838248050427189\n",
      "Step - 1368, Loss - 0.7693103503386758, Learning Rate - 0.025, magnitude of gradient - 0.2764368139544055\n",
      "Step - 1369, Loss - 0.7980272751610725, Learning Rate - 0.025, magnitude of gradient - 1.6792754734259583\n",
      "Step - 1370, Loss - 0.7460970269402895, Learning Rate - 0.025, magnitude of gradient - 0.9451491751530271\n",
      "Step - 1371, Loss - 0.7101285569763084, Learning Rate - 0.025, magnitude of gradient - 2.390027824961257\n",
      "Step - 1372, Loss - 0.5721412576771674, Learning Rate - 0.025, magnitude of gradient - 0.8874714123002158\n",
      "Step - 1373, Loss - 0.49975954846417714, Learning Rate - 0.025, magnitude of gradient - 0.5560822674349676\n",
      "Step - 1374, Loss - 0.5640303054247024, Learning Rate - 0.025, magnitude of gradient - 0.659352098894394\n",
      "Step - 1375, Loss - 0.7106286860202692, Learning Rate - 0.025, magnitude of gradient - 1.3640233568244713\n",
      "Step - 1376, Loss - 0.5754118598046076, Learning Rate - 0.025, magnitude of gradient - 1.1916567369926265\n",
      "Step - 1377, Loss - 0.5604974276603033, Learning Rate - 0.025, magnitude of gradient - 1.574665700722505\n",
      "Step - 1378, Loss - 0.7775442874517887, Learning Rate - 0.025, magnitude of gradient - 2.870866903561573\n",
      "Step - 1379, Loss - 0.5731981828604196, Learning Rate - 0.025, magnitude of gradient - 2.116450458435004\n",
      "Step - 1380, Loss - 0.6510594076870893, Learning Rate - 0.025, magnitude of gradient - 1.6165433694711389\n",
      "Step - 1381, Loss - 0.7070910095546944, Learning Rate - 0.025, magnitude of gradient - 1.4734132326892697\n",
      "Step - 1382, Loss - 0.8471793498062543, Learning Rate - 0.025, magnitude of gradient - 1.3425382385807625\n",
      "Step - 1383, Loss - 0.6766222328720464, Learning Rate - 0.025, magnitude of gradient - 1.635552527527586\n",
      "Step - 1384, Loss - 0.7552565684739481, Learning Rate - 0.025, magnitude of gradient - 1.7019805885211976\n",
      "Step - 1385, Loss - 0.8952338353976173, Learning Rate - 0.025, magnitude of gradient - 0.9505896100514377\n",
      "Step - 1386, Loss - 0.6592012624335408, Learning Rate - 0.025, magnitude of gradient - 1.8795014587602068\n",
      "Step - 1387, Loss - 0.7616880749659121, Learning Rate - 0.025, magnitude of gradient - 2.411882273578732\n",
      "Step - 1388, Loss - 0.5880829936541995, Learning Rate - 0.025, magnitude of gradient - 0.8150591536323931\n",
      "Step - 1389, Loss - 0.7142046081325311, Learning Rate - 0.025, magnitude of gradient - 2.582897559671236\n",
      "Step - 1390, Loss - 0.6956896684071803, Learning Rate - 0.025, magnitude of gradient - 0.7064345889749432\n",
      "Step - 1391, Loss - 0.7136324516173421, Learning Rate - 0.025, magnitude of gradient - 1.3829761476854898\n",
      "Step - 1392, Loss - 0.5321366051146528, Learning Rate - 0.025, magnitude of gradient - 1.9527226026359257\n",
      "Step - 1393, Loss - 0.7741747069421423, Learning Rate - 0.025, magnitude of gradient - 1.1991768072162017\n",
      "Step - 1394, Loss - 0.7722454951728457, Learning Rate - 0.025, magnitude of gradient - 1.9418100125380084\n",
      "Step - 1395, Loss - 0.7054012627620688, Learning Rate - 0.025, magnitude of gradient - 0.6463617188252674\n",
      "Step - 1396, Loss - 0.5940691095979899, Learning Rate - 0.025, magnitude of gradient - 2.194590297930971\n",
      "Step - 1397, Loss - 0.8049202188205559, Learning Rate - 0.025, magnitude of gradient - 1.5080000533360347\n",
      "Step - 1398, Loss - 0.5265170970484151, Learning Rate - 0.025, magnitude of gradient - 3.2015991602131\n",
      "Step - 1399, Loss - 0.9658416446633776, Learning Rate - 0.025, magnitude of gradient - 1.3429914300467414\n",
      "Step - 1400, Loss - 0.7751663373382108, Learning Rate - 0.025, magnitude of gradient - 4.143528363621533\n",
      "Step - 1401, Loss - 0.8394969304968412, Learning Rate - 0.025, magnitude of gradient - 1.3840957279284984\n",
      "Step - 1402, Loss - 0.7706885150456958, Learning Rate - 0.025, magnitude of gradient - 0.955542137038966\n",
      "Step - 1403, Loss - 0.593417687402555, Learning Rate - 0.025, magnitude of gradient - 1.581324445444051\n",
      "Step - 1404, Loss - 0.7272211076065344, Learning Rate - 0.025, magnitude of gradient - 1.2765592480472183\n",
      "Step - 1405, Loss - 0.6322855360681486, Learning Rate - 0.025, magnitude of gradient - 1.9195251006376894\n",
      "Step - 1406, Loss - 0.57271316691242, Learning Rate - 0.025, magnitude of gradient - 0.8110177793040214\n",
      "Step - 1407, Loss - 0.6052475334403242, Learning Rate - 0.025, magnitude of gradient - 1.1173779445114629\n",
      "Step - 1408, Loss - 0.8093139446449048, Learning Rate - 0.025, magnitude of gradient - 1.6479088580145385\n",
      "Step - 1409, Loss - 0.7406002069988922, Learning Rate - 0.025, magnitude of gradient - 3.2463096074525795\n",
      "Step - 1410, Loss - 0.7310037475231738, Learning Rate - 0.025, magnitude of gradient - 1.026018904187411\n",
      "Step - 1411, Loss - 0.8389580735158939, Learning Rate - 0.025, magnitude of gradient - 0.4519588973352054\n",
      "Step - 1412, Loss - 0.49631820556572137, Learning Rate - 0.025, magnitude of gradient - 1.4509791148866449\n",
      "Step - 1413, Loss - 0.5913352217099164, Learning Rate - 0.025, magnitude of gradient - 0.97985742078258\n",
      "Step - 1414, Loss - 0.6985943536379933, Learning Rate - 0.025, magnitude of gradient - 1.3245886032297487\n",
      "Step - 1415, Loss - 0.7759496533885193, Learning Rate - 0.025, magnitude of gradient - 1.557752614872349\n",
      "Step - 1416, Loss - 0.5594933877747614, Learning Rate - 0.025, magnitude of gradient - 1.2453870245617102\n",
      "Step - 1417, Loss - 0.6001444793542277, Learning Rate - 0.025, magnitude of gradient - 0.5390293014107963\n",
      "Step - 1418, Loss - 0.703718711152191, Learning Rate - 0.025, magnitude of gradient - 3.066674798708696\n",
      "Step - 1419, Loss - 0.9792873116260664, Learning Rate - 0.025, magnitude of gradient - 1.3350204624821336\n",
      "Step - 1420, Loss - 0.8951590442756395, Learning Rate - 0.025, magnitude of gradient - 1.3445997174003532\n",
      "Step - 1421, Loss - 0.6765739848499402, Learning Rate - 0.025, magnitude of gradient - 0.5624907622117638\n",
      "Step - 1422, Loss - 0.732651744726206, Learning Rate - 0.025, magnitude of gradient - 4.18427148167177\n",
      "Step - 1423, Loss - 0.7777727499695701, Learning Rate - 0.025, magnitude of gradient - 2.275520943794478\n",
      "Step - 1424, Loss - 0.7582570704674092, Learning Rate - 0.025, magnitude of gradient - 0.8669308369048883\n",
      "Step - 1425, Loss - 0.8968196226857446, Learning Rate - 0.025, magnitude of gradient - 2.302140308749514\n",
      "Step - 1426, Loss - 0.7978153281975712, Learning Rate - 0.025, magnitude of gradient - 1.273895603626228\n",
      "Step - 1427, Loss - 0.6492628730158113, Learning Rate - 0.025, magnitude of gradient - 2.13304863008019\n",
      "Step - 1428, Loss - 0.8572293696616513, Learning Rate - 0.025, magnitude of gradient - 2.1734177029172352\n",
      "Step - 1429, Loss - 0.7629421965882884, Learning Rate - 0.025, magnitude of gradient - 2.875545068831483\n",
      "Step - 1430, Loss - 0.6100874387822897, Learning Rate - 0.025, magnitude of gradient - 1.2612103832404842\n",
      "Step - 1431, Loss - 0.8303047269598367, Learning Rate - 0.025, magnitude of gradient - 1.6536187931944872\n",
      "Step - 1432, Loss - 0.6934976410427669, Learning Rate - 0.025, magnitude of gradient - 1.1752367398599053\n",
      "Step - 1433, Loss - 0.5708250956184809, Learning Rate - 0.025, magnitude of gradient - 1.4780973299789244\n",
      "Step - 1434, Loss - 0.6080573625083713, Learning Rate - 0.025, magnitude of gradient - 1.6654308716420447\n",
      "Step - 1435, Loss - 0.8477718324825052, Learning Rate - 0.025, magnitude of gradient - 3.0340860570918515\n",
      "Step - 1436, Loss - 0.614744786169779, Learning Rate - 0.025, magnitude of gradient - 0.8127409135980562\n",
      "Step - 1437, Loss - 0.7914742887449455, Learning Rate - 0.025, magnitude of gradient - 1.7334170686206685\n",
      "Step - 1438, Loss - 0.57866517123992, Learning Rate - 0.025, magnitude of gradient - 2.6871327691075577\n",
      "Step - 1439, Loss - 0.7848122181528722, Learning Rate - 0.025, magnitude of gradient - 3.0513806326574455\n",
      "Step - 1440, Loss - 0.8277188240129001, Learning Rate - 0.025, magnitude of gradient - 1.834745400373369\n",
      "Step - 1441, Loss - 0.6785501817503345, Learning Rate - 0.025, magnitude of gradient - 2.3878791185969126\n",
      "Step - 1442, Loss - 0.5398405142745288, Learning Rate - 0.025, magnitude of gradient - 0.7709640503579257\n",
      "Step - 1443, Loss - 0.6729900187714563, Learning Rate - 0.025, magnitude of gradient - 0.22835030721626148\n",
      "Step - 1444, Loss - 0.5073170733703637, Learning Rate - 0.025, magnitude of gradient - 2.1899271829827454\n",
      "Step - 1445, Loss - 0.6125228601872154, Learning Rate - 0.025, magnitude of gradient - 2.929490107511104\n",
      "Step - 1446, Loss - 0.5363866265136001, Learning Rate - 0.025, magnitude of gradient - 1.0655445511303918\n",
      "Step - 1447, Loss - 0.7802148458851887, Learning Rate - 0.025, magnitude of gradient - 2.959750878361859\n",
      "Step - 1448, Loss - 0.6382185157352772, Learning Rate - 0.025, magnitude of gradient - 1.1283022185554419\n",
      "Step - 1449, Loss - 0.5818309344204268, Learning Rate - 0.025, magnitude of gradient - 1.818809798154548\n",
      "Step - 1450, Loss - 0.701591715743235, Learning Rate - 0.025, magnitude of gradient - 2.1463370290935657\n",
      "Step - 1451, Loss - 0.7398039906063332, Learning Rate - 0.025, magnitude of gradient - 1.5995047040793844\n",
      "Step - 1452, Loss - 0.6188504190225869, Learning Rate - 0.025, magnitude of gradient - 1.5151448271083976\n",
      "Step - 1453, Loss - 0.7541488255820867, Learning Rate - 0.025, magnitude of gradient - 1.7368739868072147\n",
      "Step - 1454, Loss - 0.7897878524138215, Learning Rate - 0.025, magnitude of gradient - 1.11319051735262\n",
      "Step - 1455, Loss - 0.7895808586703021, Learning Rate - 0.025, magnitude of gradient - 1.1083441797188411\n",
      "Step - 1456, Loss - 0.7605608265222594, Learning Rate - 0.025, magnitude of gradient - 0.6861730204354446\n",
      "Step - 1457, Loss - 0.7464623386278033, Learning Rate - 0.025, magnitude of gradient - 1.3903459204176365\n",
      "Step - 1458, Loss - 0.889594198398604, Learning Rate - 0.025, magnitude of gradient - 2.8769631244765166\n",
      "Step - 1459, Loss - 0.8645523330638105, Learning Rate - 0.025, magnitude of gradient - 2.371097855128659\n",
      "Step - 1460, Loss - 0.9542140478287716, Learning Rate - 0.025, magnitude of gradient - 2.113457536705387\n",
      "Step - 1461, Loss - 0.7599093103128635, Learning Rate - 0.025, magnitude of gradient - 1.060619130423035\n",
      "Step - 1462, Loss - 0.8361811513527213, Learning Rate - 0.025, magnitude of gradient - 1.9098936061473588\n",
      "Step - 1463, Loss - 0.8792395660784295, Learning Rate - 0.025, magnitude of gradient - 1.1952851854869158\n",
      "Step - 1464, Loss - 0.9671681911370766, Learning Rate - 0.025, magnitude of gradient - 0.8541731417409173\n",
      "Step - 1465, Loss - 0.6772333003228871, Learning Rate - 0.025, magnitude of gradient - 1.7342279963831095\n",
      "Step - 1466, Loss - 0.9409032295378937, Learning Rate - 0.025, magnitude of gradient - 1.0269803458391598\n",
      "Step - 1467, Loss - 0.8144760607567891, Learning Rate - 0.025, magnitude of gradient - 2.0069227712719435\n",
      "Step - 1468, Loss - 0.7877040618356197, Learning Rate - 0.025, magnitude of gradient - 2.273919686898802\n",
      "Step - 1469, Loss - 0.8049023436680058, Learning Rate - 0.025, magnitude of gradient - 1.0977565909562539\n",
      "Step - 1470, Loss - 0.8381496070210861, Learning Rate - 0.025, magnitude of gradient - 3.16085386573595\n",
      "Step - 1471, Loss - 0.6453635464050249, Learning Rate - 0.025, magnitude of gradient - 3.5374216475321663\n",
      "Step - 1472, Loss - 0.7713968687553145, Learning Rate - 0.025, magnitude of gradient - 2.6484826899238243\n",
      "Step - 1473, Loss - 0.7275997111201086, Learning Rate - 0.025, magnitude of gradient - 1.4231608942719551\n",
      "Step - 1474, Loss - 0.5821272674998905, Learning Rate - 0.025, magnitude of gradient - 1.0987513129303539\n",
      "Step - 1475, Loss - 0.6577854369306948, Learning Rate - 0.025, magnitude of gradient - 0.7128681605656184\n",
      "Step - 1476, Loss - 0.7578425844083541, Learning Rate - 0.025, magnitude of gradient - 2.1032445255753878\n",
      "Step - 1477, Loss - 0.6630991919814491, Learning Rate - 0.025, magnitude of gradient - 1.6012850549653068\n",
      "Step - 1478, Loss - 0.55750806892889, Learning Rate - 0.025, magnitude of gradient - 1.333518468199307\n",
      "Step - 1479, Loss - 0.5848144749507499, Learning Rate - 0.025, magnitude of gradient - 1.8902425127580882\n",
      "Step - 1480, Loss - 0.7663238698248798, Learning Rate - 0.025, magnitude of gradient - 1.2149265824788935\n",
      "Step - 1481, Loss - 0.7674930836362341, Learning Rate - 0.025, magnitude of gradient - 0.8245257576870287\n",
      "Step - 1482, Loss - 0.6450973925538219, Learning Rate - 0.025, magnitude of gradient - 1.6893013903357965\n",
      "Step - 1483, Loss - 0.8244783624116425, Learning Rate - 0.025, magnitude of gradient - 3.817150608279523\n",
      "Step - 1484, Loss - 0.7606536974944222, Learning Rate - 0.025, magnitude of gradient - 0.38300177024672166\n",
      "Step - 1485, Loss - 0.5757542990723821, Learning Rate - 0.025, magnitude of gradient - 2.2235296687189616\n",
      "Step - 1486, Loss - 0.6769761406676468, Learning Rate - 0.025, magnitude of gradient - 2.0135153384186832\n",
      "Step - 1487, Loss - 0.6148917344181448, Learning Rate - 0.025, magnitude of gradient - 1.142885709524\n",
      "Step - 1488, Loss - 0.8670303631910968, Learning Rate - 0.025, magnitude of gradient - 2.810181570371659\n",
      "Step - 1489, Loss - 0.9505265180093385, Learning Rate - 0.025, magnitude of gradient - 1.3095179493418223\n",
      "Step - 1490, Loss - 0.6363129215494963, Learning Rate - 0.025, magnitude of gradient - 3.3636430662497325\n",
      "Step - 1491, Loss - 0.8003397032760348, Learning Rate - 0.025, magnitude of gradient - 1.377179519534499\n",
      "Step - 1492, Loss - 0.6229377495850418, Learning Rate - 0.025, magnitude of gradient - 1.5947196533735255\n",
      "Step - 1493, Loss - 0.6147942061448388, Learning Rate - 0.025, magnitude of gradient - 0.8106547362522794\n",
      "Step - 1494, Loss - 0.7862555780003251, Learning Rate - 0.025, magnitude of gradient - 2.6037830358300202\n",
      "Step - 1495, Loss - 0.8587817383572538, Learning Rate - 0.025, magnitude of gradient - 0.8491527016339253\n",
      "Step - 1496, Loss - 0.8026779548418923, Learning Rate - 0.025, magnitude of gradient - 0.8814333742265045\n",
      "Step - 1497, Loss - 0.681346417131846, Learning Rate - 0.025, magnitude of gradient - 1.7702025939595407\n",
      "Step - 1498, Loss - 0.764936967551807, Learning Rate - 0.025, magnitude of gradient - 1.778004570917154\n",
      "Step - 1499, Loss - 0.7050217807922663, Learning Rate - 0.025, magnitude of gradient - 1.7323795725030149\n",
      "Step - 1500, Loss - 0.638209140150388, Learning Rate - 0.025, magnitude of gradient - 1.2992774548996386\n",
      "Step - 1501, Loss - 0.5488450284795896, Learning Rate - 0.025, magnitude of gradient - 4.201597057952843\n",
      "Step - 1502, Loss - 0.6943565301111936, Learning Rate - 0.025, magnitude of gradient - 2.008956843112504\n",
      "Step - 1503, Loss - 0.5868238154916283, Learning Rate - 0.025, magnitude of gradient - 2.164627401032347\n",
      "Step - 1504, Loss - 0.7805218143913155, Learning Rate - 0.025, magnitude of gradient - 1.9496614308280977\n",
      "Step - 1505, Loss - 0.8158102532229223, Learning Rate - 0.025, magnitude of gradient - 0.8829584150124221\n",
      "Step - 1506, Loss - 0.7175373061324287, Learning Rate - 0.025, magnitude of gradient - 0.8961757559767589\n",
      "Step - 1507, Loss - 0.8119976298002063, Learning Rate - 0.025, magnitude of gradient - 1.297154564434339\n",
      "Step - 1508, Loss - 0.666767584076952, Learning Rate - 0.025, magnitude of gradient - 2.9881506899890584\n",
      "Step - 1509, Loss - 0.5353457405356806, Learning Rate - 0.025, magnitude of gradient - 1.4724294930255115\n",
      "Step - 1510, Loss - 0.8253452528413455, Learning Rate - 0.025, magnitude of gradient - 2.6333667850612943\n",
      "Step - 1511, Loss - 0.7444647621215239, Learning Rate - 0.025, magnitude of gradient - 1.4544117960930194\n",
      "Step - 1512, Loss - 0.7318013892313191, Learning Rate - 0.025, magnitude of gradient - 2.050448252437078\n",
      "Step - 1513, Loss - 0.662877150343329, Learning Rate - 0.025, magnitude of gradient - 1.337973389271869\n",
      "Step - 1514, Loss - 0.6026025242223764, Learning Rate - 0.025, magnitude of gradient - 2.6199712415031975\n",
      "Step - 1515, Loss - 0.5647340836568525, Learning Rate - 0.025, magnitude of gradient - 1.8363971787084175\n",
      "Step - 1516, Loss - 0.6063987904855359, Learning Rate - 0.025, magnitude of gradient - 0.7493554825575881\n",
      "Step - 1517, Loss - 0.5223543285766123, Learning Rate - 0.025, magnitude of gradient - 2.7001949515253454\n",
      "Step - 1518, Loss - 0.4863120225652047, Learning Rate - 0.025, magnitude of gradient - 0.8672311623943824\n",
      "Step - 1519, Loss - 0.780049280302408, Learning Rate - 0.025, magnitude of gradient - 2.107358698297675\n",
      "Step - 1520, Loss - 0.5361284113527018, Learning Rate - 0.025, magnitude of gradient - 1.3069335172140975\n",
      "Step - 1521, Loss - 0.9876002178667576, Learning Rate - 0.025, magnitude of gradient - 2.911031751425731\n",
      "Step - 1522, Loss - 0.6563047223286484, Learning Rate - 0.025, magnitude of gradient - 3.672335745444185\n",
      "Step - 1523, Loss - 0.6751572483924173, Learning Rate - 0.025, magnitude of gradient - 2.8727495801313974\n",
      "Step - 1524, Loss - 0.8374403513598466, Learning Rate - 0.025, magnitude of gradient - 1.4829789248842757\n",
      "Step - 1525, Loss - 0.9428673249798708, Learning Rate - 0.025, magnitude of gradient - 1.5325358535085254\n",
      "Step - 1526, Loss - 0.6783405238153122, Learning Rate - 0.025, magnitude of gradient - 0.993844926375731\n",
      "Step - 1527, Loss - 0.5927339410958705, Learning Rate - 0.025, magnitude of gradient - 2.700125542763523\n",
      "Step - 1528, Loss - 0.711468627728478, Learning Rate - 0.025, magnitude of gradient - 0.9872058746427776\n",
      "Step - 1529, Loss - 0.7074122695614113, Learning Rate - 0.025, magnitude of gradient - 2.2646405581299844\n",
      "Step - 1530, Loss - 0.6949716751384329, Learning Rate - 0.025, magnitude of gradient - 1.388646962746256\n",
      "Step - 1531, Loss - 0.7337552851583211, Learning Rate - 0.025, magnitude of gradient - 3.2970303922703357\n",
      "Step - 1532, Loss - 0.8461887613516405, Learning Rate - 0.025, magnitude of gradient - 1.7145130465288616\n",
      "Step - 1533, Loss - 0.7302710933237933, Learning Rate - 0.025, magnitude of gradient - 2.9943827791042805\n",
      "Step - 1534, Loss - 0.7635480431969723, Learning Rate - 0.025, magnitude of gradient - 2.091055541703888\n",
      "Step - 1535, Loss - 0.6309632975548587, Learning Rate - 0.025, magnitude of gradient - 2.9485218473754085\n",
      "Step - 1536, Loss - 0.5952144708516415, Learning Rate - 0.025, magnitude of gradient - 1.4774193239684426\n",
      "Step - 1537, Loss - 1.0617221964203776, Learning Rate - 0.025, magnitude of gradient - 3.3429217794135644\n",
      "Step - 1538, Loss - 0.6287953396083276, Learning Rate - 0.025, magnitude of gradient - 3.0712342892729105\n",
      "Step - 1539, Loss - 0.5945627013845891, Learning Rate - 0.025, magnitude of gradient - 2.0169161838097405\n",
      "Step - 1540, Loss - 0.607001805209918, Learning Rate - 0.025, magnitude of gradient - 1.6084348223499363\n",
      "Step - 1541, Loss - 0.6999730430164669, Learning Rate - 0.025, magnitude of gradient - 1.2947130975587886\n",
      "Step - 1542, Loss - 0.7222357397849054, Learning Rate - 0.025, magnitude of gradient - 3.1290265194775175\n",
      "Step - 1543, Loss - 0.5782383362103529, Learning Rate - 0.025, magnitude of gradient - 2.582991541368449\n",
      "Step - 1544, Loss - 0.8086844146758632, Learning Rate - 0.025, magnitude of gradient - 3.810427566078813\n",
      "Step - 1545, Loss - 0.9299421403459718, Learning Rate - 0.025, magnitude of gradient - 1.3382144192767398\n",
      "Step - 1546, Loss - 0.6748905261508398, Learning Rate - 0.025, magnitude of gradient - 2.827287315946025\n",
      "Step - 1547, Loss - 0.6528417021256219, Learning Rate - 0.025, magnitude of gradient - 1.0507997705111338\n",
      "Step - 1548, Loss - 0.6687962475281916, Learning Rate - 0.025, magnitude of gradient - 1.7233438080018604\n",
      "Step - 1549, Loss - 0.6080240410939682, Learning Rate - 0.025, magnitude of gradient - 0.22662956669039336\n",
      "Step - 1550, Loss - 0.5867414477377932, Learning Rate - 0.025, magnitude of gradient - 2.6528715942891448\n",
      "Step - 1551, Loss - 0.5905822819646882, Learning Rate - 0.025, magnitude of gradient - 0.586916328765397\n",
      "Step - 1552, Loss - 0.7755076017664402, Learning Rate - 0.025, magnitude of gradient - 2.245316716300014\n",
      "Step - 1553, Loss - 0.6536073867970678, Learning Rate - 0.025, magnitude of gradient - 1.37040948236663\n",
      "Step - 1554, Loss - 0.8065923977722281, Learning Rate - 0.025, magnitude of gradient - 1.689556573837197\n",
      "Step - 1555, Loss - 0.7559920545054846, Learning Rate - 0.025, magnitude of gradient - 2.3334870654475046\n",
      "Step - 1556, Loss - 0.7458774543329812, Learning Rate - 0.025, magnitude of gradient - 1.0976847703803767\n",
      "Step - 1557, Loss - 0.5960972175709465, Learning Rate - 0.025, magnitude of gradient - 1.172517100819895\n",
      "Step - 1558, Loss - 0.799227451689899, Learning Rate - 0.025, magnitude of gradient - 1.7180721259373581\n",
      "Step - 1559, Loss - 0.8152116756820638, Learning Rate - 0.025, magnitude of gradient - 5.133398466533519\n",
      "Step - 1560, Loss - 0.7013149745205113, Learning Rate - 0.025, magnitude of gradient - 1.1085619416987322\n",
      "Step - 1561, Loss - 0.6657916931564327, Learning Rate - 0.025, magnitude of gradient - 3.160826243388009\n",
      "Step - 1562, Loss - 0.6381050452135895, Learning Rate - 0.025, magnitude of gradient - 1.974002652102598\n",
      "Step - 1563, Loss - 0.7248395137946385, Learning Rate - 0.025, magnitude of gradient - 0.7264435589911011\n",
      "Step - 1564, Loss - 0.7819133951576238, Learning Rate - 0.025, magnitude of gradient - 1.2236044314884595\n",
      "Step - 1565, Loss - 0.7081451403162138, Learning Rate - 0.025, magnitude of gradient - 0.8999877506391649\n",
      "Step - 1566, Loss - 0.6621113149405708, Learning Rate - 0.025, magnitude of gradient - 0.2670487253941341\n",
      "Step - 1567, Loss - 0.7466578129046092, Learning Rate - 0.025, magnitude of gradient - 1.402946914145586\n",
      "Step - 1568, Loss - 0.7787217093619659, Learning Rate - 0.025, magnitude of gradient - 0.9715310569865326\n",
      "Step - 1569, Loss - 0.9385121594640804, Learning Rate - 0.025, magnitude of gradient - 1.4647792037150142\n",
      "Step - 1570, Loss - 0.711997204873273, Learning Rate - 0.025, magnitude of gradient - 0.5516399114113876\n",
      "Step - 1571, Loss - 0.6447703565982392, Learning Rate - 0.025, magnitude of gradient - 1.5900301759262094\n",
      "Step - 1572, Loss - 0.9031065041852403, Learning Rate - 0.025, magnitude of gradient - 1.2504476147973032\n",
      "Step - 1573, Loss - 0.5855983508603936, Learning Rate - 0.025, magnitude of gradient - 2.157877581961885\n",
      "Step - 1574, Loss - 0.84182125638788, Learning Rate - 0.025, magnitude of gradient - 2.124648884106436\n",
      "Step - 1575, Loss - 0.6702454486216356, Learning Rate - 0.025, magnitude of gradient - 1.8861807452062485\n",
      "Step - 1576, Loss - 0.7406281987679317, Learning Rate - 0.025, magnitude of gradient - 2.0935602017602926\n",
      "Step - 1577, Loss - 0.5512590711798195, Learning Rate - 0.025, magnitude of gradient - 1.639597840842432\n",
      "Step - 1578, Loss - 0.6848023708164163, Learning Rate - 0.025, magnitude of gradient - 1.8882738735525397\n",
      "Step - 1579, Loss - 0.6794800108039862, Learning Rate - 0.025, magnitude of gradient - 2.2586630765926095\n",
      "Step - 1580, Loss - 0.6359196994648482, Learning Rate - 0.025, magnitude of gradient - 1.6162963799119092\n",
      "Step - 1581, Loss - 0.7688364801729483, Learning Rate - 0.025, magnitude of gradient - 2.4147228744223064\n",
      "Step - 1582, Loss - 0.6571448125467725, Learning Rate - 0.025, magnitude of gradient - 1.9437525680548209\n",
      "Step - 1583, Loss - 0.7901800217658523, Learning Rate - 0.025, magnitude of gradient - 1.2034256290950338\n",
      "Step - 1584, Loss - 0.7714154480635438, Learning Rate - 0.025, magnitude of gradient - 0.7377003053959308\n",
      "Step - 1585, Loss - 0.8329071632295472, Learning Rate - 0.025, magnitude of gradient - 0.7634867872456925\n",
      "Step - 1586, Loss - 0.9091447447923804, Learning Rate - 0.025, magnitude of gradient - 3.2709509378708033\n",
      "Step - 1587, Loss - 0.7360741458667964, Learning Rate - 0.025, magnitude of gradient - 1.3797418475076422\n",
      "Step - 1588, Loss - 0.7384324463445577, Learning Rate - 0.025, magnitude of gradient - 0.7953778365466524\n",
      "Step - 1589, Loss - 0.7792398071078666, Learning Rate - 0.025, magnitude of gradient - 1.1572251424121456\n",
      "Step - 1590, Loss - 0.8940330157166541, Learning Rate - 0.025, magnitude of gradient - 1.2211088636290728\n",
      "Step - 1591, Loss - 0.7181193046796419, Learning Rate - 0.025, magnitude of gradient - 1.183474802843641\n",
      "Step - 1592, Loss - 0.7185190454225048, Learning Rate - 0.025, magnitude of gradient - 1.528675004150431\n",
      "Step - 1593, Loss - 0.7959751416460056, Learning Rate - 0.025, magnitude of gradient - 2.329858238018839\n",
      "Step - 1594, Loss - 0.6852447802922405, Learning Rate - 0.025, magnitude of gradient - 1.0773972237540235\n",
      "Step - 1595, Loss - 0.678327958206044, Learning Rate - 0.025, magnitude of gradient - 2.7744733088166478\n",
      "Step - 1596, Loss - 0.5492260905773301, Learning Rate - 0.025, magnitude of gradient - 0.8643330366151839\n",
      "Step - 1597, Loss - 0.8309299200941942, Learning Rate - 0.025, magnitude of gradient - 1.7824092929562316\n",
      "Step - 1598, Loss - 0.6739010943201403, Learning Rate - 0.025, magnitude of gradient - 1.108313964002836\n",
      "Step - 1599, Loss - 0.6443150412132572, Learning Rate - 0.025, magnitude of gradient - 1.0750396405069678\n",
      "Step - 1600, Loss - 0.7073088416398285, Learning Rate - 0.025, magnitude of gradient - 0.669170617418258\n",
      "Step - 1601, Loss - 0.5967311380870456, Learning Rate - 0.025, magnitude of gradient - 0.9787193276860702\n",
      "Step - 1602, Loss - 0.6289320576423334, Learning Rate - 0.025, magnitude of gradient - 1.3028675684516733\n",
      "Step - 1603, Loss - 0.689109590159757, Learning Rate - 0.025, magnitude of gradient - 1.1055759964231402\n",
      "Step - 1604, Loss - 0.8083037006228471, Learning Rate - 0.025, magnitude of gradient - 1.3930222898063855\n",
      "Step - 1605, Loss - 0.7622158295539697, Learning Rate - 0.025, magnitude of gradient - 2.459725935146623\n",
      "Step - 1606, Loss - 0.9634164984126128, Learning Rate - 0.025, magnitude of gradient - 1.177913346112098\n",
      "Step - 1607, Loss - 0.8091030793911933, Learning Rate - 0.025, magnitude of gradient - 2.7440647938753817\n",
      "Step - 1608, Loss - 0.6939509132660722, Learning Rate - 0.025, magnitude of gradient - 1.9465429649145212\n",
      "Step - 1609, Loss - 0.6000789750029475, Learning Rate - 0.025, magnitude of gradient - 4.257835415052968\n",
      "Step - 1610, Loss - 0.841537888978001, Learning Rate - 0.025, magnitude of gradient - 2.3046145547443104\n",
      "Step - 1611, Loss - 0.6408689031201064, Learning Rate - 0.025, magnitude of gradient - 1.3905535386775711\n",
      "Step - 1612, Loss - 0.9438427086150678, Learning Rate - 0.025, magnitude of gradient - 2.4078963785480316\n",
      "Step - 1613, Loss - 0.7702488364330995, Learning Rate - 0.025, magnitude of gradient - 2.5388640912690303\n",
      "Step - 1614, Loss - 0.8235351204797703, Learning Rate - 0.025, magnitude of gradient - 2.8654473527689635\n",
      "Step - 1615, Loss - 0.6180191348928527, Learning Rate - 0.025, magnitude of gradient - 2.40983281773204\n",
      "Step - 1616, Loss - 0.8120480027663595, Learning Rate - 0.025, magnitude of gradient - 0.4404536531666536\n",
      "Step - 1617, Loss - 0.916232922333691, Learning Rate - 0.025, magnitude of gradient - 1.3534937429107872\n",
      "Step - 1618, Loss - 0.677172720952798, Learning Rate - 0.025, magnitude of gradient - 1.7226967164698992\n",
      "Step - 1619, Loss - 0.5787443866878377, Learning Rate - 0.025, magnitude of gradient - 1.7838655261862286\n",
      "Step - 1620, Loss - 0.7543197090846363, Learning Rate - 0.025, magnitude of gradient - 1.8529900279095648\n",
      "Step - 1621, Loss - 0.6184157774243906, Learning Rate - 0.025, magnitude of gradient - 1.0219876636256733\n",
      "Step - 1622, Loss - 0.7584616387228108, Learning Rate - 0.025, magnitude of gradient - 1.4265218910436208\n",
      "Step - 1623, Loss - 0.6991161050464242, Learning Rate - 0.025, magnitude of gradient - 2.387870258235436\n",
      "Step - 1624, Loss - 0.6206214017471485, Learning Rate - 0.025, magnitude of gradient - 1.3072806750915396\n",
      "Step - 1625, Loss - 0.7331773968198654, Learning Rate - 0.025, magnitude of gradient - 1.9221752256561595\n",
      "Step - 1626, Loss - 0.4979798491090807, Learning Rate - 0.025, magnitude of gradient - 2.38715786401524\n",
      "Step - 1627, Loss - 0.5224605495602528, Learning Rate - 0.025, magnitude of gradient - 0.39524429158670465\n",
      "Step - 1628, Loss - 0.7131404903333336, Learning Rate - 0.025, magnitude of gradient - 1.5285915591326855\n",
      "Step - 1629, Loss - 0.5839473386918232, Learning Rate - 0.025, magnitude of gradient - 1.783813800172804\n",
      "Step - 1630, Loss - 0.606740200528444, Learning Rate - 0.025, magnitude of gradient - 0.8409524458186236\n",
      "Step - 1631, Loss - 0.6901585212363402, Learning Rate - 0.025, magnitude of gradient - 2.4786715038971594\n",
      "Step - 1632, Loss - 0.5932260947787238, Learning Rate - 0.025, magnitude of gradient - 1.580717948869505\n",
      "Step - 1633, Loss - 0.5878961169770331, Learning Rate - 0.025, magnitude of gradient - 0.7407753682133612\n",
      "Step - 1634, Loss - 0.7489559469468876, Learning Rate - 0.025, magnitude of gradient - 0.3793924645190685\n",
      "Step - 1635, Loss - 0.6173123712157912, Learning Rate - 0.025, magnitude of gradient - 1.7061581711543057\n",
      "Step - 1636, Loss - 0.6029234483440624, Learning Rate - 0.025, magnitude of gradient - 3.005430818361608\n",
      "Step - 1637, Loss - 0.7682943685053706, Learning Rate - 0.025, magnitude of gradient - 2.855418637460689\n",
      "Step - 1638, Loss - 0.7635114429244529, Learning Rate - 0.025, magnitude of gradient - 1.160632973345298\n",
      "Step - 1639, Loss - 0.9181315106326711, Learning Rate - 0.025, magnitude of gradient - 4.424409984823721\n",
      "Step - 1640, Loss - 0.6569035167955795, Learning Rate - 0.025, magnitude of gradient - 1.2910773803442923\n",
      "Step - 1641, Loss - 0.6606784374781766, Learning Rate - 0.025, magnitude of gradient - 1.4560809904953118\n",
      "Step - 1642, Loss - 0.8270598310775757, Learning Rate - 0.025, magnitude of gradient - 0.6712469403424453\n",
      "Step - 1643, Loss - 0.6475094336768025, Learning Rate - 0.025, magnitude of gradient - 0.8838244538912406\n",
      "Step - 1644, Loss - 0.8277025171030534, Learning Rate - 0.025, magnitude of gradient - 1.141074333229023\n",
      "Step - 1645, Loss - 0.4718350493994306, Learning Rate - 0.025, magnitude of gradient - 1.601120365591702\n",
      "Step - 1646, Loss - 0.8270651493074919, Learning Rate - 0.025, magnitude of gradient - 1.9449003128191102\n",
      "Step - 1647, Loss - 0.6097245290345443, Learning Rate - 0.025, magnitude of gradient - 3.8380575279875995\n",
      "Step - 1648, Loss - 0.656491994966053, Learning Rate - 0.025, magnitude of gradient - 1.3905579602762417\n",
      "Step - 1649, Loss - 0.5589509729107549, Learning Rate - 0.025, magnitude of gradient - 0.37943581384722536\n",
      "Step - 1650, Loss - 0.910013390484814, Learning Rate - 0.025, magnitude of gradient - 2.250234441137659\n",
      "Step - 1651, Loss - 0.6454642558239213, Learning Rate - 0.025, magnitude of gradient - 2.8836768176986514\n",
      "Step - 1652, Loss - 0.898238733988626, Learning Rate - 0.025, magnitude of gradient - 1.334642048991713\n",
      "Step - 1653, Loss - 0.8721869103468278, Learning Rate - 0.025, magnitude of gradient - 1.9645368447044433\n",
      "Step - 1654, Loss - 0.7475662496628803, Learning Rate - 0.025, magnitude of gradient - 1.9154564751757457\n",
      "Step - 1655, Loss - 0.7997563470190503, Learning Rate - 0.025, magnitude of gradient - 2.4694252584490397\n",
      "Step - 1656, Loss - 0.7276015288594769, Learning Rate - 0.025, magnitude of gradient - 1.4357946745084278\n",
      "Step - 1657, Loss - 0.7395623451837033, Learning Rate - 0.025, magnitude of gradient - 3.1824382803947886\n",
      "Step - 1658, Loss - 0.8200952099292771, Learning Rate - 0.025, magnitude of gradient - 1.9380299785137731\n",
      "Step - 1659, Loss - 0.7182917248194158, Learning Rate - 0.025, magnitude of gradient - 2.361145407189776\n",
      "Step - 1660, Loss - 0.6265589074505701, Learning Rate - 0.025, magnitude of gradient - 1.4165920090681168\n",
      "Step - 1661, Loss - 0.6253037233982912, Learning Rate - 0.025, magnitude of gradient - 1.308902894416612\n",
      "Step - 1662, Loss - 0.6675873971497408, Learning Rate - 0.025, magnitude of gradient - 3.2535309726098993\n",
      "Step - 1663, Loss - 0.3955079358839571, Learning Rate - 0.025, magnitude of gradient - 0.5687412166989885\n",
      "Step - 1664, Loss - 0.8504928649363315, Learning Rate - 0.025, magnitude of gradient - 2.85807979211473\n",
      "Step - 1665, Loss - 0.6867255136043244, Learning Rate - 0.025, magnitude of gradient - 0.752721759364087\n",
      "Step - 1666, Loss - 0.7574134666580832, Learning Rate - 0.025, magnitude of gradient - 1.1840040342166807\n",
      "Step - 1667, Loss - 0.6846160545198656, Learning Rate - 0.025, magnitude of gradient - 1.4751736507197963\n",
      "Step - 1668, Loss - 0.749968948920031, Learning Rate - 0.025, magnitude of gradient - 1.9182214591463265\n",
      "Step - 1669, Loss - 0.7506494341272579, Learning Rate - 0.025, magnitude of gradient - 1.4700195452376448\n",
      "Step - 1670, Loss - 0.8393203223087724, Learning Rate - 0.025, magnitude of gradient - 2.0520558794716384\n",
      "Step - 1671, Loss - 0.7341121923124525, Learning Rate - 0.025, magnitude of gradient - 1.705817501587578\n",
      "Step - 1672, Loss - 0.8198730349306591, Learning Rate - 0.025, magnitude of gradient - 2.715796187134872\n",
      "Step - 1673, Loss - 0.4640903408736262, Learning Rate - 0.025, magnitude of gradient - 0.6933797880441873\n",
      "Step - 1674, Loss - 0.7354291709046545, Learning Rate - 0.025, magnitude of gradient - 1.5539503539862187\n",
      "Step - 1675, Loss - 0.8020268642892164, Learning Rate - 0.025, magnitude of gradient - 0.7847190001005575\n",
      "Step - 1676, Loss - 0.8034875132255282, Learning Rate - 0.025, magnitude of gradient - 0.6412196618141316\n",
      "Step - 1677, Loss - 0.8954030430855641, Learning Rate - 0.025, magnitude of gradient - 1.579883563926296\n",
      "Step - 1678, Loss - 0.6522116820381063, Learning Rate - 0.025, magnitude of gradient - 1.2637337564747233\n",
      "Step - 1679, Loss - 0.8000813825903057, Learning Rate - 0.025, magnitude of gradient - 0.8665722556412235\n",
      "Step - 1680, Loss - 0.6791520226841776, Learning Rate - 0.025, magnitude of gradient - 0.3317873758510974\n",
      "Step - 1681, Loss - 0.7853844160323687, Learning Rate - 0.025, magnitude of gradient - 0.6100904008078113\n",
      "Step - 1682, Loss - 0.7276347767901593, Learning Rate - 0.025, magnitude of gradient - 0.5145400168321097\n",
      "Step - 1683, Loss - 0.745334843647421, Learning Rate - 0.025, magnitude of gradient - 1.654470603777947\n",
      "Step - 1684, Loss - 0.6287192499016376, Learning Rate - 0.025, magnitude of gradient - 0.8693341899677721\n",
      "Step - 1685, Loss - 0.860718472988552, Learning Rate - 0.025, magnitude of gradient - 1.7341740534325676\n",
      "Step - 1686, Loss - 0.7390623856834934, Learning Rate - 0.025, magnitude of gradient - 1.136434211800991\n",
      "Step - 1687, Loss - 0.6234824642747425, Learning Rate - 0.025, magnitude of gradient - 0.6521220224983378\n",
      "Step - 1688, Loss - 0.8482402724171475, Learning Rate - 0.025, magnitude of gradient - 2.2762585195378064\n",
      "Step - 1689, Loss - 0.7391480821428953, Learning Rate - 0.025, magnitude of gradient - 1.7760019742706261\n",
      "Step - 1690, Loss - 0.5566813646132343, Learning Rate - 0.025, magnitude of gradient - 1.5829371578973495\n",
      "Step - 1691, Loss - 0.7569639801104509, Learning Rate - 0.025, magnitude of gradient - 1.3124682306262583\n",
      "Step - 1692, Loss - 0.7323537651844563, Learning Rate - 0.025, magnitude of gradient - 0.9766906892221432\n",
      "Step - 1693, Loss - 0.56219184557839, Learning Rate - 0.025, magnitude of gradient - 1.81120238189321\n",
      "Step - 1694, Loss - 0.6471141320078938, Learning Rate - 0.025, magnitude of gradient - 1.4184842799136648\n",
      "Step - 1695, Loss - 0.6635426249864307, Learning Rate - 0.025, magnitude of gradient - 1.9979278184307434\n",
      "Step - 1696, Loss - 0.7058769802742209, Learning Rate - 0.025, magnitude of gradient - 2.341281763762417\n",
      "Step - 1697, Loss - 0.8383098731871115, Learning Rate - 0.025, magnitude of gradient - 3.608845875374626\n",
      "Step - 1698, Loss - 0.9640172448887487, Learning Rate - 0.025, magnitude of gradient - 1.5772990550199575\n",
      "Step - 1699, Loss - 0.7653482986060204, Learning Rate - 0.025, magnitude of gradient - 2.1491870019777783\n",
      "Step - 1700, Loss - 0.8725435774542009, Learning Rate - 0.025, magnitude of gradient - 1.8033478601602162\n",
      "Step - 1701, Loss - 0.6735484531367034, Learning Rate - 0.025, magnitude of gradient - 0.9590714113691744\n",
      "Step - 1702, Loss - 0.6731227727884939, Learning Rate - 0.025, magnitude of gradient - 1.748456991693923\n",
      "Step - 1703, Loss - 0.6810459125404899, Learning Rate - 0.025, magnitude of gradient - 2.611522310590704\n",
      "Step - 1704, Loss - 0.7225271924232367, Learning Rate - 0.025, magnitude of gradient - 1.22565454521375\n",
      "Step - 1705, Loss - 0.6491640052210398, Learning Rate - 0.025, magnitude of gradient - 3.9436699941148348\n",
      "Step - 1706, Loss - 0.5388912665458214, Learning Rate - 0.025, magnitude of gradient - 0.7941050501613258\n",
      "Step - 1707, Loss - 0.7687911163675573, Learning Rate - 0.025, magnitude of gradient - 2.7731562643135415\n",
      "Step - 1708, Loss - 0.7660364756944005, Learning Rate - 0.025, magnitude of gradient - 2.5220321610768557\n",
      "Step - 1709, Loss - 0.6787149730084866, Learning Rate - 0.025, magnitude of gradient - 2.7678551945131598\n",
      "Step - 1710, Loss - 0.6366519358143525, Learning Rate - 0.025, magnitude of gradient - 0.6494773229442768\n",
      "Step - 1711, Loss - 0.6576692216902759, Learning Rate - 0.025, magnitude of gradient - 1.6226823431960966\n",
      "Step - 1712, Loss - 0.8629665734394689, Learning Rate - 0.025, magnitude of gradient - 2.3622068206753353\n",
      "Step - 1713, Loss - 0.9563149924687515, Learning Rate - 0.025, magnitude of gradient - 1.615561196308324\n",
      "Step - 1714, Loss - 0.8886853061166811, Learning Rate - 0.025, magnitude of gradient - 2.5848230755182064\n",
      "Step - 1715, Loss - 0.6967635719077335, Learning Rate - 0.025, magnitude of gradient - 0.43702368273989395\n",
      "Step - 1716, Loss - 0.6751689965391314, Learning Rate - 0.025, magnitude of gradient - 0.9019929878015578\n",
      "Step - 1717, Loss - 0.6826386901561319, Learning Rate - 0.025, magnitude of gradient - 2.6513813077513384\n",
      "Step - 1718, Loss - 0.6312201636921642, Learning Rate - 0.025, magnitude of gradient - 2.4618576828191014\n",
      "Step - 1719, Loss - 0.6796040059638491, Learning Rate - 0.025, magnitude of gradient - 1.5055379798610948\n",
      "Step - 1720, Loss - 0.6149757240336189, Learning Rate - 0.025, magnitude of gradient - 0.965166665207811\n",
      "Step - 1721, Loss - 0.6301296056914696, Learning Rate - 0.025, magnitude of gradient - 1.240397951531572\n",
      "Step - 1722, Loss - 0.774714241540416, Learning Rate - 0.025, magnitude of gradient - 1.287942517495594\n",
      "Step - 1723, Loss - 0.5761711582576888, Learning Rate - 0.025, magnitude of gradient - 1.4196048296956671\n",
      "Step - 1724, Loss - 0.8342233846303, Learning Rate - 0.025, magnitude of gradient - 1.3188687748078856\n",
      "Step - 1725, Loss - 0.769190147903674, Learning Rate - 0.025, magnitude of gradient - 2.6131915374531953\n",
      "Step - 1726, Loss - 0.8177158371563436, Learning Rate - 0.025, magnitude of gradient - 2.310241553414672\n",
      "Step - 1727, Loss - 0.7682677840847688, Learning Rate - 0.025, magnitude of gradient - 0.5597695946299136\n",
      "Step - 1728, Loss - 0.5607543884684674, Learning Rate - 0.025, magnitude of gradient - 1.8634500437901536\n",
      "Step - 1729, Loss - 0.7574575015075257, Learning Rate - 0.025, magnitude of gradient - 1.1862436166460268\n",
      "Step - 1730, Loss - 0.7824493863379478, Learning Rate - 0.025, magnitude of gradient - 1.0558976774227835\n",
      "Step - 1731, Loss - 0.8363189414644321, Learning Rate - 0.025, magnitude of gradient - 5.517868926717281\n",
      "Step - 1732, Loss - 0.6482725388447672, Learning Rate - 0.025, magnitude of gradient - 2.7270714114044647\n",
      "Step - 1733, Loss - 0.6637821493120919, Learning Rate - 0.025, magnitude of gradient - 1.3873104459585754\n",
      "Step - 1734, Loss - 0.5447738595364229, Learning Rate - 0.025, magnitude of gradient - 1.597079578510052\n",
      "Step - 1735, Loss - 0.9095861938326405, Learning Rate - 0.025, magnitude of gradient - 3.1066381273959576\n",
      "Step - 1736, Loss - 0.8058208112183635, Learning Rate - 0.025, magnitude of gradient - 1.9706945438493317\n",
      "Step - 1737, Loss - 0.6305316868927641, Learning Rate - 0.025, magnitude of gradient - 0.6145233207959109\n",
      "Step - 1738, Loss - 0.6310918164910476, Learning Rate - 0.025, magnitude of gradient - 2.6645313875484393\n",
      "Step - 1739, Loss - 0.8252911784507917, Learning Rate - 0.025, magnitude of gradient - 2.39926972941955\n",
      "Step - 1740, Loss - 0.8069030368552113, Learning Rate - 0.025, magnitude of gradient - 1.049575837981802\n",
      "Step - 1741, Loss - 0.5437861556319057, Learning Rate - 0.025, magnitude of gradient - 1.4624126122962182\n",
      "Step - 1742, Loss - 0.6705442079831444, Learning Rate - 0.025, magnitude of gradient - 1.5765626906580854\n",
      "Step - 1743, Loss - 0.6939047192680219, Learning Rate - 0.025, magnitude of gradient - 2.0433600258639535\n",
      "Step - 1744, Loss - 0.7335191307379012, Learning Rate - 0.025, magnitude of gradient - 0.6688535157624448\n",
      "Step - 1745, Loss - 0.7446170341275162, Learning Rate - 0.025, magnitude of gradient - 3.1835677875437174\n",
      "Step - 1746, Loss - 0.6021706653724797, Learning Rate - 0.025, magnitude of gradient - 2.9380894203169077\n",
      "Step - 1747, Loss - 0.7194449801663317, Learning Rate - 0.025, magnitude of gradient - 2.7153828396704935\n",
      "Step - 1748, Loss - 0.5054423844631559, Learning Rate - 0.025, magnitude of gradient - 3.153294238242087\n",
      "Step - 1749, Loss - 0.6984686001762854, Learning Rate - 0.025, magnitude of gradient - 2.627014111082018\n",
      "Step - 1750, Loss - 0.6927913443436525, Learning Rate - 0.025, magnitude of gradient - 2.7617413875309063\n",
      "Step - 1751, Loss - 0.75935051190876, Learning Rate - 0.025, magnitude of gradient - 2.287890088149873\n",
      "Step - 1752, Loss - 0.6308441849150332, Learning Rate - 0.025, magnitude of gradient - 0.8664951378166084\n",
      "Step - 1753, Loss - 0.7504237387724454, Learning Rate - 0.025, magnitude of gradient - 2.455762344433501\n",
      "Step - 1754, Loss - 0.8787208718255947, Learning Rate - 0.025, magnitude of gradient - 1.9902554058912618\n",
      "Step - 1755, Loss - 0.8904880872273204, Learning Rate - 0.025, magnitude of gradient - 2.2045526695631352\n",
      "Step - 1756, Loss - 0.7220764310167649, Learning Rate - 0.025, magnitude of gradient - 2.0664734652337207\n",
      "Step - 1757, Loss - 0.6949901734142443, Learning Rate - 0.025, magnitude of gradient - 1.60764986478622\n",
      "Step - 1758, Loss - 0.7769303857854409, Learning Rate - 0.025, magnitude of gradient - 1.5326142631892354\n",
      "Step - 1759, Loss - 0.7501761893747022, Learning Rate - 0.025, magnitude of gradient - 1.3798473617108065\n",
      "Step - 1760, Loss - 0.8654476064545071, Learning Rate - 0.025, magnitude of gradient - 0.6117438084716135\n",
      "Step - 1761, Loss - 0.7959842073361638, Learning Rate - 0.025, magnitude of gradient - 0.8073157400575078\n",
      "Step - 1762, Loss - 0.7545170109397565, Learning Rate - 0.025, magnitude of gradient - 2.4487318506471056\n",
      "Step - 1763, Loss - 0.6789409633856807, Learning Rate - 0.025, magnitude of gradient - 2.127539977088325\n",
      "Step - 1764, Loss - 0.7028252872101917, Learning Rate - 0.025, magnitude of gradient - 1.0667404244277887\n",
      "Step - 1765, Loss - 0.7195673446542961, Learning Rate - 0.025, magnitude of gradient - 1.997554032106306\n",
      "Step - 1766, Loss - 0.7770561793991582, Learning Rate - 0.025, magnitude of gradient - 1.701829189081829\n",
      "Step - 1767, Loss - 0.6370125920263041, Learning Rate - 0.025, magnitude of gradient - 1.7996333909527156\n",
      "Step - 1768, Loss - 0.6015907203820335, Learning Rate - 0.025, magnitude of gradient - 1.4408097114489553\n",
      "Step - 1769, Loss - 0.6375665863190654, Learning Rate - 0.025, magnitude of gradient - 2.076603687527583\n",
      "Step - 1770, Loss - 0.7611519614596159, Learning Rate - 0.025, magnitude of gradient - 3.017541623240024\n",
      "Step - 1771, Loss - 0.7255478076913293, Learning Rate - 0.025, magnitude of gradient - 1.5857432115046994\n",
      "Step - 1772, Loss - 0.6068105870135695, Learning Rate - 0.025, magnitude of gradient - 1.2573768617184224\n",
      "Step - 1773, Loss - 0.7556415639809702, Learning Rate - 0.025, magnitude of gradient - 1.4652403138414891\n",
      "Step - 1774, Loss - 0.7046386956103975, Learning Rate - 0.025, magnitude of gradient - 0.7122674117228629\n",
      "Step - 1775, Loss - 0.878333349272992, Learning Rate - 0.025, magnitude of gradient - 2.8287670149465916\n",
      "Step - 1776, Loss - 0.6169948971973536, Learning Rate - 0.025, magnitude of gradient - 2.4053526683913504\n",
      "Step - 1777, Loss - 0.6554258840191947, Learning Rate - 0.025, magnitude of gradient - 2.084898678086968\n",
      "Step - 1778, Loss - 0.7547705137772022, Learning Rate - 0.025, magnitude of gradient - 3.155917295790653\n",
      "Step - 1779, Loss - 0.6570493476751502, Learning Rate - 0.025, magnitude of gradient - 3.128596441641368\n",
      "Step - 1780, Loss - 0.73938904434995, Learning Rate - 0.025, magnitude of gradient - 0.33778689156859387\n",
      "Step - 1781, Loss - 0.8151328118659908, Learning Rate - 0.025, magnitude of gradient - 1.912623911144937\n",
      "Step - 1782, Loss - 0.6195272954405064, Learning Rate - 0.025, magnitude of gradient - 1.0431831808645506\n",
      "Step - 1783, Loss - 0.7138799714326266, Learning Rate - 0.025, magnitude of gradient - 2.8248213532221595\n",
      "Step - 1784, Loss - 0.8692567005925823, Learning Rate - 0.025, magnitude of gradient - 1.4521303542565838\n",
      "Step - 1785, Loss - 0.6437804444868442, Learning Rate - 0.025, magnitude of gradient - 1.5243450640297664\n",
      "Step - 1786, Loss - 0.7199614234994071, Learning Rate - 0.025, magnitude of gradient - 0.6415038708770069\n",
      "Step - 1787, Loss - 0.6442025448751301, Learning Rate - 0.025, magnitude of gradient - 2.4109483048106104\n",
      "Step - 1788, Loss - 0.6880927786089602, Learning Rate - 0.025, magnitude of gradient - 1.037714473226108\n",
      "Step - 1789, Loss - 0.5594971489240234, Learning Rate - 0.025, magnitude of gradient - 2.1959273715475947\n",
      "Step - 1790, Loss - 0.714100850890694, Learning Rate - 0.025, magnitude of gradient - 0.5922571643427221\n",
      "Step - 1791, Loss - 0.677362060448052, Learning Rate - 0.025, magnitude of gradient - 1.2758427670521049\n",
      "Step - 1792, Loss - 0.6227083775714455, Learning Rate - 0.025, magnitude of gradient - 0.2328744136382808\n",
      "Step - 1793, Loss - 0.583710809658381, Learning Rate - 0.025, magnitude of gradient - 1.188778665039611\n",
      "Step - 1794, Loss - 0.723103174283322, Learning Rate - 0.025, magnitude of gradient - 0.9052900158095486\n",
      "Step - 1795, Loss - 0.6631920247479599, Learning Rate - 0.025, magnitude of gradient - 1.5518452170391421\n",
      "Step - 1796, Loss - 0.770877702198745, Learning Rate - 0.025, magnitude of gradient - 1.9938413472809768\n",
      "Step - 1797, Loss - 0.7104563731625039, Learning Rate - 0.025, magnitude of gradient - 3.6317422938709565\n",
      "Step - 1798, Loss - 0.5853900997114088, Learning Rate - 0.025, magnitude of gradient - 1.3386991465533702\n",
      "Step - 1799, Loss - 0.7748405574481331, Learning Rate - 0.025, magnitude of gradient - 0.8127005432862331\n",
      "Step - 1800, Loss - 0.5448834873194293, Learning Rate - 0.025, magnitude of gradient - 3.775123940675465\n",
      "Step - 1801, Loss - 0.5231587886193765, Learning Rate - 0.025, magnitude of gradient - 1.4547694498573713\n",
      "Step - 1802, Loss - 0.9104985808739433, Learning Rate - 0.025, magnitude of gradient - 3.390781443706327\n",
      "Step - 1803, Loss - 0.7461044338141893, Learning Rate - 0.025, magnitude of gradient - 2.220418772776407\n",
      "Step - 1804, Loss - 0.9345718630286819, Learning Rate - 0.025, magnitude of gradient - 3.4661364753329647\n",
      "Step - 1805, Loss - 0.8371956718504197, Learning Rate - 0.025, magnitude of gradient - 1.9836371186698012\n",
      "Step - 1806, Loss - 0.8554423313994404, Learning Rate - 0.025, magnitude of gradient - 1.8058264349443742\n",
      "Step - 1807, Loss - 0.5076709851934851, Learning Rate - 0.025, magnitude of gradient - 1.0169506141220466\n",
      "Step - 1808, Loss - 0.6955968364914571, Learning Rate - 0.025, magnitude of gradient - 1.6023213410281347\n",
      "Step - 1809, Loss - 0.6382687011945027, Learning Rate - 0.025, magnitude of gradient - 1.4800919424965113\n",
      "Step - 1810, Loss - 0.7840551203953369, Learning Rate - 0.025, magnitude of gradient - 1.5667356925535099\n",
      "Step - 1811, Loss - 0.797131839761831, Learning Rate - 0.025, magnitude of gradient - 1.2294195206604313\n",
      "Step - 1812, Loss - 0.7740974448667872, Learning Rate - 0.025, magnitude of gradient - 1.2457552415845523\n",
      "Step - 1813, Loss - 0.701937943222442, Learning Rate - 0.025, magnitude of gradient - 1.9962190500027939\n",
      "Step - 1814, Loss - 0.6727570170641363, Learning Rate - 0.025, magnitude of gradient - 0.4613599594441334\n",
      "Step - 1815, Loss - 0.8555146741548846, Learning Rate - 0.025, magnitude of gradient - 1.2762001366551083\n",
      "Step - 1816, Loss - 0.6804998562414128, Learning Rate - 0.025, magnitude of gradient - 0.8656672997209224\n",
      "Step - 1817, Loss - 0.5973075656875264, Learning Rate - 0.025, magnitude of gradient - 1.6843022234451643\n",
      "Step - 1818, Loss - 0.6418040484152636, Learning Rate - 0.025, magnitude of gradient - 1.6466822134704973\n",
      "Step - 1819, Loss - 0.9785314243826168, Learning Rate - 0.025, magnitude of gradient - 4.100363718727259\n",
      "Step - 1820, Loss - 0.6637519011757458, Learning Rate - 0.025, magnitude of gradient - 1.7727104998754237\n",
      "Step - 1821, Loss - 0.6706135172219035, Learning Rate - 0.025, magnitude of gradient - 1.4385100510458593\n",
      "Step - 1822, Loss - 0.7333546563868533, Learning Rate - 0.025, magnitude of gradient - 3.0145443220075223\n",
      "Step - 1823, Loss - 0.679249972357708, Learning Rate - 0.025, magnitude of gradient - 1.3363329443659426\n",
      "Step - 1824, Loss - 0.7874696422890415, Learning Rate - 0.025, magnitude of gradient - 2.246804343012812\n",
      "Step - 1825, Loss - 0.8609667801441501, Learning Rate - 0.025, magnitude of gradient - 0.8543027585561428\n",
      "Step - 1826, Loss - 0.7699651473757854, Learning Rate - 0.025, magnitude of gradient - 1.388189717935599\n",
      "Step - 1827, Loss - 0.655422668241453, Learning Rate - 0.025, magnitude of gradient - 0.4683965873401684\n",
      "Step - 1828, Loss - 0.663813222833828, Learning Rate - 0.025, magnitude of gradient - 1.7815145391255238\n",
      "Step - 1829, Loss - 0.6861638462783535, Learning Rate - 0.025, magnitude of gradient - 0.5886259639022028\n",
      "Step - 1830, Loss - 0.7090069201242266, Learning Rate - 0.025, magnitude of gradient - 0.507147447892441\n",
      "Step - 1831, Loss - 0.595051475877868, Learning Rate - 0.025, magnitude of gradient - 2.8516940420177948\n",
      "Step - 1832, Loss - 0.7991605515180813, Learning Rate - 0.025, magnitude of gradient - 1.5448485792701179\n",
      "Step - 1833, Loss - 0.754023767540157, Learning Rate - 0.025, magnitude of gradient - 1.6145337967281843\n",
      "Step - 1834, Loss - 0.8601196848090559, Learning Rate - 0.025, magnitude of gradient - 1.1594055166356136\n",
      "Step - 1835, Loss - 0.9290086009001223, Learning Rate - 0.025, magnitude of gradient - 1.664768864912167\n",
      "Step - 1836, Loss - 0.5046481620929418, Learning Rate - 0.025, magnitude of gradient - 1.3056698224696761\n",
      "Step - 1837, Loss - 0.73847301009277, Learning Rate - 0.025, magnitude of gradient - 1.2454319247485381\n",
      "Step - 1838, Loss - 0.8088908124379364, Learning Rate - 0.025, magnitude of gradient - 2.564166547513011\n",
      "Step - 1839, Loss - 0.6836346919805929, Learning Rate - 0.025, magnitude of gradient - 2.1616745426611943\n",
      "Step - 1840, Loss - 0.7083262167005713, Learning Rate - 0.025, magnitude of gradient - 1.1632541593696772\n",
      "Step - 1841, Loss - 0.7701960953588615, Learning Rate - 0.025, magnitude of gradient - 3.2370754935645283\n",
      "Step - 1842, Loss - 0.46612056585153033, Learning Rate - 0.025, magnitude of gradient - 1.6981709705050316\n",
      "Step - 1843, Loss - 0.7049454487561279, Learning Rate - 0.025, magnitude of gradient - 3.5150454242710603\n",
      "Step - 1844, Loss - 0.8929738931719032, Learning Rate - 0.025, magnitude of gradient - 1.3828520858625915\n",
      "Step - 1845, Loss - 0.9397880998990199, Learning Rate - 0.025, magnitude of gradient - 3.0248381605505004\n",
      "Step - 1846, Loss - 0.7371532995688456, Learning Rate - 0.025, magnitude of gradient - 2.236298580141988\n",
      "Step - 1847, Loss - 0.9024335306408585, Learning Rate - 0.025, magnitude of gradient - 1.6267435493033784\n",
      "Step - 1848, Loss - 0.7831704983901444, Learning Rate - 0.025, magnitude of gradient - 2.0532083895932183\n",
      "Step - 1849, Loss - 0.6916495213779779, Learning Rate - 0.025, magnitude of gradient - 0.9692848876980041\n",
      "Step - 1850, Loss - 0.6108575227485353, Learning Rate - 0.025, magnitude of gradient - 1.4282203312947017\n",
      "Step - 1851, Loss - 0.6614287364118911, Learning Rate - 0.025, magnitude of gradient - 1.9606885270462209\n",
      "Step - 1852, Loss - 0.68750030985784, Learning Rate - 0.025, magnitude of gradient - 1.181663112126319\n",
      "Step - 1853, Loss - 0.7987946401299175, Learning Rate - 0.025, magnitude of gradient - 1.6805341205534188\n",
      "Step - 1854, Loss - 0.722105589272948, Learning Rate - 0.025, magnitude of gradient - 1.1135506787821208\n",
      "Step - 1855, Loss - 0.8574243585547137, Learning Rate - 0.025, magnitude of gradient - 0.8700204037715388\n",
      "Step - 1856, Loss - 0.8187508654746106, Learning Rate - 0.025, magnitude of gradient - 0.9346154120698272\n",
      "Step - 1857, Loss - 0.811485050779202, Learning Rate - 0.025, magnitude of gradient - 0.7580114485595637\n",
      "Step - 1858, Loss - 0.7422835521133253, Learning Rate - 0.025, magnitude of gradient - 0.5447432957872712\n",
      "Step - 1859, Loss - 0.7807810383870479, Learning Rate - 0.025, magnitude of gradient - 1.5271317619490674\n",
      "Step - 1860, Loss - 0.8476647882956809, Learning Rate - 0.025, magnitude of gradient - 2.851658602842032\n",
      "Step - 1861, Loss - 0.6816021778473795, Learning Rate - 0.025, magnitude of gradient - 2.0004534860378906\n",
      "Step - 1862, Loss - 0.8359348604738874, Learning Rate - 0.025, magnitude of gradient - 1.3586345748614952\n",
      "Step - 1863, Loss - 0.7630359020906939, Learning Rate - 0.025, magnitude of gradient - 1.5627821195198202\n",
      "Step - 1864, Loss - 0.8975054536875599, Learning Rate - 0.025, magnitude of gradient - 2.473691331267696\n",
      "Step - 1865, Loss - 0.465520045484862, Learning Rate - 0.025, magnitude of gradient - 0.6176338870826469\n",
      "Step - 1866, Loss - 0.8953891252982553, Learning Rate - 0.025, magnitude of gradient - 2.0228994387843566\n",
      "Step - 1867, Loss - 0.5466611978445277, Learning Rate - 0.025, magnitude of gradient - 1.3766951333791686\n",
      "Step - 1868, Loss - 0.7112492729369402, Learning Rate - 0.025, magnitude of gradient - 0.8548440041380958\n",
      "Step - 1869, Loss - 0.8189389892256372, Learning Rate - 0.025, magnitude of gradient - 2.770068750320138\n",
      "Step - 1870, Loss - 0.6507218190158606, Learning Rate - 0.025, magnitude of gradient - 2.0340454429338313\n",
      "Step - 1871, Loss - 0.6549359556761291, Learning Rate - 0.025, magnitude of gradient - 1.9277429418969871\n",
      "Step - 1872, Loss - 0.6820955468985658, Learning Rate - 0.025, magnitude of gradient - 1.9009035747052578\n",
      "Step - 1873, Loss - 0.6677921674925713, Learning Rate - 0.025, magnitude of gradient - 0.6105055624918088\n",
      "Step - 1874, Loss - 0.9187196168817233, Learning Rate - 0.025, magnitude of gradient - 2.448735292091741\n",
      "Step - 1875, Loss - 0.5472164506836891, Learning Rate - 0.025, magnitude of gradient - 1.6719663881741063\n",
      "Step - 1876, Loss - 0.7941599068821499, Learning Rate - 0.025, magnitude of gradient - 2.3038103739055966\n",
      "Step - 1877, Loss - 0.6759099565665847, Learning Rate - 0.025, magnitude of gradient - 0.6529289426135899\n",
      "Step - 1878, Loss - 0.7006855324977992, Learning Rate - 0.025, magnitude of gradient - 2.4514372710994836\n",
      "Step - 1879, Loss - 0.8988234385381146, Learning Rate - 0.025, magnitude of gradient - 2.555673742844961\n",
      "Step - 1880, Loss - 0.7789198516403902, Learning Rate - 0.025, magnitude of gradient - 2.8734196644701973\n",
      "Step - 1881, Loss - 0.8162316308352249, Learning Rate - 0.025, magnitude of gradient - 1.2531403798446301\n",
      "Step - 1882, Loss - 0.9441422620091784, Learning Rate - 0.025, magnitude of gradient - 2.486180608276839\n",
      "Step - 1883, Loss - 0.5583041594111171, Learning Rate - 0.025, magnitude of gradient - 1.170746176432838\n",
      "Step - 1884, Loss - 0.575073651390041, Learning Rate - 0.025, magnitude of gradient - 2.9189501550396733\n",
      "Step - 1885, Loss - 0.5801395831512893, Learning Rate - 0.025, magnitude of gradient - 0.9302148603050482\n",
      "Step - 1886, Loss - 0.780852988299199, Learning Rate - 0.025, magnitude of gradient - 2.574753463472257\n",
      "Step - 1887, Loss - 0.6991481135764063, Learning Rate - 0.025, magnitude of gradient - 1.6933188568194013\n",
      "Step - 1888, Loss - 0.6267846206145885, Learning Rate - 0.025, magnitude of gradient - 0.37303048798711225\n",
      "Step - 1889, Loss - 0.9013202438391883, Learning Rate - 0.025, magnitude of gradient - 1.208831543147437\n",
      "Step - 1890, Loss - 0.6290641709070225, Learning Rate - 0.025, magnitude of gradient - 1.8342361682336406\n",
      "Step - 1891, Loss - 0.7631312364842948, Learning Rate - 0.025, magnitude of gradient - 1.1083088282469236\n",
      "Step - 1892, Loss - 0.5811302803076718, Learning Rate - 0.025, magnitude of gradient - 3.595202585583987\n",
      "Step - 1893, Loss - 0.6873794534941551, Learning Rate - 0.025, magnitude of gradient - 0.6188034589724455\n",
      "Step - 1894, Loss - 0.6589703940173788, Learning Rate - 0.025, magnitude of gradient - 1.5486017961232388\n",
      "Step - 1895, Loss - 0.686715529894922, Learning Rate - 0.025, magnitude of gradient - 2.42648977129212\n",
      "Step - 1896, Loss - 0.7532044307008212, Learning Rate - 0.025, magnitude of gradient - 3.0400236558343456\n",
      "Step - 1897, Loss - 0.709272937952334, Learning Rate - 0.025, magnitude of gradient - 0.8491565766270156\n",
      "Step - 1898, Loss - 0.546590331534323, Learning Rate - 0.025, magnitude of gradient - 2.4163660023446374\n",
      "Step - 1899, Loss - 0.6514496654225158, Learning Rate - 0.025, magnitude of gradient - 3.774382887496093\n",
      "Step - 1900, Loss - 0.6928148920644039, Learning Rate - 0.025, magnitude of gradient - 0.2272788863806466\n",
      "Step - 1901, Loss - 1.028428221947898, Learning Rate - 0.025, magnitude of gradient - 3.1975161435073125\n",
      "Step - 1902, Loss - 0.8616218039120558, Learning Rate - 0.025, magnitude of gradient - 1.4144750571482096\n",
      "Step - 1903, Loss - 0.5751369709718329, Learning Rate - 0.025, magnitude of gradient - 3.7943491815550994\n",
      "Step - 1904, Loss - 0.645516745578964, Learning Rate - 0.025, magnitude of gradient - 1.2278025683357836\n",
      "Step - 1905, Loss - 0.82674142721718, Learning Rate - 0.025, magnitude of gradient - 1.9547189921599462\n",
      "Step - 1906, Loss - 0.7981599059279079, Learning Rate - 0.025, magnitude of gradient - 1.5610292112442263\n",
      "Step - 1907, Loss - 0.9649869884407614, Learning Rate - 0.025, magnitude of gradient - 1.7643737873474734\n",
      "Step - 1908, Loss - 0.7174199394888554, Learning Rate - 0.025, magnitude of gradient - 2.510147776863159\n",
      "Step - 1909, Loss - 0.8214312601299909, Learning Rate - 0.025, magnitude of gradient - 1.3749652781345625\n",
      "Step - 1910, Loss - 0.9297977651141959, Learning Rate - 0.025, magnitude of gradient - 0.658207167876523\n",
      "Step - 1911, Loss - 0.5791187755514803, Learning Rate - 0.025, magnitude of gradient - 2.8096296861309797\n",
      "Step - 1912, Loss - 0.6720236702018834, Learning Rate - 0.025, magnitude of gradient - 4.07445316592709\n",
      "Step - 1913, Loss - 0.5915169600727935, Learning Rate - 0.025, magnitude of gradient - 0.9811757623915833\n",
      "Step - 1914, Loss - 0.6664767035805217, Learning Rate - 0.025, magnitude of gradient - 1.6088771964870372\n",
      "Step - 1915, Loss - 0.58615426439053, Learning Rate - 0.025, magnitude of gradient - 0.48340195406764525\n",
      "Step - 1916, Loss - 0.609803604584163, Learning Rate - 0.025, magnitude of gradient - 0.9494546632361318\n",
      "Step - 1917, Loss - 0.7451513551312064, Learning Rate - 0.025, magnitude of gradient - 1.4875806887675378\n",
      "Step - 1918, Loss - 0.7223449468666523, Learning Rate - 0.025, magnitude of gradient - 2.615114857008653\n",
      "Step - 1919, Loss - 0.6676527997646998, Learning Rate - 0.025, magnitude of gradient - 1.438046477472246\n",
      "Step - 1920, Loss - 0.6772688765727655, Learning Rate - 0.025, magnitude of gradient - 3.8211208425812915\n",
      "Step - 1921, Loss - 0.8406982273349896, Learning Rate - 0.025, magnitude of gradient - 2.7595411052197494\n",
      "Step - 1922, Loss - 0.806437424749323, Learning Rate - 0.025, magnitude of gradient - 1.2880583284710896\n",
      "Step - 1923, Loss - 0.7789596100117055, Learning Rate - 0.025, magnitude of gradient - 0.44294482659082735\n",
      "Step - 1924, Loss - 0.8266454764989593, Learning Rate - 0.025, magnitude of gradient - 2.446221628836664\n",
      "Step - 1925, Loss - 0.747431643903646, Learning Rate - 0.025, magnitude of gradient - 2.042279566658805\n",
      "Step - 1926, Loss - 0.7089964018447238, Learning Rate - 0.025, magnitude of gradient - 0.7389557944133537\n",
      "Step - 1927, Loss - 0.843088043669806, Learning Rate - 0.025, magnitude of gradient - 1.5978751641068505\n",
      "Step - 1928, Loss - 0.6098061206033037, Learning Rate - 0.025, magnitude of gradient - 2.54661174728505\n",
      "Step - 1929, Loss - 0.6508275674157706, Learning Rate - 0.025, magnitude of gradient - 0.8164449307927336\n",
      "Step - 1930, Loss - 0.6773151571064947, Learning Rate - 0.025, magnitude of gradient - 0.9516920731412148\n",
      "Step - 1931, Loss - 0.8806081164268159, Learning Rate - 0.025, magnitude of gradient - 0.611246900645195\n",
      "Step - 1932, Loss - 0.8897570275642847, Learning Rate - 0.025, magnitude of gradient - 2.368907584010739\n",
      "Step - 1933, Loss - 0.7118467381984008, Learning Rate - 0.025, magnitude of gradient - 1.647471922894601\n",
      "Step - 1934, Loss - 0.7122418074765386, Learning Rate - 0.025, magnitude of gradient - 2.047787051137204\n",
      "Step - 1935, Loss - 0.6577085922353079, Learning Rate - 0.025, magnitude of gradient - 0.6400832212687354\n",
      "Step - 1936, Loss - 0.6498749944850719, Learning Rate - 0.025, magnitude of gradient - 0.8665571767183945\n",
      "Step - 1937, Loss - 0.767523154378495, Learning Rate - 0.025, magnitude of gradient - 1.093014005609974\n",
      "Step - 1938, Loss - 0.7958404148309859, Learning Rate - 0.025, magnitude of gradient - 1.9045994073576755\n",
      "Step - 1939, Loss - 0.74953324757452, Learning Rate - 0.025, magnitude of gradient - 1.8212475936330517\n",
      "Step - 1940, Loss - 0.8366641391908729, Learning Rate - 0.025, magnitude of gradient - 2.018266202578326\n",
      "Step - 1941, Loss - 0.6157194424666941, Learning Rate - 0.025, magnitude of gradient - 0.9313074748247325\n",
      "Step - 1942, Loss - 0.7170346663466443, Learning Rate - 0.025, magnitude of gradient - 2.748560290984887\n",
      "Step - 1943, Loss - 0.6649357910132361, Learning Rate - 0.025, magnitude of gradient - 0.5134368157502103\n",
      "Step - 1944, Loss - 0.6460348717144591, Learning Rate - 0.025, magnitude of gradient - 0.37400438088853555\n",
      "Step - 1945, Loss - 0.7721050359571611, Learning Rate - 0.025, magnitude of gradient - 1.323072350461054\n",
      "Step - 1946, Loss - 0.8230661718683911, Learning Rate - 0.025, magnitude of gradient - 3.054440224834559\n",
      "Step - 1947, Loss - 0.5981962302003438, Learning Rate - 0.025, magnitude of gradient - 0.925884809786191\n",
      "Step - 1948, Loss - 0.549275088819836, Learning Rate - 0.025, magnitude of gradient - 1.9908075592049708\n",
      "Step - 1949, Loss - 0.8206964527395818, Learning Rate - 0.025, magnitude of gradient - 1.732415726568554\n",
      "Step - 1950, Loss - 0.8331490790049827, Learning Rate - 0.025, magnitude of gradient - 1.1359203138131486\n",
      "Step - 1951, Loss - 0.7668565716946191, Learning Rate - 0.025, magnitude of gradient - 1.66714755494989\n",
      "Step - 1952, Loss - 0.6043192176703808, Learning Rate - 0.025, magnitude of gradient - 1.1658252081522231\n",
      "Step - 1953, Loss - 0.8467872714378404, Learning Rate - 0.025, magnitude of gradient - 1.9167517245468149\n",
      "Step - 1954, Loss - 0.5956891521598382, Learning Rate - 0.025, magnitude of gradient - 1.3830935980953412\n",
      "Step - 1955, Loss - 0.7676195869249682, Learning Rate - 0.025, magnitude of gradient - 0.8238488379359739\n",
      "Step - 1956, Loss - 0.47661647777698607, Learning Rate - 0.025, magnitude of gradient - 1.1392970920131011\n",
      "Step - 1957, Loss - 0.7191316623775176, Learning Rate - 0.025, magnitude of gradient - 1.7223171775577202\n",
      "Step - 1958, Loss - 0.5668393348334867, Learning Rate - 0.025, magnitude of gradient - 1.7231715692582052\n",
      "Step - 1959, Loss - 0.6626963223158583, Learning Rate - 0.025, magnitude of gradient - 0.5452399974298923\n",
      "Step - 1960, Loss - 0.7387989490944757, Learning Rate - 0.025, magnitude of gradient - 1.7498852329928527\n",
      "Step - 1961, Loss - 0.634988484607086, Learning Rate - 0.025, magnitude of gradient - 2.4909515370057282\n",
      "Step - 1962, Loss - 0.6500625974832788, Learning Rate - 0.025, magnitude of gradient - 1.8506424142248357\n",
      "Step - 1963, Loss - 0.6717496270337224, Learning Rate - 0.025, magnitude of gradient - 0.998162103700541\n",
      "Step - 1964, Loss - 0.810860519758678, Learning Rate - 0.025, magnitude of gradient - 2.139572701025174\n",
      "Step - 1965, Loss - 0.6801733718798266, Learning Rate - 0.025, magnitude of gradient - 0.9239333283811907\n",
      "Step - 1966, Loss - 0.4671424204438916, Learning Rate - 0.025, magnitude of gradient - 1.5972175974700922\n",
      "Step - 1967, Loss - 0.6747843540301957, Learning Rate - 0.025, magnitude of gradient - 3.2283262470502443\n",
      "Step - 1968, Loss - 0.7755712920694315, Learning Rate - 0.025, magnitude of gradient - 0.8137517335793935\n",
      "Step - 1969, Loss - 0.6435605992932412, Learning Rate - 0.025, magnitude of gradient - 1.6316569272788408\n",
      "Step - 1970, Loss - 0.6865131423132976, Learning Rate - 0.025, magnitude of gradient - 2.0695582161180885\n",
      "Step - 1971, Loss - 0.653076069164018, Learning Rate - 0.025, magnitude of gradient - 0.33506353378815645\n",
      "Step - 1972, Loss - 0.9630716266049024, Learning Rate - 0.025, magnitude of gradient - 2.7283218726694654\n",
      "Step - 1973, Loss - 0.8024568897497641, Learning Rate - 0.025, magnitude of gradient - 0.9975079023346307\n",
      "Step - 1974, Loss - 0.7656067291980351, Learning Rate - 0.025, magnitude of gradient - 2.1886358855150543\n",
      "Step - 1975, Loss - 0.7212370365613913, Learning Rate - 0.025, magnitude of gradient - 0.7164816782814297\n",
      "Step - 1976, Loss - 0.7859598059030295, Learning Rate - 0.025, magnitude of gradient - 0.6024930686770653\n",
      "Step - 1977, Loss - 0.6873326704633501, Learning Rate - 0.025, magnitude of gradient - 3.1408839427447517\n",
      "Step - 1978, Loss - 0.8060183296903815, Learning Rate - 0.025, magnitude of gradient - 0.4622420813949978\n",
      "Step - 1979, Loss - 0.4599686521031023, Learning Rate - 0.025, magnitude of gradient - 2.6164736947044833\n",
      "Step - 1980, Loss - 0.6066365460194318, Learning Rate - 0.025, magnitude of gradient - 0.8961766091239608\n",
      "Step - 1981, Loss - 0.6861004952660663, Learning Rate - 0.025, magnitude of gradient - 3.537389632045842\n",
      "Step - 1982, Loss - 0.7969412632053557, Learning Rate - 0.025, magnitude of gradient - 2.129079001581982\n",
      "Step - 1983, Loss - 0.7221172505612852, Learning Rate - 0.025, magnitude of gradient - 2.6508118656078197\n",
      "Step - 1984, Loss - 0.8754692431334059, Learning Rate - 0.025, magnitude of gradient - 1.3562818795173623\n",
      "Step - 1985, Loss - 1.0471164474427732, Learning Rate - 0.025, magnitude of gradient - 1.3452845797995683\n",
      "Step - 1986, Loss - 0.8292038170763458, Learning Rate - 0.025, magnitude of gradient - 1.1134005125756754\n",
      "Step - 1987, Loss - 0.7735594644366985, Learning Rate - 0.025, magnitude of gradient - 3.0992770449117275\n",
      "Step - 1988, Loss - 0.8339499410750075, Learning Rate - 0.025, magnitude of gradient - 2.6211299999925397\n",
      "Step - 1989, Loss - 0.7001712577551769, Learning Rate - 0.025, magnitude of gradient - 0.7843553610700128\n",
      "Step - 1990, Loss - 0.6169532247734753, Learning Rate - 0.025, magnitude of gradient - 2.2088020742516155\n",
      "Step - 1991, Loss - 0.6164538013225079, Learning Rate - 0.025, magnitude of gradient - 1.656799943735429\n",
      "Step - 1992, Loss - 0.6330212636410242, Learning Rate - 0.025, magnitude of gradient - 1.1512490201609424\n",
      "Step - 1993, Loss - 0.6204232045373517, Learning Rate - 0.025, magnitude of gradient - 1.4190419713415914\n",
      "Step - 1994, Loss - 0.5167781052023157, Learning Rate - 0.025, magnitude of gradient - 1.5997174990556047\n",
      "Step - 1995, Loss - 0.7032030641758208, Learning Rate - 0.025, magnitude of gradient - 3.11939100969609\n",
      "Step - 1996, Loss - 0.5708628819214958, Learning Rate - 0.025, magnitude of gradient - 0.8875208157551291\n",
      "Step - 1997, Loss - 0.7342101534965002, Learning Rate - 0.025, magnitude of gradient - 1.4947071612002878\n",
      "Step - 1998, Loss - 0.791785279112923, Learning Rate - 0.025, magnitude of gradient - 1.2348999949075037\n",
      "Step - 1999, Loss - 0.6628720245372204, Learning Rate - 0.025, magnitude of gradient - 1.5001853885422245\n",
      "Step - 2000, Loss - 0.7656132490420324, Learning Rate - 0.025, magnitude of gradient - 1.468873030924387\n",
      "Step - 2001, Loss - 0.8791927461114571, Learning Rate - 0.0125, magnitude of gradient - 1.1751336384268174\n",
      "Step - 2002, Loss - 0.7949166847992715, Learning Rate - 0.0125, magnitude of gradient - 0.5933017358234437\n",
      "Step - 2003, Loss - 0.6354450296801079, Learning Rate - 0.0125, magnitude of gradient - 2.423603743880862\n",
      "Step - 2004, Loss - 0.7464838040779831, Learning Rate - 0.0125, magnitude of gradient - 0.5641623205651631\n",
      "Step - 2005, Loss - 0.6944429791843446, Learning Rate - 0.0125, magnitude of gradient - 0.9651070566951516\n",
      "Step - 2006, Loss - 0.870246869086602, Learning Rate - 0.0125, magnitude of gradient - 1.6425477801135504\n",
      "Step - 2007, Loss - 0.665924084910854, Learning Rate - 0.0125, magnitude of gradient - 1.9474731744113327\n",
      "Step - 2008, Loss - 0.7465309238236911, Learning Rate - 0.0125, magnitude of gradient - 1.8271511774458085\n",
      "Step - 2009, Loss - 0.7901689261698286, Learning Rate - 0.0125, magnitude of gradient - 2.1716197039889815\n",
      "Step - 2010, Loss - 0.5563729847789318, Learning Rate - 0.0125, magnitude of gradient - 0.7404572608284948\n",
      "Step - 2011, Loss - 0.8598981414958855, Learning Rate - 0.0125, magnitude of gradient - 1.2258312016652313\n",
      "Step - 2012, Loss - 0.6618608823177149, Learning Rate - 0.0125, magnitude of gradient - 0.9674066244294848\n",
      "Step - 2013, Loss - 0.6053131257642204, Learning Rate - 0.0125, magnitude of gradient - 1.2348399647873052\n",
      "Step - 2014, Loss - 0.7707265206148135, Learning Rate - 0.0125, magnitude of gradient - 0.8006918576798233\n",
      "Step - 2015, Loss - 0.5399471980309316, Learning Rate - 0.0125, magnitude of gradient - 1.2151739716819983\n",
      "Step - 2016, Loss - 0.6131148241654367, Learning Rate - 0.0125, magnitude of gradient - 1.7721635813644299\n",
      "Step - 2017, Loss - 0.5882664873185801, Learning Rate - 0.0125, magnitude of gradient - 0.9715374094610513\n",
      "Step - 2018, Loss - 0.7790263933716153, Learning Rate - 0.0125, magnitude of gradient - 0.3699436219978866\n",
      "Step - 2019, Loss - 0.6943849835129366, Learning Rate - 0.0125, magnitude of gradient - 1.2283710446153029\n",
      "Step - 2020, Loss - 0.6846592548476368, Learning Rate - 0.0125, magnitude of gradient - 0.8517718149335877\n",
      "Step - 2021, Loss - 0.9554400540836717, Learning Rate - 0.0125, magnitude of gradient - 1.5962877431029299\n",
      "Step - 2022, Loss - 0.7395612743164381, Learning Rate - 0.0125, magnitude of gradient - 1.123327207672956\n",
      "Step - 2023, Loss - 0.5522065060124762, Learning Rate - 0.0125, magnitude of gradient - 1.4217836213953965\n",
      "Step - 2024, Loss - 0.802020966899542, Learning Rate - 0.0125, magnitude of gradient - 0.5661503707807681\n",
      "Step - 2025, Loss - 0.6001339125375489, Learning Rate - 0.0125, magnitude of gradient - 0.9071650125543256\n",
      "Step - 2026, Loss - 0.5934648450754108, Learning Rate - 0.0125, magnitude of gradient - 0.4224847961110985\n",
      "Step - 2027, Loss - 0.6434150750906709, Learning Rate - 0.0125, magnitude of gradient - 1.614823169681276\n",
      "Step - 2028, Loss - 0.5578895207180288, Learning Rate - 0.0125, magnitude of gradient - 1.526862682228848\n",
      "Step - 2029, Loss - 0.6243814226033905, Learning Rate - 0.0125, magnitude of gradient - 2.5928919380707516\n",
      "Step - 2030, Loss - 0.7954489779417966, Learning Rate - 0.0125, magnitude of gradient - 1.4348982806037418\n",
      "Step - 2031, Loss - 0.6817772505900317, Learning Rate - 0.0125, magnitude of gradient - 1.9357576858737247\n",
      "Step - 2032, Loss - 0.7284476961246225, Learning Rate - 0.0125, magnitude of gradient - 1.4378587793585489\n",
      "Step - 2033, Loss - 0.657478681467947, Learning Rate - 0.0125, magnitude of gradient - 2.2614924803865186\n",
      "Step - 2034, Loss - 0.7880181364721689, Learning Rate - 0.0125, magnitude of gradient - 2.4328508183745656\n",
      "Step - 2035, Loss - 0.70732303340231, Learning Rate - 0.0125, magnitude of gradient - 1.1954843598556522\n",
      "Step - 2036, Loss - 0.7818473748706979, Learning Rate - 0.0125, magnitude of gradient - 1.42918079614919\n",
      "Step - 2037, Loss - 0.9687411923739548, Learning Rate - 0.0125, magnitude of gradient - 1.5391032769112643\n",
      "Step - 2038, Loss - 0.7907948094064225, Learning Rate - 0.0125, magnitude of gradient - 2.505381306051018\n",
      "Step - 2039, Loss - 0.6880789404630969, Learning Rate - 0.0125, magnitude of gradient - 0.6542090127596043\n",
      "Step - 2040, Loss - 1.049512893108691, Learning Rate - 0.0125, magnitude of gradient - 1.0940251998069128\n",
      "Step - 2041, Loss - 0.838699735411597, Learning Rate - 0.0125, magnitude of gradient - 0.9874235528334875\n",
      "Step - 2042, Loss - 0.7003747057571912, Learning Rate - 0.0125, magnitude of gradient - 1.3450605807083986\n",
      "Step - 2043, Loss - 0.6493166308735853, Learning Rate - 0.0125, magnitude of gradient - 0.9293808644317704\n",
      "Step - 2044, Loss - 0.6596295911365551, Learning Rate - 0.0125, magnitude of gradient - 1.291726503842013\n",
      "Step - 2045, Loss - 0.6166971025319034, Learning Rate - 0.0125, magnitude of gradient - 0.98279074928443\n",
      "Step - 2046, Loss - 0.7130642802084003, Learning Rate - 0.0125, magnitude of gradient - 1.1519975533000566\n",
      "Step - 2047, Loss - 0.7875596967919266, Learning Rate - 0.0125, magnitude of gradient - 1.5065847290561658\n",
      "Step - 2048, Loss - 0.685608328352954, Learning Rate - 0.0125, magnitude of gradient - 1.8560213394345344\n",
      "Step - 2049, Loss - 0.5957610658670333, Learning Rate - 0.0125, magnitude of gradient - 1.6299578354213589\n",
      "Step - 2050, Loss - 0.9183749874747524, Learning Rate - 0.0125, magnitude of gradient - 2.6452924375310682\n",
      "Step - 2051, Loss - 0.6994210625203081, Learning Rate - 0.0125, magnitude of gradient - 0.9235697951530639\n",
      "Step - 2052, Loss - 0.6777156776403853, Learning Rate - 0.0125, magnitude of gradient - 1.7111422600606592\n",
      "Step - 2053, Loss - 0.7710876810460712, Learning Rate - 0.0125, magnitude of gradient - 2.370047428632697\n",
      "Step - 2054, Loss - 0.6571667230685266, Learning Rate - 0.0125, magnitude of gradient - 1.0274160741211589\n",
      "Step - 2055, Loss - 0.6641197836500232, Learning Rate - 0.0125, magnitude of gradient - 2.625064642060922\n",
      "Step - 2056, Loss - 0.49028424076758437, Learning Rate - 0.0125, magnitude of gradient - 1.2179074751464227\n",
      "Step - 2057, Loss - 0.6477483718573095, Learning Rate - 0.0125, magnitude of gradient - 1.0520314441673864\n",
      "Step - 2058, Loss - 0.6925675286917952, Learning Rate - 0.0125, magnitude of gradient - 1.5860715352333226\n",
      "Step - 2059, Loss - 0.5056549145073407, Learning Rate - 0.0125, magnitude of gradient - 0.7734879756643819\n",
      "Step - 2060, Loss - 0.6362986789694565, Learning Rate - 0.0125, magnitude of gradient - 1.967393897521297\n",
      "Step - 2061, Loss - 0.662579891554399, Learning Rate - 0.0125, magnitude of gradient - 0.690766057154385\n",
      "Step - 2062, Loss - 0.6287819921695759, Learning Rate - 0.0125, magnitude of gradient - 0.9623597043027898\n",
      "Step - 2063, Loss - 0.5774692561762623, Learning Rate - 0.0125, magnitude of gradient - 2.2569366995510123\n",
      "Step - 2064, Loss - 0.696230955577895, Learning Rate - 0.0125, magnitude of gradient - 1.3764462430061997\n",
      "Step - 2065, Loss - 0.5994397047848687, Learning Rate - 0.0125, magnitude of gradient - 1.716741736812586\n",
      "Step - 2066, Loss - 0.9360748710030538, Learning Rate - 0.0125, magnitude of gradient - 1.2615410955549102\n",
      "Step - 2067, Loss - 0.5419576464891306, Learning Rate - 0.0125, magnitude of gradient - 0.6335780288120744\n",
      "Step - 2068, Loss - 0.8593357389837789, Learning Rate - 0.0125, magnitude of gradient - 2.158491897206394\n",
      "Step - 2069, Loss - 0.6596450592041184, Learning Rate - 0.0125, magnitude of gradient - 1.5980192564335773\n",
      "Step - 2070, Loss - 0.8323329831350632, Learning Rate - 0.0125, magnitude of gradient - 1.2856876544652334\n",
      "Step - 2071, Loss - 0.5158314439067114, Learning Rate - 0.0125, magnitude of gradient - 0.6228012599762287\n",
      "Step - 2072, Loss - 0.7372243385518471, Learning Rate - 0.0125, magnitude of gradient - 0.6327405867668798\n",
      "Step - 2073, Loss - 0.6769575412485942, Learning Rate - 0.0125, magnitude of gradient - 1.427512765086288\n",
      "Step - 2074, Loss - 0.6444104334531973, Learning Rate - 0.0125, magnitude of gradient - 0.5757782683156529\n",
      "Step - 2075, Loss - 0.6266026677574148, Learning Rate - 0.0125, magnitude of gradient - 1.0521513118007317\n",
      "Step - 2076, Loss - 0.7793241094737376, Learning Rate - 0.0125, magnitude of gradient - 2.0179978427543217\n",
      "Step - 2077, Loss - 0.726070109199032, Learning Rate - 0.0125, magnitude of gradient - 1.193860124354371\n",
      "Step - 2078, Loss - 0.7755874172917335, Learning Rate - 0.0125, magnitude of gradient - 1.7554534730998819\n",
      "Step - 2079, Loss - 0.8445460610062897, Learning Rate - 0.0125, magnitude of gradient - 0.6477161302140396\n",
      "Step - 2080, Loss - 0.7748233949550811, Learning Rate - 0.0125, magnitude of gradient - 2.098423664177808\n",
      "Step - 2081, Loss - 0.5091456474824971, Learning Rate - 0.0125, magnitude of gradient - 0.6438445974299185\n",
      "Step - 2082, Loss - 0.5803662687417532, Learning Rate - 0.0125, magnitude of gradient - 1.8923442414508254\n",
      "Step - 2083, Loss - 0.8476872073152475, Learning Rate - 0.0125, magnitude of gradient - 2.56066573128129\n",
      "Step - 2084, Loss - 0.6877849899568867, Learning Rate - 0.0125, magnitude of gradient - 0.9918379481753172\n",
      "Step - 2085, Loss - 0.5901090840152129, Learning Rate - 0.0125, magnitude of gradient - 1.7231707689572628\n",
      "Step - 2086, Loss - 0.587632611414432, Learning Rate - 0.0125, magnitude of gradient - 1.114312004351451\n",
      "Step - 2087, Loss - 0.5273547423817524, Learning Rate - 0.0125, magnitude of gradient - 1.029502075568689\n",
      "Step - 2088, Loss - 0.7649164415975939, Learning Rate - 0.0125, magnitude of gradient - 2.849765660681219\n",
      "Step - 2089, Loss - 0.712055859382038, Learning Rate - 0.0125, magnitude of gradient - 1.5191823060024616\n",
      "Step - 2090, Loss - 0.6677154218630731, Learning Rate - 0.0125, magnitude of gradient - 0.5519823749040056\n",
      "Step - 2091, Loss - 0.5037252168667077, Learning Rate - 0.0125, magnitude of gradient - 0.6229996052298065\n",
      "Step - 2092, Loss - 0.8502641148188136, Learning Rate - 0.0125, magnitude of gradient - 2.829257640442928\n",
      "Step - 2093, Loss - 0.549093781908784, Learning Rate - 0.0125, magnitude of gradient - 0.7837841617937145\n",
      "Step - 2094, Loss - 0.795938761028592, Learning Rate - 0.0125, magnitude of gradient - 3.359384854211392\n",
      "Step - 2095, Loss - 0.7426416815856387, Learning Rate - 0.0125, magnitude of gradient - 2.659047941120965\n",
      "Step - 2096, Loss - 0.692667941969158, Learning Rate - 0.0125, magnitude of gradient - 2.2747236841581744\n",
      "Step - 2097, Loss - 0.72670701278742, Learning Rate - 0.0125, magnitude of gradient - 2.3400089880990445\n",
      "Step - 2098, Loss - 0.5439086474662856, Learning Rate - 0.0125, magnitude of gradient - 1.3029277457318869\n",
      "Step - 2099, Loss - 0.7059879372630369, Learning Rate - 0.0125, magnitude of gradient - 2.4160983884095986\n",
      "Step - 2100, Loss - 0.6528861934067425, Learning Rate - 0.0125, magnitude of gradient - 0.9349144217866693\n",
      "Step - 2101, Loss - 0.6698091508042217, Learning Rate - 0.0125, magnitude of gradient - 1.592996612744858\n",
      "Step - 2102, Loss - 0.6252141501478897, Learning Rate - 0.0125, magnitude of gradient - 1.1859116648199755\n",
      "Step - 2103, Loss - 0.7218872233964438, Learning Rate - 0.0125, magnitude of gradient - 1.1104601105294971\n",
      "Step - 2104, Loss - 0.6023884987229633, Learning Rate - 0.0125, magnitude of gradient - 1.7345752161618286\n",
      "Step - 2105, Loss - 0.8879026227625192, Learning Rate - 0.0125, magnitude of gradient - 1.17477937033108\n",
      "Step - 2106, Loss - 0.9216875677486567, Learning Rate - 0.0125, magnitude of gradient - 0.7004563771431281\n",
      "Step - 2107, Loss - 0.8119455765391015, Learning Rate - 0.0125, magnitude of gradient - 0.5551621064688094\n",
      "Step - 2108, Loss - 0.6656109161354524, Learning Rate - 0.0125, magnitude of gradient - 0.9552095159002226\n",
      "Step - 2109, Loss - 0.8374718392112285, Learning Rate - 0.0125, magnitude of gradient - 0.7222162852477156\n",
      "Step - 2110, Loss - 0.9076503286523177, Learning Rate - 0.0125, magnitude of gradient - 0.6245580533040226\n",
      "Step - 2111, Loss - 0.7320259880698449, Learning Rate - 0.0125, magnitude of gradient - 2.6594004701443836\n",
      "Step - 2112, Loss - 0.6289920894919517, Learning Rate - 0.0125, magnitude of gradient - 1.7459069055548164\n",
      "Step - 2113, Loss - 0.7992760173810863, Learning Rate - 0.0125, magnitude of gradient - 0.8052003027240318\n",
      "Step - 2114, Loss - 0.7270681964232616, Learning Rate - 0.0125, magnitude of gradient - 0.8937769298342897\n",
      "Step - 2115, Loss - 0.597518783226122, Learning Rate - 0.0125, magnitude of gradient - 2.0680290405087374\n",
      "Step - 2116, Loss - 1.0176394289989932, Learning Rate - 0.0125, magnitude of gradient - 3.399176276601939\n",
      "Step - 2117, Loss - 0.6706703307553918, Learning Rate - 0.0125, magnitude of gradient - 1.437637272785215\n",
      "Step - 2118, Loss - 0.6409230940319613, Learning Rate - 0.0125, magnitude of gradient - 0.7015923547297479\n",
      "Step - 2119, Loss - 0.7847014271797024, Learning Rate - 0.0125, magnitude of gradient - 1.050888674981854\n",
      "Step - 2120, Loss - 0.7938299420885773, Learning Rate - 0.0125, magnitude of gradient - 0.5265792343832958\n",
      "Step - 2121, Loss - 0.7334907733319723, Learning Rate - 0.0125, magnitude of gradient - 1.0179693067019493\n",
      "Step - 2122, Loss - 0.7952629224143885, Learning Rate - 0.0125, magnitude of gradient - 1.1135512289827207\n",
      "Step - 2123, Loss - 0.7040514485503482, Learning Rate - 0.0125, magnitude of gradient - 2.4862723786584597\n",
      "Step - 2124, Loss - 0.7580537583119129, Learning Rate - 0.0125, magnitude of gradient - 1.8366886420579849\n",
      "Step - 2125, Loss - 0.6933282389739793, Learning Rate - 0.0125, magnitude of gradient - 2.0454845067030742\n",
      "Step - 2126, Loss - 0.7620622820259897, Learning Rate - 0.0125, magnitude of gradient - 1.1246855506458386\n",
      "Step - 2127, Loss - 0.7543166299367775, Learning Rate - 0.0125, magnitude of gradient - 1.4090629185714878\n",
      "Step - 2128, Loss - 0.7251667484890169, Learning Rate - 0.0125, magnitude of gradient - 2.7597237381597117\n",
      "Step - 2129, Loss - 0.6326082704093549, Learning Rate - 0.0125, magnitude of gradient - 1.8247195803189122\n",
      "Step - 2130, Loss - 0.7954649796335438, Learning Rate - 0.0125, magnitude of gradient - 2.792110997772201\n",
      "Step - 2131, Loss - 0.6340876861024679, Learning Rate - 0.0125, magnitude of gradient - 0.8317725926002353\n",
      "Step - 2132, Loss - 0.8404982220746439, Learning Rate - 0.0125, magnitude of gradient - 1.7555084041291413\n",
      "Step - 2133, Loss - 0.5005628535259621, Learning Rate - 0.0125, magnitude of gradient - 1.6465062551177978\n",
      "Step - 2134, Loss - 0.537772213290184, Learning Rate - 0.0125, magnitude of gradient - 1.374230952114726\n",
      "Step - 2135, Loss - 0.7060843955001194, Learning Rate - 0.0125, magnitude of gradient - 1.9729301595349726\n",
      "Step - 2136, Loss - 0.7061888951847823, Learning Rate - 0.0125, magnitude of gradient - 1.26343411321702\n",
      "Step - 2137, Loss - 0.5904909305881911, Learning Rate - 0.0125, magnitude of gradient - 1.5382569246737565\n",
      "Step - 2138, Loss - 0.7627734455986391, Learning Rate - 0.0125, magnitude of gradient - 1.7085679548861628\n",
      "Step - 2139, Loss - 0.6811550845041047, Learning Rate - 0.0125, magnitude of gradient - 1.6644654762179187\n",
      "Step - 2140, Loss - 0.6098615166578889, Learning Rate - 0.0125, magnitude of gradient - 1.7760627631610686\n",
      "Step - 2141, Loss - 0.6773149981346086, Learning Rate - 0.0125, magnitude of gradient - 0.6033897396452093\n",
      "Step - 2142, Loss - 0.8575045953032291, Learning Rate - 0.0125, magnitude of gradient - 1.0052950377326706\n",
      "Step - 2143, Loss - 0.805021580025086, Learning Rate - 0.0125, magnitude of gradient - 2.064031019114439\n",
      "Step - 2144, Loss - 0.5755383484899144, Learning Rate - 0.0125, magnitude of gradient - 0.9924389697243319\n",
      "Step - 2145, Loss - 0.5969975038899831, Learning Rate - 0.0125, magnitude of gradient - 1.8454926122815558\n",
      "Step - 2146, Loss - 0.8126772055342338, Learning Rate - 0.0125, magnitude of gradient - 2.233406536799749\n",
      "Step - 2147, Loss - 0.846014419729601, Learning Rate - 0.0125, magnitude of gradient - 1.8443349491388175\n",
      "Step - 2148, Loss - 0.8256530266800846, Learning Rate - 0.0125, magnitude of gradient - 0.9311543697878847\n",
      "Step - 2149, Loss - 0.6376296187487495, Learning Rate - 0.0125, magnitude of gradient - 1.7960551649642162\n",
      "Step - 2150, Loss - 0.5363515948436381, Learning Rate - 0.0125, magnitude of gradient - 1.047720981780391\n",
      "Step - 2151, Loss - 0.6805266277687838, Learning Rate - 0.0125, magnitude of gradient - 1.2431806069239268\n",
      "Step - 2152, Loss - 0.7421935472050125, Learning Rate - 0.0125, magnitude of gradient - 1.7675321491433087\n",
      "Step - 2153, Loss - 0.7405000196247684, Learning Rate - 0.0125, magnitude of gradient - 1.045754525283138\n",
      "Step - 2154, Loss - 0.8777988616140662, Learning Rate - 0.0125, magnitude of gradient - 1.2153334374986942\n",
      "Step - 2155, Loss - 0.9238387754139621, Learning Rate - 0.0125, magnitude of gradient - 2.4383316020650883\n",
      "Step - 2156, Loss - 0.7694973956707984, Learning Rate - 0.0125, magnitude of gradient - 0.9727445461324822\n",
      "Step - 2157, Loss - 0.6492694686750308, Learning Rate - 0.0125, magnitude of gradient - 1.9056240089360117\n",
      "Step - 2158, Loss - 0.9802904878139885, Learning Rate - 0.0125, magnitude of gradient - 3.418391012207761\n",
      "Step - 2159, Loss - 0.6578232486089297, Learning Rate - 0.0125, magnitude of gradient - 1.27558553002906\n",
      "Step - 2160, Loss - 0.9301242226038291, Learning Rate - 0.0125, magnitude of gradient - 1.093245007021482\n",
      "Step - 2161, Loss - 0.6547304838506328, Learning Rate - 0.0125, magnitude of gradient - 2.3046380035953185\n",
      "Step - 2162, Loss - 0.6147350182985324, Learning Rate - 0.0125, magnitude of gradient - 0.7233936554865811\n",
      "Step - 2163, Loss - 0.750389625000663, Learning Rate - 0.0125, magnitude of gradient - 2.0853372263113363\n",
      "Step - 2164, Loss - 0.7921334082032426, Learning Rate - 0.0125, magnitude of gradient - 1.16995385658165\n",
      "Step - 2165, Loss - 0.5003723296952733, Learning Rate - 0.0125, magnitude of gradient - 1.5868373349584746\n",
      "Step - 2166, Loss - 0.621732791723973, Learning Rate - 0.0125, magnitude of gradient - 2.592997996379586\n",
      "Step - 2167, Loss - 0.6427940560388189, Learning Rate - 0.0125, magnitude of gradient - 0.40570200909427534\n",
      "Step - 2168, Loss - 0.7681796861552798, Learning Rate - 0.0125, magnitude of gradient - 1.8114501492617392\n",
      "Step - 2169, Loss - 0.9080186877556092, Learning Rate - 0.0125, magnitude of gradient - 2.526461932917235\n",
      "Step - 2170, Loss - 0.6008198558425879, Learning Rate - 0.0125, magnitude of gradient - 2.064796386808526\n",
      "Step - 2171, Loss - 0.6723371822507326, Learning Rate - 0.0125, magnitude of gradient - 1.1597132707018354\n",
      "Step - 2172, Loss - 0.7642656478941167, Learning Rate - 0.0125, magnitude of gradient - 1.259393636157787\n",
      "Step - 2173, Loss - 0.6362732151074546, Learning Rate - 0.0125, magnitude of gradient - 2.0338393180040426\n",
      "Step - 2174, Loss - 0.8845180602995822, Learning Rate - 0.0125, magnitude of gradient - 0.9752061742966531\n",
      "Step - 2175, Loss - 0.7012626455967217, Learning Rate - 0.0125, magnitude of gradient - 1.3587893245205682\n",
      "Step - 2176, Loss - 0.770603753498728, Learning Rate - 0.0125, magnitude of gradient - 1.8030696277684215\n",
      "Step - 2177, Loss - 0.6128740471901262, Learning Rate - 0.0125, magnitude of gradient - 2.00876980965947\n",
      "Step - 2178, Loss - 0.5591261386856408, Learning Rate - 0.0125, magnitude of gradient - 0.9518477142306013\n",
      "Step - 2179, Loss - 0.7451253950484035, Learning Rate - 0.0125, magnitude of gradient - 2.387252287086586\n",
      "Step - 2180, Loss - 0.8478676989699697, Learning Rate - 0.0125, magnitude of gradient - 2.5862200875973307\n",
      "Step - 2181, Loss - 0.7266386483938503, Learning Rate - 0.0125, magnitude of gradient - 0.4398042980770075\n",
      "Step - 2182, Loss - 0.7228410632839335, Learning Rate - 0.0125, magnitude of gradient - 0.9606836001096201\n",
      "Step - 2183, Loss - 0.4839606420776709, Learning Rate - 0.0125, magnitude of gradient - 1.0526096855285343\n",
      "Step - 2184, Loss - 0.6697817650307993, Learning Rate - 0.0125, magnitude of gradient - 0.6242613986647029\n",
      "Step - 2185, Loss - 0.7729552161536529, Learning Rate - 0.0125, magnitude of gradient - 1.1352661190064417\n",
      "Step - 2186, Loss - 0.6310590612743954, Learning Rate - 0.0125, magnitude of gradient - 2.20221614725084\n",
      "Step - 2187, Loss - 0.811757254421577, Learning Rate - 0.0125, magnitude of gradient - 1.3705742886277836\n",
      "Step - 2188, Loss - 0.6180625491224468, Learning Rate - 0.0125, magnitude of gradient - 2.1413867028694775\n",
      "Step - 2189, Loss - 0.7267320120953135, Learning Rate - 0.0125, magnitude of gradient - 3.2957638120740897\n",
      "Step - 2190, Loss - 0.915733196239022, Learning Rate - 0.0125, magnitude of gradient - 2.1175913148394834\n",
      "Step - 2191, Loss - 0.6204023969224846, Learning Rate - 0.0125, magnitude of gradient - 1.8822477585991004\n",
      "Step - 2192, Loss - 0.6758339246242218, Learning Rate - 0.0125, magnitude of gradient - 1.7985170869213316\n",
      "Step - 2193, Loss - 0.6644292059500201, Learning Rate - 0.0125, magnitude of gradient - 0.5243093986619627\n",
      "Step - 2194, Loss - 0.7384986701759501, Learning Rate - 0.0125, magnitude of gradient - 0.8148777207752376\n",
      "Step - 2195, Loss - 0.5776522168940998, Learning Rate - 0.0125, magnitude of gradient - 2.224219061960889\n",
      "Step - 2196, Loss - 0.6459922781908491, Learning Rate - 0.0125, magnitude of gradient - 2.513756795542827\n",
      "Step - 2197, Loss - 0.5766995455891021, Learning Rate - 0.0125, magnitude of gradient - 2.2084708152068684\n",
      "Step - 2198, Loss - 0.7614861032946093, Learning Rate - 0.0125, magnitude of gradient - 1.121568607582297\n",
      "Step - 2199, Loss - 0.8709983315215137, Learning Rate - 0.0125, magnitude of gradient - 1.1509141307950121\n",
      "Step - 2200, Loss - 0.7501580097864888, Learning Rate - 0.0125, magnitude of gradient - 0.5803526288950402\n",
      "Step - 2201, Loss - 0.69667140875789, Learning Rate - 0.0125, magnitude of gradient - 2.021195765070394\n",
      "Step - 2202, Loss - 0.6822618374870033, Learning Rate - 0.0125, magnitude of gradient - 1.1973002303330629\n",
      "Step - 2203, Loss - 0.5673405422336584, Learning Rate - 0.0125, magnitude of gradient - 2.075642558935226\n",
      "Step - 2204, Loss - 0.555196796339361, Learning Rate - 0.0125, magnitude of gradient - 1.979563735427082\n",
      "Step - 2205, Loss - 0.6254589760281583, Learning Rate - 0.0125, magnitude of gradient - 1.3222310524295002\n",
      "Step - 2206, Loss - 0.7008591111490098, Learning Rate - 0.0125, magnitude of gradient - 0.9900507034551105\n",
      "Step - 2207, Loss - 0.6237209198145562, Learning Rate - 0.0125, magnitude of gradient - 1.3515266670095576\n",
      "Step - 2208, Loss - 0.4753691436217724, Learning Rate - 0.0125, magnitude of gradient - 1.193407368263414\n",
      "Step - 2209, Loss - 0.5389623538984426, Learning Rate - 0.0125, magnitude of gradient - 2.3966523391751378\n",
      "Step - 2210, Loss - 0.8203194926598483, Learning Rate - 0.0125, magnitude of gradient - 1.907458757819235\n",
      "Step - 2211, Loss - 0.7600308880030985, Learning Rate - 0.0125, magnitude of gradient - 0.8191721628667612\n",
      "Step - 2212, Loss - 0.7481629387998752, Learning Rate - 0.0125, magnitude of gradient - 1.1710497659738943\n",
      "Step - 2213, Loss - 0.6865110011217381, Learning Rate - 0.0125, magnitude of gradient - 0.848343599026349\n",
      "Step - 2214, Loss - 0.7016761189708293, Learning Rate - 0.0125, magnitude of gradient - 1.6500679056478151\n",
      "Step - 2215, Loss - 0.6764542020356621, Learning Rate - 0.0125, magnitude of gradient - 0.8846843709573001\n",
      "Step - 2216, Loss - 0.6789156110912244, Learning Rate - 0.0125, magnitude of gradient - 1.3136368257391746\n",
      "Step - 2217, Loss - 0.6695823769395485, Learning Rate - 0.0125, magnitude of gradient - 2.137348717727588\n",
      "Step - 2218, Loss - 0.5862747546482556, Learning Rate - 0.0125, magnitude of gradient - 1.1584862755105678\n",
      "Step - 2219, Loss - 0.7401631573461793, Learning Rate - 0.0125, magnitude of gradient - 1.5532718718506568\n",
      "Step - 2220, Loss - 0.6494319462133648, Learning Rate - 0.0125, magnitude of gradient - 1.525270735228021\n",
      "Step - 2221, Loss - 0.606417806785227, Learning Rate - 0.0125, magnitude of gradient - 0.805906907145235\n",
      "Step - 2222, Loss - 0.6651715023132578, Learning Rate - 0.0125, magnitude of gradient - 1.3708630065745055\n",
      "Step - 2223, Loss - 0.7502094891219844, Learning Rate - 0.0125, magnitude of gradient - 0.7578430604194049\n",
      "Step - 2224, Loss - 0.7795744495644148, Learning Rate - 0.0125, magnitude of gradient - 1.261573132717067\n",
      "Step - 2225, Loss - 0.7931720131240254, Learning Rate - 0.0125, magnitude of gradient - 1.8885885166674847\n",
      "Step - 2226, Loss - 0.7415753734050394, Learning Rate - 0.0125, magnitude of gradient - 1.3130218509669036\n",
      "Step - 2227, Loss - 0.9855490638900957, Learning Rate - 0.0125, magnitude of gradient - 1.1819294245523941\n",
      "Step - 2228, Loss - 0.7027653411176156, Learning Rate - 0.0125, magnitude of gradient - 1.2373249865216842\n",
      "Step - 2229, Loss - 0.4765706816368321, Learning Rate - 0.0125, magnitude of gradient - 2.019880622175753\n",
      "Step - 2230, Loss - 0.7234182746220642, Learning Rate - 0.0125, magnitude of gradient - 0.6527761623866044\n",
      "Step - 2231, Loss - 0.8018873978673224, Learning Rate - 0.0125, magnitude of gradient - 1.6132867348216826\n",
      "Step - 2232, Loss - 0.7815079174736801, Learning Rate - 0.0125, magnitude of gradient - 0.7682874628374725\n",
      "Step - 2233, Loss - 0.8122492009964459, Learning Rate - 0.0125, magnitude of gradient - 3.2644052210096675\n",
      "Step - 2234, Loss - 0.773465594682369, Learning Rate - 0.0125, magnitude of gradient - 0.8500200951747366\n",
      "Step - 2235, Loss - 0.7611820002468977, Learning Rate - 0.0125, magnitude of gradient - 1.845244015013375\n",
      "Step - 2236, Loss - 0.8116905776785576, Learning Rate - 0.0125, magnitude of gradient - 0.5509866186182454\n",
      "Step - 2237, Loss - 0.6120918168632257, Learning Rate - 0.0125, magnitude of gradient - 1.0105263151138695\n",
      "Step - 2238, Loss - 0.7518734162008118, Learning Rate - 0.0125, magnitude of gradient - 2.1481429161735783\n",
      "Step - 2239, Loss - 0.6944114722066265, Learning Rate - 0.0125, magnitude of gradient - 1.0259491036829984\n",
      "Step - 2240, Loss - 0.764944505031283, Learning Rate - 0.0125, magnitude of gradient - 0.7594238418923461\n",
      "Step - 2241, Loss - 0.8484726962445948, Learning Rate - 0.0125, magnitude of gradient - 0.993688979825336\n",
      "Step - 2242, Loss - 0.7396050813914702, Learning Rate - 0.0125, magnitude of gradient - 1.3554454071295994\n",
      "Step - 2243, Loss - 0.7156960628285363, Learning Rate - 0.0125, magnitude of gradient - 0.8035313342198432\n",
      "Step - 2244, Loss - 0.7502600120841122, Learning Rate - 0.0125, magnitude of gradient - 1.776848680017573\n",
      "Step - 2245, Loss - 0.5121976866997634, Learning Rate - 0.0125, magnitude of gradient - 1.2540611288520584\n",
      "Step - 2246, Loss - 0.622966845596157, Learning Rate - 0.0125, magnitude of gradient - 1.5105276889677608\n",
      "Step - 2247, Loss - 0.530904667294918, Learning Rate - 0.0125, magnitude of gradient - 1.1542289520813538\n",
      "Step - 2248, Loss - 0.7484791884004867, Learning Rate - 0.0125, magnitude of gradient - 2.438978553207821\n",
      "Step - 2249, Loss - 0.5273071083891818, Learning Rate - 0.0125, magnitude of gradient - 1.3867175660657844\n",
      "Step - 2250, Loss - 0.543901176477129, Learning Rate - 0.0125, magnitude of gradient - 1.5070283928365866\n",
      "Step - 2251, Loss - 0.5194070602148453, Learning Rate - 0.0125, magnitude of gradient - 1.9315048272524344\n",
      "Step - 2252, Loss - 0.6890088002816767, Learning Rate - 0.0125, magnitude of gradient - 1.32042101962669\n",
      "Step - 2253, Loss - 0.6999175995192211, Learning Rate - 0.0125, magnitude of gradient - 0.9973875010402313\n",
      "Step - 2254, Loss - 0.6592323968154568, Learning Rate - 0.0125, magnitude of gradient - 0.6332907213495036\n",
      "Step - 2255, Loss - 0.649704919322399, Learning Rate - 0.0125, magnitude of gradient - 0.8211494157771845\n",
      "Step - 2256, Loss - 0.6561354782738814, Learning Rate - 0.0125, magnitude of gradient - 2.090668585536597\n",
      "Step - 2257, Loss - 0.8529250839767164, Learning Rate - 0.0125, magnitude of gradient - 2.033325321903977\n",
      "Step - 2258, Loss - 0.6476444531274093, Learning Rate - 0.0125, magnitude of gradient - 1.413789508938865\n",
      "Step - 2259, Loss - 0.6834141184149747, Learning Rate - 0.0125, magnitude of gradient - 2.9512950663390214\n",
      "Step - 2260, Loss - 0.9133238687557538, Learning Rate - 0.0125, magnitude of gradient - 2.550128388973544\n",
      "Step - 2261, Loss - 0.7946730660115475, Learning Rate - 0.0125, magnitude of gradient - 1.1494526051503697\n",
      "Step - 2262, Loss - 0.8506353377786284, Learning Rate - 0.0125, magnitude of gradient - 2.6333039501352946\n",
      "Step - 2263, Loss - 0.6194993353714644, Learning Rate - 0.0125, magnitude of gradient - 1.3033780417767022\n",
      "Step - 2264, Loss - 0.5049747103907827, Learning Rate - 0.0125, magnitude of gradient - 1.0604038713086097\n",
      "Step - 2265, Loss - 0.8137438058746844, Learning Rate - 0.0125, magnitude of gradient - 2.2178723984263415\n",
      "Step - 2266, Loss - 0.6737067630457755, Learning Rate - 0.0125, magnitude of gradient - 1.283560840915189\n",
      "Step - 2267, Loss - 0.5058189665227778, Learning Rate - 0.0125, magnitude of gradient - 1.1745507390903418\n",
      "Step - 2268, Loss - 0.5718869661882852, Learning Rate - 0.0125, magnitude of gradient - 1.1060981753445263\n",
      "Step - 2269, Loss - 0.7131905541091121, Learning Rate - 0.0125, magnitude of gradient - 1.2725022735166043\n",
      "Step - 2270, Loss - 0.5426815429952554, Learning Rate - 0.0125, magnitude of gradient - 0.8573001844369508\n",
      "Step - 2271, Loss - 0.591628276771608, Learning Rate - 0.0125, magnitude of gradient - 2.076949792005512\n",
      "Step - 2272, Loss - 0.7863739939522503, Learning Rate - 0.0125, magnitude of gradient - 1.6779368715774023\n",
      "Step - 2273, Loss - 0.6785524005843092, Learning Rate - 0.0125, magnitude of gradient - 1.7374797173455998\n",
      "Step - 2274, Loss - 0.7596065365068738, Learning Rate - 0.0125, magnitude of gradient - 1.4749670850467946\n",
      "Step - 2275, Loss - 0.7551580398124942, Learning Rate - 0.0125, magnitude of gradient - 3.316841257295055\n",
      "Step - 2276, Loss - 0.8562302395649466, Learning Rate - 0.0125, magnitude of gradient - 1.8308183663528956\n",
      "Step - 2277, Loss - 0.7787266706505896, Learning Rate - 0.0125, magnitude of gradient - 0.7226897645067801\n",
      "Step - 2278, Loss - 0.6627597084940833, Learning Rate - 0.0125, magnitude of gradient - 2.23998706853466\n",
      "Step - 2279, Loss - 0.6031780768597111, Learning Rate - 0.0125, magnitude of gradient - 1.6999100742210105\n",
      "Step - 2280, Loss - 0.7350921474002754, Learning Rate - 0.0125, magnitude of gradient - 2.528670984391441\n",
      "Step - 2281, Loss - 0.6646846096427175, Learning Rate - 0.0125, magnitude of gradient - 1.3775219805746015\n",
      "Step - 2282, Loss - 0.6388761165148642, Learning Rate - 0.0125, magnitude of gradient - 1.5060514460587207\n",
      "Step - 2283, Loss - 0.9104549163098629, Learning Rate - 0.0125, magnitude of gradient - 1.9512412572293272\n",
      "Step - 2284, Loss - 0.6731395856017603, Learning Rate - 0.0125, magnitude of gradient - 2.9510717377113234\n",
      "Step - 2285, Loss - 0.5466051839743314, Learning Rate - 0.0125, magnitude of gradient - 1.3753716494293426\n",
      "Step - 2286, Loss - 0.6242531647994193, Learning Rate - 0.0125, magnitude of gradient - 1.1305402744952924\n",
      "Step - 2287, Loss - 0.6570203727419921, Learning Rate - 0.0125, magnitude of gradient - 1.5677163614685639\n",
      "Step - 2288, Loss - 0.8684112846917817, Learning Rate - 0.0125, magnitude of gradient - 1.852752545459735\n",
      "Step - 2289, Loss - 0.7140308506451976, Learning Rate - 0.0125, magnitude of gradient - 0.390500148761781\n",
      "Step - 2290, Loss - 0.6867266396241325, Learning Rate - 0.0125, magnitude of gradient - 1.9721110308590075\n",
      "Step - 2291, Loss - 0.613643915580051, Learning Rate - 0.0125, magnitude of gradient - 2.4172182893790746\n",
      "Step - 2292, Loss - 0.7712342130193617, Learning Rate - 0.0125, magnitude of gradient - 2.208207436623525\n",
      "Step - 2293, Loss - 0.7448603005376601, Learning Rate - 0.0125, magnitude of gradient - 1.2337129679116676\n",
      "Step - 2294, Loss - 0.7066986896020283, Learning Rate - 0.0125, magnitude of gradient - 2.8956186631783303\n",
      "Step - 2295, Loss - 0.5893152570597959, Learning Rate - 0.0125, magnitude of gradient - 1.3308792285679645\n",
      "Step - 2296, Loss - 0.7335391475917548, Learning Rate - 0.0125, magnitude of gradient - 2.535559668132396\n",
      "Step - 2297, Loss - 0.8282494166667949, Learning Rate - 0.0125, magnitude of gradient - 1.2228675764560204\n",
      "Step - 2298, Loss - 0.8046684300062219, Learning Rate - 0.0125, magnitude of gradient - 1.379570987583227\n",
      "Step - 2299, Loss - 0.5140548915738236, Learning Rate - 0.0125, magnitude of gradient - 1.0897317539327491\n",
      "Step - 2300, Loss - 0.6791127823316861, Learning Rate - 0.0125, magnitude of gradient - 1.7308837372012156\n",
      "Step - 2301, Loss - 0.7938291158318636, Learning Rate - 0.0125, magnitude of gradient - 1.1425937211737154\n",
      "Step - 2302, Loss - 0.6589115027530548, Learning Rate - 0.0125, magnitude of gradient - 1.4780271088958803\n",
      "Step - 2303, Loss - 0.7256159205913274, Learning Rate - 0.0125, magnitude of gradient - 2.3806570679596866\n",
      "Step - 2304, Loss - 0.7243529121430332, Learning Rate - 0.0125, magnitude of gradient - 0.8546639884420694\n",
      "Step - 2305, Loss - 0.6514377529197105, Learning Rate - 0.0125, magnitude of gradient - 2.6918935958507677\n",
      "Step - 2306, Loss - 0.6359823406005618, Learning Rate - 0.0125, magnitude of gradient - 1.3167687851440133\n",
      "Step - 2307, Loss - 0.6667690961778534, Learning Rate - 0.0125, magnitude of gradient - 0.95081095318324\n",
      "Step - 2308, Loss - 0.6802562651452594, Learning Rate - 0.0125, magnitude of gradient - 2.541537088097368\n",
      "Step - 2309, Loss - 0.5544473022327476, Learning Rate - 0.0125, magnitude of gradient - 1.1048777968080183\n",
      "Step - 2310, Loss - 0.6761058123833628, Learning Rate - 0.0125, magnitude of gradient - 1.8377964402708649\n",
      "Step - 2311, Loss - 0.6860735869308898, Learning Rate - 0.0125, magnitude of gradient - 1.285225648521351\n",
      "Step - 2312, Loss - 0.7461449691353017, Learning Rate - 0.0125, magnitude of gradient - 2.2208697962752533\n",
      "Step - 2313, Loss - 0.6946591514263033, Learning Rate - 0.0125, magnitude of gradient - 1.2925622024090127\n",
      "Step - 2314, Loss - 0.733515113214329, Learning Rate - 0.0125, magnitude of gradient - 0.6796017952602106\n",
      "Step - 2315, Loss - 0.7957979206933331, Learning Rate - 0.0125, magnitude of gradient - 1.0288081742404074\n",
      "Step - 2316, Loss - 0.7098836247756902, Learning Rate - 0.0125, magnitude of gradient - 1.2728132078609045\n",
      "Step - 2317, Loss - 0.9807515795246626, Learning Rate - 0.0125, magnitude of gradient - 0.7052668602159656\n",
      "Step - 2318, Loss - 0.6652112681813442, Learning Rate - 0.0125, magnitude of gradient - 1.6010932282973573\n",
      "Step - 2319, Loss - 0.6588113372171105, Learning Rate - 0.0125, magnitude of gradient - 2.689170668519567\n",
      "Step - 2320, Loss - 0.8087613155437803, Learning Rate - 0.0125, magnitude of gradient - 1.80436141468634\n",
      "Step - 2321, Loss - 0.625987522909872, Learning Rate - 0.0125, magnitude of gradient - 1.395612724078555\n",
      "Step - 2322, Loss - 0.748213219252161, Learning Rate - 0.0125, magnitude of gradient - 0.7244814882863638\n",
      "Step - 2323, Loss - 0.585235222962761, Learning Rate - 0.0125, magnitude of gradient - 1.6743405956305883\n",
      "Step - 2324, Loss - 0.61064191890996, Learning Rate - 0.0125, magnitude of gradient - 1.3935838060240398\n",
      "Step - 2325, Loss - 0.651374123144837, Learning Rate - 0.0125, magnitude of gradient - 1.3753491767083117\n",
      "Step - 2326, Loss - 0.5555455474476716, Learning Rate - 0.0125, magnitude of gradient - 2.4888901756417074\n",
      "Step - 2327, Loss - 0.853953982481748, Learning Rate - 0.0125, magnitude of gradient - 2.3788771205866683\n",
      "Step - 2328, Loss - 0.8793526377417402, Learning Rate - 0.0125, magnitude of gradient - 1.8801056485764254\n",
      "Step - 2329, Loss - 0.7936916220496717, Learning Rate - 0.0125, magnitude of gradient - 1.727314793116037\n",
      "Step - 2330, Loss - 0.7486162755420989, Learning Rate - 0.0125, magnitude of gradient - 4.703322376088892\n",
      "Step - 2331, Loss - 0.7141136930483385, Learning Rate - 0.0125, magnitude of gradient - 0.6718989581161251\n",
      "Step - 2332, Loss - 0.7180195663598643, Learning Rate - 0.0125, magnitude of gradient - 0.4881067860031412\n",
      "Step - 2333, Loss - 0.9222490355607326, Learning Rate - 0.0125, magnitude of gradient - 2.167295183645771\n",
      "Step - 2334, Loss - 0.7304131312171311, Learning Rate - 0.0125, magnitude of gradient - 1.4779937733973167\n",
      "Step - 2335, Loss - 0.5838680791038757, Learning Rate - 0.0125, magnitude of gradient - 1.6779263762096606\n",
      "Step - 2336, Loss - 0.5599934726119773, Learning Rate - 0.0125, magnitude of gradient - 1.5834467333510582\n",
      "Step - 2337, Loss - 0.4993698916250101, Learning Rate - 0.0125, magnitude of gradient - 1.6396951928914512\n",
      "Step - 2338, Loss - 0.6287080322479687, Learning Rate - 0.0125, magnitude of gradient - 0.802505168171068\n",
      "Step - 2339, Loss - 0.7698906466221241, Learning Rate - 0.0125, magnitude of gradient - 1.9111873841005749\n",
      "Step - 2340, Loss - 0.613076084479357, Learning Rate - 0.0125, magnitude of gradient - 1.3060768206503155\n",
      "Step - 2341, Loss - 0.9673572906953214, Learning Rate - 0.0125, magnitude of gradient - 3.0003506984056627\n",
      "Step - 2342, Loss - 0.6178504049356104, Learning Rate - 0.0125, magnitude of gradient - 0.8956033018145714\n",
      "Step - 2343, Loss - 0.606960210405346, Learning Rate - 0.0125, magnitude of gradient - 1.3381439667169877\n",
      "Step - 2344, Loss - 0.5268166204438145, Learning Rate - 0.0125, magnitude of gradient - 1.0481733377575666\n",
      "Step - 2345, Loss - 0.6905985182287809, Learning Rate - 0.0125, magnitude of gradient - 0.7860890056997333\n",
      "Step - 2346, Loss - 0.5094440705441786, Learning Rate - 0.0125, magnitude of gradient - 2.1429637238418513\n",
      "Step - 2347, Loss - 0.6633931495928398, Learning Rate - 0.0125, magnitude of gradient - 1.6438784438436111\n",
      "Step - 2348, Loss - 0.6667980282482174, Learning Rate - 0.0125, magnitude of gradient - 1.3468184656772406\n",
      "Step - 2349, Loss - 0.8306607144603686, Learning Rate - 0.0125, magnitude of gradient - 3.116321669314642\n",
      "Step - 2350, Loss - 0.7760028919263133, Learning Rate - 0.0125, magnitude of gradient - 1.671974514651422\n",
      "Step - 2351, Loss - 0.5473624409087311, Learning Rate - 0.0125, magnitude of gradient - 1.8915683709095537\n",
      "Step - 2352, Loss - 0.7947901211231796, Learning Rate - 0.0125, magnitude of gradient - 0.9488945375931925\n",
      "Step - 2353, Loss - 0.6764102598602608, Learning Rate - 0.0125, magnitude of gradient - 2.5218746026537744\n",
      "Step - 2354, Loss - 0.8874789405016675, Learning Rate - 0.0125, magnitude of gradient - 2.6609172506126315\n",
      "Step - 2355, Loss - 0.6219047570287461, Learning Rate - 0.0125, magnitude of gradient - 0.9401324726052014\n",
      "Step - 2356, Loss - 0.5919216945718542, Learning Rate - 0.0125, magnitude of gradient - 0.774945574377744\n",
      "Step - 2357, Loss - 0.6295437271872164, Learning Rate - 0.0125, magnitude of gradient - 1.7207903955755934\n",
      "Step - 2358, Loss - 0.5399432680990295, Learning Rate - 0.0125, magnitude of gradient - 1.0900121286188014\n",
      "Step - 2359, Loss - 0.6682264071396865, Learning Rate - 0.0125, magnitude of gradient - 1.6410264145629132\n",
      "Step - 2360, Loss - 0.650169357760327, Learning Rate - 0.0125, magnitude of gradient - 2.0915023091272036\n",
      "Step - 2361, Loss - 0.8590069203198839, Learning Rate - 0.0125, magnitude of gradient - 2.6157373000877966\n",
      "Step - 2362, Loss - 0.6576686758822674, Learning Rate - 0.0125, magnitude of gradient - 1.621551543505269\n",
      "Step - 2363, Loss - 0.860068843991809, Learning Rate - 0.0125, magnitude of gradient - 1.728384771721844\n",
      "Step - 2364, Loss - 0.5608395438192583, Learning Rate - 0.0125, magnitude of gradient - 1.2703540042171788\n",
      "Step - 2365, Loss - 0.6752245053445632, Learning Rate - 0.0125, magnitude of gradient - 1.0758691561943934\n",
      "Step - 2366, Loss - 0.7452116493809763, Learning Rate - 0.0125, magnitude of gradient - 0.4468996905626504\n",
      "Step - 2367, Loss - 0.6998170675937884, Learning Rate - 0.0125, magnitude of gradient - 1.5934127086210303\n",
      "Step - 2368, Loss - 0.7628094164370882, Learning Rate - 0.0125, magnitude of gradient - 2.177475513348916\n",
      "Step - 2369, Loss - 0.6782816319217077, Learning Rate - 0.0125, magnitude of gradient - 1.0840690884022113\n",
      "Step - 2370, Loss - 0.7038262652328258, Learning Rate - 0.0125, magnitude of gradient - 3.516778974371963\n",
      "Step - 2371, Loss - 0.7784379238209029, Learning Rate - 0.0125, magnitude of gradient - 1.8676956317052682\n",
      "Step - 2372, Loss - 0.7693189479059435, Learning Rate - 0.0125, magnitude of gradient - 1.4264075941801557\n",
      "Step - 2373, Loss - 0.5521459104206179, Learning Rate - 0.0125, magnitude of gradient - 0.5763689825167724\n",
      "Step - 2374, Loss - 0.6631659651753198, Learning Rate - 0.0125, magnitude of gradient - 4.206830412897329\n",
      "Step - 2375, Loss - 0.645119383207552, Learning Rate - 0.0125, magnitude of gradient - 0.9553998020911748\n",
      "Step - 2376, Loss - 0.6084392117205426, Learning Rate - 0.0125, magnitude of gradient - 1.478585584154513\n",
      "Step - 2377, Loss - 0.8010262641051265, Learning Rate - 0.0125, magnitude of gradient - 0.9126892503929481\n",
      "Step - 2378, Loss - 0.8655606710540577, Learning Rate - 0.0125, magnitude of gradient - 2.889043933750006\n",
      "Step - 2379, Loss - 0.766401385238383, Learning Rate - 0.0125, magnitude of gradient - 2.093387713835747\n",
      "Step - 2380, Loss - 0.8747425382649577, Learning Rate - 0.0125, magnitude of gradient - 1.6092219649119668\n",
      "Step - 2381, Loss - 0.6917541245148568, Learning Rate - 0.0125, magnitude of gradient - 1.843148019378594\n",
      "Step - 2382, Loss - 0.7166724175642776, Learning Rate - 0.0125, magnitude of gradient - 1.733666778960957\n",
      "Step - 2383, Loss - 0.6330979165596454, Learning Rate - 0.0125, magnitude of gradient - 1.226497520662207\n",
      "Step - 2384, Loss - 0.7655851226411104, Learning Rate - 0.0125, magnitude of gradient - 1.9927971454331628\n",
      "Step - 2385, Loss - 0.7961706545403515, Learning Rate - 0.0125, magnitude of gradient - 2.889063935041922\n",
      "Step - 2386, Loss - 0.6133481823185369, Learning Rate - 0.0125, magnitude of gradient - 1.642125483375586\n",
      "Step - 2387, Loss - 0.7743141779075698, Learning Rate - 0.0125, magnitude of gradient - 1.4568639121566718\n",
      "Step - 2388, Loss - 0.6038701112231227, Learning Rate - 0.0125, magnitude of gradient - 1.1516937131037608\n",
      "Step - 2389, Loss - 0.6705588160876181, Learning Rate - 0.0125, magnitude of gradient - 1.3756961350673929\n",
      "Step - 2390, Loss - 0.6211659726369388, Learning Rate - 0.0125, magnitude of gradient - 1.1991971797952512\n",
      "Step - 2391, Loss - 0.6407287384149525, Learning Rate - 0.0125, magnitude of gradient - 2.5366451416752693\n",
      "Step - 2392, Loss - 0.7702431455622115, Learning Rate - 0.0125, magnitude of gradient - 1.612370475636457\n",
      "Step - 2393, Loss - 0.6059363487697857, Learning Rate - 0.0125, magnitude of gradient - 0.6776598477310632\n",
      "Step - 2394, Loss - 0.6943972621412579, Learning Rate - 0.0125, magnitude of gradient - 1.4906352233952322\n",
      "Step - 2395, Loss - 0.7055574934391022, Learning Rate - 0.0125, magnitude of gradient - 2.5429792315510684\n",
      "Step - 2396, Loss - 0.7755188147250526, Learning Rate - 0.0125, magnitude of gradient - 0.5861884375216725\n",
      "Step - 2397, Loss - 0.6310264704984867, Learning Rate - 0.0125, magnitude of gradient - 2.475795486301514\n",
      "Step - 2398, Loss - 0.6732048620322846, Learning Rate - 0.0125, magnitude of gradient - 2.3867967916427957\n",
      "Step - 2399, Loss - 0.6499326638126798, Learning Rate - 0.0125, magnitude of gradient - 0.473821162775585\n",
      "Step - 2400, Loss - 0.5834328298326791, Learning Rate - 0.0125, magnitude of gradient - 2.311734877438897\n",
      "Step - 2401, Loss - 0.7056283774403653, Learning Rate - 0.0125, magnitude of gradient - 1.454077481051154\n",
      "Step - 2402, Loss - 0.7090269317926483, Learning Rate - 0.0125, magnitude of gradient - 1.6092989773205204\n",
      "Step - 2403, Loss - 0.6759194231612842, Learning Rate - 0.0125, magnitude of gradient - 0.7999335788667131\n",
      "Step - 2404, Loss - 0.8651571731901984, Learning Rate - 0.0125, magnitude of gradient - 2.5624542587752344\n",
      "Step - 2405, Loss - 0.6573933918356158, Learning Rate - 0.0125, magnitude of gradient - 1.1388485192797326\n",
      "Step - 2406, Loss - 0.8262222722625637, Learning Rate - 0.0125, magnitude of gradient - 0.5893199388492473\n",
      "Step - 2407, Loss - 0.7669371238953977, Learning Rate - 0.0125, magnitude of gradient - 1.5338761956118263\n",
      "Step - 2408, Loss - 0.6715979008168486, Learning Rate - 0.0125, magnitude of gradient - 1.657979470393694\n",
      "Step - 2409, Loss - 0.7772228559237113, Learning Rate - 0.0125, magnitude of gradient - 2.303546445727798\n",
      "Step - 2410, Loss - 0.7459205663330214, Learning Rate - 0.0125, magnitude of gradient - 1.4866444769645661\n",
      "Step - 2411, Loss - 0.7705505610361728, Learning Rate - 0.0125, magnitude of gradient - 0.8140593612116532\n",
      "Step - 2412, Loss - 0.6210967775582725, Learning Rate - 0.0125, magnitude of gradient - 0.5364756314279943\n",
      "Step - 2413, Loss - 0.5502589541173717, Learning Rate - 0.0125, magnitude of gradient - 1.1906377678468623\n",
      "Step - 2414, Loss - 0.8517500859022793, Learning Rate - 0.0125, magnitude of gradient - 1.381455850540838\n",
      "Step - 2415, Loss - 0.7587972754950618, Learning Rate - 0.0125, magnitude of gradient - 0.8732878394652491\n",
      "Step - 2416, Loss - 0.7204808234574333, Learning Rate - 0.0125, magnitude of gradient - 1.7966466428656862\n",
      "Step - 2417, Loss - 0.804728417043441, Learning Rate - 0.0125, magnitude of gradient - 2.148064587213483\n",
      "Step - 2418, Loss - 0.6491755757824729, Learning Rate - 0.0125, magnitude of gradient - 1.0640023512765946\n",
      "Step - 2419, Loss - 0.7335342201596498, Learning Rate - 0.0125, magnitude of gradient - 0.8586835717759479\n",
      "Step - 2420, Loss - 0.7294537497196821, Learning Rate - 0.0125, magnitude of gradient - 0.6783862694957601\n",
      "Step - 2421, Loss - 0.6204703276205701, Learning Rate - 0.0125, magnitude of gradient - 0.6796254920000016\n",
      "Step - 2422, Loss - 0.7280420828178829, Learning Rate - 0.0125, magnitude of gradient - 0.852191554996004\n",
      "Step - 2423, Loss - 0.710876039942929, Learning Rate - 0.0125, magnitude of gradient - 2.1722753182408603\n",
      "Step - 2424, Loss - 0.6990051230685154, Learning Rate - 0.0125, magnitude of gradient - 1.5847844394200776\n",
      "Step - 2425, Loss - 0.650000165040044, Learning Rate - 0.0125, magnitude of gradient - 0.7906701140688346\n",
      "Step - 2426, Loss - 0.7751918353661893, Learning Rate - 0.0125, magnitude of gradient - 1.4160552895494514\n",
      "Step - 2427, Loss - 0.9234514685224668, Learning Rate - 0.0125, magnitude of gradient - 3.2039978671862324\n",
      "Step - 2428, Loss - 0.5036677094500769, Learning Rate - 0.0125, magnitude of gradient - 1.0248143264533092\n",
      "Step - 2429, Loss - 0.8417589601998339, Learning Rate - 0.0125, magnitude of gradient - 3.4020406226365467\n",
      "Step - 2430, Loss - 0.647503702404179, Learning Rate - 0.0125, magnitude of gradient - 0.41928343656759115\n",
      "Step - 2431, Loss - 0.5104313314928417, Learning Rate - 0.0125, magnitude of gradient - 0.9757337515595884\n",
      "Step - 2432, Loss - 0.9297678311913792, Learning Rate - 0.0125, magnitude of gradient - 2.1212690900070426\n",
      "Step - 2433, Loss - 0.6319753700881359, Learning Rate - 0.0125, magnitude of gradient - 1.9301673519248688\n",
      "Step - 2434, Loss - 0.5491412552205001, Learning Rate - 0.0125, magnitude of gradient - 1.10341671486389\n",
      "Step - 2435, Loss - 0.6303188830533892, Learning Rate - 0.0125, magnitude of gradient - 1.6384279116605258\n",
      "Step - 2436, Loss - 0.9430588329148227, Learning Rate - 0.0125, magnitude of gradient - 1.879223405369016\n",
      "Step - 2437, Loss - 0.6794445513804589, Learning Rate - 0.0125, magnitude of gradient - 2.61176792003309\n",
      "Step - 2438, Loss - 0.6440297608684233, Learning Rate - 0.0125, magnitude of gradient - 1.915732057216835\n",
      "Step - 2439, Loss - 0.8655505964168098, Learning Rate - 0.0125, magnitude of gradient - 3.1833417951561023\n",
      "Step - 2440, Loss - 0.7216255076932928, Learning Rate - 0.0125, magnitude of gradient - 0.6429607852753306\n",
      "Step - 2441, Loss - 0.8973548039463735, Learning Rate - 0.0125, magnitude of gradient - 1.600449861526565\n",
      "Step - 2442, Loss - 0.7208283774840543, Learning Rate - 0.0125, magnitude of gradient - 1.115178186060746\n",
      "Step - 2443, Loss - 0.5547240508062479, Learning Rate - 0.0125, magnitude of gradient - 0.5829353818547293\n",
      "Step - 2444, Loss - 0.7633143996082339, Learning Rate - 0.0125, magnitude of gradient - 1.3829758165695498\n",
      "Step - 2445, Loss - 0.6856616574819272, Learning Rate - 0.0125, magnitude of gradient - 1.0216652542098517\n",
      "Step - 2446, Loss - 0.6657983095292659, Learning Rate - 0.0125, magnitude of gradient - 1.3120900426643014\n",
      "Step - 2447, Loss - 0.854594918936051, Learning Rate - 0.0125, magnitude of gradient - 2.0585074773170833\n",
      "Step - 2448, Loss - 0.7720092266715975, Learning Rate - 0.0125, magnitude of gradient - 0.4160061145819755\n",
      "Step - 2449, Loss - 0.6327530705010562, Learning Rate - 0.0125, magnitude of gradient - 1.4897562849398485\n",
      "Step - 2450, Loss - 0.7008823926149299, Learning Rate - 0.0125, magnitude of gradient - 2.205216491428623\n",
      "Step - 2451, Loss - 0.7647733454171145, Learning Rate - 0.0125, magnitude of gradient - 0.9878157459898682\n",
      "Step - 2452, Loss - 0.8698021701414074, Learning Rate - 0.0125, magnitude of gradient - 1.1281091202100282\n",
      "Step - 2453, Loss - 0.731846490299072, Learning Rate - 0.0125, magnitude of gradient - 1.2221848594871634\n",
      "Step - 2454, Loss - 0.7373251914735783, Learning Rate - 0.0125, magnitude of gradient - 1.7303708191646128\n",
      "Step - 2455, Loss - 0.6001029545005822, Learning Rate - 0.0125, magnitude of gradient - 1.4486346186541725\n",
      "Step - 2456, Loss - 0.5206109077677633, Learning Rate - 0.0125, magnitude of gradient - 1.6869555579506372\n",
      "Step - 2457, Loss - 0.8203513963899036, Learning Rate - 0.0125, magnitude of gradient - 2.4231592427717406\n",
      "Step - 2458, Loss - 0.9041649117345727, Learning Rate - 0.0125, magnitude of gradient - 1.132064829685793\n",
      "Step - 2459, Loss - 0.544490457084869, Learning Rate - 0.0125, magnitude of gradient - 1.193331621363502\n",
      "Step - 2460, Loss - 0.6630605210366627, Learning Rate - 0.0125, magnitude of gradient - 1.1375486744799939\n",
      "Step - 2461, Loss - 0.787579962133072, Learning Rate - 0.0125, magnitude of gradient - 2.1001685599170394\n",
      "Step - 2462, Loss - 0.7128131825524988, Learning Rate - 0.0125, magnitude of gradient - 1.2661453403017058\n",
      "Step - 2463, Loss - 0.8912365574944473, Learning Rate - 0.0125, magnitude of gradient - 0.5590842253082484\n",
      "Step - 2464, Loss - 0.8785685715871838, Learning Rate - 0.0125, magnitude of gradient - 0.5423990722607536\n",
      "Step - 2465, Loss - 0.792873684952232, Learning Rate - 0.0125, magnitude of gradient - 2.2371067429975184\n",
      "Step - 2466, Loss - 0.6905650282617279, Learning Rate - 0.0125, magnitude of gradient - 0.7772019172532574\n",
      "Step - 2467, Loss - 0.6494792952768952, Learning Rate - 0.0125, magnitude of gradient - 1.270884366412921\n",
      "Step - 2468, Loss - 0.690864252784431, Learning Rate - 0.0125, magnitude of gradient - 1.406765547928311\n",
      "Step - 2469, Loss - 0.6745303245391736, Learning Rate - 0.0125, magnitude of gradient - 1.0785500522443348\n",
      "Step - 2470, Loss - 0.8074201114383459, Learning Rate - 0.0125, magnitude of gradient - 1.7501315369826929\n",
      "Step - 2471, Loss - 0.7063636828358848, Learning Rate - 0.0125, magnitude of gradient - 2.2200654152255774\n",
      "Step - 2472, Loss - 0.5809659289033513, Learning Rate - 0.0125, magnitude of gradient - 1.7576173203472047\n",
      "Step - 2473, Loss - 0.730350891828186, Learning Rate - 0.0125, magnitude of gradient - 2.135339309711215\n",
      "Step - 2474, Loss - 0.9195398165508943, Learning Rate - 0.0125, magnitude of gradient - 0.8942952715343088\n",
      "Step - 2475, Loss - 0.7039836069659815, Learning Rate - 0.0125, magnitude of gradient - 0.871338177263313\n",
      "Step - 2476, Loss - 0.9082411112667165, Learning Rate - 0.0125, magnitude of gradient - 1.2363971221256325\n",
      "Step - 2477, Loss - 0.6989511210517567, Learning Rate - 0.0125, magnitude of gradient - 3.0517789282887406\n",
      "Step - 2478, Loss - 0.6956375012452075, Learning Rate - 0.0125, magnitude of gradient - 1.164722562937175\n",
      "Step - 2479, Loss - 0.6590146927596616, Learning Rate - 0.0125, magnitude of gradient - 1.0481908182615043\n",
      "Step - 2480, Loss - 0.781186941770987, Learning Rate - 0.0125, magnitude of gradient - 2.953482808642218\n",
      "Step - 2481, Loss - 0.7323420908181639, Learning Rate - 0.0125, magnitude of gradient - 0.9502248501146378\n",
      "Step - 2482, Loss - 0.6756299603944405, Learning Rate - 0.0125, magnitude of gradient - 0.5141568092758296\n",
      "Step - 2483, Loss - 0.6789416297241828, Learning Rate - 0.0125, magnitude of gradient - 0.9525629221285776\n",
      "Step - 2484, Loss - 0.7295624264946011, Learning Rate - 0.0125, magnitude of gradient - 1.6509983465878562\n",
      "Step - 2485, Loss - 0.6701587165341535, Learning Rate - 0.0125, magnitude of gradient - 0.7879149131715438\n",
      "Step - 2486, Loss - 0.6076546385408954, Learning Rate - 0.0125, magnitude of gradient - 0.5540977627322481\n",
      "Step - 2487, Loss - 0.6637269907172226, Learning Rate - 0.0125, magnitude of gradient - 0.7094359744217122\n",
      "Step - 2488, Loss - 0.8092220438226024, Learning Rate - 0.0125, magnitude of gradient - 1.2430550926372186\n",
      "Step - 2489, Loss - 0.6547077010721176, Learning Rate - 0.0125, magnitude of gradient - 1.6976449142261247\n",
      "Step - 2490, Loss - 0.70183776692593, Learning Rate - 0.0125, magnitude of gradient - 1.327471774873824\n",
      "Step - 2491, Loss - 0.7517396733427986, Learning Rate - 0.0125, magnitude of gradient - 0.9539768709164337\n",
      "Step - 2492, Loss - 0.5371404601684988, Learning Rate - 0.0125, magnitude of gradient - 1.028624948304553\n",
      "Step - 2493, Loss - 0.6684989149786649, Learning Rate - 0.0125, magnitude of gradient - 0.9861938832859506\n",
      "Step - 2494, Loss - 0.7612642804024286, Learning Rate - 0.0125, magnitude of gradient - 1.2509902579024839\n",
      "Step - 2495, Loss - 0.735767424710367, Learning Rate - 0.0125, magnitude of gradient - 2.122716030844355\n",
      "Step - 2496, Loss - 0.7552795760759984, Learning Rate - 0.0125, magnitude of gradient - 1.8340756664529034\n",
      "Step - 2497, Loss - 0.7741989449912817, Learning Rate - 0.0125, magnitude of gradient - 0.7419028429513476\n",
      "Step - 2498, Loss - 0.8109331540326711, Learning Rate - 0.0125, magnitude of gradient - 1.7563491314909514\n",
      "Step - 2499, Loss - 0.6872907992086461, Learning Rate - 0.0125, magnitude of gradient - 0.7449808107017908\n",
      "Step - 2500, Loss - 0.6880690573202614, Learning Rate - 0.0125, magnitude of gradient - 0.9078381703308804\n",
      "Step - 2501, Loss - 0.8119016934891155, Learning Rate - 0.0125, magnitude of gradient - 2.4176336185393597\n",
      "Step - 2502, Loss - 0.50253697318626, Learning Rate - 0.0125, magnitude of gradient - 1.0634422216933859\n",
      "Step - 2503, Loss - 0.6683038992400778, Learning Rate - 0.0125, magnitude of gradient - 1.5973483060688847\n",
      "Step - 2504, Loss - 0.7367594816632883, Learning Rate - 0.0125, magnitude of gradient - 1.4246629658857233\n",
      "Step - 2505, Loss - 0.6377891585402151, Learning Rate - 0.0125, magnitude of gradient - 1.2495816454718867\n",
      "Step - 2506, Loss - 0.8097237366696484, Learning Rate - 0.0125, magnitude of gradient - 1.5037050494687505\n",
      "Step - 2507, Loss - 0.7310990631096383, Learning Rate - 0.0125, magnitude of gradient - 1.5673413800258635\n",
      "Step - 2508, Loss - 0.8396503206440845, Learning Rate - 0.0125, magnitude of gradient - 0.8889286846198529\n",
      "Step - 2509, Loss - 0.5111662821366836, Learning Rate - 0.0125, magnitude of gradient - 1.9242692371473422\n",
      "Step - 2510, Loss - 0.762491432818255, Learning Rate - 0.0125, magnitude of gradient - 0.6037383554510003\n",
      "Step - 2511, Loss - 0.7248213372531495, Learning Rate - 0.0125, magnitude of gradient - 1.3238507948211689\n",
      "Step - 2512, Loss - 0.6995469492222239, Learning Rate - 0.0125, magnitude of gradient - 1.300856905316953\n",
      "Step - 2513, Loss - 0.7364166856111838, Learning Rate - 0.0125, magnitude of gradient - 2.824949049896948\n",
      "Step - 2514, Loss - 0.7338056796839661, Learning Rate - 0.0125, magnitude of gradient - 1.5040319792706505\n",
      "Step - 2515, Loss - 0.5981367258975465, Learning Rate - 0.0125, magnitude of gradient - 2.018312185554467\n",
      "Step - 2516, Loss - 0.6222071698345267, Learning Rate - 0.0125, magnitude of gradient - 0.9604791845498136\n",
      "Step - 2517, Loss - 0.635490351119402, Learning Rate - 0.0125, magnitude of gradient - 1.064002716749477\n",
      "Step - 2518, Loss - 0.7001139515071204, Learning Rate - 0.0125, magnitude of gradient - 0.5262559329695816\n",
      "Step - 2519, Loss - 0.6835728294881458, Learning Rate - 0.0125, magnitude of gradient - 2.355100253809825\n",
      "Step - 2520, Loss - 0.6048232563359108, Learning Rate - 0.0125, magnitude of gradient - 1.204864720963513\n",
      "Step - 2521, Loss - 0.6090309063762575, Learning Rate - 0.0125, magnitude of gradient - 2.362791982422336\n",
      "Step - 2522, Loss - 0.7251873968528353, Learning Rate - 0.0125, magnitude of gradient - 2.209537562378409\n",
      "Step - 2523, Loss - 0.8806373247771104, Learning Rate - 0.0125, magnitude of gradient - 1.357386156369376\n",
      "Step - 2524, Loss - 0.839511820033799, Learning Rate - 0.0125, magnitude of gradient - 1.5713016553213983\n",
      "Step - 2525, Loss - 0.6066922264387447, Learning Rate - 0.0125, magnitude of gradient - 0.243878151615333\n",
      "Step - 2526, Loss - 0.7597482395275024, Learning Rate - 0.0125, magnitude of gradient - 1.7524231557055883\n",
      "Step - 2527, Loss - 0.7479845573832051, Learning Rate - 0.0125, magnitude of gradient - 1.1852054083995367\n",
      "Step - 2528, Loss - 0.8913092593540796, Learning Rate - 0.0125, magnitude of gradient - 3.280762360004965\n",
      "Step - 2529, Loss - 0.8564788701114234, Learning Rate - 0.0125, magnitude of gradient - 3.358995997690886\n",
      "Step - 2530, Loss - 0.74717151108106, Learning Rate - 0.0125, magnitude of gradient - 2.8355077284081958\n",
      "Step - 2531, Loss - 0.8478796457762678, Learning Rate - 0.0125, magnitude of gradient - 1.1557111250802548\n",
      "Step - 2532, Loss - 0.7973103357711236, Learning Rate - 0.0125, magnitude of gradient - 0.6044397125201607\n",
      "Step - 2533, Loss - 0.7110883884982202, Learning Rate - 0.0125, magnitude of gradient - 1.2558749818424932\n",
      "Step - 2534, Loss - 0.6999614514628367, Learning Rate - 0.0125, magnitude of gradient - 0.6654001120146926\n",
      "Step - 2535, Loss - 0.6910473441668324, Learning Rate - 0.0125, magnitude of gradient - 1.5405528280774035\n",
      "Step - 2536, Loss - 0.8558817667976661, Learning Rate - 0.0125, magnitude of gradient - 1.1505417107294045\n",
      "Step - 2537, Loss - 0.7327451111367635, Learning Rate - 0.0125, magnitude of gradient - 1.1706307891421455\n",
      "Step - 2538, Loss - 0.6900962239800079, Learning Rate - 0.0125, magnitude of gradient - 0.453056964285075\n",
      "Step - 2539, Loss - 0.6328027557034819, Learning Rate - 0.0125, magnitude of gradient - 2.011773769241315\n",
      "Step - 2540, Loss - 0.749097448634083, Learning Rate - 0.0125, magnitude of gradient - 1.586289853900035\n",
      "Step - 2541, Loss - 0.7676002101671281, Learning Rate - 0.0125, magnitude of gradient - 1.36762062704248\n",
      "Step - 2542, Loss - 0.658839790590757, Learning Rate - 0.0125, magnitude of gradient - 0.4689793404407659\n",
      "Step - 2543, Loss - 0.7514377974278955, Learning Rate - 0.0125, magnitude of gradient - 1.369398846944601\n",
      "Step - 2544, Loss - 0.5427304521158527, Learning Rate - 0.0125, magnitude of gradient - 0.9385822900525935\n",
      "Step - 2545, Loss - 0.7457918221872896, Learning Rate - 0.0125, magnitude of gradient - 1.6481483907613117\n",
      "Step - 2546, Loss - 0.5745069698371237, Learning Rate - 0.0125, magnitude of gradient - 1.6032722654548293\n",
      "Step - 2547, Loss - 0.6561309011071408, Learning Rate - 0.0125, magnitude of gradient - 0.5533414258653347\n",
      "Step - 2548, Loss - 0.7462736309309981, Learning Rate - 0.0125, magnitude of gradient - 2.012905032547337\n",
      "Step - 2549, Loss - 0.643915673437674, Learning Rate - 0.0125, magnitude of gradient - 2.193948672512863\n",
      "Step - 2550, Loss - 0.7533065387608444, Learning Rate - 0.0125, magnitude of gradient - 0.7193088615792266\n",
      "Step - 2551, Loss - 0.7264472026396014, Learning Rate - 0.0125, magnitude of gradient - 1.0909258989062118\n",
      "Step - 2552, Loss - 0.6959022166320383, Learning Rate - 0.0125, magnitude of gradient - 0.9220719823526177\n",
      "Step - 2553, Loss - 0.6063885571101383, Learning Rate - 0.0125, magnitude of gradient - 3.4108476849018357\n",
      "Step - 2554, Loss - 0.8739234941271394, Learning Rate - 0.0125, magnitude of gradient - 2.2638082379838096\n",
      "Step - 2555, Loss - 0.6388132969010589, Learning Rate - 0.0125, magnitude of gradient - 0.8114263781259539\n",
      "Step - 2556, Loss - 0.6745231092953488, Learning Rate - 0.0125, magnitude of gradient - 1.2415222810121755\n",
      "Step - 2557, Loss - 0.8754881906548321, Learning Rate - 0.0125, magnitude of gradient - 1.3772434145163943\n",
      "Step - 2558, Loss - 0.9236903183690077, Learning Rate - 0.0125, magnitude of gradient - 1.8210808781005015\n",
      "Step - 2559, Loss - 0.7321011025948588, Learning Rate - 0.0125, magnitude of gradient - 2.5499094212983464\n",
      "Step - 2560, Loss - 0.7112222437270984, Learning Rate - 0.0125, magnitude of gradient - 0.4780263420719822\n",
      "Step - 2561, Loss - 0.8081993146261847, Learning Rate - 0.0125, magnitude of gradient - 1.4249976003086229\n",
      "Step - 2562, Loss - 0.7027538691633707, Learning Rate - 0.0125, magnitude of gradient - 1.5668285862231446\n",
      "Step - 2563, Loss - 0.6201513270432943, Learning Rate - 0.0125, magnitude of gradient - 0.9303201082862952\n",
      "Step - 2564, Loss - 0.8472150438530991, Learning Rate - 0.0125, magnitude of gradient - 2.8966870049073474\n",
      "Step - 2565, Loss - 0.8065195813987702, Learning Rate - 0.0125, magnitude of gradient - 0.8272461073363575\n",
      "Step - 2566, Loss - 0.6737457620721043, Learning Rate - 0.0125, magnitude of gradient - 0.617975668952297\n",
      "Step - 2567, Loss - 0.7149667482556621, Learning Rate - 0.0125, magnitude of gradient - 1.0913849140070746\n",
      "Step - 2568, Loss - 0.7273063177562096, Learning Rate - 0.0125, magnitude of gradient - 2.405111959591905\n",
      "Step - 2569, Loss - 0.9565901623199194, Learning Rate - 0.0125, magnitude of gradient - 1.7297247432664824\n",
      "Step - 2570, Loss - 0.7780934512257844, Learning Rate - 0.0125, magnitude of gradient - 1.5731051386046544\n",
      "Step - 2571, Loss - 0.7912474454075321, Learning Rate - 0.0125, magnitude of gradient - 1.136434751654582\n",
      "Step - 2572, Loss - 0.8731643004870447, Learning Rate - 0.0125, magnitude of gradient - 1.3773246928666683\n",
      "Step - 2573, Loss - 0.7548852106028029, Learning Rate - 0.0125, magnitude of gradient - 1.170897799900002\n",
      "Step - 2574, Loss - 0.6375628058070915, Learning Rate - 0.0125, magnitude of gradient - 0.6800629507743922\n",
      "Step - 2575, Loss - 0.8287131084868218, Learning Rate - 0.0125, magnitude of gradient - 0.44670133741129225\n",
      "Step - 2576, Loss - 0.6466646698533238, Learning Rate - 0.0125, magnitude of gradient - 0.7656069199423683\n",
      "Step - 2577, Loss - 0.6190396825463064, Learning Rate - 0.0125, magnitude of gradient - 0.7256497113106941\n",
      "Step - 2578, Loss - 0.8131085900296198, Learning Rate - 0.0125, magnitude of gradient - 2.9762938120210998\n",
      "Step - 2579, Loss - 0.8480635978219176, Learning Rate - 0.0125, magnitude of gradient - 1.5251888459594478\n",
      "Step - 2580, Loss - 0.5361031167951388, Learning Rate - 0.0125, magnitude of gradient - 1.5810790097797944\n",
      "Step - 2581, Loss - 0.7890614692726888, Learning Rate - 0.0125, magnitude of gradient - 0.4087756934235733\n",
      "Step - 2582, Loss - 0.6680963967327153, Learning Rate - 0.0125, magnitude of gradient - 0.8298796254609603\n",
      "Step - 2583, Loss - 0.550475635186155, Learning Rate - 0.0125, magnitude of gradient - 1.6805129756775512\n",
      "Step - 2584, Loss - 0.7245557965933862, Learning Rate - 0.0125, magnitude of gradient - 1.0783600105735143\n",
      "Step - 2585, Loss - 0.7884602253671597, Learning Rate - 0.0125, magnitude of gradient - 1.9467542210669715\n",
      "Step - 2586, Loss - 0.720361421664113, Learning Rate - 0.0125, magnitude of gradient - 0.602928980382\n",
      "Step - 2587, Loss - 0.6602286323736477, Learning Rate - 0.0125, magnitude of gradient - 1.1712754921302608\n",
      "Step - 2588, Loss - 0.5109411253479083, Learning Rate - 0.0125, magnitude of gradient - 1.268056899788565\n",
      "Step - 2589, Loss - 0.8023798296089992, Learning Rate - 0.0125, magnitude of gradient - 2.059561009227133\n",
      "Step - 2590, Loss - 0.7190013265930236, Learning Rate - 0.0125, magnitude of gradient - 2.6454480913614358\n",
      "Step - 2591, Loss - 0.6935883773269521, Learning Rate - 0.0125, magnitude of gradient - 1.0721811647489647\n",
      "Step - 2592, Loss - 0.7787503091292954, Learning Rate - 0.0125, magnitude of gradient - 0.8930424917184309\n",
      "Step - 2593, Loss - 0.5279301661228372, Learning Rate - 0.0125, magnitude of gradient - 2.380020501762659\n",
      "Step - 2594, Loss - 0.7291002589177622, Learning Rate - 0.0125, magnitude of gradient - 0.6784086606325669\n",
      "Step - 2595, Loss - 0.4505453006676481, Learning Rate - 0.0125, magnitude of gradient - 1.8165107964026874\n",
      "Step - 2596, Loss - 0.749642872843291, Learning Rate - 0.0125, magnitude of gradient - 2.180579426670704\n",
      "Step - 2597, Loss - 0.7389903976154293, Learning Rate - 0.0125, magnitude of gradient - 1.2475540820692126\n",
      "Step - 2598, Loss - 0.5643212638600029, Learning Rate - 0.0125, magnitude of gradient - 1.3215642363712077\n",
      "Step - 2599, Loss - 0.4843687618017483, Learning Rate - 0.0125, magnitude of gradient - 0.9042847665453875\n",
      "Step - 2600, Loss - 0.515980123443233, Learning Rate - 0.0125, magnitude of gradient - 2.529404296318404\n",
      "Step - 2601, Loss - 0.6342328267351565, Learning Rate - 0.0125, magnitude of gradient - 1.9749252845579996\n",
      "Step - 2602, Loss - 0.7559022529254301, Learning Rate - 0.0125, magnitude of gradient - 0.7094925677130942\n",
      "Step - 2603, Loss - 0.6161686857973454, Learning Rate - 0.0125, magnitude of gradient - 1.1986708405573594\n",
      "Step - 2604, Loss - 0.695330221026487, Learning Rate - 0.0125, magnitude of gradient - 1.498814912068116\n",
      "Step - 2605, Loss - 0.8772076172412823, Learning Rate - 0.0125, magnitude of gradient - 2.0923166255722125\n",
      "Step - 2606, Loss - 0.512606264000028, Learning Rate - 0.0125, magnitude of gradient - 0.748025947933673\n",
      "Step - 2607, Loss - 0.8248549469187004, Learning Rate - 0.0125, magnitude of gradient - 2.607034031807334\n",
      "Step - 2608, Loss - 0.7922135496865192, Learning Rate - 0.0125, magnitude of gradient - 1.7451967245910687\n",
      "Step - 2609, Loss - 0.7804808879989855, Learning Rate - 0.0125, magnitude of gradient - 0.6132466303561303\n",
      "Step - 2610, Loss - 0.7907780487204076, Learning Rate - 0.0125, magnitude of gradient - 1.0699387779246938\n",
      "Step - 2611, Loss - 0.7056790381488851, Learning Rate - 0.0125, magnitude of gradient - 0.801005136128132\n",
      "Step - 2612, Loss - 0.8505609928693577, Learning Rate - 0.0125, magnitude of gradient - 0.9439377324631829\n",
      "Step - 2613, Loss - 0.6277939734624369, Learning Rate - 0.0125, magnitude of gradient - 0.7896268228358448\n",
      "Step - 2614, Loss - 0.7490206279908412, Learning Rate - 0.0125, magnitude of gradient - 1.3705613357957012\n",
      "Step - 2615, Loss - 0.5707518204193158, Learning Rate - 0.0125, magnitude of gradient - 1.2279137856930824\n",
      "Step - 2616, Loss - 0.76868402257922, Learning Rate - 0.0125, magnitude of gradient - 1.0306391739292757\n",
      "Step - 2617, Loss - 0.9971259146652302, Learning Rate - 0.0125, magnitude of gradient - 1.2977431349463968\n",
      "Step - 2618, Loss - 0.6959650606710608, Learning Rate - 0.0125, magnitude of gradient - 1.1598723156870472\n",
      "Step - 2619, Loss - 0.816670878801316, Learning Rate - 0.0125, magnitude of gradient - 2.025522556544287\n",
      "Step - 2620, Loss - 0.6437309640963701, Learning Rate - 0.0125, magnitude of gradient - 1.3183770453979893\n",
      "Step - 2621, Loss - 0.9677414802476907, Learning Rate - 0.0125, magnitude of gradient - 2.135384312841987\n",
      "Step - 2622, Loss - 0.6048767499189671, Learning Rate - 0.0125, magnitude of gradient - 0.8768621786240745\n",
      "Step - 2623, Loss - 0.7508163071779852, Learning Rate - 0.0125, magnitude of gradient - 2.0261219324843736\n",
      "Step - 2624, Loss - 0.726115121536209, Learning Rate - 0.0125, magnitude of gradient - 0.9325506073468721\n",
      "Step - 2625, Loss - 0.6385727636482854, Learning Rate - 0.0125, magnitude of gradient - 2.3422501263486977\n",
      "Step - 2626, Loss - 0.7779701616437209, Learning Rate - 0.0125, magnitude of gradient - 1.6347144749532125\n",
      "Step - 2627, Loss - 0.7919399333679987, Learning Rate - 0.0125, magnitude of gradient - 2.648450647984357\n",
      "Step - 2628, Loss - 0.9095761889572752, Learning Rate - 0.0125, magnitude of gradient - 1.2966796447279112\n",
      "Step - 2629, Loss - 0.6674699634254537, Learning Rate - 0.0125, magnitude of gradient - 3.048588565704417\n",
      "Step - 2630, Loss - 0.778624644020983, Learning Rate - 0.0125, magnitude of gradient - 0.8845768354316418\n",
      "Step - 2631, Loss - 0.8679947372082499, Learning Rate - 0.0125, magnitude of gradient - 3.3876731093380408\n",
      "Step - 2632, Loss - 0.7568127892633294, Learning Rate - 0.0125, magnitude of gradient - 1.9553403235484674\n",
      "Step - 2633, Loss - 0.6582716909051802, Learning Rate - 0.0125, magnitude of gradient - 1.503075784555201\n",
      "Step - 2634, Loss - 0.7440584076104311, Learning Rate - 0.0125, magnitude of gradient - 1.7204946400307881\n",
      "Step - 2635, Loss - 0.7348383331349753, Learning Rate - 0.0125, magnitude of gradient - 2.29723333703304\n",
      "Step - 2636, Loss - 0.9903727257547422, Learning Rate - 0.0125, magnitude of gradient - 2.773022716222659\n",
      "Step - 2637, Loss - 0.5972041842285647, Learning Rate - 0.0125, magnitude of gradient - 1.4192467257163885\n",
      "Step - 2638, Loss - 0.5969192413725797, Learning Rate - 0.0125, magnitude of gradient - 1.2154032550787224\n",
      "Step - 2639, Loss - 0.5728996793955404, Learning Rate - 0.0125, magnitude of gradient - 0.8043475531373986\n",
      "Step - 2640, Loss - 0.7407668844200501, Learning Rate - 0.0125, magnitude of gradient - 1.5186096188059948\n",
      "Step - 2641, Loss - 0.574853288982337, Learning Rate - 0.0125, magnitude of gradient - 1.6892891119670508\n",
      "Step - 2642, Loss - 0.7588210383652623, Learning Rate - 0.0125, magnitude of gradient - 1.3820071246145467\n",
      "Step - 2643, Loss - 0.6274768108373323, Learning Rate - 0.0125, magnitude of gradient - 0.36507311474523035\n",
      "Step - 2644, Loss - 0.6531446119137824, Learning Rate - 0.0125, magnitude of gradient - 1.5378622512623763\n",
      "Step - 2645, Loss - 0.5560179374200834, Learning Rate - 0.0125, magnitude of gradient - 0.552486218867552\n",
      "Step - 2646, Loss - 0.9130132514047538, Learning Rate - 0.0125, magnitude of gradient - 3.8617875009061846\n",
      "Step - 2647, Loss - 0.6904981816152513, Learning Rate - 0.0125, magnitude of gradient - 3.0497395648848387\n",
      "Step - 2648, Loss - 0.6028970361060808, Learning Rate - 0.0125, magnitude of gradient - 0.8319909252872425\n",
      "Step - 2649, Loss - 0.854965824997543, Learning Rate - 0.0125, magnitude of gradient - 0.6036008984292311\n",
      "Step - 2650, Loss - 0.6719838547468053, Learning Rate - 0.0125, magnitude of gradient - 0.6963422449996464\n",
      "Step - 2651, Loss - 0.6806945961505101, Learning Rate - 0.0125, magnitude of gradient - 0.8950064199591482\n",
      "Step - 2652, Loss - 0.7127190723041426, Learning Rate - 0.0125, magnitude of gradient - 0.3046714812685868\n",
      "Step - 2653, Loss - 0.6119016146874426, Learning Rate - 0.0125, magnitude of gradient - 1.1408544104757186\n",
      "Step - 2654, Loss - 0.7514590687672844, Learning Rate - 0.0125, magnitude of gradient - 0.4255918753476749\n",
      "Step - 2655, Loss - 0.5534249246260973, Learning Rate - 0.0125, magnitude of gradient - 2.3461505647460856\n",
      "Step - 2656, Loss - 0.8590649736576103, Learning Rate - 0.0125, magnitude of gradient - 2.0828217643260665\n",
      "Step - 2657, Loss - 0.748448262000551, Learning Rate - 0.0125, magnitude of gradient - 1.7854895709418137\n",
      "Step - 2658, Loss - 0.6890195476590227, Learning Rate - 0.0125, magnitude of gradient - 1.878786910731134\n",
      "Step - 2659, Loss - 0.8560738998392401, Learning Rate - 0.0125, magnitude of gradient - 2.6663952373827913\n",
      "Step - 2660, Loss - 0.6656353366399501, Learning Rate - 0.0125, magnitude of gradient - 1.2210022395088724\n",
      "Step - 2661, Loss - 0.7288553008153629, Learning Rate - 0.0125, magnitude of gradient - 1.4284698199108457\n",
      "Step - 2662, Loss - 0.5140114686216879, Learning Rate - 0.0125, magnitude of gradient - 2.4966642378037243\n",
      "Step - 2663, Loss - 0.7145864630904977, Learning Rate - 0.0125, magnitude of gradient - 2.760695669397638\n",
      "Step - 2664, Loss - 0.4954415548632556, Learning Rate - 0.0125, magnitude of gradient - 1.0037312511939855\n",
      "Step - 2665, Loss - 0.7427819082995744, Learning Rate - 0.0125, magnitude of gradient - 2.224987359833788\n",
      "Step - 2666, Loss - 0.5670706895372198, Learning Rate - 0.0125, magnitude of gradient - 2.5661907225989466\n",
      "Step - 2667, Loss - 0.6887685912595036, Learning Rate - 0.0125, magnitude of gradient - 1.5940330018258195\n",
      "Step - 2668, Loss - 0.6922655253291001, Learning Rate - 0.0125, magnitude of gradient - 1.08092295855234\n",
      "Step - 2669, Loss - 0.853754584749121, Learning Rate - 0.0125, magnitude of gradient - 0.9733478597042725\n",
      "Step - 2670, Loss - 0.6680365500892049, Learning Rate - 0.0125, magnitude of gradient - 1.400543909040613\n",
      "Step - 2671, Loss - 0.7567295089170181, Learning Rate - 0.0125, magnitude of gradient - 0.6454926318725723\n",
      "Step - 2672, Loss - 0.7910833052587339, Learning Rate - 0.0125, magnitude of gradient - 0.45021781013920226\n",
      "Step - 2673, Loss - 0.8231445509490152, Learning Rate - 0.0125, magnitude of gradient - 0.9285762658155383\n",
      "Step - 2674, Loss - 0.7116657671614972, Learning Rate - 0.0125, magnitude of gradient - 0.9497655609221802\n",
      "Step - 2675, Loss - 0.6387579151708543, Learning Rate - 0.0125, magnitude of gradient - 1.294027296253058\n",
      "Step - 2676, Loss - 0.7942775833502571, Learning Rate - 0.0125, magnitude of gradient - 2.116347102589306\n",
      "Step - 2677, Loss - 0.9181813747703922, Learning Rate - 0.0125, magnitude of gradient - 0.8996470829953487\n",
      "Step - 2678, Loss - 0.614306457072517, Learning Rate - 0.0125, magnitude of gradient - 1.403847925566779\n",
      "Step - 2679, Loss - 0.7715132910922639, Learning Rate - 0.0125, magnitude of gradient - 1.006589989244662\n",
      "Step - 2680, Loss - 0.6748750333281268, Learning Rate - 0.0125, magnitude of gradient - 1.510393430611476\n",
      "Step - 2681, Loss - 0.6348134537914019, Learning Rate - 0.0125, magnitude of gradient - 1.8715942463071924\n",
      "Step - 2682, Loss - 0.7989974806111664, Learning Rate - 0.0125, magnitude of gradient - 1.7738924774801168\n",
      "Step - 2683, Loss - 0.7994459820637931, Learning Rate - 0.0125, magnitude of gradient - 1.2124362747714301\n",
      "Step - 2684, Loss - 0.6517047074716187, Learning Rate - 0.0125, magnitude of gradient - 1.6784912264568075\n",
      "Step - 2685, Loss - 0.7443953183510186, Learning Rate - 0.0125, magnitude of gradient - 1.0650902647576845\n",
      "Step - 2686, Loss - 0.9299491143383896, Learning Rate - 0.0125, magnitude of gradient - 1.8740912218038985\n",
      "Step - 2687, Loss - 0.6375757198155383, Learning Rate - 0.0125, magnitude of gradient - 1.1306579920338762\n",
      "Step - 2688, Loss - 0.6141310217909776, Learning Rate - 0.0125, magnitude of gradient - 1.1724262648790036\n",
      "Step - 2689, Loss - 0.6149731611826189, Learning Rate - 0.0125, magnitude of gradient - 1.9945136523389262\n",
      "Step - 2690, Loss - 0.9026837461173647, Learning Rate - 0.0125, magnitude of gradient - 3.162417512932797\n",
      "Step - 2691, Loss - 0.7588519731407972, Learning Rate - 0.0125, magnitude of gradient - 0.6635170369634036\n",
      "Step - 2692, Loss - 0.7142409561978007, Learning Rate - 0.0125, magnitude of gradient - 1.5865211357583218\n",
      "Step - 2693, Loss - 0.6065928696430304, Learning Rate - 0.0125, magnitude of gradient - 1.3760323527204457\n",
      "Step - 2694, Loss - 0.7053895062201473, Learning Rate - 0.0125, magnitude of gradient - 2.1572594016347466\n",
      "Step - 2695, Loss - 0.7091098656746868, Learning Rate - 0.0125, magnitude of gradient - 1.0507564617293363\n",
      "Step - 2696, Loss - 0.6081081605314802, Learning Rate - 0.0125, magnitude of gradient - 2.75251053244208\n",
      "Step - 2697, Loss - 0.6283966989587261, Learning Rate - 0.0125, magnitude of gradient - 1.03302844111546\n",
      "Step - 2698, Loss - 0.7075020111554583, Learning Rate - 0.0125, magnitude of gradient - 0.852892041059814\n",
      "Step - 2699, Loss - 0.7098368011038702, Learning Rate - 0.0125, magnitude of gradient - 1.9497427847066113\n",
      "Step - 2700, Loss - 0.7927640416214782, Learning Rate - 0.0125, magnitude of gradient - 2.49716660933645\n",
      "Step - 2701, Loss - 0.6993327068281614, Learning Rate - 0.0125, magnitude of gradient - 1.6803616513456632\n",
      "Step - 2702, Loss - 0.6963374465669605, Learning Rate - 0.0125, magnitude of gradient - 1.3049257239542382\n",
      "Step - 2703, Loss - 0.6667104591329824, Learning Rate - 0.0125, magnitude of gradient - 1.5424866364325156\n",
      "Step - 2704, Loss - 0.5314852288473408, Learning Rate - 0.0125, magnitude of gradient - 4.2454062280060185\n",
      "Step - 2705, Loss - 0.6526435816038498, Learning Rate - 0.0125, magnitude of gradient - 1.1675919294994492\n",
      "Step - 2706, Loss - 0.7100254267687609, Learning Rate - 0.0125, magnitude of gradient - 1.8740236082611703\n",
      "Step - 2707, Loss - 0.6732952613389375, Learning Rate - 0.0125, magnitude of gradient - 0.6470752298512455\n",
      "Step - 2708, Loss - 0.6977826210966869, Learning Rate - 0.0125, magnitude of gradient - 0.6222652665205994\n",
      "Step - 2709, Loss - 0.7148298409093751, Learning Rate - 0.0125, magnitude of gradient - 1.8808651166984347\n",
      "Step - 2710, Loss - 0.7386034442821562, Learning Rate - 0.0125, magnitude of gradient - 2.017389325912817\n",
      "Step - 2711, Loss - 0.7690094642995043, Learning Rate - 0.0125, magnitude of gradient - 1.3670979437961617\n",
      "Step - 2712, Loss - 0.747869791072897, Learning Rate - 0.0125, magnitude of gradient - 0.3227088888544071\n",
      "Step - 2713, Loss - 0.8067032090293862, Learning Rate - 0.0125, magnitude of gradient - 1.2628307577542464\n",
      "Step - 2714, Loss - 0.5994987768447, Learning Rate - 0.0125, magnitude of gradient - 0.8848230076887226\n",
      "Step - 2715, Loss - 0.6674898433126742, Learning Rate - 0.0125, magnitude of gradient - 3.151628265042154\n",
      "Step - 2716, Loss - 0.7232939679444585, Learning Rate - 0.0125, magnitude of gradient - 0.7747992454725776\n",
      "Step - 2717, Loss - 0.6557505375455752, Learning Rate - 0.0125, magnitude of gradient - 1.532088947848645\n",
      "Step - 2718, Loss - 0.6103384384342535, Learning Rate - 0.0125, magnitude of gradient - 1.3903318822582151\n",
      "Step - 2719, Loss - 0.5998632227050598, Learning Rate - 0.0125, magnitude of gradient - 1.546453142885557\n",
      "Step - 2720, Loss - 0.5145620808485708, Learning Rate - 0.0125, magnitude of gradient - 0.9953486319848838\n",
      "Step - 2721, Loss - 0.7159848885673185, Learning Rate - 0.0125, magnitude of gradient - 1.4673631676399166\n",
      "Step - 2722, Loss - 0.7540251066831456, Learning Rate - 0.0125, magnitude of gradient - 0.9212569962025263\n",
      "Step - 2723, Loss - 0.8727981351938957, Learning Rate - 0.0125, magnitude of gradient - 1.7005170788451442\n",
      "Step - 2724, Loss - 0.5322412124861318, Learning Rate - 0.0125, magnitude of gradient - 0.654516214322393\n",
      "Step - 2725, Loss - 0.7513633776082249, Learning Rate - 0.0125, magnitude of gradient - 0.1646788583826863\n",
      "Step - 2726, Loss - 0.6504512048625454, Learning Rate - 0.0125, magnitude of gradient - 0.5000378301435723\n",
      "Step - 2727, Loss - 0.6162176310603051, Learning Rate - 0.0125, magnitude of gradient - 0.6214729498529903\n",
      "Step - 2728, Loss - 0.7663943346087448, Learning Rate - 0.0125, magnitude of gradient - 0.8942229813391901\n",
      "Step - 2729, Loss - 0.7982345759444462, Learning Rate - 0.0125, magnitude of gradient - 1.5620457908929832\n",
      "Step - 2730, Loss - 0.729473162091605, Learning Rate - 0.0125, magnitude of gradient - 1.3730877912310038\n",
      "Step - 2731, Loss - 0.725615514254954, Learning Rate - 0.0125, magnitude of gradient - 2.2023931917584485\n",
      "Step - 2732, Loss - 0.59495213100202, Learning Rate - 0.0125, magnitude of gradient - 0.46358104142847684\n",
      "Step - 2733, Loss - 0.710607237968873, Learning Rate - 0.0125, magnitude of gradient - 3.383996725907624\n",
      "Step - 2734, Loss - 0.7181170075178924, Learning Rate - 0.0125, magnitude of gradient - 0.710706502319117\n",
      "Step - 2735, Loss - 0.898474796089832, Learning Rate - 0.0125, magnitude of gradient - 1.9797321906226362\n",
      "Step - 2736, Loss - 0.666885806632554, Learning Rate - 0.0125, magnitude of gradient - 1.976868261982352\n",
      "Step - 2737, Loss - 0.7136649237498559, Learning Rate - 0.0125, magnitude of gradient - 1.2867142509665046\n",
      "Step - 2738, Loss - 0.8242469702386276, Learning Rate - 0.0125, magnitude of gradient - 1.1533766139908195\n",
      "Step - 2739, Loss - 0.5224065071566476, Learning Rate - 0.0125, magnitude of gradient - 1.2060953595997712\n",
      "Step - 2740, Loss - 0.8158671124427103, Learning Rate - 0.0125, magnitude of gradient - 0.9829516469724237\n",
      "Step - 2741, Loss - 0.604379248987057, Learning Rate - 0.0125, magnitude of gradient - 1.3087365456556095\n",
      "Step - 2742, Loss - 0.7016059253480331, Learning Rate - 0.0125, magnitude of gradient - 2.48974231527178\n",
      "Step - 2743, Loss - 0.7780892188351881, Learning Rate - 0.0125, magnitude of gradient - 0.6968410359055569\n",
      "Step - 2744, Loss - 0.8923053967960001, Learning Rate - 0.0125, magnitude of gradient - 3.2073491713815\n",
      "Step - 2745, Loss - 0.6597762959825754, Learning Rate - 0.0125, magnitude of gradient - 1.842536293648938\n",
      "Step - 2746, Loss - 0.6692955057849368, Learning Rate - 0.0125, magnitude of gradient - 1.7620337547539617\n",
      "Step - 2747, Loss - 0.6077899681945261, Learning Rate - 0.0125, magnitude of gradient - 0.8246565454515192\n",
      "Step - 2748, Loss - 0.9012521116404857, Learning Rate - 0.0125, magnitude of gradient - 3.185491896567958\n",
      "Step - 2749, Loss - 0.6604026558540633, Learning Rate - 0.0125, magnitude of gradient - 1.6515972278678304\n",
      "Step - 2750, Loss - 0.7482771968837088, Learning Rate - 0.0125, magnitude of gradient - 1.9077556697261635\n",
      "Step - 2751, Loss - 0.782842458210494, Learning Rate - 0.0125, magnitude of gradient - 1.5839242090956214\n",
      "Step - 2752, Loss - 0.5594670226802961, Learning Rate - 0.0125, magnitude of gradient - 1.221686452181182\n",
      "Step - 2753, Loss - 0.728231123930551, Learning Rate - 0.0125, magnitude of gradient - 2.2363180883148845\n",
      "Step - 2754, Loss - 0.6227231635960216, Learning Rate - 0.0125, magnitude of gradient - 1.9124198819039115\n",
      "Step - 2755, Loss - 0.7545895435742124, Learning Rate - 0.0125, magnitude of gradient - 1.6325384031566912\n",
      "Step - 2756, Loss - 0.49609495131620546, Learning Rate - 0.0125, magnitude of gradient - 1.0936449209906052\n",
      "Step - 2757, Loss - 0.6496333615001314, Learning Rate - 0.0125, magnitude of gradient - 1.0158503121109872\n",
      "Step - 2758, Loss - 0.6780372926126585, Learning Rate - 0.0125, magnitude of gradient - 1.6172467889703548\n",
      "Step - 2759, Loss - 0.7555264745622832, Learning Rate - 0.0125, magnitude of gradient - 1.0651492314074504\n",
      "Step - 2760, Loss - 0.643339333944529, Learning Rate - 0.0125, magnitude of gradient - 1.2384032409962367\n",
      "Step - 2761, Loss - 0.6667593721645604, Learning Rate - 0.0125, magnitude of gradient - 0.7517270504410727\n",
      "Step - 2762, Loss - 0.8087317432435464, Learning Rate - 0.0125, magnitude of gradient - 0.4197364322978015\n",
      "Step - 2763, Loss - 0.41402043092558066, Learning Rate - 0.0125, magnitude of gradient - 2.764131994521641\n",
      "Step - 2764, Loss - 0.7641586579881057, Learning Rate - 0.0125, magnitude of gradient - 1.123552764187968\n",
      "Step - 2765, Loss - 0.7823004101999721, Learning Rate - 0.0125, magnitude of gradient - 1.1030380404792604\n",
      "Step - 2766, Loss - 0.5485252169621129, Learning Rate - 0.0125, magnitude of gradient - 2.6009678271299657\n",
      "Step - 2767, Loss - 0.7492753760703493, Learning Rate - 0.0125, magnitude of gradient - 0.9125572284388246\n",
      "Step - 2768, Loss - 0.7694578296987595, Learning Rate - 0.0125, magnitude of gradient - 0.39414250767832765\n",
      "Step - 2769, Loss - 0.6083936370753806, Learning Rate - 0.0125, magnitude of gradient - 1.7206263931582608\n",
      "Step - 2770, Loss - 0.614933178905256, Learning Rate - 0.0125, magnitude of gradient - 0.8206967120485605\n",
      "Step - 2771, Loss - 0.7210847716705266, Learning Rate - 0.0125, magnitude of gradient - 1.9208276631419838\n",
      "Step - 2772, Loss - 0.7532895977293963, Learning Rate - 0.0125, magnitude of gradient - 1.1539379471879252\n",
      "Step - 2773, Loss - 0.5759725688728482, Learning Rate - 0.0125, magnitude of gradient - 1.296046989607247\n",
      "Step - 2774, Loss - 0.6121341262308327, Learning Rate - 0.0125, magnitude of gradient - 1.8293913106259436\n",
      "Step - 2775, Loss - 0.7153926683836822, Learning Rate - 0.0125, magnitude of gradient - 0.6005071384041606\n",
      "Step - 2776, Loss - 0.49166915747242523, Learning Rate - 0.0125, magnitude of gradient - 1.3106650655061782\n",
      "Step - 2777, Loss - 0.6539267670899807, Learning Rate - 0.0125, magnitude of gradient - 1.6566276850127983\n",
      "Step - 2778, Loss - 0.8621364988767297, Learning Rate - 0.0125, magnitude of gradient - 2.1360752774196574\n",
      "Step - 2779, Loss - 0.7409650303535905, Learning Rate - 0.0125, magnitude of gradient - 2.2771142948445893\n",
      "Step - 2780, Loss - 0.595686161273034, Learning Rate - 0.0125, magnitude of gradient - 0.7105155726803818\n",
      "Step - 2781, Loss - 0.4418527645294555, Learning Rate - 0.0125, magnitude of gradient - 0.5335223279681602\n",
      "Step - 2782, Loss - 0.5719844752813872, Learning Rate - 0.0125, magnitude of gradient - 1.7680577711786725\n",
      "Step - 2783, Loss - 0.5376484893161309, Learning Rate - 0.0125, magnitude of gradient - 0.8805656404654039\n",
      "Step - 2784, Loss - 0.7726097933437164, Learning Rate - 0.0125, magnitude of gradient - 2.7152365941580685\n",
      "Step - 2785, Loss - 0.5525930843079153, Learning Rate - 0.0125, magnitude of gradient - 1.6230922083365358\n",
      "Step - 2786, Loss - 0.9029192178427016, Learning Rate - 0.0125, magnitude of gradient - 2.172099525772755\n",
      "Step - 2787, Loss - 0.771574705626249, Learning Rate - 0.0125, magnitude of gradient - 1.0719114007597608\n",
      "Step - 2788, Loss - 0.7222555237593133, Learning Rate - 0.0125, magnitude of gradient - 0.7789922681568747\n",
      "Step - 2789, Loss - 0.574523057480918, Learning Rate - 0.0125, magnitude of gradient - 1.7592494475632863\n",
      "Step - 2790, Loss - 0.5112587299371036, Learning Rate - 0.0125, magnitude of gradient - 1.431349901742741\n",
      "Step - 2791, Loss - 0.9631534720824955, Learning Rate - 0.0125, magnitude of gradient - 2.6025383203861385\n",
      "Step - 2792, Loss - 0.7889465200815805, Learning Rate - 0.0125, magnitude of gradient - 2.342166758567015\n",
      "Step - 2793, Loss - 0.8531203735758628, Learning Rate - 0.0125, magnitude of gradient - 2.4233811545328265\n",
      "Step - 2794, Loss - 0.715278899601247, Learning Rate - 0.0125, magnitude of gradient - 2.2751343270487205\n",
      "Step - 2795, Loss - 0.7136662684871797, Learning Rate - 0.0125, magnitude of gradient - 1.572242337088847\n",
      "Step - 2796, Loss - 0.6171147490131024, Learning Rate - 0.0125, magnitude of gradient - 1.1045654460179328\n",
      "Step - 2797, Loss - 0.7574415173302732, Learning Rate - 0.0125, magnitude of gradient - 0.9389488703640686\n",
      "Step - 2798, Loss - 0.43081538784190393, Learning Rate - 0.0125, magnitude of gradient - 1.1760964232958264\n",
      "Step - 2799, Loss - 0.8157301671085725, Learning Rate - 0.0125, magnitude of gradient - 2.3173054359582155\n",
      "Step - 2800, Loss - 0.7972888932250982, Learning Rate - 0.0125, magnitude of gradient - 2.2227379834598615\n",
      "Step - 2801, Loss - 0.6067647909674079, Learning Rate - 0.0125, magnitude of gradient - 1.620026456268425\n",
      "Step - 2802, Loss - 0.7802912947432088, Learning Rate - 0.0125, magnitude of gradient - 2.456690470110269\n",
      "Step - 2803, Loss - 0.8114781245878689, Learning Rate - 0.0125, magnitude of gradient - 2.1459089644542506\n",
      "Step - 2804, Loss - 0.7122859216526217, Learning Rate - 0.0125, magnitude of gradient - 0.2227414676249876\n",
      "Step - 2805, Loss - 0.8803681474917261, Learning Rate - 0.0125, magnitude of gradient - 1.3024159700421987\n",
      "Step - 2806, Loss - 0.7891854519491244, Learning Rate - 0.0125, magnitude of gradient - 1.3345981565647707\n",
      "Step - 2807, Loss - 0.7178920082937746, Learning Rate - 0.0125, magnitude of gradient - 1.5015454420384213\n",
      "Step - 2808, Loss - 0.630794194220731, Learning Rate - 0.0125, magnitude of gradient - 2.5783390078763526\n",
      "Step - 2809, Loss - 0.7971820204560779, Learning Rate - 0.0125, magnitude of gradient - 1.2186029601822246\n",
      "Step - 2810, Loss - 0.5668916493140872, Learning Rate - 0.0125, magnitude of gradient - 0.643321465824845\n",
      "Step - 2811, Loss - 0.7339925308032215, Learning Rate - 0.0125, magnitude of gradient - 3.4479635462985905\n",
      "Step - 2812, Loss - 0.821843023793843, Learning Rate - 0.0125, magnitude of gradient - 1.1249854801489672\n",
      "Step - 2813, Loss - 0.8370474775356421, Learning Rate - 0.0125, magnitude of gradient - 1.3413303028733126\n",
      "Step - 2814, Loss - 0.737370453816887, Learning Rate - 0.0125, magnitude of gradient - 1.017339568376662\n",
      "Step - 2815, Loss - 0.8562034418871803, Learning Rate - 0.0125, magnitude of gradient - 1.0639124982805512\n",
      "Step - 2816, Loss - 0.5814257438120279, Learning Rate - 0.0125, magnitude of gradient - 1.3622823271374611\n",
      "Step - 2817, Loss - 0.6191288696445492, Learning Rate - 0.0125, magnitude of gradient - 0.4640297449195796\n",
      "Step - 2818, Loss - 0.6323279123259221, Learning Rate - 0.0125, magnitude of gradient - 1.6310381515484904\n",
      "Step - 2819, Loss - 0.7556477271196269, Learning Rate - 0.0125, magnitude of gradient - 1.182094138229401\n",
      "Step - 2820, Loss - 0.6021234064227763, Learning Rate - 0.0125, magnitude of gradient - 1.9609154240582403\n",
      "Step - 2821, Loss - 0.862596758493243, Learning Rate - 0.0125, magnitude of gradient - 1.601040000060761\n",
      "Step - 2822, Loss - 0.8401177357614537, Learning Rate - 0.0125, magnitude of gradient - 2.097247582072423\n",
      "Step - 2823, Loss - 0.5897925429996761, Learning Rate - 0.0125, magnitude of gradient - 1.045970882858918\n",
      "Step - 2824, Loss - 0.6418671325447389, Learning Rate - 0.0125, magnitude of gradient - 1.4739097445667844\n",
      "Step - 2825, Loss - 0.600375667488949, Learning Rate - 0.0125, magnitude of gradient - 2.1743304710195304\n",
      "Step - 2826, Loss - 0.504216432459052, Learning Rate - 0.0125, magnitude of gradient - 1.205543088529495\n",
      "Step - 2827, Loss - 0.45102110228339, Learning Rate - 0.0125, magnitude of gradient - 1.7804735901746238\n",
      "Step - 2828, Loss - 0.669747136498944, Learning Rate - 0.0125, magnitude of gradient - 2.070371858679727\n",
      "Step - 2829, Loss - 0.7520130528801897, Learning Rate - 0.0125, magnitude of gradient - 2.2557435974816715\n",
      "Step - 2830, Loss - 0.8260235876754509, Learning Rate - 0.0125, magnitude of gradient - 1.632955311098127\n",
      "Step - 2831, Loss - 0.893114351768978, Learning Rate - 0.0125, magnitude of gradient - 1.339756783340744\n",
      "Step - 2832, Loss - 0.5700578781546701, Learning Rate - 0.0125, magnitude of gradient - 1.2161833035831675\n",
      "Step - 2833, Loss - 0.673486872402771, Learning Rate - 0.0125, magnitude of gradient - 1.36681990791457\n",
      "Step - 2834, Loss - 0.6962444415607714, Learning Rate - 0.0125, magnitude of gradient - 1.8904134632526446\n",
      "Step - 2835, Loss - 0.7464268641752415, Learning Rate - 0.0125, magnitude of gradient - 1.9986369437288074\n",
      "Step - 2836, Loss - 0.6549719558745135, Learning Rate - 0.0125, magnitude of gradient - 0.7819942779302516\n",
      "Step - 2837, Loss - 0.8112825591881552, Learning Rate - 0.0125, magnitude of gradient - 1.3618415047460626\n",
      "Step - 2838, Loss - 0.6190361544500436, Learning Rate - 0.0125, magnitude of gradient - 2.4886975876942956\n",
      "Step - 2839, Loss - 0.6620111386616325, Learning Rate - 0.0125, magnitude of gradient - 1.2052001327516866\n",
      "Step - 2840, Loss - 0.5321613104160355, Learning Rate - 0.0125, magnitude of gradient - 1.150411025747863\n",
      "Step - 2841, Loss - 0.8517600345816988, Learning Rate - 0.0125, magnitude of gradient - 0.9060694328366906\n",
      "Step - 2842, Loss - 0.49469383032816633, Learning Rate - 0.0125, magnitude of gradient - 2.537876122415976\n",
      "Step - 2843, Loss - 0.8055383019275291, Learning Rate - 0.0125, magnitude of gradient - 1.5441345672002944\n",
      "Step - 2844, Loss - 0.8614791647552286, Learning Rate - 0.0125, magnitude of gradient - 1.1956551683208338\n",
      "Step - 2845, Loss - 0.621551395889121, Learning Rate - 0.0125, magnitude of gradient - 1.9936236020877238\n",
      "Step - 2846, Loss - 0.8626831640828235, Learning Rate - 0.0125, magnitude of gradient - 1.8836804423681537\n",
      "Step - 2847, Loss - 0.6820334233871621, Learning Rate - 0.0125, magnitude of gradient - 1.0223090734809355\n",
      "Step - 2848, Loss - 0.7383681332492855, Learning Rate - 0.0125, magnitude of gradient - 0.7514634956251998\n",
      "Step - 2849, Loss - 0.6053427964487924, Learning Rate - 0.0125, magnitude of gradient - 1.014230189576548\n",
      "Step - 2850, Loss - 0.7976118933582501, Learning Rate - 0.0125, magnitude of gradient - 1.4583223602065212\n",
      "Step - 2851, Loss - 0.6013135220121151, Learning Rate - 0.0125, magnitude of gradient - 2.083051746498023\n",
      "Step - 2852, Loss - 0.7235851018186464, Learning Rate - 0.0125, magnitude of gradient - 2.315169690931695\n",
      "Step - 2853, Loss - 0.8767167570445757, Learning Rate - 0.0125, magnitude of gradient - 2.6253111040706587\n",
      "Step - 2854, Loss - 0.41929415749215354, Learning Rate - 0.0125, magnitude of gradient - 1.8539590230697223\n",
      "Step - 2855, Loss - 0.6757695142019857, Learning Rate - 0.0125, magnitude of gradient - 1.53854383911065\n",
      "Step - 2856, Loss - 0.6759436002600396, Learning Rate - 0.0125, magnitude of gradient - 1.7615898295344912\n",
      "Step - 2857, Loss - 0.5226011556385748, Learning Rate - 0.0125, magnitude of gradient - 0.912802452113637\n",
      "Step - 2858, Loss - 0.823724566816069, Learning Rate - 0.0125, magnitude of gradient - 1.5524744298484305\n",
      "Step - 2859, Loss - 0.8293733890318935, Learning Rate - 0.0125, magnitude of gradient - 1.4630747432945397\n",
      "Step - 2860, Loss - 0.569944752849691, Learning Rate - 0.0125, magnitude of gradient - 2.4820221904199355\n",
      "Step - 2861, Loss - 0.7561085631281538, Learning Rate - 0.0125, magnitude of gradient - 0.9178823494061522\n",
      "Step - 2862, Loss - 0.8801693147303217, Learning Rate - 0.0125, magnitude of gradient - 1.5803657357571976\n",
      "Step - 2863, Loss - 0.5593717611858935, Learning Rate - 0.0125, magnitude of gradient - 0.695612357025138\n",
      "Step - 2864, Loss - 0.7560758505736689, Learning Rate - 0.0125, magnitude of gradient - 0.9311613584298754\n",
      "Step - 2865, Loss - 0.6819961375167993, Learning Rate - 0.0125, magnitude of gradient - 1.8348560002419256\n",
      "Step - 2866, Loss - 0.7520541046785729, Learning Rate - 0.0125, magnitude of gradient - 0.8923138239967786\n",
      "Step - 2867, Loss - 0.770992533630348, Learning Rate - 0.0125, magnitude of gradient - 0.7672975660721785\n",
      "Step - 2868, Loss - 0.5357504666911059, Learning Rate - 0.0125, magnitude of gradient - 0.8592970167762731\n",
      "Step - 2869, Loss - 0.6649454838344961, Learning Rate - 0.0125, magnitude of gradient - 0.5767521557952278\n",
      "Step - 2870, Loss - 0.5763510992805985, Learning Rate - 0.0125, magnitude of gradient - 0.9381786444089544\n",
      "Step - 2871, Loss - 0.7908432006616304, Learning Rate - 0.0125, magnitude of gradient - 1.7469081610821917\n",
      "Step - 2872, Loss - 0.6409035419750284, Learning Rate - 0.0125, magnitude of gradient - 0.8663366822781846\n",
      "Step - 2873, Loss - 0.7423352550915008, Learning Rate - 0.0125, magnitude of gradient - 1.0195056442486636\n",
      "Step - 2874, Loss - 0.6961340080903915, Learning Rate - 0.0125, magnitude of gradient - 0.804247309720306\n",
      "Step - 2875, Loss - 0.6127819248989677, Learning Rate - 0.0125, magnitude of gradient - 1.0374759608918054\n",
      "Step - 2876, Loss - 0.7193844002430252, Learning Rate - 0.0125, magnitude of gradient - 1.395740048748876\n",
      "Step - 2877, Loss - 0.6078500416374207, Learning Rate - 0.0125, magnitude of gradient - 1.709176558244528\n",
      "Step - 2878, Loss - 0.7194138993846131, Learning Rate - 0.0125, magnitude of gradient - 0.594543636135098\n",
      "Step - 2879, Loss - 0.7667946091206467, Learning Rate - 0.0125, magnitude of gradient - 1.4754491670417442\n",
      "Step - 2880, Loss - 0.6082366533782146, Learning Rate - 0.0125, magnitude of gradient - 2.7977093500113233\n",
      "Step - 2881, Loss - 0.7093001589039187, Learning Rate - 0.0125, magnitude of gradient - 1.82712387511691\n",
      "Step - 2882, Loss - 0.7429243522363352, Learning Rate - 0.0125, magnitude of gradient - 1.2517706562193367\n",
      "Step - 2883, Loss - 0.7897411112943666, Learning Rate - 0.0125, magnitude of gradient - 1.3590268964223233\n",
      "Step - 2884, Loss - 0.568275452140134, Learning Rate - 0.0125, magnitude of gradient - 1.8427887856843488\n",
      "Step - 2885, Loss - 0.7294995000361384, Learning Rate - 0.0125, magnitude of gradient - 0.8528573767980671\n",
      "Step - 2886, Loss - 0.5681478171472133, Learning Rate - 0.0125, magnitude of gradient - 1.7430673307558324\n",
      "Step - 2887, Loss - 0.7251984648381037, Learning Rate - 0.0125, magnitude of gradient - 1.3404278375398797\n",
      "Step - 2888, Loss - 0.4757764251516887, Learning Rate - 0.0125, magnitude of gradient - 1.5174693697117339\n",
      "Step - 2889, Loss - 0.5701374702403961, Learning Rate - 0.0125, magnitude of gradient - 1.6549356457884714\n",
      "Step - 2890, Loss - 0.7607783987275512, Learning Rate - 0.0125, magnitude of gradient - 1.3924072854890377\n",
      "Step - 2891, Loss - 0.6428135725643555, Learning Rate - 0.0125, magnitude of gradient - 1.5146041267960133\n",
      "Step - 2892, Loss - 0.971773093214864, Learning Rate - 0.0125, magnitude of gradient - 1.447184383758988\n",
      "Step - 2893, Loss - 0.8822717195607767, Learning Rate - 0.0125, magnitude of gradient - 0.617383432023955\n",
      "Step - 2894, Loss - 0.7277035491959472, Learning Rate - 0.0125, magnitude of gradient - 0.8988368024544297\n",
      "Step - 2895, Loss - 0.532412607337458, Learning Rate - 0.0125, magnitude of gradient - 0.5325175924972199\n",
      "Step - 2896, Loss - 0.8007346459169796, Learning Rate - 0.0125, magnitude of gradient - 1.492188765764821\n",
      "Step - 2897, Loss - 0.710575561992235, Learning Rate - 0.0125, magnitude of gradient - 0.7983244711795521\n",
      "Step - 2898, Loss - 0.8678665657504488, Learning Rate - 0.0125, magnitude of gradient - 3.1614634517154077\n",
      "Step - 2899, Loss - 0.44528361807644873, Learning Rate - 0.0125, magnitude of gradient - 1.1466904093801418\n",
      "Step - 2900, Loss - 1.0224923869241918, Learning Rate - 0.0125, magnitude of gradient - 2.4662107091067176\n",
      "Step - 2901, Loss - 0.5843300633095353, Learning Rate - 0.0125, magnitude of gradient - 1.285118865810665\n",
      "Step - 2902, Loss - 0.6399298443923569, Learning Rate - 0.0125, magnitude of gradient - 0.8833718408581507\n",
      "Step - 2903, Loss - 0.8005018634580117, Learning Rate - 0.0125, magnitude of gradient - 1.4011097911025014\n",
      "Step - 2904, Loss - 0.71345186489787, Learning Rate - 0.0125, magnitude of gradient - 1.5586538138949124\n",
      "Step - 2905, Loss - 0.723236874902808, Learning Rate - 0.0125, magnitude of gradient - 1.816396999024342\n",
      "Step - 2906, Loss - 0.4987589103504977, Learning Rate - 0.0125, magnitude of gradient - 0.7617602141329616\n",
      "Step - 2907, Loss - 0.8246883102952584, Learning Rate - 0.0125, magnitude of gradient - 2.7809225999830143\n",
      "Step - 2908, Loss - 0.6024876192302808, Learning Rate - 0.0125, magnitude of gradient - 2.8514179369844577\n",
      "Step - 2909, Loss - 0.7464447339401808, Learning Rate - 0.0125, magnitude of gradient - 1.6370131784197521\n",
      "Step - 2910, Loss - 0.6903153047891064, Learning Rate - 0.0125, magnitude of gradient - 1.395868644792879\n",
      "Step - 2911, Loss - 0.6653890328258084, Learning Rate - 0.0125, magnitude of gradient - 0.4076755653861384\n",
      "Step - 2912, Loss - 0.7310597714911438, Learning Rate - 0.0125, magnitude of gradient - 1.1576605015569505\n",
      "Step - 2913, Loss - 0.7087128388761803, Learning Rate - 0.0125, magnitude of gradient - 2.036001192375621\n",
      "Step - 2914, Loss - 0.6322474096387091, Learning Rate - 0.0125, magnitude of gradient - 1.2070009763930574\n",
      "Step - 2915, Loss - 0.6288370504890872, Learning Rate - 0.0125, magnitude of gradient - 0.9652674158229614\n",
      "Step - 2916, Loss - 0.5067665599005098, Learning Rate - 0.0125, magnitude of gradient - 0.3567637223998299\n",
      "Step - 2917, Loss - 0.6597302528398633, Learning Rate - 0.0125, magnitude of gradient - 2.5466423397398836\n",
      "Step - 2918, Loss - 0.8725540695456583, Learning Rate - 0.0125, magnitude of gradient - 1.5138679060475577\n",
      "Step - 2919, Loss - 0.6998811347204392, Learning Rate - 0.0125, magnitude of gradient - 2.251622073053006\n",
      "Step - 2920, Loss - 0.7656658500781354, Learning Rate - 0.0125, magnitude of gradient - 0.7634526570136709\n",
      "Step - 2921, Loss - 0.5763141034089535, Learning Rate - 0.0125, magnitude of gradient - 1.7987109517111648\n",
      "Step - 2922, Loss - 0.5838280364973691, Learning Rate - 0.0125, magnitude of gradient - 0.6889433938385527\n",
      "Step - 2923, Loss - 0.8596294298600259, Learning Rate - 0.0125, magnitude of gradient - 0.887819113906175\n",
      "Step - 2924, Loss - 0.5787152355636603, Learning Rate - 0.0125, magnitude of gradient - 1.849388132366935\n",
      "Step - 2925, Loss - 0.5444969693950041, Learning Rate - 0.0125, magnitude of gradient - 1.2645030641855226\n",
      "Step - 2926, Loss - 0.6949762923188286, Learning Rate - 0.0125, magnitude of gradient - 1.932534238449376\n",
      "Step - 2927, Loss - 0.7765019460530819, Learning Rate - 0.0125, magnitude of gradient - 2.4141509417063536\n",
      "Step - 2928, Loss - 0.6007291630832607, Learning Rate - 0.0125, magnitude of gradient - 0.6710835822406798\n",
      "Step - 2929, Loss - 0.5624034347658164, Learning Rate - 0.0125, magnitude of gradient - 1.0566513620392803\n",
      "Step - 2930, Loss - 0.9797437625557579, Learning Rate - 0.0125, magnitude of gradient - 0.9953751305505949\n",
      "Step - 2931, Loss - 0.4381413257883503, Learning Rate - 0.0125, magnitude of gradient - 1.6396661622678894\n",
      "Step - 2932, Loss - 0.7934332646965256, Learning Rate - 0.0125, magnitude of gradient - 1.8505687562858644\n",
      "Step - 2933, Loss - 0.9122888945724472, Learning Rate - 0.0125, magnitude of gradient - 0.9555100848695919\n",
      "Step - 2934, Loss - 0.7921193171756841, Learning Rate - 0.0125, magnitude of gradient - 2.8369807346713314\n",
      "Step - 2935, Loss - 0.5864295019057991, Learning Rate - 0.0125, magnitude of gradient - 0.959177659374518\n",
      "Step - 2936, Loss - 0.7053356229043092, Learning Rate - 0.0125, magnitude of gradient - 1.1972881210186834\n",
      "Step - 2937, Loss - 0.7555909712305326, Learning Rate - 0.0125, magnitude of gradient - 0.6506531455207214\n",
      "Step - 2938, Loss - 0.8026543925648675, Learning Rate - 0.0125, magnitude of gradient - 0.7252579414553857\n",
      "Step - 2939, Loss - 0.6743244364761845, Learning Rate - 0.0125, magnitude of gradient - 1.2720962149112782\n",
      "Step - 2940, Loss - 0.8186923375290687, Learning Rate - 0.0125, magnitude of gradient - 1.1799313310152186\n",
      "Step - 2941, Loss - 0.7058836120373893, Learning Rate - 0.0125, magnitude of gradient - 0.8737250002237767\n",
      "Step - 2942, Loss - 0.9700341680739095, Learning Rate - 0.0125, magnitude of gradient - 2.68562781826273\n",
      "Step - 2943, Loss - 0.7972963095773703, Learning Rate - 0.0125, magnitude of gradient - 1.3170756638116992\n",
      "Step - 2944, Loss - 0.794780487371934, Learning Rate - 0.0125, magnitude of gradient - 1.1582893934039602\n",
      "Step - 2945, Loss - 0.8925901109432772, Learning Rate - 0.0125, magnitude of gradient - 2.0064354738640864\n",
      "Step - 2946, Loss - 0.6599335511935902, Learning Rate - 0.0125, magnitude of gradient - 0.6824071444910053\n",
      "Step - 2947, Loss - 0.7786712495578325, Learning Rate - 0.0125, magnitude of gradient - 0.9595558406478016\n",
      "Step - 2948, Loss - 0.6359101120961377, Learning Rate - 0.0125, magnitude of gradient - 1.3792927710297005\n",
      "Step - 2949, Loss - 0.7963942656220271, Learning Rate - 0.0125, magnitude of gradient - 1.9165434697001962\n",
      "Step - 2950, Loss - 0.7057964719007321, Learning Rate - 0.0125, magnitude of gradient - 1.6497678664930047\n",
      "Step - 2951, Loss - 0.5334362771233929, Learning Rate - 0.0125, magnitude of gradient - 3.269789155014311\n",
      "Step - 2952, Loss - 0.7069672920557923, Learning Rate - 0.0125, magnitude of gradient - 0.9541169243354151\n",
      "Step - 2953, Loss - 0.8351368295392818, Learning Rate - 0.0125, magnitude of gradient - 0.734745497594411\n",
      "Step - 2954, Loss - 0.6652671820352382, Learning Rate - 0.0125, magnitude of gradient - 1.8247215797631762\n",
      "Step - 2955, Loss - 0.5798913060038308, Learning Rate - 0.0125, magnitude of gradient - 2.4347557957506565\n",
      "Step - 2956, Loss - 0.7883894489119527, Learning Rate - 0.0125, magnitude of gradient - 1.3288421625528188\n",
      "Step - 2957, Loss - 0.6610965611049745, Learning Rate - 0.0125, magnitude of gradient - 0.5087518920249874\n",
      "Step - 2958, Loss - 0.8310882109417523, Learning Rate - 0.0125, magnitude of gradient - 1.680446623562513\n",
      "Step - 2959, Loss - 0.6885263294763402, Learning Rate - 0.0125, magnitude of gradient - 1.5247309545993788\n",
      "Step - 2960, Loss - 0.7548836577870814, Learning Rate - 0.0125, magnitude of gradient - 1.3536041150843958\n",
      "Step - 2961, Loss - 0.8153687473735239, Learning Rate - 0.0125, magnitude of gradient - 3.370230464095249\n",
      "Step - 2962, Loss - 0.9899449797596784, Learning Rate - 0.0125, magnitude of gradient - 2.26893437294992\n",
      "Step - 2963, Loss - 0.8547764058577014, Learning Rate - 0.0125, magnitude of gradient - 0.9307296179747055\n",
      "Step - 2964, Loss - 0.6857974111905769, Learning Rate - 0.0125, magnitude of gradient - 1.5129415350474675\n",
      "Step - 2965, Loss - 0.6774373817526901, Learning Rate - 0.0125, magnitude of gradient - 1.0941696483683991\n",
      "Step - 2966, Loss - 0.7641371203955194, Learning Rate - 0.0125, magnitude of gradient - 2.193163675005285\n",
      "Step - 2967, Loss - 0.6789008207570126, Learning Rate - 0.0125, magnitude of gradient - 1.063020303385625\n",
      "Step - 2968, Loss - 0.697145174009512, Learning Rate - 0.0125, magnitude of gradient - 2.989447540965262\n",
      "Step - 2969, Loss - 0.7543381757797888, Learning Rate - 0.0125, magnitude of gradient - 1.3809239890870963\n",
      "Step - 2970, Loss - 0.7415747323367108, Learning Rate - 0.0125, magnitude of gradient - 0.8679893456276369\n",
      "Step - 2971, Loss - 0.4838415087109314, Learning Rate - 0.0125, magnitude of gradient - 2.027140304180313\n",
      "Step - 2972, Loss - 0.6933948237066616, Learning Rate - 0.0125, magnitude of gradient - 1.909195810547644\n",
      "Step - 2973, Loss - 1.0276377521972246, Learning Rate - 0.0125, magnitude of gradient - 1.2775188114173086\n",
      "Step - 2974, Loss - 0.6451033387659577, Learning Rate - 0.0125, magnitude of gradient - 1.5504000720779954\n",
      "Step - 2975, Loss - 0.920893147945772, Learning Rate - 0.0125, magnitude of gradient - 1.2477145776180127\n",
      "Step - 2976, Loss - 0.8813369458737484, Learning Rate - 0.0125, magnitude of gradient - 1.781627008209927\n",
      "Step - 2977, Loss - 0.7242125001322495, Learning Rate - 0.0125, magnitude of gradient - 1.052778557059092\n",
      "Step - 2978, Loss - 0.846012837111063, Learning Rate - 0.0125, magnitude of gradient - 1.3573990672078597\n",
      "Step - 2979, Loss - 0.6571971812659395, Learning Rate - 0.0125, magnitude of gradient - 1.3983399327080634\n",
      "Step - 2980, Loss - 0.5245510431687097, Learning Rate - 0.0125, magnitude of gradient - 1.0863646185245288\n",
      "Step - 2981, Loss - 0.6576681324067962, Learning Rate - 0.0125, magnitude of gradient - 1.4151257606014025\n",
      "Step - 2982, Loss - 0.7384704368241909, Learning Rate - 0.0125, magnitude of gradient - 2.209772575202042\n",
      "Step - 2983, Loss - 0.6525525358041462, Learning Rate - 0.0125, magnitude of gradient - 1.0375931033742327\n",
      "Step - 2984, Loss - 0.6217239769308276, Learning Rate - 0.0125, magnitude of gradient - 1.232407804606545\n",
      "Step - 2985, Loss - 0.9835070801426777, Learning Rate - 0.0125, magnitude of gradient - 1.9279635154773607\n",
      "Step - 2986, Loss - 0.6722075294996668, Learning Rate - 0.0125, magnitude of gradient - 2.4105201822022546\n",
      "Step - 2987, Loss - 0.5610322910154384, Learning Rate - 0.0125, magnitude of gradient - 0.47759323823289535\n",
      "Step - 2988, Loss - 0.6392200879049859, Learning Rate - 0.0125, magnitude of gradient - 1.897395228955224\n",
      "Step - 2989, Loss - 0.8089836146432428, Learning Rate - 0.0125, magnitude of gradient - 3.22416830503466\n",
      "Step - 2990, Loss - 0.6411516878407716, Learning Rate - 0.0125, magnitude of gradient - 1.7765570313801389\n",
      "Step - 2991, Loss - 0.8238501405496303, Learning Rate - 0.0125, magnitude of gradient - 3.099368811472403\n",
      "Step - 2992, Loss - 0.7309076849401498, Learning Rate - 0.0125, magnitude of gradient - 0.7293912837558048\n",
      "Step - 2993, Loss - 0.6481818872962459, Learning Rate - 0.0125, magnitude of gradient - 1.781845111925394\n",
      "Step - 2994, Loss - 0.6768909299687895, Learning Rate - 0.0125, magnitude of gradient - 1.0121580642597106\n",
      "Step - 2995, Loss - 0.6908434558749159, Learning Rate - 0.0125, magnitude of gradient - 0.7819504716723877\n",
      "Step - 2996, Loss - 1.0057180386663156, Learning Rate - 0.0125, magnitude of gradient - 2.3110927995642854\n",
      "Step - 2997, Loss - 0.7970332145801393, Learning Rate - 0.0125, magnitude of gradient - 0.8368664955092479\n",
      "Step - 2998, Loss - 0.9083083577267841, Learning Rate - 0.0125, magnitude of gradient - 1.9639799796228699\n",
      "Step - 2999, Loss - 0.5662349493925678, Learning Rate - 0.0125, magnitude of gradient - 2.350122413863279\n",
      "Step - 3000, Loss - 0.531826754543472, Learning Rate - 0.0125, magnitude of gradient - 1.5038714654712635\n",
      "Step - 3001, Loss - 0.7491684865149434, Learning Rate - 0.00625, magnitude of gradient - 2.074299904271881\n",
      "Step - 3002, Loss - 0.8202078644054938, Learning Rate - 0.00625, magnitude of gradient - 1.3423600675627927\n",
      "Step - 3003, Loss - 0.6358720993054602, Learning Rate - 0.00625, magnitude of gradient - 0.9550508017634253\n",
      "Step - 3004, Loss - 0.8470282963254693, Learning Rate - 0.00625, magnitude of gradient - 1.0056459329266512\n",
      "Step - 3005, Loss - 0.8482772157271536, Learning Rate - 0.00625, magnitude of gradient - 0.538123899869204\n",
      "Step - 3006, Loss - 0.6608992534874377, Learning Rate - 0.00625, magnitude of gradient - 0.9594100491524712\n",
      "Step - 3007, Loss - 0.5818170598686088, Learning Rate - 0.00625, magnitude of gradient - 3.1946649050674014\n",
      "Step - 3008, Loss - 0.839100736681891, Learning Rate - 0.00625, magnitude of gradient - 0.9191181670524444\n",
      "Step - 3009, Loss - 0.7408333532341183, Learning Rate - 0.00625, magnitude of gradient - 2.038823389670798\n",
      "Step - 3010, Loss - 0.7307402154171234, Learning Rate - 0.00625, magnitude of gradient - 1.1018730460887007\n",
      "Step - 3011, Loss - 0.8492997882012641, Learning Rate - 0.00625, magnitude of gradient - 1.5996925518018543\n",
      "Step - 3012, Loss - 0.722908408478719, Learning Rate - 0.00625, magnitude of gradient - 1.2393978939663488\n",
      "Step - 3013, Loss - 0.673445330087888, Learning Rate - 0.00625, magnitude of gradient - 0.35806949211789335\n",
      "Step - 3014, Loss - 0.5801741702600542, Learning Rate - 0.00625, magnitude of gradient - 0.364142451909002\n",
      "Step - 3015, Loss - 0.6887868912675043, Learning Rate - 0.00625, magnitude of gradient - 0.9043779246551288\n",
      "Step - 3016, Loss - 0.644913572829625, Learning Rate - 0.00625, magnitude of gradient - 1.2222665810268043\n",
      "Step - 3017, Loss - 0.699192019967705, Learning Rate - 0.00625, magnitude of gradient - 0.39209008596602823\n",
      "Step - 3018, Loss - 0.8142504798962961, Learning Rate - 0.00625, magnitude of gradient - 1.043915011642078\n",
      "Step - 3019, Loss - 0.7097308033528591, Learning Rate - 0.00625, magnitude of gradient - 1.2167421201028619\n",
      "Step - 3020, Loss - 0.5768409689203318, Learning Rate - 0.00625, magnitude of gradient - 0.847283889349625\n",
      "Step - 3021, Loss - 0.6418769958626248, Learning Rate - 0.00625, magnitude of gradient - 1.0870563800869197\n",
      "Step - 3022, Loss - 0.4610899324294268, Learning Rate - 0.00625, magnitude of gradient - 1.409339347814658\n",
      "Step - 3023, Loss - 0.7053118647612623, Learning Rate - 0.00625, magnitude of gradient - 1.2576035030457584\n",
      "Step - 3024, Loss - 0.6355371899661952, Learning Rate - 0.00625, magnitude of gradient - 0.592522938358885\n",
      "Step - 3025, Loss - 0.7049324052768, Learning Rate - 0.00625, magnitude of gradient - 1.6198900924223145\n",
      "Step - 3026, Loss - 0.6777428962404711, Learning Rate - 0.00625, magnitude of gradient - 1.5713118496342267\n",
      "Step - 3027, Loss - 0.712615018746258, Learning Rate - 0.00625, magnitude of gradient - 0.8692711430908144\n",
      "Step - 3028, Loss - 0.7232299037469996, Learning Rate - 0.00625, magnitude of gradient - 1.9971609573508975\n",
      "Step - 3029, Loss - 0.8773132361610261, Learning Rate - 0.00625, magnitude of gradient - 0.8079690409401278\n",
      "Step - 3030, Loss - 0.6220593189672357, Learning Rate - 0.00625, magnitude of gradient - 0.2842142936890753\n",
      "Step - 3031, Loss - 0.5817688025107006, Learning Rate - 0.00625, magnitude of gradient - 1.9113593778240447\n",
      "Step - 3032, Loss - 0.6178400234160543, Learning Rate - 0.00625, magnitude of gradient - 0.99560676516871\n",
      "Step - 3033, Loss - 0.6404602287865211, Learning Rate - 0.00625, magnitude of gradient - 0.95771496892032\n",
      "Step - 3034, Loss - 0.7347689525644996, Learning Rate - 0.00625, magnitude of gradient - 2.6638231341433167\n",
      "Step - 3035, Loss - 0.6736995244923802, Learning Rate - 0.00625, magnitude of gradient - 0.595551496658277\n",
      "Step - 3036, Loss - 0.5071504342675629, Learning Rate - 0.00625, magnitude of gradient - 0.9660162771384959\n",
      "Step - 3037, Loss - 0.6453041117885374, Learning Rate - 0.00625, magnitude of gradient - 1.1628274037468629\n",
      "Step - 3038, Loss - 0.6222144794567137, Learning Rate - 0.00625, magnitude of gradient - 0.8195044570679236\n",
      "Step - 3039, Loss - 0.6168532832499362, Learning Rate - 0.00625, magnitude of gradient - 2.2009762578012677\n",
      "Step - 3040, Loss - 0.7319793864324238, Learning Rate - 0.00625, magnitude of gradient - 2.1000995041999095\n",
      "Step - 3041, Loss - 0.5535953072754108, Learning Rate - 0.00625, magnitude of gradient - 0.9030441794799531\n",
      "Step - 3042, Loss - 0.7041543360778735, Learning Rate - 0.00625, magnitude of gradient - 1.237795729304922\n",
      "Step - 3043, Loss - 0.7598848330227194, Learning Rate - 0.00625, magnitude of gradient - 2.079734108656209\n",
      "Step - 3044, Loss - 0.8073108772626082, Learning Rate - 0.00625, magnitude of gradient - 1.6084562801181195\n",
      "Step - 3045, Loss - 0.6973386610912556, Learning Rate - 0.00625, magnitude of gradient - 1.5713068144131381\n",
      "Step - 3046, Loss - 0.7734288924092665, Learning Rate - 0.00625, magnitude of gradient - 1.3394628505887904\n",
      "Step - 3047, Loss - 0.6503259177936334, Learning Rate - 0.00625, magnitude of gradient - 1.348716833649527\n",
      "Step - 3048, Loss - 0.8786996333392738, Learning Rate - 0.00625, magnitude of gradient - 1.1648973895993457\n",
      "Step - 3049, Loss - 0.8044268351411418, Learning Rate - 0.00625, magnitude of gradient - 1.2627951345395698\n",
      "Step - 3050, Loss - 0.6774749969906211, Learning Rate - 0.00625, magnitude of gradient - 0.42949510966082793\n",
      "Step - 3051, Loss - 0.9127082859768936, Learning Rate - 0.00625, magnitude of gradient - 0.773418197678987\n",
      "Step - 3052, Loss - 0.6547471410099555, Learning Rate - 0.00625, magnitude of gradient - 0.9506427150931722\n",
      "Step - 3053, Loss - 0.6719251245867589, Learning Rate - 0.00625, magnitude of gradient - 3.220329989221392\n",
      "Step - 3054, Loss - 0.5836688402537074, Learning Rate - 0.00625, magnitude of gradient - 1.0817104696970596\n",
      "Step - 3055, Loss - 0.8776836785250011, Learning Rate - 0.00625, magnitude of gradient - 2.171701443135571\n",
      "Step - 3056, Loss - 0.817687144539466, Learning Rate - 0.00625, magnitude of gradient - 1.9265367002740714\n",
      "Step - 3057, Loss - 0.65588134373207, Learning Rate - 0.00625, magnitude of gradient - 1.2346595175166428\n",
      "Step - 3058, Loss - 0.7324893984754112, Learning Rate - 0.00625, magnitude of gradient - 0.20687375384969725\n",
      "Step - 3059, Loss - 0.6216403339436296, Learning Rate - 0.00625, magnitude of gradient - 1.2154948025761476\n",
      "Step - 3060, Loss - 0.729421246043908, Learning Rate - 0.00625, magnitude of gradient - 1.900850040537529\n",
      "Step - 3061, Loss - 0.7093522173114859, Learning Rate - 0.00625, magnitude of gradient - 1.1240783943070125\n",
      "Step - 3062, Loss - 0.8624706569284439, Learning Rate - 0.00625, magnitude of gradient - 2.4279804756369985\n",
      "Step - 3063, Loss - 0.8275391152339069, Learning Rate - 0.00625, magnitude of gradient - 1.1535208219558488\n",
      "Step - 3064, Loss - 0.7279740903979609, Learning Rate - 0.00625, magnitude of gradient - 1.1842130756764366\n",
      "Step - 3065, Loss - 0.47294861409475125, Learning Rate - 0.00625, magnitude of gradient - 0.8672899009405193\n",
      "Step - 3066, Loss - 0.8587848568968616, Learning Rate - 0.00625, magnitude of gradient - 1.3927334604427308\n",
      "Step - 3067, Loss - 0.9114499036863242, Learning Rate - 0.00625, magnitude of gradient - 1.2870341608262035\n",
      "Step - 3068, Loss - 0.8261332062631209, Learning Rate - 0.00625, magnitude of gradient - 1.7567422724155963\n",
      "Step - 3069, Loss - 0.6472256887997488, Learning Rate - 0.00625, magnitude of gradient - 0.7898427822550038\n",
      "Step - 3070, Loss - 0.755007360343133, Learning Rate - 0.00625, magnitude of gradient - 1.5872967430261384\n",
      "Step - 3071, Loss - 0.7257422317552171, Learning Rate - 0.00625, magnitude of gradient - 0.9778543330443725\n",
      "Step - 3072, Loss - 0.5987051714119088, Learning Rate - 0.00625, magnitude of gradient - 0.4889910092956695\n",
      "Step - 3073, Loss - 0.7288162666396877, Learning Rate - 0.00625, magnitude of gradient - 1.127545997590663\n",
      "Step - 3074, Loss - 0.685267374348765, Learning Rate - 0.00625, magnitude of gradient - 1.200699978939427\n",
      "Step - 3075, Loss - 0.7328099345474642, Learning Rate - 0.00625, magnitude of gradient - 2.6548895574370914\n",
      "Step - 3076, Loss - 0.7317549139875443, Learning Rate - 0.00625, magnitude of gradient - 0.40080601000103006\n",
      "Step - 3077, Loss - 0.8274523939511517, Learning Rate - 0.00625, magnitude of gradient - 1.2404739078432052\n",
      "Step - 3078, Loss - 0.4791680692914829, Learning Rate - 0.00625, magnitude of gradient - 2.3638432276092156\n",
      "Step - 3079, Loss - 0.6201629633415253, Learning Rate - 0.00625, magnitude of gradient - 1.536801110175098\n",
      "Step - 3080, Loss - 0.7736268078352723, Learning Rate - 0.00625, magnitude of gradient - 0.5093711018188688\n",
      "Step - 3081, Loss - 0.6438801354941027, Learning Rate - 0.00625, magnitude of gradient - 1.2576697218438613\n",
      "Step - 3082, Loss - 0.6053631726848765, Learning Rate - 0.00625, magnitude of gradient - 0.663986527477601\n",
      "Step - 3083, Loss - 0.5102219267629087, Learning Rate - 0.00625, magnitude of gradient - 0.6149919613715787\n",
      "Step - 3084, Loss - 0.7990666245280464, Learning Rate - 0.00625, magnitude of gradient - 1.705705067420557\n",
      "Step - 3085, Loss - 0.6463268419139999, Learning Rate - 0.00625, magnitude of gradient - 0.7362044237665978\n",
      "Step - 3086, Loss - 0.7194900743110515, Learning Rate - 0.00625, magnitude of gradient - 0.809863776314068\n",
      "Step - 3087, Loss - 0.5468647498784068, Learning Rate - 0.00625, magnitude of gradient - 2.0145465881088507\n",
      "Step - 3088, Loss - 0.7128439872892405, Learning Rate - 0.00625, magnitude of gradient - 1.0293037039679662\n",
      "Step - 3089, Loss - 0.5748218254996353, Learning Rate - 0.00625, magnitude of gradient - 0.7819066179874535\n",
      "Step - 3090, Loss - 0.7306331466859932, Learning Rate - 0.00625, magnitude of gradient - 1.375083952827576\n",
      "Step - 3091, Loss - 0.7575193962569954, Learning Rate - 0.00625, magnitude of gradient - 0.7693618767269151\n",
      "Step - 3092, Loss - 0.535672674776791, Learning Rate - 0.00625, magnitude of gradient - 1.5465991721037657\n",
      "Step - 3093, Loss - 0.80287522685606, Learning Rate - 0.00625, magnitude of gradient - 1.9270210240787042\n",
      "Step - 3094, Loss - 0.6169005474655612, Learning Rate - 0.00625, magnitude of gradient - 1.5379605757892831\n",
      "Step - 3095, Loss - 0.7873439448690972, Learning Rate - 0.00625, magnitude of gradient - 0.6200879717307858\n",
      "Step - 3096, Loss - 0.749776957924306, Learning Rate - 0.00625, magnitude of gradient - 0.5896148406440372\n",
      "Step - 3097, Loss - 0.566861246087966, Learning Rate - 0.00625, magnitude of gradient - 0.2920256798387047\n",
      "Step - 3098, Loss - 0.7131786786600125, Learning Rate - 0.00625, magnitude of gradient - 0.8853600663062939\n",
      "Step - 3099, Loss - 0.5276498576688353, Learning Rate - 0.00625, magnitude of gradient - 2.29037839580586\n",
      "Step - 3100, Loss - 0.9003794382216818, Learning Rate - 0.00625, magnitude of gradient - 0.8946337372824765\n",
      "Step - 3101, Loss - 0.8696529429746316, Learning Rate - 0.00625, magnitude of gradient - 1.31755423406045\n",
      "Step - 3102, Loss - 0.8376251184738195, Learning Rate - 0.00625, magnitude of gradient - 0.5471119722118906\n",
      "Step - 3103, Loss - 0.6104890851014033, Learning Rate - 0.00625, magnitude of gradient - 1.331487823588452\n",
      "Step - 3104, Loss - 0.6968473243875589, Learning Rate - 0.00625, magnitude of gradient - 1.2794208032367957\n",
      "Step - 3105, Loss - 0.8359476620280779, Learning Rate - 0.00625, magnitude of gradient - 0.47033121035592834\n",
      "Step - 3106, Loss - 0.7350730958587889, Learning Rate - 0.00625, magnitude of gradient - 0.5516788522993262\n",
      "Step - 3107, Loss - 0.7158159932193477, Learning Rate - 0.00625, magnitude of gradient - 1.069740394910599\n",
      "Step - 3108, Loss - 0.6583874058271, Learning Rate - 0.00625, magnitude of gradient - 2.2266701556623656\n",
      "Step - 3109, Loss - 0.6798356427364816, Learning Rate - 0.00625, magnitude of gradient - 0.7235911559159116\n",
      "Step - 3110, Loss - 0.633498581064408, Learning Rate - 0.00625, magnitude of gradient - 0.7022733122698512\n",
      "Step - 3111, Loss - 0.7237061068985245, Learning Rate - 0.00625, magnitude of gradient - 1.1011852402906697\n",
      "Step - 3112, Loss - 0.6262421661644656, Learning Rate - 0.00625, magnitude of gradient - 1.4715862359083354\n",
      "Step - 3113, Loss - 0.7269166194270713, Learning Rate - 0.00625, magnitude of gradient - 1.810372559034456\n",
      "Step - 3114, Loss - 0.5249892587108224, Learning Rate - 0.00625, magnitude of gradient - 2.268603747918188\n",
      "Step - 3115, Loss - 0.7560510961201614, Learning Rate - 0.00625, magnitude of gradient - 1.8438358511032595\n",
      "Step - 3116, Loss - 0.5803510791443056, Learning Rate - 0.00625, magnitude of gradient - 0.7425238513827472\n",
      "Step - 3117, Loss - 0.5493492832693749, Learning Rate - 0.00625, magnitude of gradient - 1.5371966365645189\n",
      "Step - 3118, Loss - 0.6529423113220574, Learning Rate - 0.00625, magnitude of gradient - 0.4665797930296704\n",
      "Step - 3119, Loss - 0.7001443125295974, Learning Rate - 0.00625, magnitude of gradient - 1.2134393502912952\n",
      "Step - 3120, Loss - 0.7960106353141179, Learning Rate - 0.00625, magnitude of gradient - 1.1842109243906558\n",
      "Step - 3121, Loss - 0.8296668185364335, Learning Rate - 0.00625, magnitude of gradient - 1.6769083773334343\n",
      "Step - 3122, Loss - 0.7063391822448415, Learning Rate - 0.00625, magnitude of gradient - 1.5728695834567952\n",
      "Step - 3123, Loss - 0.7839449080484425, Learning Rate - 0.00625, magnitude of gradient - 2.028890703897823\n",
      "Step - 3124, Loss - 0.9462835719427516, Learning Rate - 0.00625, magnitude of gradient - 1.7613166980698252\n",
      "Step - 3125, Loss - 0.6708576168315448, Learning Rate - 0.00625, magnitude of gradient - 0.7668745096259219\n",
      "Step - 3126, Loss - 0.6387401245930431, Learning Rate - 0.00625, magnitude of gradient - 0.40618219006754014\n",
      "Step - 3127, Loss - 0.6586898233364729, Learning Rate - 0.00625, magnitude of gradient - 0.39987908067012645\n",
      "Step - 3128, Loss - 0.7589489824141009, Learning Rate - 0.00625, magnitude of gradient - 1.1821671622549124\n",
      "Step - 3129, Loss - 0.9018983773292537, Learning Rate - 0.00625, magnitude of gradient - 1.724087064901177\n",
      "Step - 3130, Loss - 0.7503525134031002, Learning Rate - 0.00625, magnitude of gradient - 1.6711050169950776\n",
      "Step - 3131, Loss - 0.8135749381315048, Learning Rate - 0.00625, magnitude of gradient - 0.6871043054428859\n",
      "Step - 3132, Loss - 0.6541149296436046, Learning Rate - 0.00625, magnitude of gradient - 1.758966314753991\n",
      "Step - 3133, Loss - 0.7027041543833827, Learning Rate - 0.00625, magnitude of gradient - 2.3696907449889943\n",
      "Step - 3134, Loss - 0.7230966482141575, Learning Rate - 0.00625, magnitude of gradient - 1.5787137956548902\n",
      "Step - 3135, Loss - 0.724117256134049, Learning Rate - 0.00625, magnitude of gradient - 1.1067497909381354\n",
      "Step - 3136, Loss - 0.6378353355032719, Learning Rate - 0.00625, magnitude of gradient - 2.0345675431448482\n",
      "Step - 3137, Loss - 0.5949843239986139, Learning Rate - 0.00625, magnitude of gradient - 1.5515843177499986\n",
      "Step - 3138, Loss - 0.8224539722017048, Learning Rate - 0.00625, magnitude of gradient - 1.7401305933394589\n",
      "Step - 3139, Loss - 0.7515700681264986, Learning Rate - 0.00625, magnitude of gradient - 2.2846654307702883\n",
      "Step - 3140, Loss - 0.627382112196078, Learning Rate - 0.00625, magnitude of gradient - 0.6123984505541539\n",
      "Step - 3141, Loss - 0.6756069852966177, Learning Rate - 0.00625, magnitude of gradient - 1.3553816707891564\n",
      "Step - 3142, Loss - 0.5180227357879894, Learning Rate - 0.00625, magnitude of gradient - 1.594018559355743\n",
      "Step - 3143, Loss - 0.6300668136207833, Learning Rate - 0.00625, magnitude of gradient - 1.3611853768975537\n",
      "Step - 3144, Loss - 0.6937855855917769, Learning Rate - 0.00625, magnitude of gradient - 1.3438363953489008\n",
      "Step - 3145, Loss - 0.6059769031732252, Learning Rate - 0.00625, magnitude of gradient - 2.152525081344612\n",
      "Step - 3146, Loss - 0.7229257970939869, Learning Rate - 0.00625, magnitude of gradient - 0.8024589741885777\n",
      "Step - 3147, Loss - 0.7677414554349185, Learning Rate - 0.00625, magnitude of gradient - 1.2727740862901078\n",
      "Step - 3148, Loss - 0.6753657087735825, Learning Rate - 0.00625, magnitude of gradient - 0.5038358332181818\n",
      "Step - 3149, Loss - 0.696573851258075, Learning Rate - 0.00625, magnitude of gradient - 1.5523750538051966\n",
      "Step - 3150, Loss - 0.8040441342138906, Learning Rate - 0.00625, magnitude of gradient - 0.8153921217686991\n",
      "Step - 3151, Loss - 0.6142105376553645, Learning Rate - 0.00625, magnitude of gradient - 2.065014966902858\n",
      "Step - 3152, Loss - 0.7626562965768195, Learning Rate - 0.00625, magnitude of gradient - 2.0423115043909807\n",
      "Step - 3153, Loss - 0.6849064683014389, Learning Rate - 0.00625, magnitude of gradient - 1.3587082016341603\n",
      "Step - 3154, Loss - 0.5692271362899832, Learning Rate - 0.00625, magnitude of gradient - 1.969621740107424\n",
      "Step - 3155, Loss - 0.601999525854973, Learning Rate - 0.00625, magnitude of gradient - 4.230837907382649\n",
      "Step - 3156, Loss - 0.8850503987582141, Learning Rate - 0.00625, magnitude of gradient - 1.7577717909517447\n",
      "Step - 3157, Loss - 0.5779888007340552, Learning Rate - 0.00625, magnitude of gradient - 1.6800521063808047\n",
      "Step - 3158, Loss - 0.9086484795820504, Learning Rate - 0.00625, magnitude of gradient - 3.0214915559485496\n",
      "Step - 3159, Loss - 0.7240294801087901, Learning Rate - 0.00625, magnitude of gradient - 1.9982798348883284\n",
      "Step - 3160, Loss - 0.48728682280840074, Learning Rate - 0.00625, magnitude of gradient - 0.8313026879692917\n",
      "Step - 3161, Loss - 0.8215782643714369, Learning Rate - 0.00625, magnitude of gradient - 1.2018406618058515\n",
      "Step - 3162, Loss - 0.6045561998853117, Learning Rate - 0.00625, magnitude of gradient - 0.8172399970969758\n",
      "Step - 3163, Loss - 0.7650758875239044, Learning Rate - 0.00625, magnitude of gradient - 1.721082286649126\n",
      "Step - 3164, Loss - 0.7882301450094215, Learning Rate - 0.00625, magnitude of gradient - 1.2618509905413893\n",
      "Step - 3165, Loss - 0.5699988162145331, Learning Rate - 0.00625, magnitude of gradient - 1.271996144902454\n",
      "Step - 3166, Loss - 0.550589942060316, Learning Rate - 0.00625, magnitude of gradient - 0.47356980040674174\n",
      "Step - 3167, Loss - 0.6221409860760656, Learning Rate - 0.00625, magnitude of gradient - 0.5226269142564505\n",
      "Step - 3168, Loss - 0.7776929382393667, Learning Rate - 0.00625, magnitude of gradient - 1.5381248648144323\n",
      "Step - 3169, Loss - 0.613614615235514, Learning Rate - 0.00625, magnitude of gradient - 2.0413558004603214\n",
      "Step - 3170, Loss - 0.8310836555363346, Learning Rate - 0.00625, magnitude of gradient - 2.3183714168583758\n",
      "Step - 3171, Loss - 0.8102705088406742, Learning Rate - 0.00625, magnitude of gradient - 1.0465210435133905\n",
      "Step - 3172, Loss - 0.6171890247716593, Learning Rate - 0.00625, magnitude of gradient - 1.9465550974593562\n",
      "Step - 3173, Loss - 0.6808104036899835, Learning Rate - 0.00625, magnitude of gradient - 0.5609264881334152\n",
      "Step - 3174, Loss - 0.593080760302811, Learning Rate - 0.00625, magnitude of gradient - 0.42727950643993806\n",
      "Step - 3175, Loss - 0.6836371467446226, Learning Rate - 0.00625, magnitude of gradient - 1.8038365490078787\n",
      "Step - 3176, Loss - 0.7648528507341621, Learning Rate - 0.00625, magnitude of gradient - 2.519871016683427\n",
      "Step - 3177, Loss - 0.8179202554795181, Learning Rate - 0.00625, magnitude of gradient - 2.659300389454947\n",
      "Step - 3178, Loss - 0.6987917453392171, Learning Rate - 0.00625, magnitude of gradient - 1.5615830866551121\n",
      "Step - 3179, Loss - 0.8376941949524636, Learning Rate - 0.00625, magnitude of gradient - 1.3257911391629968\n",
      "Step - 3180, Loss - 0.7283866920136098, Learning Rate - 0.00625, magnitude of gradient - 1.8581290482256994\n",
      "Step - 3181, Loss - 0.6655516242919589, Learning Rate - 0.00625, magnitude of gradient - 1.1993227354552332\n",
      "Step - 3182, Loss - 0.593297318947829, Learning Rate - 0.00625, magnitude of gradient - 1.6611929059147619\n",
      "Step - 3183, Loss - 0.644869125293105, Learning Rate - 0.00625, magnitude of gradient - 1.1816702604562161\n",
      "Step - 3184, Loss - 0.7672592388664927, Learning Rate - 0.00625, magnitude of gradient - 0.7886152280070677\n",
      "Step - 3185, Loss - 0.7977479042263518, Learning Rate - 0.00625, magnitude of gradient - 0.8130806414292416\n",
      "Step - 3186, Loss - 0.6190582625726705, Learning Rate - 0.00625, magnitude of gradient - 0.9753821069698381\n",
      "Step - 3187, Loss - 0.6020975238327598, Learning Rate - 0.00625, magnitude of gradient - 1.3316013291362705\n",
      "Step - 3188, Loss - 0.5501873229579854, Learning Rate - 0.00625, magnitude of gradient - 0.5315261585543277\n",
      "Step - 3189, Loss - 0.661377108294067, Learning Rate - 0.00625, magnitude of gradient - 0.8196895899625253\n",
      "Step - 3190, Loss - 0.6532653364549854, Learning Rate - 0.00625, magnitude of gradient - 0.9732122211784747\n",
      "Step - 3191, Loss - 0.7667424057161293, Learning Rate - 0.00625, magnitude of gradient - 1.6919691119331306\n",
      "Step - 3192, Loss - 0.6383701367546453, Learning Rate - 0.00625, magnitude of gradient - 2.7447030270874535\n",
      "Step - 3193, Loss - 0.6646904552629899, Learning Rate - 0.00625, magnitude of gradient - 0.6031463107807754\n",
      "Step - 3194, Loss - 0.7362683131214705, Learning Rate - 0.00625, magnitude of gradient - 2.2636237087423545\n",
      "Step - 3195, Loss - 0.7820447537803917, Learning Rate - 0.00625, magnitude of gradient - 1.184654174877725\n",
      "Step - 3196, Loss - 0.7074978672826431, Learning Rate - 0.00625, magnitude of gradient - 1.2552261063744345\n",
      "Step - 3197, Loss - 0.9361952261498019, Learning Rate - 0.00625, magnitude of gradient - 2.010752859522545\n",
      "Step - 3198, Loss - 0.8585590030674504, Learning Rate - 0.00625, magnitude of gradient - 1.6460224177411966\n",
      "Step - 3199, Loss - 0.5728064985268353, Learning Rate - 0.00625, magnitude of gradient - 1.693722454618882\n",
      "Step - 3200, Loss - 0.6063935103092669, Learning Rate - 0.00625, magnitude of gradient - 0.7003598982493346\n",
      "Step - 3201, Loss - 0.5565904169263459, Learning Rate - 0.00625, magnitude of gradient - 0.31319478315933785\n",
      "Step - 3202, Loss - 0.7097017912040766, Learning Rate - 0.00625, magnitude of gradient - 1.5059589686868533\n",
      "Step - 3203, Loss - 0.5826970165293541, Learning Rate - 0.00625, magnitude of gradient - 1.986002694002783\n",
      "Step - 3204, Loss - 0.6562058493825843, Learning Rate - 0.00625, magnitude of gradient - 0.5257437216753794\n",
      "Step - 3205, Loss - 0.6761113564603508, Learning Rate - 0.00625, magnitude of gradient - 1.1773958246483167\n",
      "Step - 3206, Loss - 0.6792387559100932, Learning Rate - 0.00625, magnitude of gradient - 1.2384419956405848\n",
      "Step - 3207, Loss - 0.6613751562200602, Learning Rate - 0.00625, magnitude of gradient - 1.5283906417681337\n",
      "Step - 3208, Loss - 0.5074182088019836, Learning Rate - 0.00625, magnitude of gradient - 0.8142518065211786\n",
      "Step - 3209, Loss - 0.7220915591173973, Learning Rate - 0.00625, magnitude of gradient - 0.9792117835977544\n",
      "Step - 3210, Loss - 0.6968403682589185, Learning Rate - 0.00625, magnitude of gradient - 1.7192377387531823\n",
      "Step - 3211, Loss - 0.590008329025394, Learning Rate - 0.00625, magnitude of gradient - 1.986435517488101\n",
      "Step - 3212, Loss - 0.661059645085283, Learning Rate - 0.00625, magnitude of gradient - 0.539497984186798\n",
      "Step - 3213, Loss - 0.855661377862148, Learning Rate - 0.00625, magnitude of gradient - 0.9493882697622914\n",
      "Step - 3214, Loss - 0.8080180579472127, Learning Rate - 0.00625, magnitude of gradient - 0.8767287609696023\n",
      "Step - 3215, Loss - 0.7058655426482398, Learning Rate - 0.00625, magnitude of gradient - 1.3036873213957845\n",
      "Step - 3216, Loss - 0.7191398141121875, Learning Rate - 0.00625, magnitude of gradient - 1.690820564355371\n",
      "Step - 3217, Loss - 0.7513624797522405, Learning Rate - 0.00625, magnitude of gradient - 1.8532927884689787\n",
      "Step - 3218, Loss - 0.6611494650648247, Learning Rate - 0.00625, magnitude of gradient - 1.9110473948729536\n",
      "Step - 3219, Loss - 0.6973623749617475, Learning Rate - 0.00625, magnitude of gradient - 1.5822665612682165\n",
      "Step - 3220, Loss - 0.676065367614356, Learning Rate - 0.00625, magnitude of gradient - 1.7516695237564017\n",
      "Step - 3221, Loss - 0.7407191310957177, Learning Rate - 0.00625, magnitude of gradient - 0.8452871437747657\n",
      "Step - 3222, Loss - 0.685626618671897, Learning Rate - 0.00625, magnitude of gradient - 0.727017630636878\n",
      "Step - 3223, Loss - 0.595847158304128, Learning Rate - 0.00625, magnitude of gradient - 0.5584801379063754\n",
      "Step - 3224, Loss - 0.7640163083469831, Learning Rate - 0.00625, magnitude of gradient - 0.6481425904753572\n",
      "Step - 3225, Loss - 0.8733065923364287, Learning Rate - 0.00625, magnitude of gradient - 2.057434036189655\n",
      "Step - 3226, Loss - 0.6266613972833723, Learning Rate - 0.00625, magnitude of gradient - 0.4698385429473856\n",
      "Step - 3227, Loss - 0.7493053065450603, Learning Rate - 0.00625, magnitude of gradient - 1.314222511784621\n",
      "Step - 3228, Loss - 0.7341853424070026, Learning Rate - 0.00625, magnitude of gradient - 1.326153126810063\n",
      "Step - 3229, Loss - 0.5970129370083548, Learning Rate - 0.00625, magnitude of gradient - 1.193671001741726\n",
      "Step - 3230, Loss - 0.7854003543155599, Learning Rate - 0.00625, magnitude of gradient - 1.1195559041758143\n",
      "Step - 3231, Loss - 0.8471466413867917, Learning Rate - 0.00625, magnitude of gradient - 0.7744066121557416\n",
      "Step - 3232, Loss - 0.8675122561181365, Learning Rate - 0.00625, magnitude of gradient - 0.8844877187620918\n",
      "Step - 3233, Loss - 0.7075339906777764, Learning Rate - 0.00625, magnitude of gradient - 0.6911804707791442\n",
      "Step - 3234, Loss - 0.7406515511338029, Learning Rate - 0.00625, magnitude of gradient - 1.0670237095191122\n",
      "Step - 3235, Loss - 0.6886244585714727, Learning Rate - 0.00625, magnitude of gradient - 1.0143860018822717\n",
      "Step - 3236, Loss - 0.6830238795412023, Learning Rate - 0.00625, magnitude of gradient - 0.428510326590707\n",
      "Step - 3237, Loss - 0.6331463741131834, Learning Rate - 0.00625, magnitude of gradient - 1.3042859179270414\n",
      "Step - 3238, Loss - 0.5742190797980256, Learning Rate - 0.00625, magnitude of gradient - 1.5504006104080206\n",
      "Step - 3239, Loss - 0.7491498812072604, Learning Rate - 0.00625, magnitude of gradient - 1.910989279339471\n",
      "Step - 3240, Loss - 0.6747489174800955, Learning Rate - 0.00625, magnitude of gradient - 1.0686988170425133\n",
      "Step - 3241, Loss - 0.6749304327171273, Learning Rate - 0.00625, magnitude of gradient - 0.6850397038930284\n",
      "Step - 3242, Loss - 0.6591039271396005, Learning Rate - 0.00625, magnitude of gradient - 1.6770716601575504\n",
      "Step - 3243, Loss - 0.7344467468952287, Learning Rate - 0.00625, magnitude of gradient - 2.2004884701065803\n",
      "Step - 3244, Loss - 0.7962119930314324, Learning Rate - 0.00625, magnitude of gradient - 1.7859092879439988\n",
      "Step - 3245, Loss - 0.5381273315023629, Learning Rate - 0.00625, magnitude of gradient - 1.1470332588926224\n",
      "Step - 3246, Loss - 0.9448290487294425, Learning Rate - 0.00625, magnitude of gradient - 1.9264184922537524\n",
      "Step - 3247, Loss - 0.6695297602378354, Learning Rate - 0.00625, magnitude of gradient - 1.1469305576953466\n",
      "Step - 3248, Loss - 0.6368222777735806, Learning Rate - 0.00625, magnitude of gradient - 3.2679678740967075\n",
      "Step - 3249, Loss - 0.6602315292337966, Learning Rate - 0.00625, magnitude of gradient - 1.0807485419540377\n",
      "Step - 3250, Loss - 0.6813687639959036, Learning Rate - 0.00625, magnitude of gradient - 1.6868423077757513\n",
      "Step - 3251, Loss - 0.7497541594473643, Learning Rate - 0.00625, magnitude of gradient - 0.9504185012218052\n",
      "Step - 3252, Loss - 0.8516277972195649, Learning Rate - 0.00625, magnitude of gradient - 1.2074567154328266\n",
      "Step - 3253, Loss - 0.9575675109259617, Learning Rate - 0.00625, magnitude of gradient - 0.6667514282370864\n",
      "Step - 3254, Loss - 0.5444948198550321, Learning Rate - 0.00625, magnitude of gradient - 1.8608812088593238\n",
      "Step - 3255, Loss - 0.7421800724506246, Learning Rate - 0.00625, magnitude of gradient - 0.823232064156002\n",
      "Step - 3256, Loss - 0.656509035282795, Learning Rate - 0.00625, magnitude of gradient - 1.5767812551993685\n",
      "Step - 3257, Loss - 0.7925244811025409, Learning Rate - 0.00625, magnitude of gradient - 1.0849069162774252\n",
      "Step - 3258, Loss - 0.6969630565232818, Learning Rate - 0.00625, magnitude of gradient - 0.6000436560786571\n",
      "Step - 3259, Loss - 0.811019933022523, Learning Rate - 0.00625, magnitude of gradient - 1.2793405930734976\n",
      "Step - 3260, Loss - 0.7136565041748413, Learning Rate - 0.00625, magnitude of gradient - 0.9048946911603467\n",
      "Step - 3261, Loss - 0.7574789481298203, Learning Rate - 0.00625, magnitude of gradient - 3.4355746416583015\n",
      "Step - 3262, Loss - 0.5107915477774332, Learning Rate - 0.00625, magnitude of gradient - 0.7389330972358987\n",
      "Step - 3263, Loss - 0.716024220356353, Learning Rate - 0.00625, magnitude of gradient - 1.7389438843559009\n",
      "Step - 3264, Loss - 0.7454366058260472, Learning Rate - 0.00625, magnitude of gradient - 0.7611186744160916\n",
      "Step - 3265, Loss - 0.6332045388803423, Learning Rate - 0.00625, magnitude of gradient - 0.9087817154776558\n",
      "Step - 3266, Loss - 0.7372763961698359, Learning Rate - 0.00625, magnitude of gradient - 1.131572310624649\n",
      "Step - 3267, Loss - 0.9291057500256185, Learning Rate - 0.00625, magnitude of gradient - 1.5657352020537096\n",
      "Step - 3268, Loss - 0.685431110715086, Learning Rate - 0.00625, magnitude of gradient - 0.4876473148843718\n",
      "Step - 3269, Loss - 0.733521758760989, Learning Rate - 0.00625, magnitude of gradient - 1.383820357128306\n",
      "Step - 3270, Loss - 0.7211118370119451, Learning Rate - 0.00625, magnitude of gradient - 1.9475712819633932\n",
      "Step - 3271, Loss - 0.6234903540779305, Learning Rate - 0.00625, magnitude of gradient - 1.6974971778829413\n",
      "Step - 3272, Loss - 0.763110994667216, Learning Rate - 0.00625, magnitude of gradient - 1.0350422621424398\n",
      "Step - 3273, Loss - 0.6326010820064888, Learning Rate - 0.00625, magnitude of gradient - 0.21811653957746513\n",
      "Step - 3274, Loss - 0.9028742282423279, Learning Rate - 0.00625, magnitude of gradient - 3.4603734388965997\n",
      "Step - 3275, Loss - 0.7711256733508021, Learning Rate - 0.00625, magnitude of gradient - 1.110594209090489\n",
      "Step - 3276, Loss - 0.9931863501255607, Learning Rate - 0.00625, magnitude of gradient - 1.4456857339006672\n",
      "Step - 3277, Loss - 0.8158326637086596, Learning Rate - 0.00625, magnitude of gradient - 1.0830733764177676\n",
      "Step - 3278, Loss - 0.5895254961278017, Learning Rate - 0.00625, magnitude of gradient - 1.964351235685637\n",
      "Step - 3279, Loss - 0.6893593968351701, Learning Rate - 0.00625, magnitude of gradient - 2.362837549621953\n",
      "Step - 3280, Loss - 0.8562291934729956, Learning Rate - 0.00625, magnitude of gradient - 1.0567777699182552\n",
      "Step - 3281, Loss - 0.7733024215530147, Learning Rate - 0.00625, magnitude of gradient - 0.4784097393447311\n",
      "Step - 3282, Loss - 0.946345872250534, Learning Rate - 0.00625, magnitude of gradient - 0.29827801365735135\n",
      "Step - 3283, Loss - 0.7830193827014366, Learning Rate - 0.00625, magnitude of gradient - 1.1892527711022143\n",
      "Step - 3284, Loss - 0.847806811713151, Learning Rate - 0.00625, magnitude of gradient - 1.7299064402818916\n",
      "Step - 3285, Loss - 0.6706913849997149, Learning Rate - 0.00625, magnitude of gradient - 0.9009686123333472\n",
      "Step - 3286, Loss - 0.8536963287435975, Learning Rate - 0.00625, magnitude of gradient - 1.2171222929137633\n",
      "Step - 3287, Loss - 0.7193888197977365, Learning Rate - 0.00625, magnitude of gradient - 0.950129952405696\n",
      "Step - 3288, Loss - 0.5957630286597809, Learning Rate - 0.00625, magnitude of gradient - 1.1300392004604818\n",
      "Step - 3289, Loss - 0.7601666348020717, Learning Rate - 0.00625, magnitude of gradient - 1.186801680575027\n",
      "Step - 3290, Loss - 0.6742120471457423, Learning Rate - 0.00625, magnitude of gradient - 2.849846013309161\n",
      "Step - 3291, Loss - 0.7028672746522466, Learning Rate - 0.00625, magnitude of gradient - 1.4270633654626066\n",
      "Step - 3292, Loss - 0.599125485513954, Learning Rate - 0.00625, magnitude of gradient - 1.4737692903330124\n",
      "Step - 3293, Loss - 0.6429779086670276, Learning Rate - 0.00625, magnitude of gradient - 0.5426245642825992\n",
      "Step - 3294, Loss - 0.5884524941829505, Learning Rate - 0.00625, magnitude of gradient - 0.742098573531512\n",
      "Step - 3295, Loss - 0.7497344076695643, Learning Rate - 0.00625, magnitude of gradient - 1.378522026386033\n",
      "Step - 3296, Loss - 0.7289995823572336, Learning Rate - 0.00625, magnitude of gradient - 1.7368802952189593\n",
      "Step - 3297, Loss - 0.7350822126993706, Learning Rate - 0.00625, magnitude of gradient - 1.1576703788177252\n",
      "Step - 3298, Loss - 0.5973835200828723, Learning Rate - 0.00625, magnitude of gradient - 1.9228374493577347\n",
      "Step - 3299, Loss - 0.637337127704922, Learning Rate - 0.00625, magnitude of gradient - 1.411450145927106\n",
      "Step - 3300, Loss - 0.7150082676891788, Learning Rate - 0.00625, magnitude of gradient - 1.763720330569985\n",
      "Step - 3301, Loss - 0.6306413109696243, Learning Rate - 0.00625, magnitude of gradient - 1.8037862849147321\n",
      "Step - 3302, Loss - 0.6771455840807185, Learning Rate - 0.00625, magnitude of gradient - 2.130141371269725\n",
      "Step - 3303, Loss - 0.8551731109426708, Learning Rate - 0.00625, magnitude of gradient - 1.1016639345607777\n",
      "Step - 3304, Loss - 0.6954575107806575, Learning Rate - 0.00625, magnitude of gradient - 1.141260454906827\n",
      "Step - 3305, Loss - 0.6306417899998356, Learning Rate - 0.00625, magnitude of gradient - 1.433106954844303\n",
      "Step - 3306, Loss - 0.6477035335418111, Learning Rate - 0.00625, magnitude of gradient - 1.7473298720712147\n",
      "Step - 3307, Loss - 0.6777667450277189, Learning Rate - 0.00625, magnitude of gradient - 0.9413437369624869\n",
      "Step - 3308, Loss - 0.7054767616994795, Learning Rate - 0.00625, magnitude of gradient - 2.0869918924594413\n",
      "Step - 3309, Loss - 0.7166649160119065, Learning Rate - 0.00625, magnitude of gradient - 2.0348549376370983\n",
      "Step - 3310, Loss - 0.6672797662831068, Learning Rate - 0.00625, magnitude of gradient - 2.3886744973574587\n",
      "Step - 3311, Loss - 0.5662404592208994, Learning Rate - 0.00625, magnitude of gradient - 3.106249349671397\n",
      "Step - 3312, Loss - 0.8543220188637699, Learning Rate - 0.00625, magnitude of gradient - 0.4981503097852567\n",
      "Step - 3313, Loss - 0.747060127420205, Learning Rate - 0.00625, magnitude of gradient - 1.112401837417697\n",
      "Step - 3314, Loss - 0.7642152841012013, Learning Rate - 0.00625, magnitude of gradient - 2.1572664045850405\n",
      "Step - 3315, Loss - 0.7087847621194547, Learning Rate - 0.00625, magnitude of gradient - 1.1625040163427989\n",
      "Step - 3316, Loss - 0.7667100160533229, Learning Rate - 0.00625, magnitude of gradient - 1.6066054620452064\n",
      "Step - 3317, Loss - 0.766494503809612, Learning Rate - 0.00625, magnitude of gradient - 1.2592066199008483\n",
      "Step - 3318, Loss - 0.5204863802303731, Learning Rate - 0.00625, magnitude of gradient - 2.147264888952674\n",
      "Step - 3319, Loss - 0.5561424205623303, Learning Rate - 0.00625, magnitude of gradient - 2.3039823206183137\n",
      "Step - 3320, Loss - 0.8435945860256674, Learning Rate - 0.00625, magnitude of gradient - 1.3352021437874841\n",
      "Step - 3321, Loss - 0.6764383076493321, Learning Rate - 0.00625, magnitude of gradient - 0.5074966437422075\n",
      "Step - 3322, Loss - 0.5729994184496695, Learning Rate - 0.00625, magnitude of gradient - 1.1870792158615788\n",
      "Step - 3323, Loss - 0.5232108852399593, Learning Rate - 0.00625, magnitude of gradient - 1.374719080310008\n",
      "Step - 3324, Loss - 0.6029372320039266, Learning Rate - 0.00625, magnitude of gradient - 0.9695569679021544\n",
      "Step - 3325, Loss - 0.7169122198162199, Learning Rate - 0.00625, magnitude of gradient - 1.344168929280696\n",
      "Step - 3326, Loss - 0.5672924334380995, Learning Rate - 0.00625, magnitude of gradient - 1.2428214367961956\n",
      "Step - 3327, Loss - 0.8210678504567919, Learning Rate - 0.00625, magnitude of gradient - 1.9482711481171855\n",
      "Step - 3328, Loss - 0.5963261729956663, Learning Rate - 0.00625, magnitude of gradient - 2.127912800598968\n",
      "Step - 3329, Loss - 0.7713742625678018, Learning Rate - 0.00625, magnitude of gradient - 1.3555339810625389\n",
      "Step - 3330, Loss - 0.7648031268141513, Learning Rate - 0.00625, magnitude of gradient - 1.497655547514197\n",
      "Step - 3331, Loss - 0.6690567522308825, Learning Rate - 0.00625, magnitude of gradient - 0.6803822673314147\n",
      "Step - 3332, Loss - 0.7649917166275116, Learning Rate - 0.00625, magnitude of gradient - 0.6624161510450688\n",
      "Step - 3333, Loss - 0.7089463886396032, Learning Rate - 0.00625, magnitude of gradient - 1.3026584338779976\n",
      "Step - 3334, Loss - 0.7040027906354444, Learning Rate - 0.00625, magnitude of gradient - 1.0034261195181076\n",
      "Step - 3335, Loss - 0.7848321349935619, Learning Rate - 0.00625, magnitude of gradient - 2.2453695983965254\n",
      "Step - 3336, Loss - 0.9012103198800038, Learning Rate - 0.00625, magnitude of gradient - 1.2956589266945413\n",
      "Step - 3337, Loss - 0.6484228171696005, Learning Rate - 0.00625, magnitude of gradient - 0.4437503619346565\n",
      "Step - 3338, Loss - 0.7137757271599746, Learning Rate - 0.00625, magnitude of gradient - 1.4716688734913328\n",
      "Step - 3339, Loss - 0.6052532685677001, Learning Rate - 0.00625, magnitude of gradient - 0.9736632563266064\n",
      "Step - 3340, Loss - 0.8329884096145697, Learning Rate - 0.00625, magnitude of gradient - 0.7202137999626347\n",
      "Step - 3341, Loss - 0.6991425591020238, Learning Rate - 0.00625, magnitude of gradient - 2.12330599724639\n",
      "Step - 3342, Loss - 0.5876475568416792, Learning Rate - 0.00625, magnitude of gradient - 1.442451237720644\n",
      "Step - 3343, Loss - 0.7666635454317887, Learning Rate - 0.00625, magnitude of gradient - 1.9778051403831594\n",
      "Step - 3344, Loss - 0.885079445589317, Learning Rate - 0.00625, magnitude of gradient - 0.7509847285599093\n",
      "Step - 3345, Loss - 0.6951394178099, Learning Rate - 0.00625, magnitude of gradient - 1.4000871579400536\n",
      "Step - 3346, Loss - 0.674269042327959, Learning Rate - 0.00625, magnitude of gradient - 0.6208779282342907\n",
      "Step - 3347, Loss - 0.691577686967022, Learning Rate - 0.00625, magnitude of gradient - 1.8645364693231168\n",
      "Step - 3348, Loss - 0.6623073862694878, Learning Rate - 0.00625, magnitude of gradient - 1.702027989402874\n",
      "Step - 3349, Loss - 0.685343107479528, Learning Rate - 0.00625, magnitude of gradient - 0.947067951749782\n",
      "Step - 3350, Loss - 0.7161490079441275, Learning Rate - 0.00625, magnitude of gradient - 1.5421939090293755\n",
      "Step - 3351, Loss - 0.6116934239884939, Learning Rate - 0.00625, magnitude of gradient - 0.8974643450580823\n",
      "Step - 3352, Loss - 0.6061314136911891, Learning Rate - 0.00625, magnitude of gradient - 1.5295417718979436\n",
      "Step - 3353, Loss - 0.8260220886261315, Learning Rate - 0.00625, magnitude of gradient - 2.068383095162836\n",
      "Step - 3354, Loss - 0.8660117100243436, Learning Rate - 0.00625, magnitude of gradient - 2.9803249503568\n",
      "Step - 3355, Loss - 0.6495126513656997, Learning Rate - 0.00625, magnitude of gradient - 0.8307257101954587\n",
      "Step - 3356, Loss - 0.7609536891221889, Learning Rate - 0.00625, magnitude of gradient - 1.7858299712392343\n",
      "Step - 3357, Loss - 0.7376146949949035, Learning Rate - 0.00625, magnitude of gradient - 1.2009856094067586\n",
      "Step - 3358, Loss - 0.6802924812731438, Learning Rate - 0.00625, magnitude of gradient - 0.4460102488019444\n",
      "Step - 3359, Loss - 0.8474572103017228, Learning Rate - 0.00625, magnitude of gradient - 1.6525107433366315\n",
      "Step - 3360, Loss - 0.4602785079655697, Learning Rate - 0.00625, magnitude of gradient - 1.2412221652409123\n",
      "Step - 3361, Loss - 0.8088510776610225, Learning Rate - 0.00625, magnitude of gradient - 1.1982115188902152\n",
      "Step - 3362, Loss - 0.8213817896673948, Learning Rate - 0.00625, magnitude of gradient - 0.5496646413861253\n",
      "Step - 3363, Loss - 1.0222988934292225, Learning Rate - 0.00625, magnitude of gradient - 1.0102649096753762\n",
      "Step - 3364, Loss - 0.6244909397253606, Learning Rate - 0.00625, magnitude of gradient - 1.3546437492990353\n",
      "Step - 3365, Loss - 0.6527902665160865, Learning Rate - 0.00625, magnitude of gradient - 1.578496652200085\n",
      "Step - 3366, Loss - 0.6881363992917258, Learning Rate - 0.00625, magnitude of gradient - 1.1530432148903167\n",
      "Step - 3367, Loss - 0.7561330289133996, Learning Rate - 0.00625, magnitude of gradient - 0.5896670897337993\n",
      "Step - 3368, Loss - 0.7275485145389414, Learning Rate - 0.00625, magnitude of gradient - 0.8630217990293\n",
      "Step - 3369, Loss - 0.7355107838883207, Learning Rate - 0.00625, magnitude of gradient - 1.5061467413628908\n",
      "Step - 3370, Loss - 0.7491744691189003, Learning Rate - 0.00625, magnitude of gradient - 2.0388656952143567\n",
      "Step - 3371, Loss - 0.7862753193733885, Learning Rate - 0.00625, magnitude of gradient - 0.3346764500802257\n",
      "Step - 3372, Loss - 0.6779905276181661, Learning Rate - 0.00625, magnitude of gradient - 1.9592126001464847\n",
      "Step - 3373, Loss - 0.5892361245813712, Learning Rate - 0.00625, magnitude of gradient - 1.5896727858448012\n",
      "Step - 3374, Loss - 0.8385340275658497, Learning Rate - 0.00625, magnitude of gradient - 1.711172081778924\n",
      "Step - 3375, Loss - 0.8608497896974385, Learning Rate - 0.00625, magnitude of gradient - 1.1526639515968473\n",
      "Step - 3376, Loss - 0.8482869639330121, Learning Rate - 0.00625, magnitude of gradient - 1.1533252323318703\n",
      "Step - 3377, Loss - 0.6206418540105136, Learning Rate - 0.00625, magnitude of gradient - 0.6520102847832034\n",
      "Step - 3378, Loss - 0.7367624514324354, Learning Rate - 0.00625, magnitude of gradient - 1.3793163296146789\n",
      "Step - 3379, Loss - 0.7927700809218878, Learning Rate - 0.00625, magnitude of gradient - 0.9566282394465583\n",
      "Step - 3380, Loss - 0.8519645597835306, Learning Rate - 0.00625, magnitude of gradient - 1.494247275293157\n",
      "Step - 3381, Loss - 0.8034860529265216, Learning Rate - 0.00625, magnitude of gradient - 1.119643757886958\n",
      "Step - 3382, Loss - 0.7437254851594945, Learning Rate - 0.00625, magnitude of gradient - 1.2324663672484566\n",
      "Step - 3383, Loss - 0.6605648217682687, Learning Rate - 0.00625, magnitude of gradient - 0.5534761460606843\n",
      "Step - 3384, Loss - 0.6755449630350568, Learning Rate - 0.00625, magnitude of gradient - 2.009659847718199\n",
      "Step - 3385, Loss - 0.6530149014323261, Learning Rate - 0.00625, magnitude of gradient - 0.6232409169162367\n",
      "Step - 3386, Loss - 0.7279182868648276, Learning Rate - 0.00625, magnitude of gradient - 0.8901422587901834\n",
      "Step - 3387, Loss - 0.821501495326543, Learning Rate - 0.00625, magnitude of gradient - 1.5096565640751174\n",
      "Step - 3388, Loss - 0.8247887049253754, Learning Rate - 0.00625, magnitude of gradient - 1.399157931169897\n",
      "Step - 3389, Loss - 0.6454639043451761, Learning Rate - 0.00625, magnitude of gradient - 1.0272726912524384\n",
      "Step - 3390, Loss - 0.6625567178955898, Learning Rate - 0.00625, magnitude of gradient - 1.06085940478338\n",
      "Step - 3391, Loss - 0.6380273256964537, Learning Rate - 0.00625, magnitude of gradient - 1.533368743791356\n",
      "Step - 3392, Loss - 0.6039644095958532, Learning Rate - 0.00625, magnitude of gradient - 1.0383044904316219\n",
      "Step - 3393, Loss - 0.6748618999524346, Learning Rate - 0.00625, magnitude of gradient - 0.9381870825850259\n",
      "Step - 3394, Loss - 0.6304119553824004, Learning Rate - 0.00625, magnitude of gradient - 1.800640957963688\n",
      "Step - 3395, Loss - 0.8602639682958289, Learning Rate - 0.00625, magnitude of gradient - 1.5200149735146864\n",
      "Step - 3396, Loss - 0.7081657763233682, Learning Rate - 0.00625, magnitude of gradient - 1.3976664991685304\n",
      "Step - 3397, Loss - 0.7132293248629782, Learning Rate - 0.00625, magnitude of gradient - 0.8831903160544594\n",
      "Step - 3398, Loss - 0.7512376074399125, Learning Rate - 0.00625, magnitude of gradient - 0.4475873696086627\n",
      "Step - 3399, Loss - 0.703011511267578, Learning Rate - 0.00625, magnitude of gradient - 1.043569821659904\n",
      "Step - 3400, Loss - 0.6756596485788298, Learning Rate - 0.00625, magnitude of gradient - 1.7770849963181377\n",
      "Step - 3401, Loss - 0.8438589716207421, Learning Rate - 0.00625, magnitude of gradient - 1.764825111051113\n",
      "Step - 3402, Loss - 0.715559312193507, Learning Rate - 0.00625, magnitude of gradient - 2.178216162260007\n",
      "Step - 3403, Loss - 0.7744988991607216, Learning Rate - 0.00625, magnitude of gradient - 0.7716257258312558\n",
      "Step - 3404, Loss - 0.5400460551869105, Learning Rate - 0.00625, magnitude of gradient - 1.1648807441861428\n",
      "Step - 3405, Loss - 0.7261815970473156, Learning Rate - 0.00625, magnitude of gradient - 2.3442265725026368\n",
      "Step - 3406, Loss - 0.724496542627972, Learning Rate - 0.00625, magnitude of gradient - 1.8693885277004392\n",
      "Step - 3407, Loss - 0.6590827458864486, Learning Rate - 0.00625, magnitude of gradient - 1.4057874501627914\n",
      "Step - 3408, Loss - 0.4977673628020264, Learning Rate - 0.00625, magnitude of gradient - 1.4026450569656181\n",
      "Step - 3409, Loss - 0.7226135988488471, Learning Rate - 0.00625, magnitude of gradient - 0.7444003674479577\n",
      "Step - 3410, Loss - 0.7463557781538537, Learning Rate - 0.00625, magnitude of gradient - 1.7937362911047245\n",
      "Step - 3411, Loss - 0.6306165974250774, Learning Rate - 0.00625, magnitude of gradient - 1.2025656533704938\n",
      "Step - 3412, Loss - 0.6494845308252168, Learning Rate - 0.00625, magnitude of gradient - 0.7457204773079763\n",
      "Step - 3413, Loss - 0.7558502897829459, Learning Rate - 0.00625, magnitude of gradient - 2.0235644164804376\n",
      "Step - 3414, Loss - 0.6824031036373966, Learning Rate - 0.00625, magnitude of gradient - 2.320451585915405\n",
      "Step - 3415, Loss - 0.788955570962382, Learning Rate - 0.00625, magnitude of gradient - 1.8026902839718082\n",
      "Step - 3416, Loss - 0.6426217450853703, Learning Rate - 0.00625, magnitude of gradient - 1.4534603111591837\n",
      "Step - 3417, Loss - 0.7122226825345311, Learning Rate - 0.00625, magnitude of gradient - 0.9271943938554452\n",
      "Step - 3418, Loss - 0.7903292153129281, Learning Rate - 0.00625, magnitude of gradient - 1.9949024130048414\n",
      "Step - 3419, Loss - 0.5427780508491422, Learning Rate - 0.00625, magnitude of gradient - 1.0850609046425346\n",
      "Step - 3420, Loss - 0.6549430052616527, Learning Rate - 0.00625, magnitude of gradient - 0.3733273657857935\n",
      "Step - 3421, Loss - 0.5642016306536034, Learning Rate - 0.00625, magnitude of gradient - 1.349550000805899\n",
      "Step - 3422, Loss - 0.6416404739814757, Learning Rate - 0.00625, magnitude of gradient - 1.0113953566159146\n",
      "Step - 3423, Loss - 0.7038795692468914, Learning Rate - 0.00625, magnitude of gradient - 1.0198950307530055\n",
      "Step - 3424, Loss - 0.801162924565246, Learning Rate - 0.00625, magnitude of gradient - 0.39098978871861734\n",
      "Step - 3425, Loss - 0.5615941637564257, Learning Rate - 0.00625, magnitude of gradient - 0.5942796469610916\n",
      "Step - 3426, Loss - 0.6244993026632472, Learning Rate - 0.00625, magnitude of gradient - 1.3990179556720665\n",
      "Step - 3427, Loss - 0.7105304389285203, Learning Rate - 0.00625, magnitude of gradient - 0.6964941118356405\n",
      "Step - 3428, Loss - 0.760988324375901, Learning Rate - 0.00625, magnitude of gradient - 1.2194903216896011\n",
      "Step - 3429, Loss - 0.7986221167031166, Learning Rate - 0.00625, magnitude of gradient - 2.3058013125611927\n",
      "Step - 3430, Loss - 0.5294029035644563, Learning Rate - 0.00625, magnitude of gradient - 1.8059871612624034\n",
      "Step - 3431, Loss - 0.6868475822333262, Learning Rate - 0.00625, magnitude of gradient - 2.8268271223392176\n",
      "Step - 3432, Loss - 0.7433030311828873, Learning Rate - 0.00625, magnitude of gradient - 0.4577466818552695\n",
      "Step - 3433, Loss - 0.7389283085730497, Learning Rate - 0.00625, magnitude of gradient - 1.338468408106707\n",
      "Step - 3434, Loss - 0.8644923505655842, Learning Rate - 0.00625, magnitude of gradient - 1.9581296538240505\n",
      "Step - 3435, Loss - 0.7138490259461006, Learning Rate - 0.00625, magnitude of gradient - 0.4425348890955481\n",
      "Step - 3436, Loss - 0.6817189919368589, Learning Rate - 0.00625, magnitude of gradient - 2.1091623606872725\n",
      "Step - 3437, Loss - 0.8912959189130962, Learning Rate - 0.00625, magnitude of gradient - 1.6257617615250408\n",
      "Step - 3438, Loss - 0.7323273693421392, Learning Rate - 0.00625, magnitude of gradient - 1.0819476070466592\n",
      "Step - 3439, Loss - 0.8286300184048854, Learning Rate - 0.00625, magnitude of gradient - 0.6157678494035469\n",
      "Step - 3440, Loss - 0.6889837534003512, Learning Rate - 0.00625, magnitude of gradient - 0.734859659422166\n",
      "Step - 3441, Loss - 0.7557516341801731, Learning Rate - 0.00625, magnitude of gradient - 2.8576502084301336\n",
      "Step - 3442, Loss - 0.6062024399995438, Learning Rate - 0.00625, magnitude of gradient - 1.2645505306276266\n",
      "Step - 3443, Loss - 0.863180298818421, Learning Rate - 0.00625, magnitude of gradient - 2.173697738664455\n",
      "Step - 3444, Loss - 0.8813420591799073, Learning Rate - 0.00625, magnitude of gradient - 1.4271078069106662\n",
      "Step - 3445, Loss - 0.6764850488522387, Learning Rate - 0.00625, magnitude of gradient - 1.1321282738848626\n",
      "Step - 3446, Loss - 0.49999293160885183, Learning Rate - 0.00625, magnitude of gradient - 1.8072987210380522\n",
      "Step - 3447, Loss - 0.7115813388223916, Learning Rate - 0.00625, magnitude of gradient - 1.5926830334890067\n",
      "Step - 3448, Loss - 0.8957919812271229, Learning Rate - 0.00625, magnitude of gradient - 1.1375873922784674\n",
      "Step - 3449, Loss - 0.623785434625466, Learning Rate - 0.00625, magnitude of gradient - 0.5474368231792912\n",
      "Step - 3450, Loss - 0.7530146212712061, Learning Rate - 0.00625, magnitude of gradient - 2.489752928580322\n",
      "Step - 3451, Loss - 0.6157173194146557, Learning Rate - 0.00625, magnitude of gradient - 0.3258238552257208\n",
      "Step - 3452, Loss - 0.5758830932451032, Learning Rate - 0.00625, magnitude of gradient - 0.984986085774562\n",
      "Step - 3453, Loss - 0.7585442270840617, Learning Rate - 0.00625, magnitude of gradient - 1.536126464628609\n",
      "Step - 3454, Loss - 0.8281521944844914, Learning Rate - 0.00625, magnitude of gradient - 2.0761499742863174\n",
      "Step - 3455, Loss - 0.5744668129299133, Learning Rate - 0.00625, magnitude of gradient - 1.267453980350764\n",
      "Step - 3456, Loss - 0.5269832408555233, Learning Rate - 0.00625, magnitude of gradient - 1.8707921147486826\n",
      "Step - 3457, Loss - 0.6397976685373021, Learning Rate - 0.00625, magnitude of gradient - 0.6226498684376337\n",
      "Step - 3458, Loss - 0.7254675153306273, Learning Rate - 0.00625, magnitude of gradient - 2.2744186558954094\n",
      "Step - 3459, Loss - 0.6798820776360145, Learning Rate - 0.00625, magnitude of gradient - 0.5196561905046755\n",
      "Step - 3460, Loss - 0.7454849094178368, Learning Rate - 0.00625, magnitude of gradient - 0.6221415541793333\n",
      "Step - 3461, Loss - 0.6060162675551158, Learning Rate - 0.00625, magnitude of gradient - 0.9524904272732637\n",
      "Step - 3462, Loss - 0.6150281819912236, Learning Rate - 0.00625, magnitude of gradient - 1.1621450376676123\n",
      "Step - 3463, Loss - 0.6930228383601977, Learning Rate - 0.00625, magnitude of gradient - 0.3136594342244404\n",
      "Step - 3464, Loss - 0.8471932298152145, Learning Rate - 0.00625, magnitude of gradient - 2.54781827371598\n",
      "Step - 3465, Loss - 0.6114878726196742, Learning Rate - 0.00625, magnitude of gradient - 0.9618303141267782\n",
      "Step - 3466, Loss - 0.6486917047178417, Learning Rate - 0.00625, magnitude of gradient - 1.6399435544109526\n",
      "Step - 3467, Loss - 0.5090155030256212, Learning Rate - 0.00625, magnitude of gradient - 1.5463902924342083\n",
      "Step - 3468, Loss - 0.7133439534932066, Learning Rate - 0.00625, magnitude of gradient - 0.8360047812908249\n",
      "Step - 3469, Loss - 0.6947833927474228, Learning Rate - 0.00625, magnitude of gradient - 1.8583714450011368\n",
      "Step - 3470, Loss - 0.6520849624233508, Learning Rate - 0.00625, magnitude of gradient - 0.8312454869777769\n",
      "Step - 3471, Loss - 0.6381039424927017, Learning Rate - 0.00625, magnitude of gradient - 1.1993060042174009\n",
      "Step - 3472, Loss - 0.8251533103070361, Learning Rate - 0.00625, magnitude of gradient - 1.5621780317200809\n",
      "Step - 3473, Loss - 0.7494074991358455, Learning Rate - 0.00625, magnitude of gradient - 1.0890809970300022\n",
      "Step - 3474, Loss - 0.8546505752651649, Learning Rate - 0.00625, magnitude of gradient - 1.5176789256404402\n",
      "Step - 3475, Loss - 0.7046708787989584, Learning Rate - 0.00625, magnitude of gradient - 1.4556615794455567\n",
      "Step - 3476, Loss - 0.7516054548312862, Learning Rate - 0.00625, magnitude of gradient - 1.5227741553333964\n",
      "Step - 3477, Loss - 0.5771822119070493, Learning Rate - 0.00625, magnitude of gradient - 0.6954584625826393\n",
      "Step - 3478, Loss - 0.7571515407103041, Learning Rate - 0.00625, magnitude of gradient - 1.4410247127460791\n",
      "Step - 3479, Loss - 0.713097670859, Learning Rate - 0.00625, magnitude of gradient - 1.2441174244120519\n",
      "Step - 3480, Loss - 0.734237561631202, Learning Rate - 0.00625, magnitude of gradient - 1.458398028258412\n",
      "Step - 3481, Loss - 0.7889604558210154, Learning Rate - 0.00625, magnitude of gradient - 0.9463824327986756\n",
      "Step - 3482, Loss - 0.7008646054574581, Learning Rate - 0.00625, magnitude of gradient - 0.548986432856516\n",
      "Step - 3483, Loss - 0.7710773282061802, Learning Rate - 0.00625, magnitude of gradient - 1.3721972116033971\n",
      "Step - 3484, Loss - 0.6800002290428204, Learning Rate - 0.00625, magnitude of gradient - 1.3488237068446398\n",
      "Step - 3485, Loss - 0.7551934150454205, Learning Rate - 0.00625, magnitude of gradient - 1.0952092459205918\n",
      "Step - 3486, Loss - 0.8721593749872139, Learning Rate - 0.00625, magnitude of gradient - 1.9325273168063544\n",
      "Step - 3487, Loss - 0.7904129455994467, Learning Rate - 0.00625, magnitude of gradient - 1.84186691240557\n",
      "Step - 3488, Loss - 0.7141198273222756, Learning Rate - 0.00625, magnitude of gradient - 1.6965243975224737\n",
      "Step - 3489, Loss - 0.7531817184940618, Learning Rate - 0.00625, magnitude of gradient - 1.9962727120351393\n",
      "Step - 3490, Loss - 0.7552756962510507, Learning Rate - 0.00625, magnitude of gradient - 2.132827038022153\n",
      "Step - 3491, Loss - 0.6326637758308444, Learning Rate - 0.00625, magnitude of gradient - 1.4868806838981037\n",
      "Step - 3492, Loss - 0.8379426149975162, Learning Rate - 0.00625, magnitude of gradient - 1.5252268898505659\n",
      "Step - 3493, Loss - 0.5988781166725698, Learning Rate - 0.00625, magnitude of gradient - 0.8811586576096163\n",
      "Step - 3494, Loss - 0.8889115561248575, Learning Rate - 0.00625, magnitude of gradient - 0.9466794208503521\n",
      "Step - 3495, Loss - 0.7732312230172169, Learning Rate - 0.00625, magnitude of gradient - 1.2491546304129955\n",
      "Step - 3496, Loss - 0.913884615303389, Learning Rate - 0.00625, magnitude of gradient - 1.5708816281493525\n",
      "Step - 3497, Loss - 0.5655428795116628, Learning Rate - 0.00625, magnitude of gradient - 1.131105470431365\n",
      "Step - 3498, Loss - 0.7245834924681968, Learning Rate - 0.00625, magnitude of gradient - 1.063919962856033\n",
      "Step - 3499, Loss - 0.600116518511371, Learning Rate - 0.00625, magnitude of gradient - 0.8892673888796737\n",
      "Step - 3500, Loss - 0.7308374500179232, Learning Rate - 0.00625, magnitude of gradient - 1.6182397273039648\n",
      "Step - 3501, Loss - 0.7640935486798782, Learning Rate - 0.00625, magnitude of gradient - 1.2994350080185861\n",
      "Step - 3502, Loss - 0.6700496406845711, Learning Rate - 0.00625, magnitude of gradient - 0.33217783146486685\n",
      "Step - 3503, Loss - 0.8233169596818768, Learning Rate - 0.00625, magnitude of gradient - 1.3981731933301236\n",
      "Step - 3504, Loss - 0.8414031574355714, Learning Rate - 0.00625, magnitude of gradient - 2.6085250253085848\n",
      "Step - 3505, Loss - 0.636851383654569, Learning Rate - 0.00625, magnitude of gradient - 1.2545588574775266\n",
      "Step - 3506, Loss - 0.6625999465301974, Learning Rate - 0.00625, magnitude of gradient - 1.031796369938352\n",
      "Step - 3507, Loss - 0.5464327549864217, Learning Rate - 0.00625, magnitude of gradient - 1.663534573584147\n",
      "Step - 3508, Loss - 0.6673221718957751, Learning Rate - 0.00625, magnitude of gradient - 1.2894986427580477\n",
      "Step - 3509, Loss - 0.8978216989958299, Learning Rate - 0.00625, magnitude of gradient - 1.5296048273313452\n",
      "Step - 3510, Loss - 0.7070713781431894, Learning Rate - 0.00625, magnitude of gradient - 0.6150074211214649\n",
      "Step - 3511, Loss - 0.6037091521625746, Learning Rate - 0.00625, magnitude of gradient - 1.4252422597658156\n",
      "Step - 3512, Loss - 0.5213087949464691, Learning Rate - 0.00625, magnitude of gradient - 1.3591052367971006\n",
      "Step - 3513, Loss - 0.7398991025389546, Learning Rate - 0.00625, magnitude of gradient - 1.3006201442759564\n",
      "Step - 3514, Loss - 0.5891044877618254, Learning Rate - 0.00625, magnitude of gradient - 1.887533976098876\n",
      "Step - 3515, Loss - 0.7240961200377397, Learning Rate - 0.00625, magnitude of gradient - 1.11947486707967\n",
      "Step - 3516, Loss - 0.5687586179811601, Learning Rate - 0.00625, magnitude of gradient - 1.4148415951399504\n",
      "Step - 3517, Loss - 0.5449828262288874, Learning Rate - 0.00625, magnitude of gradient - 1.125025866985304\n",
      "Step - 3518, Loss - 0.6201216749350699, Learning Rate - 0.00625, magnitude of gradient - 0.5455137536506679\n",
      "Step - 3519, Loss - 0.8366333872715637, Learning Rate - 0.00625, magnitude of gradient - 2.3668234866255493\n",
      "Step - 3520, Loss - 0.6050564274449792, Learning Rate - 0.00625, magnitude of gradient - 1.070122475797916\n",
      "Step - 3521, Loss - 0.7260260682013875, Learning Rate - 0.00625, magnitude of gradient - 0.8453050916485608\n",
      "Step - 3522, Loss - 0.8804599208134938, Learning Rate - 0.00625, magnitude of gradient - 2.1447475517268484\n",
      "Step - 3523, Loss - 0.5215707929686475, Learning Rate - 0.00625, magnitude of gradient - 0.7716157034333873\n",
      "Step - 3524, Loss - 0.7055417836483419, Learning Rate - 0.00625, magnitude of gradient - 2.1141988013798585\n",
      "Step - 3525, Loss - 0.6473116977046207, Learning Rate - 0.00625, magnitude of gradient - 1.8777475053285404\n",
      "Step - 3526, Loss - 0.6360697903523376, Learning Rate - 0.00625, magnitude of gradient - 1.73678084519123\n",
      "Step - 3527, Loss - 0.6337180939309237, Learning Rate - 0.00625, magnitude of gradient - 1.2940049635853001\n",
      "Step - 3528, Loss - 0.5590621295125863, Learning Rate - 0.00625, magnitude of gradient - 2.757069650459268\n",
      "Step - 3529, Loss - 0.7495938593939067, Learning Rate - 0.00625, magnitude of gradient - 0.9070628549412929\n",
      "Step - 3530, Loss - 0.6709124205792412, Learning Rate - 0.00625, magnitude of gradient - 0.6008910186027062\n",
      "Step - 3531, Loss - 0.7872953708271978, Learning Rate - 0.00625, magnitude of gradient - 1.203184090196302\n",
      "Step - 3532, Loss - 0.5526650141082948, Learning Rate - 0.00625, magnitude of gradient - 1.6589168167627524\n",
      "Step - 3533, Loss - 0.8838542804760436, Learning Rate - 0.00625, magnitude of gradient - 1.566344120931294\n",
      "Step - 3534, Loss - 0.6236242900873525, Learning Rate - 0.00625, magnitude of gradient - 1.2117206815506176\n",
      "Step - 3535, Loss - 0.7976155748921993, Learning Rate - 0.00625, magnitude of gradient - 2.24698782860191\n",
      "Step - 3536, Loss - 0.7356030975774603, Learning Rate - 0.00625, magnitude of gradient - 2.7547554380612116\n",
      "Step - 3537, Loss - 0.7157240646972128, Learning Rate - 0.00625, magnitude of gradient - 0.9085585412737083\n",
      "Step - 3538, Loss - 1.0347632173802441, Learning Rate - 0.00625, magnitude of gradient - 1.5318560706038782\n",
      "Step - 3539, Loss - 0.7625575868514627, Learning Rate - 0.00625, magnitude of gradient - 0.8154490676652871\n",
      "Step - 3540, Loss - 0.8500579980580129, Learning Rate - 0.00625, magnitude of gradient - 0.8225365675205347\n",
      "Step - 3541, Loss - 0.6598139673824278, Learning Rate - 0.00625, magnitude of gradient - 1.3371172453506839\n",
      "Step - 3542, Loss - 0.5484534174527735, Learning Rate - 0.00625, magnitude of gradient - 1.5433118236504035\n",
      "Step - 3543, Loss - 0.5806590515970806, Learning Rate - 0.00625, magnitude of gradient - 1.250151403176759\n",
      "Step - 3544, Loss - 0.50783386565075, Learning Rate - 0.00625, magnitude of gradient - 0.9065437612443358\n",
      "Step - 3545, Loss - 0.674491788792362, Learning Rate - 0.00625, magnitude of gradient - 0.6838453613189951\n",
      "Step - 3546, Loss - 0.6173985884326889, Learning Rate - 0.00625, magnitude of gradient - 0.673156894958152\n",
      "Step - 3547, Loss - 0.5811269185777027, Learning Rate - 0.00625, magnitude of gradient - 1.0145230697157053\n",
      "Step - 3548, Loss - 0.6445152566253763, Learning Rate - 0.00625, magnitude of gradient - 1.454833114665121\n",
      "Step - 3549, Loss - 0.6116246869955775, Learning Rate - 0.00625, magnitude of gradient - 1.1542112865017455\n",
      "Step - 3550, Loss - 0.642878660933474, Learning Rate - 0.00625, magnitude of gradient - 0.6684660139665305\n",
      "Step - 3551, Loss - 0.9691336442181726, Learning Rate - 0.00625, magnitude of gradient - 1.5630979041689166\n",
      "Step - 3552, Loss - 0.8283868673522474, Learning Rate - 0.00625, magnitude of gradient - 0.7171661738819313\n",
      "Step - 3553, Loss - 0.9344617447300967, Learning Rate - 0.00625, magnitude of gradient - 2.115222655056945\n",
      "Step - 3554, Loss - 0.7021012615800242, Learning Rate - 0.00625, magnitude of gradient - 1.5623968341347119\n",
      "Step - 3555, Loss - 0.7093343256451126, Learning Rate - 0.00625, magnitude of gradient - 2.6680511905346807\n",
      "Step - 3556, Loss - 0.8129826938640661, Learning Rate - 0.00625, magnitude of gradient - 1.563639386572519\n",
      "Step - 3557, Loss - 0.6138734837908698, Learning Rate - 0.00625, magnitude of gradient - 0.7478105097994682\n",
      "Step - 3558, Loss - 0.6318242994284515, Learning Rate - 0.00625, magnitude of gradient - 0.9095449930978395\n",
      "Step - 3559, Loss - 0.5523855738921221, Learning Rate - 0.00625, magnitude of gradient - 0.9173280075150368\n",
      "Step - 3560, Loss - 0.8640314186273487, Learning Rate - 0.00625, magnitude of gradient - 1.7175491260455436\n",
      "Step - 3561, Loss - 0.6284525886660418, Learning Rate - 0.00625, magnitude of gradient - 1.289930774163548\n",
      "Step - 3562, Loss - 0.6078591976083205, Learning Rate - 0.00625, magnitude of gradient - 0.9389696707819448\n",
      "Step - 3563, Loss - 0.7095815059952844, Learning Rate - 0.00625, magnitude of gradient - 0.9018251984183513\n",
      "Step - 3564, Loss - 0.49801932648141484, Learning Rate - 0.00625, magnitude of gradient - 0.12038162154078598\n",
      "Step - 3565, Loss - 0.8407815365206499, Learning Rate - 0.00625, magnitude of gradient - 3.061176732071491\n",
      "Step - 3566, Loss - 0.689383025794589, Learning Rate - 0.00625, magnitude of gradient - 1.913593773655502\n",
      "Step - 3567, Loss - 0.5734990493058659, Learning Rate - 0.00625, magnitude of gradient - 2.0955589422584726\n",
      "Step - 3568, Loss - 0.662742275307484, Learning Rate - 0.00625, magnitude of gradient - 0.6176052897270946\n",
      "Step - 3569, Loss - 0.7378131219475395, Learning Rate - 0.00625, magnitude of gradient - 2.432936989926662\n",
      "Step - 3570, Loss - 0.7806585940792014, Learning Rate - 0.00625, magnitude of gradient - 1.3313059737529298\n",
      "Step - 3571, Loss - 0.6791009336039371, Learning Rate - 0.00625, magnitude of gradient - 1.4565614983540074\n",
      "Step - 3572, Loss - 0.6883762654301189, Learning Rate - 0.00625, magnitude of gradient - 0.7880131371252063\n",
      "Step - 3573, Loss - 0.5974345576026793, Learning Rate - 0.00625, magnitude of gradient - 1.0753406767748037\n",
      "Step - 3574, Loss - 0.5456376084916043, Learning Rate - 0.00625, magnitude of gradient - 1.1469239164179221\n",
      "Step - 3575, Loss - 0.915645024485571, Learning Rate - 0.00625, magnitude of gradient - 1.6778444240623145\n",
      "Step - 3576, Loss - 0.8287858550072094, Learning Rate - 0.00625, magnitude of gradient - 1.8835604215116575\n",
      "Step - 3577, Loss - 0.5160694597931336, Learning Rate - 0.00625, magnitude of gradient - 3.4806255459985795\n",
      "Step - 3578, Loss - 0.662606400384508, Learning Rate - 0.00625, magnitude of gradient - 0.28050738969267514\n",
      "Step - 3579, Loss - 0.7542091745588967, Learning Rate - 0.00625, magnitude of gradient - 2.5613350641582584\n",
      "Step - 3580, Loss - 0.7593086920032056, Learning Rate - 0.00625, magnitude of gradient - 0.7942770683119279\n",
      "Step - 3581, Loss - 0.8034110613817089, Learning Rate - 0.00625, magnitude of gradient - 1.9582720961725124\n",
      "Step - 3582, Loss - 0.5954068660754204, Learning Rate - 0.00625, magnitude of gradient - 0.9583240960218509\n",
      "Step - 3583, Loss - 0.7811369206075844, Learning Rate - 0.00625, magnitude of gradient - 1.8862918476050643\n",
      "Step - 3584, Loss - 0.7487506683413806, Learning Rate - 0.00625, magnitude of gradient - 1.1392435853884362\n",
      "Step - 3585, Loss - 0.7611205625078465, Learning Rate - 0.00625, magnitude of gradient - 3.239443856707026\n",
      "Step - 3586, Loss - 0.5802653609075655, Learning Rate - 0.00625, magnitude of gradient - 1.8830539081406745\n",
      "Step - 3587, Loss - 0.7107354625266029, Learning Rate - 0.00625, magnitude of gradient - 0.9839081224258742\n",
      "Step - 3588, Loss - 0.6418333933329877, Learning Rate - 0.00625, magnitude of gradient - 1.110657286279253\n",
      "Step - 3589, Loss - 0.5987654541142777, Learning Rate - 0.00625, magnitude of gradient - 1.3650956434064854\n",
      "Step - 3590, Loss - 0.6669428277190289, Learning Rate - 0.00625, magnitude of gradient - 1.9741347456181135\n",
      "Step - 3591, Loss - 0.6824391168560119, Learning Rate - 0.00625, magnitude of gradient - 0.5900895668888431\n",
      "Step - 3592, Loss - 0.47898930403626816, Learning Rate - 0.00625, magnitude of gradient - 1.5789603123032054\n",
      "Step - 3593, Loss - 0.5008886585032629, Learning Rate - 0.00625, magnitude of gradient - 1.1316722588829347\n",
      "Step - 3594, Loss - 0.8567476243601965, Learning Rate - 0.00625, magnitude of gradient - 2.995305042624024\n",
      "Step - 3595, Loss - 0.8349072051703543, Learning Rate - 0.00625, magnitude of gradient - 3.4724465962699136\n",
      "Step - 3596, Loss - 0.633873935053552, Learning Rate - 0.00625, magnitude of gradient - 0.9330217308492935\n",
      "Step - 3597, Loss - 0.7700122698486449, Learning Rate - 0.00625, magnitude of gradient - 1.3063984433820148\n",
      "Step - 3598, Loss - 0.8057756865384222, Learning Rate - 0.00625, magnitude of gradient - 2.0466094421218943\n",
      "Step - 3599, Loss - 0.777854161216875, Learning Rate - 0.00625, magnitude of gradient - 1.898854483976846\n",
      "Step - 3600, Loss - 0.8026455575430642, Learning Rate - 0.00625, magnitude of gradient - 1.3854289116309921\n",
      "Step - 3601, Loss - 0.7315246640635793, Learning Rate - 0.00625, magnitude of gradient - 1.8408303736682838\n",
      "Step - 3602, Loss - 0.8783723888853099, Learning Rate - 0.00625, magnitude of gradient - 1.7673621648808056\n",
      "Step - 3603, Loss - 0.7083421664359592, Learning Rate - 0.00625, magnitude of gradient - 0.8379103306492798\n",
      "Step - 3604, Loss - 0.7365587933414806, Learning Rate - 0.00625, magnitude of gradient - 0.6863870170219253\n",
      "Step - 3605, Loss - 0.8069991230519208, Learning Rate - 0.00625, magnitude of gradient - 2.093437847301524\n",
      "Step - 3606, Loss - 0.7277700506577673, Learning Rate - 0.00625, magnitude of gradient - 2.64963711344122\n",
      "Step - 3607, Loss - 0.7520454812685875, Learning Rate - 0.00625, magnitude of gradient - 1.6237696838551041\n",
      "Step - 3608, Loss - 0.800573157155068, Learning Rate - 0.00625, magnitude of gradient - 0.74899589843375\n",
      "Step - 3609, Loss - 0.6530413018849444, Learning Rate - 0.00625, magnitude of gradient - 1.802588363970524\n",
      "Step - 3610, Loss - 0.705634307198298, Learning Rate - 0.00625, magnitude of gradient - 2.868944580286702\n",
      "Step - 3611, Loss - 0.728073668831129, Learning Rate - 0.00625, magnitude of gradient - 1.6672072042290813\n",
      "Step - 3612, Loss - 0.612302391769036, Learning Rate - 0.00625, magnitude of gradient - 2.169620868088497\n",
      "Step - 3613, Loss - 0.5101818696345325, Learning Rate - 0.00625, magnitude of gradient - 1.2431066262244626\n",
      "Step - 3614, Loss - 0.7162145958221333, Learning Rate - 0.00625, magnitude of gradient - 1.6311394561727022\n",
      "Step - 3615, Loss - 0.6582652192793752, Learning Rate - 0.00625, magnitude of gradient - 1.0365629716785218\n",
      "Step - 3616, Loss - 0.9392703878105135, Learning Rate - 0.00625, magnitude of gradient - 3.0085081350370286\n",
      "Step - 3617, Loss - 0.7874524780736744, Learning Rate - 0.00625, magnitude of gradient - 0.9954505406716095\n",
      "Step - 3618, Loss - 0.639157632655178, Learning Rate - 0.00625, magnitude of gradient - 0.6929630668190825\n",
      "Step - 3619, Loss - 0.7976021563041475, Learning Rate - 0.00625, magnitude of gradient - 0.6441401060327463\n",
      "Step - 3620, Loss - 0.8091357329789397, Learning Rate - 0.00625, magnitude of gradient - 0.7258174604096918\n",
      "Step - 3621, Loss - 0.7294182606529005, Learning Rate - 0.00625, magnitude of gradient - 0.9500900213865433\n",
      "Step - 3622, Loss - 0.5488335611190528, Learning Rate - 0.00625, magnitude of gradient - 1.6203565713820276\n",
      "Step - 3623, Loss - 0.7006712180939833, Learning Rate - 0.00625, magnitude of gradient - 1.3934036744577762\n",
      "Step - 3624, Loss - 0.5780859191077575, Learning Rate - 0.00625, magnitude of gradient - 1.5054676936058695\n",
      "Step - 3625, Loss - 0.6265550280194276, Learning Rate - 0.00625, magnitude of gradient - 1.5205114290961748\n",
      "Step - 3626, Loss - 0.5254275658039002, Learning Rate - 0.00625, magnitude of gradient - 0.45253973641703144\n",
      "Step - 3627, Loss - 0.6092180941342821, Learning Rate - 0.00625, magnitude of gradient - 0.9910377380679157\n",
      "Step - 3628, Loss - 0.6182615830205348, Learning Rate - 0.00625, magnitude of gradient - 1.573790549029561\n",
      "Step - 3629, Loss - 0.7143985482296092, Learning Rate - 0.00625, magnitude of gradient - 1.138135810755975\n",
      "Step - 3630, Loss - 0.720418851189107, Learning Rate - 0.00625, magnitude of gradient - 1.7546757326146787\n",
      "Step - 3631, Loss - 0.7383422033209477, Learning Rate - 0.00625, magnitude of gradient - 1.442397389408169\n",
      "Step - 3632, Loss - 0.6495875985243694, Learning Rate - 0.00625, magnitude of gradient - 2.0731773624993184\n",
      "Step - 3633, Loss - 0.7040432731604627, Learning Rate - 0.00625, magnitude of gradient - 1.295639370127594\n",
      "Step - 3634, Loss - 0.7188842324346203, Learning Rate - 0.00625, magnitude of gradient - 0.9747262749921478\n",
      "Step - 3635, Loss - 0.7969384570169988, Learning Rate - 0.00625, magnitude of gradient - 1.2000020112992513\n",
      "Step - 3636, Loss - 0.8715505344466317, Learning Rate - 0.00625, magnitude of gradient - 0.4324268159916323\n",
      "Step - 3637, Loss - 0.7379281872775318, Learning Rate - 0.00625, magnitude of gradient - 1.4610088472918559\n",
      "Step - 3638, Loss - 0.5946261769233367, Learning Rate - 0.00625, magnitude of gradient - 0.9797150170584813\n",
      "Step - 3639, Loss - 0.5032436628454194, Learning Rate - 0.00625, magnitude of gradient - 2.03532608716282\n",
      "Step - 3640, Loss - 0.7123273653316363, Learning Rate - 0.00625, magnitude of gradient - 1.388761333699838\n",
      "Step - 3641, Loss - 0.5419951191431969, Learning Rate - 0.00625, magnitude of gradient - 2.209519846041345\n",
      "Step - 3642, Loss - 0.6886883389616116, Learning Rate - 0.00625, magnitude of gradient - 1.994225860848603\n",
      "Step - 3643, Loss - 0.6784081559643111, Learning Rate - 0.00625, magnitude of gradient - 0.7227281956446754\n",
      "Step - 3644, Loss - 0.8126718044629585, Learning Rate - 0.00625, magnitude of gradient - 1.65743287622533\n",
      "Step - 3645, Loss - 0.5572907360971091, Learning Rate - 0.00625, magnitude of gradient - 0.6941478862686147\n",
      "Step - 3646, Loss - 0.5153138481825286, Learning Rate - 0.00625, magnitude of gradient - 0.5941266583149539\n",
      "Step - 3647, Loss - 0.9708632148986712, Learning Rate - 0.00625, magnitude of gradient - 3.5506091064075656\n",
      "Step - 3648, Loss - 0.6023713334724375, Learning Rate - 0.00625, magnitude of gradient - 1.2356051189432729\n",
      "Step - 3649, Loss - 0.740855175373424, Learning Rate - 0.00625, magnitude of gradient - 2.597257906424892\n",
      "Step - 3650, Loss - 0.6982012458183376, Learning Rate - 0.00625, magnitude of gradient - 0.7633520795634713\n",
      "Step - 3651, Loss - 0.6886243602610865, Learning Rate - 0.00625, magnitude of gradient - 1.3734026476497612\n",
      "Step - 3652, Loss - 0.7177614344972691, Learning Rate - 0.00625, magnitude of gradient - 2.4389486563050067\n",
      "Step - 3653, Loss - 0.7516658772197571, Learning Rate - 0.00625, magnitude of gradient - 0.6408752329017391\n",
      "Step - 3654, Loss - 0.6127976904120686, Learning Rate - 0.00625, magnitude of gradient - 0.8497092449630298\n",
      "Step - 3655, Loss - 0.6244155841212283, Learning Rate - 0.00625, magnitude of gradient - 0.6587630746907047\n",
      "Step - 3656, Loss - 0.76611553949632, Learning Rate - 0.00625, magnitude of gradient - 1.4819093869312772\n",
      "Step - 3657, Loss - 0.7135256871992628, Learning Rate - 0.00625, magnitude of gradient - 1.4101475371071903\n",
      "Step - 3658, Loss - 0.7273615278602302, Learning Rate - 0.00625, magnitude of gradient - 1.0642151140552283\n",
      "Step - 3659, Loss - 0.533762648212767, Learning Rate - 0.00625, magnitude of gradient - 1.9292226388455158\n",
      "Step - 3660, Loss - 0.7393338129509013, Learning Rate - 0.00625, magnitude of gradient - 2.556149704188106\n",
      "Step - 3661, Loss - 0.7987680378430537, Learning Rate - 0.00625, magnitude of gradient - 1.127690349011476\n",
      "Step - 3662, Loss - 0.5549636531324836, Learning Rate - 0.00625, magnitude of gradient - 0.577769686554494\n",
      "Step - 3663, Loss - 0.7572334641443841, Learning Rate - 0.00625, magnitude of gradient - 0.7272086147247279\n",
      "Step - 3664, Loss - 0.6967900770896135, Learning Rate - 0.00625, magnitude of gradient - 0.7799098526185555\n",
      "Step - 3665, Loss - 0.8226703549976748, Learning Rate - 0.00625, magnitude of gradient - 0.6299705627577658\n",
      "Step - 3666, Loss - 0.6854498109703901, Learning Rate - 0.00625, magnitude of gradient - 0.36441243708624693\n",
      "Step - 3667, Loss - 0.36759865898880106, Learning Rate - 0.00625, magnitude of gradient - 0.6266321548817788\n",
      "Step - 3668, Loss - 0.7091718792824653, Learning Rate - 0.00625, magnitude of gradient - 0.6076789272636958\n",
      "Step - 3669, Loss - 0.6213959633271521, Learning Rate - 0.00625, magnitude of gradient - 1.0240962797741444\n",
      "Step - 3670, Loss - 0.6788835130974054, Learning Rate - 0.00625, magnitude of gradient - 1.8240090191230964\n",
      "Step - 3671, Loss - 0.6416585345148005, Learning Rate - 0.00625, magnitude of gradient - 2.2704345223389097\n",
      "Step - 3672, Loss - 0.5716630732035598, Learning Rate - 0.00625, magnitude of gradient - 0.5145397572674374\n",
      "Step - 3673, Loss - 0.7180935566636272, Learning Rate - 0.00625, magnitude of gradient - 1.881326065935832\n",
      "Step - 3674, Loss - 0.8973708019219999, Learning Rate - 0.00625, magnitude of gradient - 1.9991192951730625\n",
      "Step - 3675, Loss - 0.6309319184599834, Learning Rate - 0.00625, magnitude of gradient - 1.0180762098620557\n",
      "Step - 3676, Loss - 0.6165157601255041, Learning Rate - 0.00625, magnitude of gradient - 0.37124628574866014\n",
      "Step - 3677, Loss - 0.7136610779768758, Learning Rate - 0.00625, magnitude of gradient - 0.6437040013941174\n",
      "Step - 3678, Loss - 0.7107590256068594, Learning Rate - 0.00625, magnitude of gradient - 1.0998323981040878\n",
      "Step - 3679, Loss - 0.7452749266633787, Learning Rate - 0.00625, magnitude of gradient - 0.5425733486554429\n",
      "Step - 3680, Loss - 0.578528141819977, Learning Rate - 0.00625, magnitude of gradient - 0.7298701016805234\n",
      "Step - 3681, Loss - 0.6826350345057349, Learning Rate - 0.00625, magnitude of gradient - 0.8176176810670909\n",
      "Step - 3682, Loss - 0.7450409210231597, Learning Rate - 0.00625, magnitude of gradient - 1.4246894014792337\n",
      "Step - 3683, Loss - 0.7018372705049287, Learning Rate - 0.00625, magnitude of gradient - 0.5886935175117595\n",
      "Step - 3684, Loss - 0.8072003750998412, Learning Rate - 0.00625, magnitude of gradient - 1.4818762275595074\n",
      "Step - 3685, Loss - 0.5435215525566566, Learning Rate - 0.00625, magnitude of gradient - 1.164672134875055\n",
      "Step - 3686, Loss - 0.6894744281783559, Learning Rate - 0.00625, magnitude of gradient - 0.7296166851309946\n",
      "Step - 3687, Loss - 0.6507239884302218, Learning Rate - 0.00625, magnitude of gradient - 2.0854139771144826\n",
      "Step - 3688, Loss - 0.5702858568368834, Learning Rate - 0.00625, magnitude of gradient - 0.808543326861831\n",
      "Step - 3689, Loss - 0.6556671036364473, Learning Rate - 0.00625, magnitude of gradient - 1.5042132723481674\n",
      "Step - 3690, Loss - 0.5683309182574209, Learning Rate - 0.00625, magnitude of gradient - 0.9104289651215287\n",
      "Step - 3691, Loss - 0.8701583485636823, Learning Rate - 0.00625, magnitude of gradient - 1.2243971591319778\n",
      "Step - 3692, Loss - 0.5960945402244247, Learning Rate - 0.00625, magnitude of gradient - 0.855556076974113\n",
      "Step - 3693, Loss - 0.7699605575923167, Learning Rate - 0.00625, magnitude of gradient - 2.032727109904951\n",
      "Step - 3694, Loss - 1.0123495714221729, Learning Rate - 0.00625, magnitude of gradient - 2.17700602266519\n",
      "Step - 3695, Loss - 0.874776016533325, Learning Rate - 0.00625, magnitude of gradient - 0.42920170392187157\n",
      "Step - 3696, Loss - 0.7918838275958522, Learning Rate - 0.00625, magnitude of gradient - 0.6150869259403525\n",
      "Step - 3697, Loss - 0.7392986175504366, Learning Rate - 0.00625, magnitude of gradient - 0.6910652247354082\n",
      "Step - 3698, Loss - 0.6296661594240566, Learning Rate - 0.00625, magnitude of gradient - 1.9888974308975815\n",
      "Step - 3699, Loss - 0.6049833153785643, Learning Rate - 0.00625, magnitude of gradient - 1.5667306008285777\n",
      "Step - 3700, Loss - 0.6748128367983494, Learning Rate - 0.00625, magnitude of gradient - 1.250711539362832\n",
      "Step - 3701, Loss - 0.7159166284221141, Learning Rate - 0.00625, magnitude of gradient - 0.9172639146831015\n",
      "Step - 3702, Loss - 0.6203483334934854, Learning Rate - 0.00625, magnitude of gradient - 1.6449900169414315\n",
      "Step - 3703, Loss - 0.8246634287920775, Learning Rate - 0.00625, magnitude of gradient - 2.5180950484667863\n",
      "Step - 3704, Loss - 0.8573722892507125, Learning Rate - 0.00625, magnitude of gradient - 1.767828551357779\n",
      "Step - 3705, Loss - 0.7470266475180333, Learning Rate - 0.00625, magnitude of gradient - 1.1639116235642528\n",
      "Step - 3706, Loss - 0.6348597945067276, Learning Rate - 0.00625, magnitude of gradient - 0.9953325291611556\n",
      "Step - 3707, Loss - 0.7510617806842925, Learning Rate - 0.00625, magnitude of gradient - 1.6296076476479862\n",
      "Step - 3708, Loss - 0.7551475403158288, Learning Rate - 0.00625, magnitude of gradient - 0.35658443299590875\n",
      "Step - 3709, Loss - 0.7991409684356497, Learning Rate - 0.00625, magnitude of gradient - 0.5192526794488788\n",
      "Step - 3710, Loss - 0.5768466122403841, Learning Rate - 0.00625, magnitude of gradient - 0.588685213665032\n",
      "Step - 3711, Loss - 0.6723779004582706, Learning Rate - 0.00625, magnitude of gradient - 1.1416592383579487\n",
      "Step - 3712, Loss - 0.588119731828898, Learning Rate - 0.00625, magnitude of gradient - 2.4411333698332016\n",
      "Step - 3713, Loss - 0.6002430639300568, Learning Rate - 0.00625, magnitude of gradient - 1.3389378574055442\n",
      "Step - 3714, Loss - 0.710850577691398, Learning Rate - 0.00625, magnitude of gradient - 1.886399087486637\n",
      "Step - 3715, Loss - 0.6722639765814413, Learning Rate - 0.00625, magnitude of gradient - 2.234823413904892\n",
      "Step - 3716, Loss - 0.7952663359111128, Learning Rate - 0.00625, magnitude of gradient - 1.3668083798520985\n",
      "Step - 3717, Loss - 0.5962298941492494, Learning Rate - 0.00625, magnitude of gradient - 0.9545556520502633\n",
      "Step - 3718, Loss - 0.7900459241431181, Learning Rate - 0.00625, magnitude of gradient - 2.324170241348431\n",
      "Step - 3719, Loss - 0.7422150768162755, Learning Rate - 0.00625, magnitude of gradient - 0.8974212118758119\n",
      "Step - 3720, Loss - 0.8223590750798803, Learning Rate - 0.00625, magnitude of gradient - 0.44732108057563563\n",
      "Step - 3721, Loss - 0.7350033470291568, Learning Rate - 0.00625, magnitude of gradient - 1.9202204746266274\n",
      "Step - 3722, Loss - 0.7767263367230666, Learning Rate - 0.00625, magnitude of gradient - 1.806075769700102\n",
      "Step - 3723, Loss - 0.7394267159828004, Learning Rate - 0.00625, magnitude of gradient - 0.9948344312188362\n",
      "Step - 3724, Loss - 0.7617299439029107, Learning Rate - 0.00625, magnitude of gradient - 2.4002096720842334\n",
      "Step - 3725, Loss - 0.7498170038473546, Learning Rate - 0.00625, magnitude of gradient - 0.49662490262488485\n",
      "Step - 3726, Loss - 0.6833204470321468, Learning Rate - 0.00625, magnitude of gradient - 0.7360687038288737\n",
      "Step - 3727, Loss - 0.6003762065064459, Learning Rate - 0.00625, magnitude of gradient - 1.3727151052514341\n",
      "Step - 3728, Loss - 0.547937322763843, Learning Rate - 0.00625, magnitude of gradient - 1.7346293334666185\n",
      "Step - 3729, Loss - 0.6930899731554474, Learning Rate - 0.00625, magnitude of gradient - 1.3424104647116684\n",
      "Step - 3730, Loss - 0.6292587951573361, Learning Rate - 0.00625, magnitude of gradient - 0.7357517554076547\n",
      "Step - 3731, Loss - 0.5913124271171235, Learning Rate - 0.00625, magnitude of gradient - 0.9766387681499692\n",
      "Step - 3732, Loss - 0.6009810498342121, Learning Rate - 0.00625, magnitude of gradient - 0.9802799577314204\n",
      "Step - 3733, Loss - 0.7847181036186927, Learning Rate - 0.00625, magnitude of gradient - 2.814010251128816\n",
      "Step - 3734, Loss - 0.6067178623793985, Learning Rate - 0.00625, magnitude of gradient - 0.5371860421980489\n",
      "Step - 3735, Loss - 0.5754037354354987, Learning Rate - 0.00625, magnitude of gradient - 1.7474192456228936\n",
      "Step - 3736, Loss - 0.7327838108166009, Learning Rate - 0.00625, magnitude of gradient - 0.39391547926309883\n",
      "Step - 3737, Loss - 0.6485480837036504, Learning Rate - 0.00625, magnitude of gradient - 0.9977639392738148\n",
      "Step - 3738, Loss - 0.6722226944366654, Learning Rate - 0.00625, magnitude of gradient - 0.7519616218381248\n",
      "Step - 3739, Loss - 0.7347227900902848, Learning Rate - 0.00625, magnitude of gradient - 2.04479146022991\n",
      "Step - 3740, Loss - 0.5589351420902249, Learning Rate - 0.00625, magnitude of gradient - 1.5434180546763598\n",
      "Step - 3741, Loss - 0.7185428581762863, Learning Rate - 0.00625, magnitude of gradient - 1.1202842991145197\n",
      "Step - 3742, Loss - 0.8727798755157573, Learning Rate - 0.00625, magnitude of gradient - 2.1979620308943155\n",
      "Step - 3743, Loss - 0.749316155498569, Learning Rate - 0.00625, magnitude of gradient - 1.5488017751257495\n",
      "Step - 3744, Loss - 0.5520456387741903, Learning Rate - 0.00625, magnitude of gradient - 1.9519456117511635\n",
      "Step - 3745, Loss - 0.7065345594130568, Learning Rate - 0.00625, magnitude of gradient - 1.1701386990743747\n",
      "Step - 3746, Loss - 0.8036928077554364, Learning Rate - 0.00625, magnitude of gradient - 0.8841511622175693\n",
      "Step - 3747, Loss - 0.604379236666704, Learning Rate - 0.00625, magnitude of gradient - 2.32460722323205\n",
      "Step - 3748, Loss - 0.5646200540601202, Learning Rate - 0.00625, magnitude of gradient - 0.851343538002797\n",
      "Step - 3749, Loss - 0.6715008277171115, Learning Rate - 0.00625, magnitude of gradient - 2.5515270022205527\n",
      "Step - 3750, Loss - 0.5590082783482303, Learning Rate - 0.00625, magnitude of gradient - 0.6896330267185631\n",
      "Step - 3751, Loss - 0.6977886212995816, Learning Rate - 0.00625, magnitude of gradient - 0.9068322337483217\n",
      "Step - 3752, Loss - 0.8184489305491839, Learning Rate - 0.00625, magnitude of gradient - 0.8745451655860434\n",
      "Step - 3753, Loss - 0.5745830463518061, Learning Rate - 0.00625, magnitude of gradient - 1.3214297835722333\n",
      "Step - 3754, Loss - 0.6907199793335763, Learning Rate - 0.00625, magnitude of gradient - 1.6299750717313501\n",
      "Step - 3755, Loss - 0.5019516417928716, Learning Rate - 0.00625, magnitude of gradient - 0.5969791074476428\n",
      "Step - 3756, Loss - 0.6100778773861559, Learning Rate - 0.00625, magnitude of gradient - 1.897833735353698\n",
      "Step - 3757, Loss - 0.6465015563816945, Learning Rate - 0.00625, magnitude of gradient - 1.1866364037110948\n",
      "Step - 3758, Loss - 0.7996918352610628, Learning Rate - 0.00625, magnitude of gradient - 1.920764008298267\n",
      "Step - 3759, Loss - 0.6604266686897989, Learning Rate - 0.00625, magnitude of gradient - 0.8276302192723534\n",
      "Step - 3760, Loss - 0.824385730956663, Learning Rate - 0.00625, magnitude of gradient - 2.2582756353354196\n",
      "Step - 3761, Loss - 0.9031724647538142, Learning Rate - 0.00625, magnitude of gradient - 1.890846400693877\n",
      "Step - 3762, Loss - 0.8464431211585555, Learning Rate - 0.00625, magnitude of gradient - 1.236582106177395\n",
      "Step - 3763, Loss - 0.6720265820606885, Learning Rate - 0.00625, magnitude of gradient - 1.8811264051035126\n",
      "Step - 3764, Loss - 0.5367526296360746, Learning Rate - 0.00625, magnitude of gradient - 0.5480463270253294\n",
      "Step - 3765, Loss - 0.8828085037382064, Learning Rate - 0.00625, magnitude of gradient - 2.3359534404505142\n",
      "Step - 3766, Loss - 0.8048162660144311, Learning Rate - 0.00625, magnitude of gradient - 0.4437531782744811\n",
      "Step - 3767, Loss - 0.7932452980899725, Learning Rate - 0.00625, magnitude of gradient - 0.3727019315120439\n",
      "Step - 3768, Loss - 0.7930711883120172, Learning Rate - 0.00625, magnitude of gradient - 1.0977551388104967\n",
      "Step - 3769, Loss - 0.7752809655262426, Learning Rate - 0.00625, magnitude of gradient - 2.103158558186786\n",
      "Step - 3770, Loss - 0.4872607322239061, Learning Rate - 0.00625, magnitude of gradient - 1.8396877585087763\n",
      "Step - 3771, Loss - 0.7049166675396625, Learning Rate - 0.00625, magnitude of gradient - 1.1620881344508105\n",
      "Step - 3772, Loss - 0.6483664948604753, Learning Rate - 0.00625, magnitude of gradient - 0.8973380928398843\n",
      "Step - 3773, Loss - 0.6625674874617806, Learning Rate - 0.00625, magnitude of gradient - 1.4631199628328235\n",
      "Step - 3774, Loss - 0.908051388668791, Learning Rate - 0.00625, magnitude of gradient - 1.0063281435579357\n",
      "Step - 3775, Loss - 0.49222154024882514, Learning Rate - 0.00625, magnitude of gradient - 2.23126003112437\n",
      "Step - 3776, Loss - 0.7818707741218267, Learning Rate - 0.00625, magnitude of gradient - 1.6577149302883822\n",
      "Step - 3777, Loss - 0.6292733381061827, Learning Rate - 0.00625, magnitude of gradient - 1.777423338112787\n",
      "Step - 3778, Loss - 0.4119220613395205, Learning Rate - 0.00625, magnitude of gradient - 2.503495667260981\n",
      "Step - 3779, Loss - 0.7915363615132882, Learning Rate - 0.00625, magnitude of gradient - 0.644793786362536\n",
      "Step - 3780, Loss - 0.5543903058336408, Learning Rate - 0.00625, magnitude of gradient - 1.1058839305850772\n",
      "Step - 3781, Loss - 0.6066799844787836, Learning Rate - 0.00625, magnitude of gradient - 1.718099540189668\n",
      "Step - 3782, Loss - 0.9109254949261026, Learning Rate - 0.00625, magnitude of gradient - 2.6328255381642482\n",
      "Step - 3783, Loss - 0.7378100604801219, Learning Rate - 0.00625, magnitude of gradient - 1.8634752667550358\n",
      "Step - 3784, Loss - 0.7977898694326541, Learning Rate - 0.00625, magnitude of gradient - 2.1803662394926513\n",
      "Step - 3785, Loss - 0.6356581569722854, Learning Rate - 0.00625, magnitude of gradient - 1.3396196371293587\n",
      "Step - 3786, Loss - 0.7356232034177701, Learning Rate - 0.00625, magnitude of gradient - 2.9713736549012193\n",
      "Step - 3787, Loss - 0.7304403834853708, Learning Rate - 0.00625, magnitude of gradient - 2.6740569717277483\n",
      "Step - 3788, Loss - 0.6224116236804982, Learning Rate - 0.00625, magnitude of gradient - 0.9321723253377694\n",
      "Step - 3789, Loss - 0.5761120866231528, Learning Rate - 0.00625, magnitude of gradient - 1.0608561692295386\n",
      "Step - 3790, Loss - 0.8051300052115444, Learning Rate - 0.00625, magnitude of gradient - 0.7742089075047107\n",
      "Step - 3791, Loss - 0.8504881088145458, Learning Rate - 0.00625, magnitude of gradient - 1.4945307934996923\n",
      "Step - 3792, Loss - 0.705399299478311, Learning Rate - 0.00625, magnitude of gradient - 1.7519861909373307\n",
      "Step - 3793, Loss - 0.5944636147151282, Learning Rate - 0.00625, magnitude of gradient - 1.6528911594265931\n",
      "Step - 3794, Loss - 0.5731055400948504, Learning Rate - 0.00625, magnitude of gradient - 1.1381051288062705\n",
      "Step - 3795, Loss - 0.6958004180235953, Learning Rate - 0.00625, magnitude of gradient - 1.7824026031164262\n",
      "Step - 3796, Loss - 0.8702493909479521, Learning Rate - 0.00625, magnitude of gradient - 2.123008251810139\n",
      "Step - 3797, Loss - 0.5950036323412521, Learning Rate - 0.00625, magnitude of gradient - 0.9743046782499185\n",
      "Step - 3798, Loss - 0.9251047820385134, Learning Rate - 0.00625, magnitude of gradient - 2.3605623537207467\n",
      "Step - 3799, Loss - 0.8272818160417219, Learning Rate - 0.00625, magnitude of gradient - 2.4598616438058407\n",
      "Step - 3800, Loss - 0.7516126715551589, Learning Rate - 0.00625, magnitude of gradient - 1.4517119667959621\n",
      "Step - 3801, Loss - 0.8055645613427465, Learning Rate - 0.00625, magnitude of gradient - 1.443027943871707\n",
      "Step - 3802, Loss - 0.8367333087373727, Learning Rate - 0.00625, magnitude of gradient - 2.3557042089629365\n",
      "Step - 3803, Loss - 0.8594047380392734, Learning Rate - 0.00625, magnitude of gradient - 1.294550321846027\n",
      "Step - 3804, Loss - 0.7054387705007531, Learning Rate - 0.00625, magnitude of gradient - 0.8461679806695253\n",
      "Step - 3805, Loss - 0.6245228463561606, Learning Rate - 0.00625, magnitude of gradient - 1.5054632910051529\n",
      "Step - 3806, Loss - 0.5494645378044295, Learning Rate - 0.00625, magnitude of gradient - 1.2184013862445864\n",
      "Step - 3807, Loss - 0.8981615429440052, Learning Rate - 0.00625, magnitude of gradient - 1.644202881115391\n",
      "Step - 3808, Loss - 0.8080344191002545, Learning Rate - 0.00625, magnitude of gradient - 1.706431499501116\n",
      "Step - 3809, Loss - 0.7987935181694776, Learning Rate - 0.00625, magnitude of gradient - 1.058627509794427\n",
      "Step - 3810, Loss - 0.6638915263743251, Learning Rate - 0.00625, magnitude of gradient - 0.6786424786305724\n",
      "Step - 3811, Loss - 0.657806700788907, Learning Rate - 0.00625, magnitude of gradient - 0.7714238840933543\n",
      "Step - 3812, Loss - 0.6997057253292651, Learning Rate - 0.00625, magnitude of gradient - 1.882935779153013\n",
      "Step - 3813, Loss - 0.7035993581981934, Learning Rate - 0.00625, magnitude of gradient - 0.5865888162610562\n",
      "Step - 3814, Loss - 0.9424280517597523, Learning Rate - 0.00625, magnitude of gradient - 1.7208998326022356\n",
      "Step - 3815, Loss - 0.6644045286556416, Learning Rate - 0.00625, magnitude of gradient - 1.028026566683092\n",
      "Step - 3816, Loss - 0.6000586939037109, Learning Rate - 0.00625, magnitude of gradient - 2.30451075290723\n",
      "Step - 3817, Loss - 0.8147400444691703, Learning Rate - 0.00625, magnitude of gradient - 1.101977507589526\n",
      "Step - 3818, Loss - 0.5952997607349115, Learning Rate - 0.00625, magnitude of gradient - 1.1592584408781423\n",
      "Step - 3819, Loss - 0.8371144731596045, Learning Rate - 0.00625, magnitude of gradient - 0.48517486594277165\n",
      "Step - 3820, Loss - 0.771901835631189, Learning Rate - 0.00625, magnitude of gradient - 1.8458881175841904\n",
      "Step - 3821, Loss - 0.7091881696044172, Learning Rate - 0.00625, magnitude of gradient - 0.8308629946244114\n",
      "Step - 3822, Loss - 0.86341340676744, Learning Rate - 0.00625, magnitude of gradient - 1.620437906276025\n",
      "Step - 3823, Loss - 0.6348316473825546, Learning Rate - 0.00625, magnitude of gradient - 0.90301842086238\n",
      "Step - 3824, Loss - 0.6739409860821332, Learning Rate - 0.00625, magnitude of gradient - 1.2230942495007404\n",
      "Step - 3825, Loss - 0.7085118717015327, Learning Rate - 0.00625, magnitude of gradient - 0.9278657020408576\n",
      "Step - 3826, Loss - 0.7090938731779314, Learning Rate - 0.00625, magnitude of gradient - 0.9716631432267842\n",
      "Step - 3827, Loss - 0.7746158422005226, Learning Rate - 0.00625, magnitude of gradient - 1.0904504869222829\n",
      "Step - 3828, Loss - 0.5503240982242125, Learning Rate - 0.00625, magnitude of gradient - 2.700577253739609\n",
      "Step - 3829, Loss - 0.7532850317431927, Learning Rate - 0.00625, magnitude of gradient - 0.7673986535103524\n",
      "Step - 3830, Loss - 0.5745234019241849, Learning Rate - 0.00625, magnitude of gradient - 2.411913209159864\n",
      "Step - 3831, Loss - 0.7473041758378886, Learning Rate - 0.00625, magnitude of gradient - 0.9135704748533544\n",
      "Step - 3832, Loss - 0.7783597428890219, Learning Rate - 0.00625, magnitude of gradient - 0.8835401888928993\n",
      "Step - 3833, Loss - 0.6976252112178325, Learning Rate - 0.00625, magnitude of gradient - 1.3972805648935183\n",
      "Step - 3834, Loss - 0.5720123488623504, Learning Rate - 0.00625, magnitude of gradient - 2.1683294247732534\n",
      "Step - 3835, Loss - 0.7230489912924807, Learning Rate - 0.00625, magnitude of gradient - 1.852395166927582\n",
      "Step - 3836, Loss - 0.8046356039647138, Learning Rate - 0.00625, magnitude of gradient - 1.428362152484679\n",
      "Step - 3837, Loss - 0.7897682279571562, Learning Rate - 0.00625, magnitude of gradient - 0.23798382602229665\n",
      "Step - 3838, Loss - 0.5958017869091916, Learning Rate - 0.00625, magnitude of gradient - 1.3118406360052153\n",
      "Step - 3839, Loss - 0.5775288458543146, Learning Rate - 0.00625, magnitude of gradient - 1.8403340338856635\n",
      "Step - 3840, Loss - 0.7998657595237247, Learning Rate - 0.00625, magnitude of gradient - 1.4859670436657433\n",
      "Step - 3841, Loss - 0.8041640534829896, Learning Rate - 0.00625, magnitude of gradient - 0.6479570982893201\n",
      "Step - 3842, Loss - 0.7751295203822482, Learning Rate - 0.00625, magnitude of gradient - 1.5484178775389765\n",
      "Step - 3843, Loss - 0.4790526856215081, Learning Rate - 0.00625, magnitude of gradient - 0.7160586080439264\n",
      "Step - 3844, Loss - 0.8836352754102272, Learning Rate - 0.00625, magnitude of gradient - 1.4947013167612104\n",
      "Step - 3845, Loss - 0.47087096153587576, Learning Rate - 0.00625, magnitude of gradient - 1.1377020428841556\n",
      "Step - 3846, Loss - 0.7814588129083226, Learning Rate - 0.00625, magnitude of gradient - 0.5231731088668251\n",
      "Step - 3847, Loss - 0.6261627358000434, Learning Rate - 0.00625, magnitude of gradient - 2.3695596207659175\n",
      "Step - 3848, Loss - 0.5479848372260983, Learning Rate - 0.00625, magnitude of gradient - 1.3313582946541402\n",
      "Step - 3849, Loss - 0.7966473976431769, Learning Rate - 0.00625, magnitude of gradient - 1.7372333150484975\n",
      "Step - 3850, Loss - 0.7293594503997982, Learning Rate - 0.00625, magnitude of gradient - 1.8735875954861319\n",
      "Step - 3851, Loss - 0.6550913647084211, Learning Rate - 0.00625, magnitude of gradient - 0.3843762490650084\n",
      "Step - 3852, Loss - 0.6741876852909555, Learning Rate - 0.00625, magnitude of gradient - 1.5800529743091385\n",
      "Step - 3853, Loss - 0.6012352259560307, Learning Rate - 0.00625, magnitude of gradient - 1.539450174192132\n",
      "Step - 3854, Loss - 0.5048621573377579, Learning Rate - 0.00625, magnitude of gradient - 1.04515847876594\n",
      "Step - 3855, Loss - 0.5969046871927063, Learning Rate - 0.00625, magnitude of gradient - 1.0284938905685377\n",
      "Step - 3856, Loss - 0.7457689210916632, Learning Rate - 0.00625, magnitude of gradient - 0.9623327406123255\n",
      "Step - 3857, Loss - 0.7142078697617974, Learning Rate - 0.00625, magnitude of gradient - 2.0615479883591523\n",
      "Step - 3858, Loss - 0.4431889485274939, Learning Rate - 0.00625, magnitude of gradient - 1.1882389658415717\n",
      "Step - 3859, Loss - 0.4057859149557399, Learning Rate - 0.00625, magnitude of gradient - 0.90086968443044\n",
      "Step - 3860, Loss - 0.6361431083808673, Learning Rate - 0.00625, magnitude of gradient - 1.0613826648611986\n",
      "Step - 3861, Loss - 0.7651995887515568, Learning Rate - 0.00625, magnitude of gradient - 1.269806856101851\n",
      "Step - 3862, Loss - 0.7033483987890214, Learning Rate - 0.00625, magnitude of gradient - 1.077548413644864\n",
      "Step - 3863, Loss - 0.6773500031324675, Learning Rate - 0.00625, magnitude of gradient - 0.1641007013887871\n",
      "Step - 3864, Loss - 0.8082651123519603, Learning Rate - 0.00625, magnitude of gradient - 1.2264296307492017\n",
      "Step - 3865, Loss - 0.6058519524619481, Learning Rate - 0.00625, magnitude of gradient - 0.5712902768995899\n",
      "Step - 3866, Loss - 0.5297143643873039, Learning Rate - 0.00625, magnitude of gradient - 0.7719210482400981\n",
      "Step - 3867, Loss - 0.58428662601063, Learning Rate - 0.00625, magnitude of gradient - 1.050748697884686\n",
      "Step - 3868, Loss - 0.7507939757694534, Learning Rate - 0.00625, magnitude of gradient - 1.8457776522840743\n",
      "Step - 3869, Loss - 0.581283697185524, Learning Rate - 0.00625, magnitude of gradient - 1.183441817518261\n",
      "Step - 3870, Loss - 0.8219501007700935, Learning Rate - 0.00625, magnitude of gradient - 1.6656165500923925\n",
      "Step - 3871, Loss - 0.9181241936482101, Learning Rate - 0.00625, magnitude of gradient - 1.9436294979495925\n",
      "Step - 3872, Loss - 0.8144404726821963, Learning Rate - 0.00625, magnitude of gradient - 2.080966541779457\n",
      "Step - 3873, Loss - 0.6166338677142533, Learning Rate - 0.00625, magnitude of gradient - 1.4651365338773825\n",
      "Step - 3874, Loss - 0.7929997041585104, Learning Rate - 0.00625, magnitude of gradient - 1.534122481640701\n",
      "Step - 3875, Loss - 0.5901642066363478, Learning Rate - 0.00625, magnitude of gradient - 0.8511803071894635\n",
      "Step - 3876, Loss - 0.8119324452203173, Learning Rate - 0.00625, magnitude of gradient - 1.6497903327343946\n",
      "Step - 3877, Loss - 0.6556569764797814, Learning Rate - 0.00625, magnitude of gradient - 0.5086352557627278\n",
      "Step - 3878, Loss - 0.6538813928704581, Learning Rate - 0.00625, magnitude of gradient - 1.8029385419979218\n",
      "Step - 3879, Loss - 0.6642355230601883, Learning Rate - 0.00625, magnitude of gradient - 2.6678753943454048\n",
      "Step - 3880, Loss - 0.6301260861711833, Learning Rate - 0.00625, magnitude of gradient - 1.4927224585037204\n",
      "Step - 3881, Loss - 0.8120902905638945, Learning Rate - 0.00625, magnitude of gradient - 0.4101811452151526\n",
      "Step - 3882, Loss - 0.8767712940106862, Learning Rate - 0.00625, magnitude of gradient - 2.5664507988765317\n",
      "Step - 3883, Loss - 0.9692603122546568, Learning Rate - 0.00625, magnitude of gradient - 1.671421840437425\n",
      "Step - 3884, Loss - 0.571906109590582, Learning Rate - 0.00625, magnitude of gradient - 0.4725923947585809\n",
      "Step - 3885, Loss - 0.8070227262328112, Learning Rate - 0.00625, magnitude of gradient - 2.712144218313682\n",
      "Step - 3886, Loss - 0.7823745173819664, Learning Rate - 0.00625, magnitude of gradient - 1.7771899503472837\n",
      "Step - 3887, Loss - 0.8331595581888236, Learning Rate - 0.00625, magnitude of gradient - 1.8110915295360561\n",
      "Step - 3888, Loss - 0.6694736915512457, Learning Rate - 0.00625, magnitude of gradient - 0.5884387040542599\n",
      "Step - 3889, Loss - 0.6585365501881587, Learning Rate - 0.00625, magnitude of gradient - 1.5146373160256086\n",
      "Step - 3890, Loss - 0.7231938134288284, Learning Rate - 0.00625, magnitude of gradient - 1.022748912602968\n",
      "Step - 3891, Loss - 0.7847496825169077, Learning Rate - 0.00625, magnitude of gradient - 1.6877832397356387\n",
      "Step - 3892, Loss - 0.6602717087621753, Learning Rate - 0.00625, magnitude of gradient - 3.383751340545188\n",
      "Step - 3893, Loss - 0.7724197033888043, Learning Rate - 0.00625, magnitude of gradient - 1.9542080792071288\n",
      "Step - 3894, Loss - 0.843111212890082, Learning Rate - 0.00625, magnitude of gradient - 0.5972281083225152\n",
      "Step - 3895, Loss - 0.7595584114271577, Learning Rate - 0.00625, magnitude of gradient - 0.8243387545099198\n",
      "Step - 3896, Loss - 0.7001055189781282, Learning Rate - 0.00625, magnitude of gradient - 1.450812902865495\n",
      "Step - 3897, Loss - 0.7010566484035758, Learning Rate - 0.00625, magnitude of gradient - 0.5543105617791009\n",
      "Step - 3898, Loss - 0.8352633225663979, Learning Rate - 0.00625, magnitude of gradient - 1.287165578181717\n",
      "Step - 3899, Loss - 0.7397793975800042, Learning Rate - 0.00625, magnitude of gradient - 0.7364692298309475\n",
      "Step - 3900, Loss - 0.7022026799647251, Learning Rate - 0.00625, magnitude of gradient - 1.0356128587195483\n",
      "Step - 3901, Loss - 0.6056617730095757, Learning Rate - 0.00625, magnitude of gradient - 0.9927157062720846\n",
      "Step - 3902, Loss - 0.8392679117177652, Learning Rate - 0.00625, magnitude of gradient - 1.5063276633831928\n",
      "Step - 3903, Loss - 0.6387448684191851, Learning Rate - 0.00625, magnitude of gradient - 1.266599106987728\n",
      "Step - 3904, Loss - 0.8743813363506275, Learning Rate - 0.00625, magnitude of gradient - 2.3939068087365203\n",
      "Step - 3905, Loss - 0.7596554798119436, Learning Rate - 0.00625, magnitude of gradient - 1.1035077088396856\n",
      "Step - 3906, Loss - 0.5261267337484974, Learning Rate - 0.00625, magnitude of gradient - 1.9425121018676217\n",
      "Step - 3907, Loss - 0.7433286758001647, Learning Rate - 0.00625, magnitude of gradient - 1.0723740376980768\n",
      "Step - 3908, Loss - 0.740226936974084, Learning Rate - 0.00625, magnitude of gradient - 1.8653236413174794\n",
      "Step - 3909, Loss - 0.5544337062586427, Learning Rate - 0.00625, magnitude of gradient - 2.051467119749819\n",
      "Step - 3910, Loss - 0.77171618879949, Learning Rate - 0.00625, magnitude of gradient - 1.9001625596439535\n",
      "Step - 3911, Loss - 0.7015213554789406, Learning Rate - 0.00625, magnitude of gradient - 0.302495751430483\n",
      "Step - 3912, Loss - 0.7314876248433402, Learning Rate - 0.00625, magnitude of gradient - 1.2577469247189883\n",
      "Step - 3913, Loss - 0.845857614026154, Learning Rate - 0.00625, magnitude of gradient - 0.9750060491109511\n",
      "Step - 3914, Loss - 0.791510999940021, Learning Rate - 0.00625, magnitude of gradient - 2.2999150517803235\n",
      "Step - 3915, Loss - 0.6879693739416712, Learning Rate - 0.00625, magnitude of gradient - 1.646217389524779\n",
      "Step - 3916, Loss - 0.5963899335093855, Learning Rate - 0.00625, magnitude of gradient - 1.931067125774325\n",
      "Step - 3917, Loss - 0.7474476910131368, Learning Rate - 0.00625, magnitude of gradient - 0.4583768588275028\n",
      "Step - 3918, Loss - 0.5962326736776355, Learning Rate - 0.00625, magnitude of gradient - 1.0653570436268605\n",
      "Step - 3919, Loss - 0.8821308823596965, Learning Rate - 0.00625, magnitude of gradient - 0.8597644821308995\n",
      "Step - 3920, Loss - 0.8022105472134977, Learning Rate - 0.00625, magnitude of gradient - 1.5753587775788456\n",
      "Step - 3921, Loss - 0.5649287716533371, Learning Rate - 0.00625, magnitude of gradient - 0.6499693589956459\n",
      "Step - 3922, Loss - 0.7913551070372042, Learning Rate - 0.00625, magnitude of gradient - 1.6002124945028098\n",
      "Step - 3923, Loss - 0.7320893037344243, Learning Rate - 0.00625, magnitude of gradient - 1.8756472222122915\n",
      "Step - 3924, Loss - 0.75946554334563, Learning Rate - 0.00625, magnitude of gradient - 1.2197809708030056\n",
      "Step - 3925, Loss - 0.782965772073337, Learning Rate - 0.00625, magnitude of gradient - 1.2487146369130528\n",
      "Step - 3926, Loss - 0.712429106098178, Learning Rate - 0.00625, magnitude of gradient - 1.4858814337008361\n",
      "Step - 3927, Loss - 0.8284368572724489, Learning Rate - 0.00625, magnitude of gradient - 1.133885735682357\n",
      "Step - 3928, Loss - 0.8575065892350687, Learning Rate - 0.00625, magnitude of gradient - 1.6440069619493378\n",
      "Step - 3929, Loss - 0.6843413861966708, Learning Rate - 0.00625, magnitude of gradient - 1.825630971293936\n",
      "Step - 3930, Loss - 0.8042932635126133, Learning Rate - 0.00625, magnitude of gradient - 0.8872850083520878\n",
      "Step - 3931, Loss - 0.6892582871834241, Learning Rate - 0.00625, magnitude of gradient - 0.41571088809857326\n",
      "Step - 3932, Loss - 0.6978791701534817, Learning Rate - 0.00625, magnitude of gradient - 0.48248973325118516\n",
      "Step - 3933, Loss - 0.5728356838004955, Learning Rate - 0.00625, magnitude of gradient - 1.7933941668569149\n",
      "Step - 3934, Loss - 0.8115758449703324, Learning Rate - 0.00625, magnitude of gradient - 1.2217123756994532\n",
      "Step - 3935, Loss - 0.6152056209693424, Learning Rate - 0.00625, magnitude of gradient - 0.49248967101086777\n",
      "Step - 3936, Loss - 0.7436302228428864, Learning Rate - 0.00625, magnitude of gradient - 0.9908124353481632\n",
      "Step - 3937, Loss - 0.8908821859500247, Learning Rate - 0.00625, magnitude of gradient - 2.905386322076948\n",
      "Step - 3938, Loss - 0.5788612545244511, Learning Rate - 0.00625, magnitude of gradient - 1.6295592103631742\n",
      "Step - 3939, Loss - 0.6572706698355089, Learning Rate - 0.00625, magnitude of gradient - 2.3763577206121553\n",
      "Step - 3940, Loss - 0.6131614917025242, Learning Rate - 0.00625, magnitude of gradient - 1.6818674497064279\n",
      "Step - 3941, Loss - 0.473934691794721, Learning Rate - 0.00625, magnitude of gradient - 1.0452159634494562\n",
      "Step - 3942, Loss - 0.657355847303595, Learning Rate - 0.00625, magnitude of gradient - 1.5443780776434073\n",
      "Step - 3943, Loss - 0.6604950653326195, Learning Rate - 0.00625, magnitude of gradient - 1.9657125792888155\n",
      "Step - 3944, Loss - 0.6379051351618372, Learning Rate - 0.00625, magnitude of gradient - 1.8002646488913514\n",
      "Step - 3945, Loss - 0.7045882817698272, Learning Rate - 0.00625, magnitude of gradient - 0.9591599206935203\n",
      "Step - 3946, Loss - 0.650456211066935, Learning Rate - 0.00625, magnitude of gradient - 1.6413950873072205\n",
      "Step - 3947, Loss - 0.761855814093488, Learning Rate - 0.00625, magnitude of gradient - 1.4506711452349894\n",
      "Step - 3948, Loss - 0.7858513201335411, Learning Rate - 0.00625, magnitude of gradient - 0.46012230908219004\n",
      "Step - 3949, Loss - 0.7852619983114576, Learning Rate - 0.00625, magnitude of gradient - 0.706266905776199\n",
      "Step - 3950, Loss - 0.7274250767105367, Learning Rate - 0.00625, magnitude of gradient - 2.484673168804192\n",
      "Step - 3951, Loss - 0.63493739468705, Learning Rate - 0.00625, magnitude of gradient - 0.8184912253533682\n",
      "Step - 3952, Loss - 0.5615488669248357, Learning Rate - 0.00625, magnitude of gradient - 1.0353602407882296\n",
      "Step - 3953, Loss - 0.4953378637760966, Learning Rate - 0.00625, magnitude of gradient - 1.1129185979749991\n",
      "Step - 3954, Loss - 0.7885671881350043, Learning Rate - 0.00625, magnitude of gradient - 1.468555307651591\n",
      "Step - 3955, Loss - 0.7468443919325726, Learning Rate - 0.00625, magnitude of gradient - 1.8251585937665882\n",
      "Step - 3956, Loss - 0.8102733084647542, Learning Rate - 0.00625, magnitude of gradient - 1.4386630996768393\n",
      "Step - 3957, Loss - 0.44384528071663276, Learning Rate - 0.00625, magnitude of gradient - 1.0006319084663327\n",
      "Step - 3958, Loss - 0.8404183204873361, Learning Rate - 0.00625, magnitude of gradient - 0.929835800290504\n",
      "Step - 3959, Loss - 0.6926586240382504, Learning Rate - 0.00625, magnitude of gradient - 1.1421041243491472\n",
      "Step - 3960, Loss - 0.7267616317418163, Learning Rate - 0.00625, magnitude of gradient - 1.296867971996142\n",
      "Step - 3961, Loss - 0.7120868415171542, Learning Rate - 0.00625, magnitude of gradient - 1.1133413028388766\n",
      "Step - 3962, Loss - 1.030277193674165, Learning Rate - 0.00625, magnitude of gradient - 2.0117638272426968\n",
      "Step - 3963, Loss - 0.5860997884671105, Learning Rate - 0.00625, magnitude of gradient - 1.9475738468973591\n",
      "Step - 3964, Loss - 0.7817175030813419, Learning Rate - 0.00625, magnitude of gradient - 1.1549717210185506\n",
      "Step - 3965, Loss - 0.5703674110243243, Learning Rate - 0.00625, magnitude of gradient - 0.38700928629301673\n",
      "Step - 3966, Loss - 0.8445184681242817, Learning Rate - 0.00625, magnitude of gradient - 1.4208692773037126\n",
      "Step - 3967, Loss - 0.6398975774206912, Learning Rate - 0.00625, magnitude of gradient - 0.773355183957819\n",
      "Step - 3968, Loss - 0.6514010540017096, Learning Rate - 0.00625, magnitude of gradient - 0.9822949085774876\n",
      "Step - 3969, Loss - 0.6624827422519121, Learning Rate - 0.00625, magnitude of gradient - 2.1773077740939617\n",
      "Step - 3970, Loss - 0.7051221676320865, Learning Rate - 0.00625, magnitude of gradient - 1.55435018375286\n",
      "Step - 3971, Loss - 0.7912394716495844, Learning Rate - 0.00625, magnitude of gradient - 0.9821552916037605\n",
      "Step - 3972, Loss - 0.6854259270316801, Learning Rate - 0.00625, magnitude of gradient - 1.0440096138469073\n",
      "Step - 3973, Loss - 0.5153773423864114, Learning Rate - 0.00625, magnitude of gradient - 2.221333070765026\n",
      "Step - 3974, Loss - 0.7090927006339709, Learning Rate - 0.00625, magnitude of gradient - 0.41020994743181755\n",
      "Step - 3975, Loss - 0.7661229805594594, Learning Rate - 0.00625, magnitude of gradient - 0.870398342288802\n",
      "Step - 3976, Loss - 0.5610373040189609, Learning Rate - 0.00625, magnitude of gradient - 1.3285814184082345\n",
      "Step - 3977, Loss - 0.5896360609832625, Learning Rate - 0.00625, magnitude of gradient - 1.727630987497923\n",
      "Step - 3978, Loss - 0.7713050282633132, Learning Rate - 0.00625, magnitude of gradient - 2.166874092049584\n",
      "Step - 3979, Loss - 0.5909293690448768, Learning Rate - 0.00625, magnitude of gradient - 0.43372681097084476\n",
      "Step - 3980, Loss - 0.5683195821906863, Learning Rate - 0.00625, magnitude of gradient - 0.9900808198847355\n",
      "Step - 3981, Loss - 0.7241373045635238, Learning Rate - 0.00625, magnitude of gradient - 2.1092866441808695\n",
      "Step - 3982, Loss - 0.6825251666474998, Learning Rate - 0.00625, magnitude of gradient - 2.1107610568559645\n",
      "Step - 3983, Loss - 0.8303913218797252, Learning Rate - 0.00625, magnitude of gradient - 1.2449055653646202\n",
      "Step - 3984, Loss - 0.644773191407948, Learning Rate - 0.00625, magnitude of gradient - 0.834075737152208\n",
      "Step - 3985, Loss - 0.7941109977511059, Learning Rate - 0.00625, magnitude of gradient - 1.4369715850527975\n",
      "Step - 3986, Loss - 0.5506441521079313, Learning Rate - 0.00625, magnitude of gradient - 2.115805673777647\n",
      "Step - 3987, Loss - 0.6944120350770013, Learning Rate - 0.00625, magnitude of gradient - 2.370740062744911\n",
      "Step - 3988, Loss - 0.7010527367077581, Learning Rate - 0.00625, magnitude of gradient - 2.886967913588133\n",
      "Step - 3989, Loss - 0.7175121690325159, Learning Rate - 0.00625, magnitude of gradient - 0.6844860031400426\n",
      "Step - 3990, Loss - 0.6463209795479394, Learning Rate - 0.00625, magnitude of gradient - 2.6655551058252462\n",
      "Step - 3991, Loss - 0.7163629082381675, Learning Rate - 0.00625, magnitude of gradient - 1.2451132808179033\n",
      "Step - 3992, Loss - 0.6255913464095033, Learning Rate - 0.00625, magnitude of gradient - 2.1808296874717046\n",
      "Step - 3993, Loss - 0.6550526346018086, Learning Rate - 0.00625, magnitude of gradient - 1.0304512195985625\n",
      "Step - 3994, Loss - 0.6371676835966127, Learning Rate - 0.00625, magnitude of gradient - 2.5272921975057363\n",
      "Step - 3995, Loss - 0.772751691605098, Learning Rate - 0.00625, magnitude of gradient - 1.0789801015000118\n",
      "Step - 3996, Loss - 0.8734678359614961, Learning Rate - 0.00625, magnitude of gradient - 1.380191872736357\n",
      "Step - 3997, Loss - 0.6939972006938225, Learning Rate - 0.00625, magnitude of gradient - 1.5398507196955618\n",
      "Step - 3998, Loss - 0.7227925856639833, Learning Rate - 0.00625, magnitude of gradient - 0.5781764372823537\n",
      "Step - 3999, Loss - 0.6716401728620667, Learning Rate - 0.00625, magnitude of gradient - 1.739413275484859\n",
      "Step - 4000, Loss - 0.7640480172700499, Learning Rate - 0.00625, magnitude of gradient - 0.3683962546561842\n",
      "Step - 4001, Loss - 0.45011495604055946, Learning Rate - 0.003125, magnitude of gradient - 1.1599929369588335\n",
      "Step - 4002, Loss - 0.5530129877500042, Learning Rate - 0.003125, magnitude of gradient - 1.7510137446187917\n",
      "Step - 4003, Loss - 0.6680676309142278, Learning Rate - 0.003125, magnitude of gradient - 1.101429047000628\n",
      "Step - 4004, Loss - 0.8672770788662063, Learning Rate - 0.003125, magnitude of gradient - 2.249796062130821\n",
      "Step - 4005, Loss - 0.6042767962256211, Learning Rate - 0.003125, magnitude of gradient - 2.1785260988993573\n",
      "Step - 4006, Loss - 0.6025470659266086, Learning Rate - 0.003125, magnitude of gradient - 0.7840053632363093\n",
      "Step - 4007, Loss - 0.6609157691279213, Learning Rate - 0.003125, magnitude of gradient - 0.5934120832613071\n",
      "Step - 4008, Loss - 0.7463764687592584, Learning Rate - 0.003125, magnitude of gradient - 1.2268337468852824\n",
      "Step - 4009, Loss - 0.6568507214791768, Learning Rate - 0.003125, magnitude of gradient - 0.6866759377563153\n",
      "Step - 4010, Loss - 0.886798050856351, Learning Rate - 0.003125, magnitude of gradient - 0.5058384333422482\n",
      "Step - 4011, Loss - 0.6880618847014075, Learning Rate - 0.003125, magnitude of gradient - 1.118886587820536\n",
      "Step - 4012, Loss - 0.43938582278392874, Learning Rate - 0.003125, magnitude of gradient - 2.000111129319488\n",
      "Step - 4013, Loss - 0.8277069732744723, Learning Rate - 0.003125, magnitude of gradient - 0.707975446353848\n",
      "Step - 4014, Loss - 0.8616905998497513, Learning Rate - 0.003125, magnitude of gradient - 1.7550183972931102\n",
      "Step - 4015, Loss - 0.5616708233218529, Learning Rate - 0.003125, magnitude of gradient - 0.4450493566446847\n",
      "Step - 4016, Loss - 0.6346300322209483, Learning Rate - 0.003125, magnitude of gradient - 2.184221739443614\n",
      "Step - 4017, Loss - 0.8059398660655603, Learning Rate - 0.003125, magnitude of gradient - 1.0861631405769987\n",
      "Step - 4018, Loss - 0.7529878714275469, Learning Rate - 0.003125, magnitude of gradient - 1.292411613158208\n",
      "Step - 4019, Loss - 0.7705936188115815, Learning Rate - 0.003125, magnitude of gradient - 1.629723433164876\n",
      "Step - 4020, Loss - 0.7486008709215832, Learning Rate - 0.003125, magnitude of gradient - 1.0047410875438814\n",
      "Step - 4021, Loss - 0.4608652183926865, Learning Rate - 0.003125, magnitude of gradient - 1.2266280614269491\n",
      "Step - 4022, Loss - 0.8675233919910637, Learning Rate - 0.003125, magnitude of gradient - 1.2997615141133692\n",
      "Step - 4023, Loss - 0.5439100179869892, Learning Rate - 0.003125, magnitude of gradient - 1.8044425478242132\n",
      "Step - 4024, Loss - 0.7995018014385019, Learning Rate - 0.003125, magnitude of gradient - 1.1083636565294763\n",
      "Step - 4025, Loss - 0.6293712640775931, Learning Rate - 0.003125, magnitude of gradient - 1.2765844987321862\n",
      "Step - 4026, Loss - 0.6304284569225882, Learning Rate - 0.003125, magnitude of gradient - 0.9996471714364049\n",
      "Step - 4027, Loss - 0.6901907899322532, Learning Rate - 0.003125, magnitude of gradient - 1.696584022106539\n",
      "Step - 4028, Loss - 0.7018502408932038, Learning Rate - 0.003125, magnitude of gradient - 1.653478570831356\n",
      "Step - 4029, Loss - 0.5903504613442679, Learning Rate - 0.003125, magnitude of gradient - 2.2627154981926276\n",
      "Step - 4030, Loss - 0.6281956698373764, Learning Rate - 0.003125, magnitude of gradient - 0.9242430612032573\n",
      "Step - 4031, Loss - 0.6941368166025872, Learning Rate - 0.003125, magnitude of gradient - 1.5301615135572098\n",
      "Step - 4032, Loss - 0.8507828670510543, Learning Rate - 0.003125, magnitude of gradient - 0.941477190188753\n",
      "Step - 4033, Loss - 0.7991779853229792, Learning Rate - 0.003125, magnitude of gradient - 1.1913601781101775\n",
      "Step - 4034, Loss - 0.777525307992935, Learning Rate - 0.003125, magnitude of gradient - 0.6420817177385806\n",
      "Step - 4035, Loss - 0.6314616844822221, Learning Rate - 0.003125, magnitude of gradient - 1.504635376766107\n",
      "Step - 4036, Loss - 0.8153103281455318, Learning Rate - 0.003125, magnitude of gradient - 1.4409645575323518\n",
      "Step - 4037, Loss - 0.9036684474148268, Learning Rate - 0.003125, magnitude of gradient - 1.4997399037617047\n",
      "Step - 4038, Loss - 0.7573346287727726, Learning Rate - 0.003125, magnitude of gradient - 1.077469197358537\n",
      "Step - 4039, Loss - 0.8376028085401424, Learning Rate - 0.003125, magnitude of gradient - 1.2445411363181293\n",
      "Step - 4040, Loss - 0.703619451549399, Learning Rate - 0.003125, magnitude of gradient - 1.1599921547753647\n",
      "Step - 4041, Loss - 0.9305864205402475, Learning Rate - 0.003125, magnitude of gradient - 0.9949561646597129\n",
      "Step - 4042, Loss - 0.6708772268255738, Learning Rate - 0.003125, magnitude of gradient - 1.159495735905902\n",
      "Step - 4043, Loss - 0.6084139879933892, Learning Rate - 0.003125, magnitude of gradient - 0.7063356701017379\n",
      "Step - 4044, Loss - 0.7740536690539443, Learning Rate - 0.003125, magnitude of gradient - 0.6162735908660205\n",
      "Step - 4045, Loss - 0.7578712910363572, Learning Rate - 0.003125, magnitude of gradient - 1.4037141081620383\n",
      "Step - 4046, Loss - 0.7201384638833657, Learning Rate - 0.003125, magnitude of gradient - 0.6447247951441059\n",
      "Step - 4047, Loss - 0.7481003527778274, Learning Rate - 0.003125, magnitude of gradient - 1.5491721606329145\n",
      "Step - 4048, Loss - 0.715657610749303, Learning Rate - 0.003125, magnitude of gradient - 1.5419242148004568\n",
      "Step - 4049, Loss - 0.6169166030891418, Learning Rate - 0.003125, magnitude of gradient - 1.9925515405157042\n",
      "Step - 4050, Loss - 0.6996200520730755, Learning Rate - 0.003125, magnitude of gradient - 1.6571946605721932\n",
      "Step - 4051, Loss - 0.8125727857205295, Learning Rate - 0.003125, magnitude of gradient - 1.0139971462877662\n",
      "Step - 4052, Loss - 0.7195339916013331, Learning Rate - 0.003125, magnitude of gradient - 1.694048953129916\n",
      "Step - 4053, Loss - 0.952393813230054, Learning Rate - 0.003125, magnitude of gradient - 2.3699148073986027\n",
      "Step - 4054, Loss - 0.8041448801547149, Learning Rate - 0.003125, magnitude of gradient - 1.0992179570742628\n",
      "Step - 4055, Loss - 0.6250760257436179, Learning Rate - 0.003125, magnitude of gradient - 1.4672375945929974\n",
      "Step - 4056, Loss - 0.6063608495369315, Learning Rate - 0.003125, magnitude of gradient - 1.0761630663198931\n",
      "Step - 4057, Loss - 0.5229060471226417, Learning Rate - 0.003125, magnitude of gradient - 1.3751570585179436\n",
      "Step - 4058, Loss - 0.5555380626600003, Learning Rate - 0.003125, magnitude of gradient - 1.0010804502624018\n",
      "Step - 4059, Loss - 0.6668050089390631, Learning Rate - 0.003125, magnitude of gradient - 0.8416573099484911\n",
      "Step - 4060, Loss - 0.7507469933315774, Learning Rate - 0.003125, magnitude of gradient - 1.3222443455913937\n",
      "Step - 4061, Loss - 0.6898349782943741, Learning Rate - 0.003125, magnitude of gradient - 1.5705473253586462\n",
      "Step - 4062, Loss - 0.6896632371448109, Learning Rate - 0.003125, magnitude of gradient - 1.0586625230332942\n",
      "Step - 4063, Loss - 0.6997888531488539, Learning Rate - 0.003125, magnitude of gradient - 0.6726878272829819\n",
      "Step - 4064, Loss - 0.7598261399627314, Learning Rate - 0.003125, magnitude of gradient - 0.9297812523601058\n",
      "Step - 4065, Loss - 0.9116790960376503, Learning Rate - 0.003125, magnitude of gradient - 1.4863365876342634\n",
      "Step - 4066, Loss - 0.9136389211112378, Learning Rate - 0.003125, magnitude of gradient - 1.348084465407215\n",
      "Step - 4067, Loss - 0.5150563627430425, Learning Rate - 0.003125, magnitude of gradient - 1.0319595377797877\n",
      "Step - 4068, Loss - 0.6458832921159725, Learning Rate - 0.003125, magnitude of gradient - 2.263197525644475\n",
      "Step - 4069, Loss - 0.6383949733139772, Learning Rate - 0.003125, magnitude of gradient - 0.8430972423828638\n",
      "Step - 4070, Loss - 0.7833942188450909, Learning Rate - 0.003125, magnitude of gradient - 1.3207100637269973\n",
      "Step - 4071, Loss - 0.8240481604307, Learning Rate - 0.003125, magnitude of gradient - 0.9115345963360038\n",
      "Step - 4072, Loss - 0.6204680440286653, Learning Rate - 0.003125, magnitude of gradient - 0.8480721312513485\n",
      "Step - 4073, Loss - 0.8958166703256627, Learning Rate - 0.003125, magnitude of gradient - 0.8837251494589424\n",
      "Step - 4074, Loss - 0.7017001182382883, Learning Rate - 0.003125, magnitude of gradient - 0.996583033441163\n",
      "Step - 4075, Loss - 0.7064408068355262, Learning Rate - 0.003125, magnitude of gradient - 1.3644373827140868\n",
      "Step - 4076, Loss - 0.9303437875146989, Learning Rate - 0.003125, magnitude of gradient - 0.6332952357107461\n",
      "Step - 4077, Loss - 0.7301230801958449, Learning Rate - 0.003125, magnitude of gradient - 1.6193093384491597\n",
      "Step - 4078, Loss - 0.7024053581968456, Learning Rate - 0.003125, magnitude of gradient - 0.577986667181102\n",
      "Step - 4079, Loss - 0.7611776165072885, Learning Rate - 0.003125, magnitude of gradient - 2.073077199026815\n",
      "Step - 4080, Loss - 0.7249753943773225, Learning Rate - 0.003125, magnitude of gradient - 1.5144759101443541\n",
      "Step - 4081, Loss - 0.9243935283191569, Learning Rate - 0.003125, magnitude of gradient - 0.564715223001208\n",
      "Step - 4082, Loss - 0.9360754597790474, Learning Rate - 0.003125, magnitude of gradient - 2.1224614733025735\n",
      "Step - 4083, Loss - 0.6889932995704608, Learning Rate - 0.003125, magnitude of gradient - 1.5023033399101258\n",
      "Step - 4084, Loss - 0.6078653200414776, Learning Rate - 0.003125, magnitude of gradient - 1.0516078757647738\n",
      "Step - 4085, Loss - 0.7569976642455325, Learning Rate - 0.003125, magnitude of gradient - 1.1551184452706993\n",
      "Step - 4086, Loss - 0.6607959699333619, Learning Rate - 0.003125, magnitude of gradient - 1.49003631868816\n",
      "Step - 4087, Loss - 0.8113760581275186, Learning Rate - 0.003125, magnitude of gradient - 1.7096633425643941\n",
      "Step - 4088, Loss - 0.9677416207065938, Learning Rate - 0.003125, magnitude of gradient - 1.1314723511115556\n",
      "Step - 4089, Loss - 0.6376367347083263, Learning Rate - 0.003125, magnitude of gradient - 0.5095585253580025\n",
      "Step - 4090, Loss - 0.582959060370617, Learning Rate - 0.003125, magnitude of gradient - 1.8804504239532602\n",
      "Step - 4091, Loss - 0.5411802720365642, Learning Rate - 0.003125, magnitude of gradient - 0.7613261503217974\n",
      "Step - 4092, Loss - 0.6683018178972461, Learning Rate - 0.003125, magnitude of gradient - 0.8000854588924583\n",
      "Step - 4093, Loss - 0.8423863683418691, Learning Rate - 0.003125, magnitude of gradient - 1.5221373435154824\n",
      "Step - 4094, Loss - 0.6385562394240892, Learning Rate - 0.003125, magnitude of gradient - 1.4477011539571543\n",
      "Step - 4095, Loss - 0.6908672452341249, Learning Rate - 0.003125, magnitude of gradient - 1.675858526847146\n",
      "Step - 4096, Loss - 0.8077677187286827, Learning Rate - 0.003125, magnitude of gradient - 1.624575543210302\n",
      "Step - 4097, Loss - 0.8319259458745032, Learning Rate - 0.003125, magnitude of gradient - 2.201147519082044\n",
      "Step - 4098, Loss - 0.6931098714932259, Learning Rate - 0.003125, magnitude of gradient - 1.7596948611384113\n",
      "Step - 4099, Loss - 0.5845418861171285, Learning Rate - 0.003125, magnitude of gradient - 1.0585473291568184\n",
      "Step - 4100, Loss - 0.8517019793729468, Learning Rate - 0.003125, magnitude of gradient - 2.3409310658032583\n",
      "Step - 4101, Loss - 0.9162078673928107, Learning Rate - 0.003125, magnitude of gradient - 0.647228185211598\n",
      "Step - 4102, Loss - 0.5759544719687968, Learning Rate - 0.003125, magnitude of gradient - 1.450278549535554\n",
      "Step - 4103, Loss - 0.5053624305822546, Learning Rate - 0.003125, magnitude of gradient - 2.0487520803102552\n",
      "Step - 4104, Loss - 0.6393755903789693, Learning Rate - 0.003125, magnitude of gradient - 1.5753259142493015\n",
      "Step - 4105, Loss - 0.5723683461332764, Learning Rate - 0.003125, magnitude of gradient - 1.697062027223792\n",
      "Step - 4106, Loss - 0.7428420618310072, Learning Rate - 0.003125, magnitude of gradient - 1.4496904346322228\n",
      "Step - 4107, Loss - 0.5064972704521078, Learning Rate - 0.003125, magnitude of gradient - 0.9133607129054089\n",
      "Step - 4108, Loss - 0.44283576779333894, Learning Rate - 0.003125, magnitude of gradient - 0.3933705423201674\n",
      "Step - 4109, Loss - 0.6560420051702618, Learning Rate - 0.003125, magnitude of gradient - 1.1275921515544944\n",
      "Step - 4110, Loss - 0.8363083188056595, Learning Rate - 0.003125, magnitude of gradient - 0.9165368427935874\n",
      "Step - 4111, Loss - 0.8273801665676855, Learning Rate - 0.003125, magnitude of gradient - 1.6311238704384232\n",
      "Step - 4112, Loss - 0.5336313902507398, Learning Rate - 0.003125, magnitude of gradient - 0.9780608559434655\n",
      "Step - 4113, Loss - 0.821104046855392, Learning Rate - 0.003125, magnitude of gradient - 2.9335110509284084\n",
      "Step - 4114, Loss - 0.8191749254786269, Learning Rate - 0.003125, magnitude of gradient - 1.8328755380294934\n",
      "Step - 4115, Loss - 0.9527132773388962, Learning Rate - 0.003125, magnitude of gradient - 2.1263548436026936\n",
      "Step - 4116, Loss - 0.842744729231566, Learning Rate - 0.003125, magnitude of gradient - 1.6287965442515873\n",
      "Step - 4117, Loss - 0.7716021251492007, Learning Rate - 0.003125, magnitude of gradient - 1.5400085061024154\n",
      "Step - 4118, Loss - 0.6689846374879682, Learning Rate - 0.003125, magnitude of gradient - 0.5159449292103352\n",
      "Step - 4119, Loss - 0.8118955330958499, Learning Rate - 0.003125, magnitude of gradient - 1.0702124737542191\n",
      "Step - 4120, Loss - 0.5992747535976696, Learning Rate - 0.003125, magnitude of gradient - 0.9644999376880845\n",
      "Step - 4121, Loss - 0.6695569948679402, Learning Rate - 0.003125, magnitude of gradient - 1.392947677712113\n",
      "Step - 4122, Loss - 0.7879522761400952, Learning Rate - 0.003125, magnitude of gradient - 0.8354445991884213\n",
      "Step - 4123, Loss - 0.8008443603297559, Learning Rate - 0.003125, magnitude of gradient - 0.9536664462640893\n",
      "Step - 4124, Loss - 0.5533580813615588, Learning Rate - 0.003125, magnitude of gradient - 0.9363242916158973\n",
      "Step - 4125, Loss - 0.7679281351629136, Learning Rate - 0.003125, magnitude of gradient - 1.1640134141864549\n",
      "Step - 4126, Loss - 0.5157099301941955, Learning Rate - 0.003125, magnitude of gradient - 1.585797332379168\n",
      "Step - 4127, Loss - 0.6535648302237749, Learning Rate - 0.003125, magnitude of gradient - 0.5645248292996932\n",
      "Step - 4128, Loss - 0.626132394007627, Learning Rate - 0.003125, magnitude of gradient - 1.042514737131619\n",
      "Step - 4129, Loss - 0.6160539537985062, Learning Rate - 0.003125, magnitude of gradient - 0.7622517386283182\n",
      "Step - 4130, Loss - 0.6688286130382958, Learning Rate - 0.003125, magnitude of gradient - 1.715848511770055\n",
      "Step - 4131, Loss - 0.7488609747411111, Learning Rate - 0.003125, magnitude of gradient - 1.1133520381124593\n",
      "Step - 4132, Loss - 0.9617724369641121, Learning Rate - 0.003125, magnitude of gradient - 1.4655775706543155\n",
      "Step - 4133, Loss - 0.7392544796723335, Learning Rate - 0.003125, magnitude of gradient - 0.5887551851106421\n",
      "Step - 4134, Loss - 0.667818304609966, Learning Rate - 0.003125, magnitude of gradient - 0.7960123023751622\n",
      "Step - 4135, Loss - 0.5037693142051322, Learning Rate - 0.003125, magnitude of gradient - 0.619505454737115\n",
      "Step - 4136, Loss - 0.642690657702868, Learning Rate - 0.003125, magnitude of gradient - 0.2693237811576577\n",
      "Step - 4137, Loss - 0.6553200696571152, Learning Rate - 0.003125, magnitude of gradient - 1.5780062696552246\n",
      "Step - 4138, Loss - 0.8187173781021103, Learning Rate - 0.003125, magnitude of gradient - 1.4986499068929382\n",
      "Step - 4139, Loss - 0.6478205713748111, Learning Rate - 0.003125, magnitude of gradient - 0.5754862430125104\n",
      "Step - 4140, Loss - 0.6912371872929797, Learning Rate - 0.003125, magnitude of gradient - 0.8916646500788165\n",
      "Step - 4141, Loss - 0.6873252630737374, Learning Rate - 0.003125, magnitude of gradient - 0.8903493427435045\n",
      "Step - 4142, Loss - 0.62163989134177, Learning Rate - 0.003125, magnitude of gradient - 0.6262755029588627\n",
      "Step - 4143, Loss - 0.66830206638439, Learning Rate - 0.003125, magnitude of gradient - 1.2365418449033154\n",
      "Step - 4144, Loss - 0.7189927699620847, Learning Rate - 0.003125, magnitude of gradient - 0.6202524748852631\n",
      "Step - 4145, Loss - 0.5560976519260388, Learning Rate - 0.003125, magnitude of gradient - 1.551145923381706\n",
      "Step - 4146, Loss - 0.7362341366315065, Learning Rate - 0.003125, magnitude of gradient - 2.7695318271066203\n",
      "Step - 4147, Loss - 0.4922461155527537, Learning Rate - 0.003125, magnitude of gradient - 0.9495120031389376\n",
      "Step - 4148, Loss - 0.6899156174057838, Learning Rate - 0.003125, magnitude of gradient - 0.4345365655137695\n",
      "Step - 4149, Loss - 0.6449929777496147, Learning Rate - 0.003125, magnitude of gradient - 2.0461173561008166\n",
      "Step - 4150, Loss - 0.7713623961887333, Learning Rate - 0.003125, magnitude of gradient - 1.6210184264594898\n",
      "Step - 4151, Loss - 0.7102849085011175, Learning Rate - 0.003125, magnitude of gradient - 0.8920848318378747\n",
      "Step - 4152, Loss - 0.8023994748545916, Learning Rate - 0.003125, magnitude of gradient - 1.29844324035555\n",
      "Step - 4153, Loss - 0.6378031272222213, Learning Rate - 0.003125, magnitude of gradient - 1.4959903862236985\n",
      "Step - 4154, Loss - 0.7534917144569439, Learning Rate - 0.003125, magnitude of gradient - 0.6329809346412366\n",
      "Step - 4155, Loss - 0.4601215002539939, Learning Rate - 0.003125, magnitude of gradient - 0.6350830651253194\n",
      "Step - 4156, Loss - 0.7205189266353722, Learning Rate - 0.003125, magnitude of gradient - 1.2552542817726133\n",
      "Step - 4157, Loss - 0.7458297552667479, Learning Rate - 0.003125, magnitude of gradient - 0.7877507695520114\n",
      "Step - 4158, Loss - 0.40934291337293943, Learning Rate - 0.003125, magnitude of gradient - 1.9126954917552716\n",
      "Step - 4159, Loss - 0.46563452427985663, Learning Rate - 0.003125, magnitude of gradient - 0.8553110957201584\n",
      "Step - 4160, Loss - 0.6544109982806693, Learning Rate - 0.003125, magnitude of gradient - 0.8176929762312123\n",
      "Step - 4161, Loss - 0.8732943362477987, Learning Rate - 0.003125, magnitude of gradient - 0.9422214407630892\n",
      "Step - 4162, Loss - 0.7329259342732435, Learning Rate - 0.003125, magnitude of gradient - 1.2297811573039583\n",
      "Step - 4163, Loss - 0.9147869061898899, Learning Rate - 0.003125, magnitude of gradient - 2.806017522049119\n",
      "Step - 4164, Loss - 0.7560712567312542, Learning Rate - 0.003125, magnitude of gradient - 1.4243632675223317\n",
      "Step - 4165, Loss - 0.6598008035320686, Learning Rate - 0.003125, magnitude of gradient - 1.1597545943554508\n",
      "Step - 4166, Loss - 0.3998519906769239, Learning Rate - 0.003125, magnitude of gradient - 1.553148696342987\n",
      "Step - 4167, Loss - 0.5207250122285257, Learning Rate - 0.003125, magnitude of gradient - 1.2542801110602553\n",
      "Step - 4168, Loss - 0.6431516961916872, Learning Rate - 0.003125, magnitude of gradient - 0.9207838921969873\n",
      "Step - 4169, Loss - 0.7772116596028356, Learning Rate - 0.003125, magnitude of gradient - 0.64435328053226\n",
      "Step - 4170, Loss - 0.7620416327835347, Learning Rate - 0.003125, magnitude of gradient - 1.5871939935001995\n",
      "Step - 4171, Loss - 0.5070523694707886, Learning Rate - 0.003125, magnitude of gradient - 1.3262077798917924\n",
      "Step - 4172, Loss - 0.6016554999836271, Learning Rate - 0.003125, magnitude of gradient - 1.413280478946895\n",
      "Step - 4173, Loss - 0.5483678866351369, Learning Rate - 0.003125, magnitude of gradient - 1.911036745996399\n",
      "Step - 4174, Loss - 0.9167489348848827, Learning Rate - 0.003125, magnitude of gradient - 1.0137326067346326\n",
      "Step - 4175, Loss - 0.6176126264918571, Learning Rate - 0.003125, magnitude of gradient - 1.3356349860278638\n",
      "Step - 4176, Loss - 0.5993704199213972, Learning Rate - 0.003125, magnitude of gradient - 0.6355020457137747\n",
      "Step - 4177, Loss - 0.6926453891788945, Learning Rate - 0.003125, magnitude of gradient - 0.391546789700844\n",
      "Step - 4178, Loss - 0.6556755669649289, Learning Rate - 0.003125, magnitude of gradient - 1.1615428662352776\n",
      "Step - 4179, Loss - 0.6270452483276588, Learning Rate - 0.003125, magnitude of gradient - 0.2589683353088665\n",
      "Step - 4180, Loss - 0.828575978044292, Learning Rate - 0.003125, magnitude of gradient - 1.8295965813957829\n",
      "Step - 4181, Loss - 0.8781273889162318, Learning Rate - 0.003125, magnitude of gradient - 1.6296215275707233\n",
      "Step - 4182, Loss - 0.70480470504137, Learning Rate - 0.003125, magnitude of gradient - 0.9492920199510232\n",
      "Step - 4183, Loss - 0.7891347708718421, Learning Rate - 0.003125, magnitude of gradient - 1.7305097267943916\n",
      "Step - 4184, Loss - 0.5350512847206693, Learning Rate - 0.003125, magnitude of gradient - 0.7880166366108254\n",
      "Step - 4185, Loss - 0.7310837010323348, Learning Rate - 0.003125, magnitude of gradient - 1.304133481559178\n",
      "Step - 4186, Loss - 0.7457440316473919, Learning Rate - 0.003125, magnitude of gradient - 2.0307544280715146\n",
      "Step - 4187, Loss - 0.7483036628818077, Learning Rate - 0.003125, magnitude of gradient - 0.2842088220069252\n",
      "Step - 4188, Loss - 0.6745316455128274, Learning Rate - 0.003125, magnitude of gradient - 1.0719752239762372\n",
      "Step - 4189, Loss - 0.6039598629792364, Learning Rate - 0.003125, magnitude of gradient - 1.9000528065112074\n",
      "Step - 4190, Loss - 0.7577312710222965, Learning Rate - 0.003125, magnitude of gradient - 0.6733607455088558\n",
      "Step - 4191, Loss - 0.8721880619836416, Learning Rate - 0.003125, magnitude of gradient - 0.8772268769485527\n",
      "Step - 4192, Loss - 0.7682012418040424, Learning Rate - 0.003125, magnitude of gradient - 2.2306287442342256\n",
      "Step - 4193, Loss - 0.7112943794213816, Learning Rate - 0.003125, magnitude of gradient - 1.1093239618603103\n",
      "Step - 4194, Loss - 0.6590624460428866, Learning Rate - 0.003125, magnitude of gradient - 1.1437437743170764\n",
      "Step - 4195, Loss - 0.6632109577152719, Learning Rate - 0.003125, magnitude of gradient - 0.43662061994381857\n",
      "Step - 4196, Loss - 0.7959882891453217, Learning Rate - 0.003125, magnitude of gradient - 1.296221301055491\n",
      "Step - 4197, Loss - 0.7737531122342831, Learning Rate - 0.003125, magnitude of gradient - 1.1802923133560488\n",
      "Step - 4198, Loss - 0.6952501317449036, Learning Rate - 0.003125, magnitude of gradient - 1.4582748586158472\n",
      "Step - 4199, Loss - 0.6742492540645898, Learning Rate - 0.003125, magnitude of gradient - 1.3532733760602427\n",
      "Step - 4200, Loss - 0.6713165275059222, Learning Rate - 0.003125, magnitude of gradient - 2.1996007676720324\n",
      "Step - 4201, Loss - 0.7362831411419505, Learning Rate - 0.003125, magnitude of gradient - 2.80713857160542\n",
      "Step - 4202, Loss - 0.6246775115819793, Learning Rate - 0.003125, magnitude of gradient - 1.8150514328215852\n",
      "Step - 4203, Loss - 0.8180175630727529, Learning Rate - 0.003125, magnitude of gradient - 1.1141239249690171\n",
      "Step - 4204, Loss - 0.6291663327086182, Learning Rate - 0.003125, magnitude of gradient - 1.7959967125601461\n",
      "Step - 4205, Loss - 0.7907836218168988, Learning Rate - 0.003125, magnitude of gradient - 1.6757566605570735\n",
      "Step - 4206, Loss - 0.6760383351551299, Learning Rate - 0.003125, magnitude of gradient - 1.1463880808785225\n",
      "Step - 4207, Loss - 0.6906902021749466, Learning Rate - 0.003125, magnitude of gradient - 1.1315791757853229\n",
      "Step - 4208, Loss - 0.7224628491497849, Learning Rate - 0.003125, magnitude of gradient - 1.052817929188918\n",
      "Step - 4209, Loss - 0.5488977880393928, Learning Rate - 0.003125, magnitude of gradient - 1.1638452416188039\n",
      "Step - 4210, Loss - 0.6894663087000857, Learning Rate - 0.003125, magnitude of gradient - 0.9653212765225453\n",
      "Step - 4211, Loss - 0.5708156320968782, Learning Rate - 0.003125, magnitude of gradient - 0.8453053364736904\n",
      "Step - 4212, Loss - 0.6729397337718938, Learning Rate - 0.003125, magnitude of gradient - 1.0629593357369351\n",
      "Step - 4213, Loss - 0.7934834063461027, Learning Rate - 0.003125, magnitude of gradient - 1.5873048350388874\n",
      "Step - 4214, Loss - 0.4970538451312668, Learning Rate - 0.003125, magnitude of gradient - 0.8411865955033209\n",
      "Step - 4215, Loss - 0.975298960543905, Learning Rate - 0.003125, magnitude of gradient - 1.4349420828008779\n",
      "Step - 4216, Loss - 0.7922938085434015, Learning Rate - 0.003125, magnitude of gradient - 1.2550588085711165\n",
      "Step - 4217, Loss - 0.657686997496975, Learning Rate - 0.003125, magnitude of gradient - 0.9400312432242809\n",
      "Step - 4218, Loss - 0.7410883370670411, Learning Rate - 0.003125, magnitude of gradient - 1.2186955808989435\n",
      "Step - 4219, Loss - 0.8110995241413265, Learning Rate - 0.003125, magnitude of gradient - 0.7102963450676817\n",
      "Step - 4220, Loss - 0.5648734542137852, Learning Rate - 0.003125, magnitude of gradient - 1.2110375482527942\n",
      "Step - 4221, Loss - 0.6675193345566411, Learning Rate - 0.003125, magnitude of gradient - 1.07585437059695\n",
      "Step - 4222, Loss - 0.6519222406498271, Learning Rate - 0.003125, magnitude of gradient - 1.284754134153466\n",
      "Step - 4223, Loss - 0.6889958341990154, Learning Rate - 0.003125, magnitude of gradient - 1.3994066080448437\n",
      "Step - 4224, Loss - 0.5617593704945016, Learning Rate - 0.003125, magnitude of gradient - 1.025479973097845\n",
      "Step - 4225, Loss - 0.8977090939720066, Learning Rate - 0.003125, magnitude of gradient - 0.8295667619464457\n",
      "Step - 4226, Loss - 0.6656330573046925, Learning Rate - 0.003125, magnitude of gradient - 0.8649314760786853\n",
      "Step - 4227, Loss - 0.6675660455681864, Learning Rate - 0.003125, magnitude of gradient - 1.2699903132471837\n",
      "Step - 4228, Loss - 0.7361493321331777, Learning Rate - 0.003125, magnitude of gradient - 0.4349625245865647\n",
      "Step - 4229, Loss - 0.7463298410900157, Learning Rate - 0.003125, magnitude of gradient - 1.967405849616457\n",
      "Step - 4230, Loss - 0.8632553080052519, Learning Rate - 0.003125, magnitude of gradient - 2.4562386005650714\n",
      "Step - 4231, Loss - 0.8619152741777897, Learning Rate - 0.003125, magnitude of gradient - 2.6054235626841504\n",
      "Step - 4232, Loss - 0.7856951367990459, Learning Rate - 0.003125, magnitude of gradient - 1.4768080057411628\n",
      "Step - 4233, Loss - 0.7683033024371313, Learning Rate - 0.003125, magnitude of gradient - 1.9658948632472055\n",
      "Step - 4234, Loss - 0.7245077952308802, Learning Rate - 0.003125, magnitude of gradient - 0.9555281705860103\n",
      "Step - 4235, Loss - 0.6283856987875754, Learning Rate - 0.003125, magnitude of gradient - 0.9473069446676845\n",
      "Step - 4236, Loss - 0.5914786206477406, Learning Rate - 0.003125, magnitude of gradient - 0.6300003766380472\n",
      "Step - 4237, Loss - 0.6469247368913144, Learning Rate - 0.003125, magnitude of gradient - 1.914004545565835\n",
      "Step - 4238, Loss - 0.6627733570643151, Learning Rate - 0.003125, magnitude of gradient - 1.4701559305799723\n",
      "Step - 4239, Loss - 0.7565407438076895, Learning Rate - 0.003125, magnitude of gradient - 2.28034703770634\n",
      "Step - 4240, Loss - 0.7578173464919806, Learning Rate - 0.003125, magnitude of gradient - 2.203057963297821\n",
      "Step - 4241, Loss - 0.5247971235569151, Learning Rate - 0.003125, magnitude of gradient - 2.328394846129082\n",
      "Step - 4242, Loss - 0.684452274589403, Learning Rate - 0.003125, magnitude of gradient - 2.1113931646016466\n",
      "Step - 4243, Loss - 0.5466289714321838, Learning Rate - 0.003125, magnitude of gradient - 1.9377143379111899\n",
      "Step - 4244, Loss - 0.5901169423035504, Learning Rate - 0.003125, magnitude of gradient - 1.4554137598927765\n",
      "Step - 4245, Loss - 0.6630833690502062, Learning Rate - 0.003125, magnitude of gradient - 1.5978659917065423\n",
      "Step - 4246, Loss - 0.8742778885024324, Learning Rate - 0.003125, magnitude of gradient - 2.1366434025422008\n",
      "Step - 4247, Loss - 0.6835763576150458, Learning Rate - 0.003125, magnitude of gradient - 0.5440907844660942\n",
      "Step - 4248, Loss - 0.7257387854887553, Learning Rate - 0.003125, magnitude of gradient - 2.7671610357396497\n",
      "Step - 4249, Loss - 0.593094548763813, Learning Rate - 0.003125, magnitude of gradient - 1.107085244551219\n",
      "Step - 4250, Loss - 0.6828933573587807, Learning Rate - 0.003125, magnitude of gradient - 1.6948550780390197\n",
      "Step - 4251, Loss - 0.5414117195074678, Learning Rate - 0.003125, magnitude of gradient - 1.2119790356976359\n",
      "Step - 4252, Loss - 0.7486272120355576, Learning Rate - 0.003125, magnitude of gradient - 0.48783267332919067\n",
      "Step - 4253, Loss - 0.7333178626346408, Learning Rate - 0.003125, magnitude of gradient - 1.5725406168092977\n",
      "Step - 4254, Loss - 0.6516743381125691, Learning Rate - 0.003125, magnitude of gradient - 0.6458061561444615\n",
      "Step - 4255, Loss - 0.7440997541404324, Learning Rate - 0.003125, magnitude of gradient - 1.3545363675428177\n",
      "Step - 4256, Loss - 0.8269124386815975, Learning Rate - 0.003125, magnitude of gradient - 1.3193346338969116\n",
      "Step - 4257, Loss - 0.6336031720698404, Learning Rate - 0.003125, magnitude of gradient - 0.7159340962469131\n",
      "Step - 4258, Loss - 0.6679664073606301, Learning Rate - 0.003125, magnitude of gradient - 1.4273241034897104\n",
      "Step - 4259, Loss - 0.7952522857627614, Learning Rate - 0.003125, magnitude of gradient - 0.8085063997831817\n",
      "Step - 4260, Loss - 0.6382122008865322, Learning Rate - 0.003125, magnitude of gradient - 1.3125720018356208\n",
      "Step - 4261, Loss - 0.6822304806512406, Learning Rate - 0.003125, magnitude of gradient - 0.8399234163539826\n",
      "Step - 4262, Loss - 0.8007626074601875, Learning Rate - 0.003125, magnitude of gradient - 1.4344603431299427\n",
      "Step - 4263, Loss - 0.5182473131172776, Learning Rate - 0.003125, magnitude of gradient - 1.3353135428263179\n",
      "Step - 4264, Loss - 0.7017566279140293, Learning Rate - 0.003125, magnitude of gradient - 1.0418941603368634\n",
      "Step - 4265, Loss - 0.704920841761686, Learning Rate - 0.003125, magnitude of gradient - 1.1559552998688976\n",
      "Step - 4266, Loss - 0.7666060495775728, Learning Rate - 0.003125, magnitude of gradient - 0.8607858247505608\n",
      "Step - 4267, Loss - 0.47655364903076614, Learning Rate - 0.003125, magnitude of gradient - 0.43300455453526815\n",
      "Step - 4268, Loss - 0.6343789477571466, Learning Rate - 0.003125, magnitude of gradient - 1.6013715387512342\n",
      "Step - 4269, Loss - 0.7030460935936965, Learning Rate - 0.003125, magnitude of gradient - 1.0519701556079812\n",
      "Step - 4270, Loss - 0.649859097004361, Learning Rate - 0.003125, magnitude of gradient - 3.326765049550311\n",
      "Step - 4271, Loss - 0.597978389902466, Learning Rate - 0.003125, magnitude of gradient - 1.973525471434805\n",
      "Step - 4272, Loss - 0.7629371342171253, Learning Rate - 0.003125, magnitude of gradient - 2.776751062113878\n",
      "Step - 4273, Loss - 0.6900959624744827, Learning Rate - 0.003125, magnitude of gradient - 0.9719526989475125\n",
      "Step - 4274, Loss - 0.8110776514361628, Learning Rate - 0.003125, magnitude of gradient - 1.3849796966882377\n",
      "Step - 4275, Loss - 0.7283281020822315, Learning Rate - 0.003125, magnitude of gradient - 0.8401421937473498\n",
      "Step - 4276, Loss - 0.6421195065152193, Learning Rate - 0.003125, magnitude of gradient - 1.022778144739557\n",
      "Step - 4277, Loss - 0.6280577198313497, Learning Rate - 0.003125, magnitude of gradient - 1.1688698040027796\n",
      "Step - 4278, Loss - 0.7163732188162512, Learning Rate - 0.003125, magnitude of gradient - 0.6798502373370642\n",
      "Step - 4279, Loss - 0.5179335419114981, Learning Rate - 0.003125, magnitude of gradient - 0.48482746148458056\n",
      "Step - 4280, Loss - 0.7158791227483425, Learning Rate - 0.003125, magnitude of gradient - 1.5444320707678458\n",
      "Step - 4281, Loss - 0.6850098879206139, Learning Rate - 0.003125, magnitude of gradient - 2.4543658540921784\n",
      "Step - 4282, Loss - 0.6197662682971289, Learning Rate - 0.003125, magnitude of gradient - 0.744859546387379\n",
      "Step - 4283, Loss - 0.7267535250610949, Learning Rate - 0.003125, magnitude of gradient - 2.214579369979166\n",
      "Step - 4284, Loss - 0.720263791450511, Learning Rate - 0.003125, magnitude of gradient - 1.8530798217521267\n",
      "Step - 4285, Loss - 0.6207159279974747, Learning Rate - 0.003125, magnitude of gradient - 1.2493435297079856\n",
      "Step - 4286, Loss - 0.7935894398735607, Learning Rate - 0.003125, magnitude of gradient - 1.0277251670762746\n",
      "Step - 4287, Loss - 0.7261905763206805, Learning Rate - 0.003125, magnitude of gradient - 1.2850719751458248\n",
      "Step - 4288, Loss - 0.6953933525328239, Learning Rate - 0.003125, magnitude of gradient - 1.151656865115101\n",
      "Step - 4289, Loss - 0.6235875910243747, Learning Rate - 0.003125, magnitude of gradient - 1.3093167405196813\n",
      "Step - 4290, Loss - 0.6914225444239365, Learning Rate - 0.003125, magnitude of gradient - 1.3570459732287912\n",
      "Step - 4291, Loss - 0.7721449502666506, Learning Rate - 0.003125, magnitude of gradient - 1.247627491296854\n",
      "Step - 4292, Loss - 0.8934444040622589, Learning Rate - 0.003125, magnitude of gradient - 1.8175953937968168\n",
      "Step - 4293, Loss - 0.5705479786068086, Learning Rate - 0.003125, magnitude of gradient - 1.6099217013942453\n",
      "Step - 4294, Loss - 0.48457966966870625, Learning Rate - 0.003125, magnitude of gradient - 1.0401516562393565\n",
      "Step - 4295, Loss - 0.711986134422409, Learning Rate - 0.003125, magnitude of gradient - 1.9050850645127135\n",
      "Step - 4296, Loss - 0.607900132158984, Learning Rate - 0.003125, magnitude of gradient - 1.3824161260031955\n",
      "Step - 4297, Loss - 0.5530121842362311, Learning Rate - 0.003125, magnitude of gradient - 0.9472493277092804\n",
      "Step - 4298, Loss - 0.7438693990604447, Learning Rate - 0.003125, magnitude of gradient - 0.9518995967902981\n",
      "Step - 4299, Loss - 0.7737581381422748, Learning Rate - 0.003125, magnitude of gradient - 1.597875994747038\n",
      "Step - 4300, Loss - 0.7616472456392412, Learning Rate - 0.003125, magnitude of gradient - 2.0580115330537048\n",
      "Step - 4301, Loss - 0.6168170180709502, Learning Rate - 0.003125, magnitude of gradient - 1.8035047472387487\n",
      "Step - 4302, Loss - 0.7159184595804463, Learning Rate - 0.003125, magnitude of gradient - 0.9670763235110633\n",
      "Step - 4303, Loss - 0.8825992018482856, Learning Rate - 0.003125, magnitude of gradient - 1.1263516305733223\n",
      "Step - 4304, Loss - 0.5829700733571355, Learning Rate - 0.003125, magnitude of gradient - 0.7409807930014621\n",
      "Step - 4305, Loss - 0.6367826015713942, Learning Rate - 0.003125, magnitude of gradient - 1.115337052585287\n",
      "Step - 4306, Loss - 0.5943499962701714, Learning Rate - 0.003125, magnitude of gradient - 1.0669439716974771\n",
      "Step - 4307, Loss - 0.7139617040524368, Learning Rate - 0.003125, magnitude of gradient - 1.1567399333445147\n",
      "Step - 4308, Loss - 0.4785045269900732, Learning Rate - 0.003125, magnitude of gradient - 0.4438402114438066\n",
      "Step - 4309, Loss - 0.7319596565911468, Learning Rate - 0.003125, magnitude of gradient - 0.700409870955612\n",
      "Step - 4310, Loss - 0.7298196470446423, Learning Rate - 0.003125, magnitude of gradient - 0.8993964018350911\n",
      "Step - 4311, Loss - 0.7467272064195889, Learning Rate - 0.003125, magnitude of gradient - 0.9777694846360656\n",
      "Step - 4312, Loss - 0.689742807321734, Learning Rate - 0.003125, magnitude of gradient - 1.6965241082830302\n",
      "Step - 4313, Loss - 0.6711632752300539, Learning Rate - 0.003125, magnitude of gradient - 2.2897488777745254\n",
      "Step - 4314, Loss - 0.6286808701539242, Learning Rate - 0.003125, magnitude of gradient - 1.9548741426714162\n",
      "Step - 4315, Loss - 0.849277280810159, Learning Rate - 0.003125, magnitude of gradient - 1.5160241112821575\n",
      "Step - 4316, Loss - 0.8436746110922922, Learning Rate - 0.003125, magnitude of gradient - 1.1527101122261614\n",
      "Step - 4317, Loss - 0.7029632788619151, Learning Rate - 0.003125, magnitude of gradient - 1.0123760986322665\n",
      "Step - 4318, Loss - 0.7176282358228392, Learning Rate - 0.003125, magnitude of gradient - 0.5529028545442872\n",
      "Step - 4319, Loss - 0.5947767321602573, Learning Rate - 0.003125, magnitude of gradient - 1.108309360980469\n",
      "Step - 4320, Loss - 0.7068867527576159, Learning Rate - 0.003125, magnitude of gradient - 0.8753714897393026\n",
      "Step - 4321, Loss - 0.8452937957921627, Learning Rate - 0.003125, magnitude of gradient - 2.9007099037153146\n",
      "Step - 4322, Loss - 0.603787036622511, Learning Rate - 0.003125, magnitude of gradient - 1.1684020525269703\n",
      "Step - 4323, Loss - 0.6965806439953632, Learning Rate - 0.003125, magnitude of gradient - 0.8550035598359491\n",
      "Step - 4324, Loss - 0.6502409682578599, Learning Rate - 0.003125, magnitude of gradient - 0.4928119895337745\n",
      "Step - 4325, Loss - 0.5929280066502827, Learning Rate - 0.003125, magnitude of gradient - 1.4058025938230145\n",
      "Step - 4326, Loss - 0.5325489534027648, Learning Rate - 0.003125, magnitude of gradient - 2.1627973139678027\n",
      "Step - 4327, Loss - 0.8052214104875532, Learning Rate - 0.003125, magnitude of gradient - 0.6896535506239704\n",
      "Step - 4328, Loss - 1.1083230030637137, Learning Rate - 0.003125, magnitude of gradient - 2.7530902898180214\n",
      "Step - 4329, Loss - 0.6634508870526848, Learning Rate - 0.003125, magnitude of gradient - 1.1008557555744074\n",
      "Step - 4330, Loss - 0.7710961971072056, Learning Rate - 0.003125, magnitude of gradient - 1.4640274963758646\n",
      "Step - 4331, Loss - 0.6443655506861503, Learning Rate - 0.003125, magnitude of gradient - 1.4681629329184265\n",
      "Step - 4332, Loss - 0.7373736080487809, Learning Rate - 0.003125, magnitude of gradient - 2.1991501226390873\n",
      "Step - 4333, Loss - 0.7946415866553643, Learning Rate - 0.003125, magnitude of gradient - 1.0607069718831552\n",
      "Step - 4334, Loss - 0.7915739362736116, Learning Rate - 0.003125, magnitude of gradient - 1.5416759248556438\n",
      "Step - 4335, Loss - 0.8228208605453753, Learning Rate - 0.003125, magnitude of gradient - 2.4545765442552128\n",
      "Step - 4336, Loss - 0.750083531533678, Learning Rate - 0.003125, magnitude of gradient - 1.460223871425335\n",
      "Step - 4337, Loss - 0.7010599908292504, Learning Rate - 0.003125, magnitude of gradient - 0.34376751594366606\n",
      "Step - 4338, Loss - 0.4762980135188806, Learning Rate - 0.003125, magnitude of gradient - 1.6438765515608378\n",
      "Step - 4339, Loss - 0.7176431661404141, Learning Rate - 0.003125, magnitude of gradient - 0.800449058222779\n",
      "Step - 4340, Loss - 0.7255911370707464, Learning Rate - 0.003125, magnitude of gradient - 1.2090117970445997\n",
      "Step - 4341, Loss - 0.6917086981804049, Learning Rate - 0.003125, magnitude of gradient - 0.7139796479125579\n",
      "Step - 4342, Loss - 0.660070718707807, Learning Rate - 0.003125, magnitude of gradient - 1.2083216288631342\n",
      "Step - 4343, Loss - 0.6527848764862555, Learning Rate - 0.003125, magnitude of gradient - 0.6502108693938167\n",
      "Step - 4344, Loss - 0.7610332528132306, Learning Rate - 0.003125, magnitude of gradient - 0.8880520693139846\n",
      "Step - 4345, Loss - 0.6084939808285108, Learning Rate - 0.003125, magnitude of gradient - 1.1983631269262067\n",
      "Step - 4346, Loss - 0.41135655697998963, Learning Rate - 0.003125, magnitude of gradient - 2.412012898430885\n",
      "Step - 4347, Loss - 0.7168760962408567, Learning Rate - 0.003125, magnitude of gradient - 2.141549453734742\n",
      "Step - 4348, Loss - 0.8196836314739011, Learning Rate - 0.003125, magnitude of gradient - 1.2950044423442437\n",
      "Step - 4349, Loss - 0.7383062284752169, Learning Rate - 0.003125, magnitude of gradient - 1.2346040349029592\n",
      "Step - 4350, Loss - 0.6742099632203917, Learning Rate - 0.003125, magnitude of gradient - 0.6812174391103919\n",
      "Step - 4351, Loss - 0.8507386295081278, Learning Rate - 0.003125, magnitude of gradient - 1.8394102555966485\n",
      "Step - 4352, Loss - 0.5915758472964063, Learning Rate - 0.003125, magnitude of gradient - 0.6692175209465742\n",
      "Step - 4353, Loss - 0.6996423084501067, Learning Rate - 0.003125, magnitude of gradient - 0.8259938372337418\n",
      "Step - 4354, Loss - 0.6932651285375118, Learning Rate - 0.003125, magnitude of gradient - 1.1395003496807656\n",
      "Step - 4355, Loss - 0.5489067583898305, Learning Rate - 0.003125, magnitude of gradient - 1.3880089079369757\n",
      "Step - 4356, Loss - 0.6555958948658698, Learning Rate - 0.003125, magnitude of gradient - 1.8256652592032097\n",
      "Step - 4357, Loss - 0.6158209900319156, Learning Rate - 0.003125, magnitude of gradient - 1.6030978911808038\n",
      "Step - 4358, Loss - 0.7198285856860307, Learning Rate - 0.003125, magnitude of gradient - 0.8149910448194321\n",
      "Step - 4359, Loss - 0.5709017789427769, Learning Rate - 0.003125, magnitude of gradient - 1.985519954808124\n",
      "Step - 4360, Loss - 0.8539717058458778, Learning Rate - 0.003125, magnitude of gradient - 1.8557209404793262\n",
      "Step - 4361, Loss - 0.7396151835710846, Learning Rate - 0.003125, magnitude of gradient - 1.206384396776162\n",
      "Step - 4362, Loss - 0.7248368184244063, Learning Rate - 0.003125, magnitude of gradient - 0.36223929186929343\n",
      "Step - 4363, Loss - 0.7321929404636475, Learning Rate - 0.003125, magnitude of gradient - 1.5652690019853073\n",
      "Step - 4364, Loss - 0.5476694517428281, Learning Rate - 0.003125, magnitude of gradient - 0.5935396324288228\n",
      "Step - 4365, Loss - 0.7350314237294979, Learning Rate - 0.003125, magnitude of gradient - 1.0068482331208453\n",
      "Step - 4366, Loss - 0.5918819234396907, Learning Rate - 0.003125, magnitude of gradient - 1.926172888048761\n",
      "Step - 4367, Loss - 0.6337920604798992, Learning Rate - 0.003125, magnitude of gradient - 1.4036099640416801\n",
      "Step - 4368, Loss - 0.7688771744025467, Learning Rate - 0.003125, magnitude of gradient - 1.8565225826432883\n",
      "Step - 4369, Loss - 0.945639375368676, Learning Rate - 0.003125, magnitude of gradient - 2.5843040922308456\n",
      "Step - 4370, Loss - 0.5887980866548411, Learning Rate - 0.003125, magnitude of gradient - 1.4605049386219275\n",
      "Step - 4371, Loss - 0.5903760079989255, Learning Rate - 0.003125, magnitude of gradient - 0.7186044860781646\n",
      "Step - 4372, Loss - 0.6255381506646464, Learning Rate - 0.003125, magnitude of gradient - 1.2265970916530229\n",
      "Step - 4373, Loss - 0.7081024655514475, Learning Rate - 0.003125, magnitude of gradient - 0.39085681035196496\n",
      "Step - 4374, Loss - 0.7335562162658652, Learning Rate - 0.003125, magnitude of gradient - 1.6414026585593309\n",
      "Step - 4375, Loss - 0.6472617427270928, Learning Rate - 0.003125, magnitude of gradient - 1.5052043759741798\n",
      "Step - 4376, Loss - 0.8275449379710864, Learning Rate - 0.003125, magnitude of gradient - 1.6227353875529817\n",
      "Step - 4377, Loss - 0.8196205620750929, Learning Rate - 0.003125, magnitude of gradient - 0.9040065586281706\n",
      "Step - 4378, Loss - 0.5534098615241044, Learning Rate - 0.003125, magnitude of gradient - 0.8652178590711125\n",
      "Step - 4379, Loss - 0.9164211215028178, Learning Rate - 0.003125, magnitude of gradient - 1.5986146790484792\n",
      "Step - 4380, Loss - 0.6388512474719792, Learning Rate - 0.003125, magnitude of gradient - 0.8283168872383965\n",
      "Step - 4381, Loss - 0.8582082343127357, Learning Rate - 0.003125, magnitude of gradient - 1.1646720279959415\n",
      "Step - 4382, Loss - 0.71708657402374, Learning Rate - 0.003125, magnitude of gradient - 0.45855474664727086\n",
      "Step - 4383, Loss - 0.5491860250121147, Learning Rate - 0.003125, magnitude of gradient - 0.9901816939532428\n",
      "Step - 4384, Loss - 0.4916955244169068, Learning Rate - 0.003125, magnitude of gradient - 0.9809700965806386\n",
      "Step - 4385, Loss - 0.5861623417554959, Learning Rate - 0.003125, magnitude of gradient - 0.6377308915431882\n",
      "Step - 4386, Loss - 0.944289505898378, Learning Rate - 0.003125, magnitude of gradient - 2.4496669091566314\n",
      "Step - 4387, Loss - 0.6129454105504561, Learning Rate - 0.003125, magnitude of gradient - 0.2954275613097419\n",
      "Step - 4388, Loss - 0.8294869605234412, Learning Rate - 0.003125, magnitude of gradient - 0.7664494280710084\n",
      "Step - 4389, Loss - 0.7511706778436864, Learning Rate - 0.003125, magnitude of gradient - 1.8372001634591069\n",
      "Step - 4390, Loss - 0.7234898371814711, Learning Rate - 0.003125, magnitude of gradient - 0.9897917743856228\n",
      "Step - 4391, Loss - 0.6529380053549946, Learning Rate - 0.003125, magnitude of gradient - 1.9689444816676225\n",
      "Step - 4392, Loss - 0.7572186839009613, Learning Rate - 0.003125, magnitude of gradient - 0.8987307173845316\n",
      "Step - 4393, Loss - 0.8316895810841748, Learning Rate - 0.003125, magnitude of gradient - 0.522582814669534\n",
      "Step - 4394, Loss - 0.7009329263203508, Learning Rate - 0.003125, magnitude of gradient - 1.1154543340922312\n",
      "Step - 4395, Loss - 0.7971815541911488, Learning Rate - 0.003125, magnitude of gradient - 2.2003137045125882\n",
      "Step - 4396, Loss - 0.627682558861038, Learning Rate - 0.003125, magnitude of gradient - 0.9034699409232545\n",
      "Step - 4397, Loss - 0.7756814707500748, Learning Rate - 0.003125, magnitude of gradient - 0.8294673685745516\n",
      "Step - 4398, Loss - 0.7730191206519057, Learning Rate - 0.003125, magnitude of gradient - 1.1918123513577736\n",
      "Step - 4399, Loss - 0.760163857165726, Learning Rate - 0.003125, magnitude of gradient - 1.1218254051135705\n",
      "Step - 4400, Loss - 0.7410266412353805, Learning Rate - 0.003125, magnitude of gradient - 0.6113296023161097\n",
      "Step - 4401, Loss - 0.7755113605600132, Learning Rate - 0.003125, magnitude of gradient - 1.8152757858367214\n",
      "Step - 4402, Loss - 0.7422136787425192, Learning Rate - 0.003125, magnitude of gradient - 2.0940451788639036\n",
      "Step - 4403, Loss - 0.6111500714926859, Learning Rate - 0.003125, magnitude of gradient - 1.0257996834207397\n",
      "Step - 4404, Loss - 0.6621118180065508, Learning Rate - 0.003125, magnitude of gradient - 1.8712631691622639\n",
      "Step - 4405, Loss - 0.7880944087934837, Learning Rate - 0.003125, magnitude of gradient - 1.2385277041629181\n",
      "Step - 4406, Loss - 0.5904926507122067, Learning Rate - 0.003125, magnitude of gradient - 2.0235972075553845\n",
      "Step - 4407, Loss - 0.6659883420450219, Learning Rate - 0.003125, magnitude of gradient - 1.7179487632078514\n",
      "Step - 4408, Loss - 0.7106240093858842, Learning Rate - 0.003125, magnitude of gradient - 1.2549103206105399\n",
      "Step - 4409, Loss - 0.6619287846452488, Learning Rate - 0.003125, magnitude of gradient - 0.5741746163590614\n",
      "Step - 4410, Loss - 0.7879642952888346, Learning Rate - 0.003125, magnitude of gradient - 0.9866593028134243\n",
      "Step - 4411, Loss - 0.7554981458762065, Learning Rate - 0.003125, magnitude of gradient - 0.4740604487171647\n",
      "Step - 4412, Loss - 0.7043213345312977, Learning Rate - 0.003125, magnitude of gradient - 0.9516008050457919\n",
      "Step - 4413, Loss - 0.5672884447964155, Learning Rate - 0.003125, magnitude of gradient - 1.8872373245085245\n",
      "Step - 4414, Loss - 0.5972434467350889, Learning Rate - 0.003125, magnitude of gradient - 0.8267368181234878\n",
      "Step - 4415, Loss - 0.6791399451618542, Learning Rate - 0.003125, magnitude of gradient - 1.4056159612721182\n",
      "Step - 4416, Loss - 0.6673474555547602, Learning Rate - 0.003125, magnitude of gradient - 0.5148072887354483\n",
      "Step - 4417, Loss - 0.783887197072503, Learning Rate - 0.003125, magnitude of gradient - 0.8016428997941957\n",
      "Step - 4418, Loss - 0.6347859507463579, Learning Rate - 0.003125, magnitude of gradient - 0.4595303817945844\n",
      "Step - 4419, Loss - 0.6811629191983433, Learning Rate - 0.003125, magnitude of gradient - 0.3748677361468071\n",
      "Step - 4420, Loss - 0.7495879753172403, Learning Rate - 0.003125, magnitude of gradient - 0.6634537433027039\n",
      "Step - 4421, Loss - 0.7784513707818235, Learning Rate - 0.003125, magnitude of gradient - 0.9777741269964786\n",
      "Step - 4422, Loss - 0.7252815390089333, Learning Rate - 0.003125, magnitude of gradient - 0.8487871025819043\n",
      "Step - 4423, Loss - 0.8440447604457251, Learning Rate - 0.003125, magnitude of gradient - 1.429844741364512\n",
      "Step - 4424, Loss - 0.5346853317619276, Learning Rate - 0.003125, magnitude of gradient - 1.7277384814475754\n",
      "Step - 4425, Loss - 0.711734119980138, Learning Rate - 0.003125, magnitude of gradient - 1.3044079068184604\n",
      "Step - 4426, Loss - 0.7550478203775163, Learning Rate - 0.003125, magnitude of gradient - 1.2650066606109844\n",
      "Step - 4427, Loss - 0.4826680257834512, Learning Rate - 0.003125, magnitude of gradient - 1.849485597336335\n",
      "Step - 4428, Loss - 0.6395103356152052, Learning Rate - 0.003125, magnitude of gradient - 1.6438194401931545\n",
      "Step - 4429, Loss - 0.7400951819798779, Learning Rate - 0.003125, magnitude of gradient - 0.8079120284925809\n",
      "Step - 4430, Loss - 0.6398747908015084, Learning Rate - 0.003125, magnitude of gradient - 0.6239474812953707\n",
      "Step - 4431, Loss - 0.7939605086601608, Learning Rate - 0.003125, magnitude of gradient - 0.2740999483680131\n",
      "Step - 4432, Loss - 0.7543635940129108, Learning Rate - 0.003125, magnitude of gradient - 1.7093126259982172\n",
      "Step - 4433, Loss - 0.7734167090077531, Learning Rate - 0.003125, magnitude of gradient - 1.888495549312144\n",
      "Step - 4434, Loss - 0.5581075882260353, Learning Rate - 0.003125, magnitude of gradient - 0.8627214756581789\n",
      "Step - 4435, Loss - 0.7786006089912143, Learning Rate - 0.003125, magnitude of gradient - 1.231270151800929\n",
      "Step - 4436, Loss - 0.6591340171644711, Learning Rate - 0.003125, magnitude of gradient - 1.1240977442579043\n",
      "Step - 4437, Loss - 0.7231846096244414, Learning Rate - 0.003125, magnitude of gradient - 1.697172861974923\n",
      "Step - 4438, Loss - 0.6602043994879248, Learning Rate - 0.003125, magnitude of gradient - 0.9499022651970368\n",
      "Step - 4439, Loss - 0.7563342524140614, Learning Rate - 0.003125, magnitude of gradient - 1.1720660594387895\n",
      "Step - 4440, Loss - 0.6321554717441096, Learning Rate - 0.003125, magnitude of gradient - 1.0250235709310587\n",
      "Step - 4441, Loss - 0.7688107497159039, Learning Rate - 0.003125, magnitude of gradient - 1.4499006719956657\n",
      "Step - 4442, Loss - 0.7443452075164259, Learning Rate - 0.003125, magnitude of gradient - 1.3254559308664748\n",
      "Step - 4443, Loss - 0.66462389858314, Learning Rate - 0.003125, magnitude of gradient - 0.9764575457110615\n",
      "Step - 4444, Loss - 0.8246426112605185, Learning Rate - 0.003125, magnitude of gradient - 2.63381360933108\n",
      "Step - 4445, Loss - 0.5004776543615206, Learning Rate - 0.003125, magnitude of gradient - 0.48248267481151014\n",
      "Step - 4446, Loss - 0.8175944097893121, Learning Rate - 0.003125, magnitude of gradient - 0.5954398490141621\n",
      "Step - 4447, Loss - 0.8086811758909029, Learning Rate - 0.003125, magnitude of gradient - 1.3666912117392727\n",
      "Step - 4448, Loss - 0.7697127738152225, Learning Rate - 0.003125, magnitude of gradient - 0.3453942577293416\n",
      "Step - 4449, Loss - 0.6851683005590079, Learning Rate - 0.003125, magnitude of gradient - 0.7794037430204017\n",
      "Step - 4450, Loss - 0.8896377888632476, Learning Rate - 0.003125, magnitude of gradient - 2.6933064671277442\n",
      "Step - 4451, Loss - 0.6462554694610666, Learning Rate - 0.003125, magnitude of gradient - 1.0108662609530372\n",
      "Step - 4452, Loss - 0.8211352231733027, Learning Rate - 0.003125, magnitude of gradient - 1.218463916235324\n",
      "Step - 4453, Loss - 0.7481585558153742, Learning Rate - 0.003125, magnitude of gradient - 0.5627106563760664\n",
      "Step - 4454, Loss - 0.7852384764613605, Learning Rate - 0.003125, magnitude of gradient - 0.7068070427883739\n",
      "Step - 4455, Loss - 0.6957275316874916, Learning Rate - 0.003125, magnitude of gradient - 1.1915263078814193\n",
      "Step - 4456, Loss - 0.5769065633819042, Learning Rate - 0.003125, magnitude of gradient - 1.1991412079446022\n",
      "Step - 4457, Loss - 0.8074419580764038, Learning Rate - 0.003125, magnitude of gradient - 2.2880063708569987\n",
      "Step - 4458, Loss - 0.6071926194527195, Learning Rate - 0.003125, magnitude of gradient - 1.3549277943145024\n",
      "Step - 4459, Loss - 0.5752846401745365, Learning Rate - 0.003125, magnitude of gradient - 2.316927104113962\n",
      "Step - 4460, Loss - 0.797704001150128, Learning Rate - 0.003125, magnitude of gradient - 1.4809053264718568\n",
      "Step - 4461, Loss - 0.653023850893896, Learning Rate - 0.003125, magnitude of gradient - 1.1105647755704804\n",
      "Step - 4462, Loss - 0.5950893500934009, Learning Rate - 0.003125, magnitude of gradient - 1.8706947111948853\n",
      "Step - 4463, Loss - 0.5503001321405397, Learning Rate - 0.003125, magnitude of gradient - 1.6294265847077896\n",
      "Step - 4464, Loss - 0.7419258587713284, Learning Rate - 0.003125, magnitude of gradient - 0.41862305519936993\n",
      "Step - 4465, Loss - 0.7583657410791504, Learning Rate - 0.003125, magnitude of gradient - 0.8684631340581035\n",
      "Step - 4466, Loss - 0.6878675996829268, Learning Rate - 0.003125, magnitude of gradient - 0.4786275585836723\n",
      "Step - 4467, Loss - 0.7508294474570338, Learning Rate - 0.003125, magnitude of gradient - 0.9885032182770763\n",
      "Step - 4468, Loss - 0.8095408484094851, Learning Rate - 0.003125, magnitude of gradient - 1.4618629572101558\n",
      "Step - 4469, Loss - 0.7333245870210482, Learning Rate - 0.003125, magnitude of gradient - 0.7278683337461492\n",
      "Step - 4470, Loss - 0.5456060994436875, Learning Rate - 0.003125, magnitude of gradient - 2.50047689733252\n",
      "Step - 4471, Loss - 0.7734579791214575, Learning Rate - 0.003125, magnitude of gradient - 0.6600350148824387\n",
      "Step - 4472, Loss - 0.6390840387707282, Learning Rate - 0.003125, magnitude of gradient - 1.153611541399409\n",
      "Step - 4473, Loss - 0.7437073830841577, Learning Rate - 0.003125, magnitude of gradient - 1.3594357145131357\n",
      "Step - 4474, Loss - 0.5511574661716824, Learning Rate - 0.003125, magnitude of gradient - 1.093505295213014\n",
      "Step - 4475, Loss - 0.8200222284023073, Learning Rate - 0.003125, magnitude of gradient - 0.9736774387659658\n",
      "Step - 4476, Loss - 0.7239249632522956, Learning Rate - 0.003125, magnitude of gradient - 0.8311111198511025\n",
      "Step - 4477, Loss - 0.5386893275543838, Learning Rate - 0.003125, magnitude of gradient - 1.6031786458622619\n",
      "Step - 4478, Loss - 0.7813424536821045, Learning Rate - 0.003125, magnitude of gradient - 0.7189762679844035\n",
      "Step - 4479, Loss - 0.6567505398192202, Learning Rate - 0.003125, magnitude of gradient - 1.0053350850125227\n",
      "Step - 4480, Loss - 0.7348833748156248, Learning Rate - 0.003125, magnitude of gradient - 2.509103432269949\n",
      "Step - 4481, Loss - 0.6707656724832531, Learning Rate - 0.003125, magnitude of gradient - 1.1610102523430177\n",
      "Step - 4482, Loss - 0.6505093354769291, Learning Rate - 0.003125, magnitude of gradient - 1.4141711267837118\n",
      "Step - 4483, Loss - 0.5053448660832756, Learning Rate - 0.003125, magnitude of gradient - 0.9589129627837064\n",
      "Step - 4484, Loss - 0.8024155094891556, Learning Rate - 0.003125, magnitude of gradient - 1.3594581186931078\n",
      "Step - 4485, Loss - 0.7281220886140255, Learning Rate - 0.003125, magnitude of gradient - 1.0410411354571472\n",
      "Step - 4486, Loss - 0.4643491711408626, Learning Rate - 0.003125, magnitude of gradient - 0.9481742425372306\n",
      "Step - 4487, Loss - 0.6597816717082219, Learning Rate - 0.003125, magnitude of gradient - 0.9597059980445317\n",
      "Step - 4488, Loss - 0.6256997995937856, Learning Rate - 0.003125, magnitude of gradient - 1.7033894664590126\n",
      "Step - 4489, Loss - 0.6565595794759908, Learning Rate - 0.003125, magnitude of gradient - 1.403879005718179\n",
      "Step - 4490, Loss - 0.8545071243784973, Learning Rate - 0.003125, magnitude of gradient - 0.8512480364973576\n",
      "Step - 4491, Loss - 0.5892665051587945, Learning Rate - 0.003125, magnitude of gradient - 1.944808549328745\n",
      "Step - 4492, Loss - 0.6034433918739823, Learning Rate - 0.003125, magnitude of gradient - 0.962566573819185\n",
      "Step - 4493, Loss - 0.733637731303728, Learning Rate - 0.003125, magnitude of gradient - 0.4180790757563253\n",
      "Step - 4494, Loss - 0.6701937689525694, Learning Rate - 0.003125, magnitude of gradient - 2.158896399983479\n",
      "Step - 4495, Loss - 0.7185089089455884, Learning Rate - 0.003125, magnitude of gradient - 1.043788738459024\n",
      "Step - 4496, Loss - 0.7705446104028579, Learning Rate - 0.003125, magnitude of gradient - 0.9045964561126865\n",
      "Step - 4497, Loss - 0.7220898490439742, Learning Rate - 0.003125, magnitude of gradient - 1.3304887155294525\n",
      "Step - 4498, Loss - 0.6306038743099132, Learning Rate - 0.003125, magnitude of gradient - 1.4975583706516866\n",
      "Step - 4499, Loss - 0.635365289401349, Learning Rate - 0.003125, magnitude of gradient - 1.0410631567006101\n",
      "Step - 4500, Loss - 0.7983921152160307, Learning Rate - 0.003125, magnitude of gradient - 1.1924172814812666\n",
      "Step - 4501, Loss - 0.6482462685888664, Learning Rate - 0.003125, magnitude of gradient - 0.4026073623524324\n",
      "Step - 4502, Loss - 0.9756154262398734, Learning Rate - 0.003125, magnitude of gradient - 3.36511626171603\n",
      "Step - 4503, Loss - 0.8028440613890949, Learning Rate - 0.003125, magnitude of gradient - 0.8483647410438211\n",
      "Step - 4504, Loss - 0.9078197273534534, Learning Rate - 0.003125, magnitude of gradient - 2.8017839745870745\n",
      "Step - 4505, Loss - 0.642445734146404, Learning Rate - 0.003125, magnitude of gradient - 0.9036076519489779\n",
      "Step - 4506, Loss - 0.7555130838683435, Learning Rate - 0.003125, magnitude of gradient - 0.5944686669411341\n",
      "Step - 4507, Loss - 0.981952294729997, Learning Rate - 0.003125, magnitude of gradient - 2.0299818945239494\n",
      "Step - 4508, Loss - 0.7591874663971524, Learning Rate - 0.003125, magnitude of gradient - 0.9593948350313647\n",
      "Step - 4509, Loss - 0.945401727922397, Learning Rate - 0.003125, magnitude of gradient - 0.872274566916774\n",
      "Step - 4510, Loss - 0.8745680438631068, Learning Rate - 0.003125, magnitude of gradient - 2.938052298454353\n",
      "Step - 4511, Loss - 0.6900917115926238, Learning Rate - 0.003125, magnitude of gradient - 1.341711985448619\n",
      "Step - 4512, Loss - 0.7400582916955943, Learning Rate - 0.003125, magnitude of gradient - 1.4784890652643217\n",
      "Step - 4513, Loss - 0.6120052795900199, Learning Rate - 0.003125, magnitude of gradient - 1.5381113790878802\n",
      "Step - 4514, Loss - 0.7430562188305987, Learning Rate - 0.003125, magnitude of gradient - 2.2949494533132\n",
      "Step - 4515, Loss - 0.6146027055273293, Learning Rate - 0.003125, magnitude of gradient - 1.961971772116699\n",
      "Step - 4516, Loss - 0.6467348380510276, Learning Rate - 0.003125, magnitude of gradient - 1.0491869931016555\n",
      "Step - 4517, Loss - 0.6830031166737252, Learning Rate - 0.003125, magnitude of gradient - 2.1411856969703913\n",
      "Step - 4518, Loss - 0.6478971922830261, Learning Rate - 0.003125, magnitude of gradient - 1.026465879743811\n",
      "Step - 4519, Loss - 0.711082270530129, Learning Rate - 0.003125, magnitude of gradient - 1.3057824856639215\n",
      "Step - 4520, Loss - 0.5826178838306882, Learning Rate - 0.003125, magnitude of gradient - 0.5452955455301066\n",
      "Step - 4521, Loss - 0.7547884183792, Learning Rate - 0.003125, magnitude of gradient - 2.07514837602854\n",
      "Step - 4522, Loss - 0.8617225205896207, Learning Rate - 0.003125, magnitude of gradient - 1.785377509494906\n",
      "Step - 4523, Loss - 0.7268927493844854, Learning Rate - 0.003125, magnitude of gradient - 1.3932541998998471\n",
      "Step - 4524, Loss - 0.5877517895483431, Learning Rate - 0.003125, magnitude of gradient - 2.598202611541203\n",
      "Step - 4525, Loss - 0.6238818290983715, Learning Rate - 0.003125, magnitude of gradient - 0.5112611549207003\n",
      "Step - 4526, Loss - 0.6240001792455802, Learning Rate - 0.003125, magnitude of gradient - 0.5772281485605585\n",
      "Step - 4527, Loss - 0.8829658423499965, Learning Rate - 0.003125, magnitude of gradient - 0.690023482192355\n",
      "Step - 4528, Loss - 0.6599924497165786, Learning Rate - 0.003125, magnitude of gradient - 0.4666133522515559\n",
      "Step - 4529, Loss - 0.7510603641473906, Learning Rate - 0.003125, magnitude of gradient - 1.0056758924290001\n",
      "Step - 4530, Loss - 0.7267496170583354, Learning Rate - 0.003125, magnitude of gradient - 0.8247779687236546\n",
      "Step - 4531, Loss - 0.8644678424186947, Learning Rate - 0.003125, magnitude of gradient - 1.7190706350754836\n",
      "Step - 4532, Loss - 0.7168702623203991, Learning Rate - 0.003125, magnitude of gradient - 0.498886592852699\n",
      "Step - 4533, Loss - 0.7656473767684899, Learning Rate - 0.003125, magnitude of gradient - 1.7702282372664622\n",
      "Step - 4534, Loss - 0.5165225012536241, Learning Rate - 0.003125, magnitude of gradient - 1.7383491947467686\n",
      "Step - 4535, Loss - 0.6881207551970299, Learning Rate - 0.003125, magnitude of gradient - 0.7519419920777299\n",
      "Step - 4536, Loss - 0.7086543641683009, Learning Rate - 0.003125, magnitude of gradient - 1.1897774127939997\n",
      "Step - 4537, Loss - 0.6727317454635311, Learning Rate - 0.003125, magnitude of gradient - 1.0804873680431741\n",
      "Step - 4538, Loss - 0.7375635391420582, Learning Rate - 0.003125, magnitude of gradient - 1.3068841656760828\n",
      "Step - 4539, Loss - 0.35793094598819114, Learning Rate - 0.003125, magnitude of gradient - 1.0464218540664583\n",
      "Step - 4540, Loss - 0.6896021203478503, Learning Rate - 0.003125, magnitude of gradient - 1.9309011918411245\n",
      "Step - 4541, Loss - 0.7146104702427649, Learning Rate - 0.003125, magnitude of gradient - 0.6071263483617281\n",
      "Step - 4542, Loss - 0.8635814211952286, Learning Rate - 0.003125, magnitude of gradient - 3.287435358477964\n",
      "Step - 4543, Loss - 0.5875374284475512, Learning Rate - 0.003125, magnitude of gradient - 1.6871225544653081\n",
      "Step - 4544, Loss - 0.8089652553909826, Learning Rate - 0.003125, magnitude of gradient - 2.78012584244731\n",
      "Step - 4545, Loss - 0.6658552954480399, Learning Rate - 0.003125, magnitude of gradient - 0.8613853027287323\n",
      "Step - 4546, Loss - 0.7743982260983043, Learning Rate - 0.003125, magnitude of gradient - 0.6703198463032256\n",
      "Step - 4547, Loss - 0.7895806730198871, Learning Rate - 0.003125, magnitude of gradient - 0.660950358597025\n",
      "Step - 4548, Loss - 0.6286640942599345, Learning Rate - 0.003125, magnitude of gradient - 2.136391559400946\n",
      "Step - 4549, Loss - 0.8446817287272147, Learning Rate - 0.003125, magnitude of gradient - 0.9488274191865085\n",
      "Step - 4550, Loss - 0.733223315457126, Learning Rate - 0.003125, magnitude of gradient - 0.30185117913818954\n",
      "Step - 4551, Loss - 0.5891285464495513, Learning Rate - 0.003125, magnitude of gradient - 1.435966455852143\n",
      "Step - 4552, Loss - 0.8217705959982234, Learning Rate - 0.003125, magnitude of gradient - 1.478178699941229\n",
      "Step - 4553, Loss - 0.5177387275682612, Learning Rate - 0.003125, magnitude of gradient - 1.4062163893788002\n",
      "Step - 4554, Loss - 0.6845851614011897, Learning Rate - 0.003125, magnitude of gradient - 1.4573120755119693\n",
      "Step - 4555, Loss - 0.9344790436028271, Learning Rate - 0.003125, magnitude of gradient - 2.7770529050528294\n",
      "Step - 4556, Loss - 0.6123657821814208, Learning Rate - 0.003125, magnitude of gradient - 0.42548519431661114\n",
      "Step - 4557, Loss - 0.7465560891899076, Learning Rate - 0.003125, magnitude of gradient - 0.8404852758543145\n",
      "Step - 4558, Loss - 0.832835044063712, Learning Rate - 0.003125, magnitude of gradient - 0.9332938921316987\n",
      "Step - 4559, Loss - 0.6256530635081007, Learning Rate - 0.003125, magnitude of gradient - 1.2475426379886507\n",
      "Step - 4560, Loss - 0.8022469758208773, Learning Rate - 0.003125, magnitude of gradient - 2.1940379716430467\n",
      "Step - 4561, Loss - 0.7334330207768991, Learning Rate - 0.003125, magnitude of gradient - 0.5203571887494327\n",
      "Step - 4562, Loss - 0.7763017856134288, Learning Rate - 0.003125, magnitude of gradient - 0.9560015368294138\n",
      "Step - 4563, Loss - 0.5834218767021746, Learning Rate - 0.003125, magnitude of gradient - 1.2554087061292907\n",
      "Step - 4564, Loss - 0.8147001018268745, Learning Rate - 0.003125, magnitude of gradient - 1.6550934842263787\n",
      "Step - 4565, Loss - 0.5140277263298745, Learning Rate - 0.003125, magnitude of gradient - 1.1196732268716407\n",
      "Step - 4566, Loss - 0.6865057094698992, Learning Rate - 0.003125, magnitude of gradient - 1.1428420802474606\n",
      "Step - 4567, Loss - 0.571796949518348, Learning Rate - 0.003125, magnitude of gradient - 1.0381255357138581\n",
      "Step - 4568, Loss - 0.650463683629204, Learning Rate - 0.003125, magnitude of gradient - 1.3616923473009228\n",
      "Step - 4569, Loss - 0.7256708197271422, Learning Rate - 0.003125, magnitude of gradient - 0.6257420304856024\n",
      "Step - 4570, Loss - 0.5576410302195169, Learning Rate - 0.003125, magnitude of gradient - 1.7118505926772658\n",
      "Step - 4571, Loss - 0.7444918130126146, Learning Rate - 0.003125, magnitude of gradient - 0.5229328867463243\n",
      "Step - 4572, Loss - 0.6770813120193384, Learning Rate - 0.003125, magnitude of gradient - 1.4059543368093403\n",
      "Step - 4573, Loss - 0.7138321063225747, Learning Rate - 0.003125, magnitude of gradient - 1.6066022307583419\n",
      "Step - 4574, Loss - 0.627301860520191, Learning Rate - 0.003125, magnitude of gradient - 0.41353982980308834\n",
      "Step - 4575, Loss - 0.8315723615133033, Learning Rate - 0.003125, magnitude of gradient - 1.8606794390595789\n",
      "Step - 4576, Loss - 0.5349927966719006, Learning Rate - 0.003125, magnitude of gradient - 0.5475705675479943\n",
      "Step - 4577, Loss - 0.6659609044636559, Learning Rate - 0.003125, magnitude of gradient - 1.3840804461937364\n",
      "Step - 4578, Loss - 0.740535969906978, Learning Rate - 0.003125, magnitude of gradient - 0.4317435167441082\n",
      "Step - 4579, Loss - 0.8240075298178582, Learning Rate - 0.003125, magnitude of gradient - 5.477877119709649\n",
      "Step - 4580, Loss - 0.6627137713806308, Learning Rate - 0.003125, magnitude of gradient - 0.4010983951509618\n",
      "Step - 4581, Loss - 0.8729098625565711, Learning Rate - 0.003125, magnitude of gradient - 1.2959707857692118\n",
      "Step - 4582, Loss - 0.6666410152993478, Learning Rate - 0.003125, magnitude of gradient - 1.598033452159594\n",
      "Step - 4583, Loss - 0.6902289109353481, Learning Rate - 0.003125, magnitude of gradient - 0.7350277477044371\n",
      "Step - 4584, Loss - 0.4877004879655014, Learning Rate - 0.003125, magnitude of gradient - 3.7943240046508353\n",
      "Step - 4585, Loss - 0.7094987636210824, Learning Rate - 0.003125, magnitude of gradient - 0.8060409608655164\n",
      "Step - 4586, Loss - 0.49880007483894206, Learning Rate - 0.003125, magnitude of gradient - 1.5662701883193717\n",
      "Step - 4587, Loss - 0.6046012919731107, Learning Rate - 0.003125, magnitude of gradient - 0.42723924714619493\n",
      "Step - 4588, Loss - 0.6483810907497678, Learning Rate - 0.003125, magnitude of gradient - 0.916347865721489\n",
      "Step - 4589, Loss - 0.5332378198886186, Learning Rate - 0.003125, magnitude of gradient - 1.6699603627665145\n",
      "Step - 4590, Loss - 0.8045389630065287, Learning Rate - 0.003125, magnitude of gradient - 2.0646471014345593\n",
      "Step - 4591, Loss - 0.5519011298583024, Learning Rate - 0.003125, magnitude of gradient - 0.5244376783825785\n",
      "Step - 4592, Loss - 0.654801664884431, Learning Rate - 0.003125, magnitude of gradient - 0.9773619075259934\n",
      "Step - 4593, Loss - 0.6491907990122037, Learning Rate - 0.003125, magnitude of gradient - 0.5073721530355509\n",
      "Step - 4594, Loss - 0.6112427615233141, Learning Rate - 0.003125, magnitude of gradient - 0.9860309266516449\n",
      "Step - 4595, Loss - 0.8596568846960942, Learning Rate - 0.003125, magnitude of gradient - 1.012119275330877\n",
      "Step - 4596, Loss - 0.8401329514654285, Learning Rate - 0.003125, magnitude of gradient - 0.9769794296280044\n",
      "Step - 4597, Loss - 0.8082816745544962, Learning Rate - 0.003125, magnitude of gradient - 0.9774546911526035\n",
      "Step - 4598, Loss - 0.7360928574130929, Learning Rate - 0.003125, magnitude of gradient - 1.105113622902834\n",
      "Step - 4599, Loss - 0.6337359975530727, Learning Rate - 0.003125, magnitude of gradient - 1.7452778225973151\n",
      "Step - 4600, Loss - 0.738452923464308, Learning Rate - 0.003125, magnitude of gradient - 0.697407586200796\n",
      "Step - 4601, Loss - 0.6169192606455477, Learning Rate - 0.003125, magnitude of gradient - 1.2555046408451618\n",
      "Step - 4602, Loss - 0.6107320464586317, Learning Rate - 0.003125, magnitude of gradient - 0.8291289421157834\n",
      "Step - 4603, Loss - 0.61064902729498, Learning Rate - 0.003125, magnitude of gradient - 0.7796504523648878\n",
      "Step - 4604, Loss - 0.7510975111252984, Learning Rate - 0.003125, magnitude of gradient - 1.087651910396462\n",
      "Step - 4605, Loss - 0.6225291129942041, Learning Rate - 0.003125, magnitude of gradient - 1.7397887956297737\n",
      "Step - 4606, Loss - 0.6165268479242703, Learning Rate - 0.003125, magnitude of gradient - 0.9382251088593667\n",
      "Step - 4607, Loss - 0.7087747244688234, Learning Rate - 0.003125, magnitude of gradient - 1.9077535648920698\n",
      "Step - 4608, Loss - 0.5266896028214539, Learning Rate - 0.003125, magnitude of gradient - 1.0736797172883754\n",
      "Step - 4609, Loss - 0.6823472060303301, Learning Rate - 0.003125, magnitude of gradient - 0.6014647877320974\n",
      "Step - 4610, Loss - 0.8913006195618789, Learning Rate - 0.003125, magnitude of gradient - 1.7903192086876056\n",
      "Step - 4611, Loss - 0.6377398991355836, Learning Rate - 0.003125, magnitude of gradient - 0.9444421259167494\n",
      "Step - 4612, Loss - 0.689839259839037, Learning Rate - 0.003125, magnitude of gradient - 0.9181177479857185\n",
      "Step - 4613, Loss - 0.6567295184023099, Learning Rate - 0.003125, magnitude of gradient - 1.6806903408919167\n",
      "Step - 4614, Loss - 0.7705146542720568, Learning Rate - 0.003125, magnitude of gradient - 0.7192845966724235\n",
      "Step - 4615, Loss - 0.7123415267882773, Learning Rate - 0.003125, magnitude of gradient - 1.272262545628235\n",
      "Step - 4616, Loss - 0.9384120656341797, Learning Rate - 0.003125, magnitude of gradient - 2.014026728900622\n",
      "Step - 4617, Loss - 0.7443270527644767, Learning Rate - 0.003125, magnitude of gradient - 3.5251609826940307\n",
      "Step - 4618, Loss - 0.7505675828347848, Learning Rate - 0.003125, magnitude of gradient - 1.2656254413622339\n",
      "Step - 4619, Loss - 0.7693482598519008, Learning Rate - 0.003125, magnitude of gradient - 0.8627703136354229\n",
      "Step - 4620, Loss - 0.6055881458184895, Learning Rate - 0.003125, magnitude of gradient - 0.7332284350925371\n",
      "Step - 4621, Loss - 0.6542318410019821, Learning Rate - 0.003125, magnitude of gradient - 0.8060528672810632\n",
      "Step - 4622, Loss - 0.5995175669069771, Learning Rate - 0.003125, magnitude of gradient - 2.851442552855482\n",
      "Step - 4623, Loss - 0.6561174589296601, Learning Rate - 0.003125, magnitude of gradient - 0.655792567030829\n",
      "Step - 4624, Loss - 0.77801999583726, Learning Rate - 0.003125, magnitude of gradient - 1.2103367310470357\n",
      "Step - 4625, Loss - 0.747740065871744, Learning Rate - 0.003125, magnitude of gradient - 1.0931315032302618\n",
      "Step - 4626, Loss - 0.948056696047863, Learning Rate - 0.003125, magnitude of gradient - 1.8866433241121279\n",
      "Step - 4627, Loss - 0.5849364539421302, Learning Rate - 0.003125, magnitude of gradient - 1.344020293041606\n",
      "Step - 4628, Loss - 0.937030210817421, Learning Rate - 0.003125, magnitude of gradient - 3.414286896930356\n",
      "Step - 4629, Loss - 0.6629891187119032, Learning Rate - 0.003125, magnitude of gradient - 0.7556295646553153\n",
      "Step - 4630, Loss - 0.6941006773007157, Learning Rate - 0.003125, magnitude of gradient - 1.3526054258665627\n",
      "Step - 4631, Loss - 0.738803072007526, Learning Rate - 0.003125, magnitude of gradient - 1.3990276698839184\n",
      "Step - 4632, Loss - 0.7258454714995999, Learning Rate - 0.003125, magnitude of gradient - 1.8428886636003567\n",
      "Step - 4633, Loss - 0.7310380909900734, Learning Rate - 0.003125, magnitude of gradient - 1.0315411269456132\n",
      "Step - 4634, Loss - 0.6954041335700314, Learning Rate - 0.003125, magnitude of gradient - 1.12363096858202\n",
      "Step - 4635, Loss - 0.5595097000771866, Learning Rate - 0.003125, magnitude of gradient - 0.9653304963088725\n",
      "Step - 4636, Loss - 0.661317035448603, Learning Rate - 0.003125, magnitude of gradient - 2.2403287792251385\n",
      "Step - 4637, Loss - 0.8437217202040774, Learning Rate - 0.003125, magnitude of gradient - 2.9931600979028046\n",
      "Step - 4638, Loss - 0.6033135571200208, Learning Rate - 0.003125, magnitude of gradient - 1.880541806481875\n",
      "Step - 4639, Loss - 0.6658951224530323, Learning Rate - 0.003125, magnitude of gradient - 0.6427911193723416\n",
      "Step - 4640, Loss - 0.7150380134508684, Learning Rate - 0.003125, magnitude of gradient - 3.3052308012854192\n",
      "Step - 4641, Loss - 0.6749887452492287, Learning Rate - 0.003125, magnitude of gradient - 0.32380253099047857\n",
      "Step - 4642, Loss - 0.7535537097557645, Learning Rate - 0.003125, magnitude of gradient - 1.2881562503684245\n",
      "Step - 4643, Loss - 0.6390245487642265, Learning Rate - 0.003125, magnitude of gradient - 1.6328888428761397\n",
      "Step - 4644, Loss - 0.723558151414852, Learning Rate - 0.003125, magnitude of gradient - 0.39578550655478445\n",
      "Step - 4645, Loss - 0.8132437276260778, Learning Rate - 0.003125, magnitude of gradient - 1.2083444413581674\n",
      "Step - 4646, Loss - 0.6886668960643891, Learning Rate - 0.003125, magnitude of gradient - 1.991111823464313\n",
      "Step - 4647, Loss - 0.6862543624349707, Learning Rate - 0.003125, magnitude of gradient - 0.6748088082373085\n",
      "Step - 4648, Loss - 0.7870869956103317, Learning Rate - 0.003125, magnitude of gradient - 0.37947764290456726\n",
      "Step - 4649, Loss - 0.8051995572547672, Learning Rate - 0.003125, magnitude of gradient - 0.7328863261013654\n",
      "Step - 4650, Loss - 0.6383214488222725, Learning Rate - 0.003125, magnitude of gradient - 0.9013734625879635\n",
      "Step - 4651, Loss - 0.6904638685813743, Learning Rate - 0.003125, magnitude of gradient - 2.375114639300877\n",
      "Step - 4652, Loss - 0.6866698230993932, Learning Rate - 0.003125, magnitude of gradient - 0.7751495233062564\n",
      "Step - 4653, Loss - 0.7478071754120613, Learning Rate - 0.003125, magnitude of gradient - 0.6154386175391442\n",
      "Step - 4654, Loss - 0.6870580914086828, Learning Rate - 0.003125, magnitude of gradient - 0.7785510094165344\n",
      "Step - 4655, Loss - 0.7832605235844114, Learning Rate - 0.003125, magnitude of gradient - 1.7187641537052472\n",
      "Step - 4656, Loss - 0.5053185705077616, Learning Rate - 0.003125, magnitude of gradient - 1.1734917756469998\n",
      "Step - 4657, Loss - 0.8065218371637217, Learning Rate - 0.003125, magnitude of gradient - 0.7795564273292407\n",
      "Step - 4658, Loss - 0.6584440400571787, Learning Rate - 0.003125, magnitude of gradient - 0.6967616952457782\n",
      "Step - 4659, Loss - 0.38686786584094834, Learning Rate - 0.003125, magnitude of gradient - 2.13797926361668\n",
      "Step - 4660, Loss - 0.44202184896587554, Learning Rate - 0.003125, magnitude of gradient - 1.2912761628382017\n",
      "Step - 4661, Loss - 0.758393213457208, Learning Rate - 0.003125, magnitude of gradient - 2.2086321230617014\n",
      "Step - 4662, Loss - 0.8335550626592881, Learning Rate - 0.003125, magnitude of gradient - 0.5684243736674766\n",
      "Step - 4663, Loss - 0.6407651048695137, Learning Rate - 0.003125, magnitude of gradient - 1.0330790322455217\n",
      "Step - 4664, Loss - 0.6274071000172013, Learning Rate - 0.003125, magnitude of gradient - 1.549824980447618\n",
      "Step - 4665, Loss - 0.8890869437760508, Learning Rate - 0.003125, magnitude of gradient - 1.1000436023215927\n",
      "Step - 4666, Loss - 0.9748731159891276, Learning Rate - 0.003125, magnitude of gradient - 0.8837210970396597\n",
      "Step - 4667, Loss - 0.5381558128184564, Learning Rate - 0.003125, magnitude of gradient - 1.815736146498342\n",
      "Step - 4668, Loss - 0.6163168409192183, Learning Rate - 0.003125, magnitude of gradient - 1.6247971729545025\n",
      "Step - 4669, Loss - 0.5929667567014343, Learning Rate - 0.003125, magnitude of gradient - 0.6224234148324747\n",
      "Step - 4670, Loss - 0.7225661269969511, Learning Rate - 0.003125, magnitude of gradient - 1.1060232844395537\n",
      "Step - 4671, Loss - 0.7205429979580759, Learning Rate - 0.003125, magnitude of gradient - 2.0042300397902966\n",
      "Step - 4672, Loss - 0.9057656807951229, Learning Rate - 0.003125, magnitude of gradient - 1.7968388224714629\n",
      "Step - 4673, Loss - 0.6547298337077606, Learning Rate - 0.003125, magnitude of gradient - 1.5740513938174545\n",
      "Step - 4674, Loss - 0.52776385029891, Learning Rate - 0.003125, magnitude of gradient - 1.6634054763458515\n",
      "Step - 4675, Loss - 0.4805947154465464, Learning Rate - 0.003125, magnitude of gradient - 0.420302318573025\n",
      "Step - 4676, Loss - 0.7804343385363597, Learning Rate - 0.003125, magnitude of gradient - 0.7283994797450941\n",
      "Step - 4677, Loss - 0.6244527315055707, Learning Rate - 0.003125, magnitude of gradient - 0.8573686985890618\n",
      "Step - 4678, Loss - 0.6653840622147511, Learning Rate - 0.003125, magnitude of gradient - 1.192065210030875\n",
      "Step - 4679, Loss - 0.6919879474038754, Learning Rate - 0.003125, magnitude of gradient - 1.7520237393878733\n",
      "Step - 4680, Loss - 0.7637085130940426, Learning Rate - 0.003125, magnitude of gradient - 0.8049853735540117\n",
      "Step - 4681, Loss - 0.8014699282152056, Learning Rate - 0.003125, magnitude of gradient - 2.3158759442486243\n",
      "Step - 4682, Loss - 0.8582661347392889, Learning Rate - 0.003125, magnitude of gradient - 1.3620566647730266\n",
      "Step - 4683, Loss - 0.7512872718488904, Learning Rate - 0.003125, magnitude of gradient - 1.834005414483123\n",
      "Step - 4684, Loss - 0.8781547499999411, Learning Rate - 0.003125, magnitude of gradient - 1.0339460759947616\n",
      "Step - 4685, Loss - 0.657489536802092, Learning Rate - 0.003125, magnitude of gradient - 1.483752790619211\n",
      "Step - 4686, Loss - 0.6425946384367631, Learning Rate - 0.003125, magnitude of gradient - 0.5180982439340435\n",
      "Step - 4687, Loss - 0.6066382148526447, Learning Rate - 0.003125, magnitude of gradient - 1.0381378931601668\n",
      "Step - 4688, Loss - 0.5799627052191654, Learning Rate - 0.003125, magnitude of gradient - 2.0505898936732705\n",
      "Step - 4689, Loss - 0.6528958574508993, Learning Rate - 0.003125, magnitude of gradient - 1.2429407981051654\n",
      "Step - 4690, Loss - 0.606695838838748, Learning Rate - 0.003125, magnitude of gradient - 1.4337526451439355\n",
      "Step - 4691, Loss - 0.580091547293492, Learning Rate - 0.003125, magnitude of gradient - 1.3101926057074058\n",
      "Step - 4692, Loss - 0.7070624578355411, Learning Rate - 0.003125, magnitude of gradient - 1.4613150800711276\n",
      "Step - 4693, Loss - 0.698203843294773, Learning Rate - 0.003125, magnitude of gradient - 0.7795494473037593\n",
      "Step - 4694, Loss - 0.6778634797149096, Learning Rate - 0.003125, magnitude of gradient - 1.319865175030949\n",
      "Step - 4695, Loss - 0.5289203381695768, Learning Rate - 0.003125, magnitude of gradient - 0.5376318129473853\n",
      "Step - 4696, Loss - 0.5564113265233286, Learning Rate - 0.003125, magnitude of gradient - 1.311100131169459\n",
      "Step - 4697, Loss - 0.5376950182878858, Learning Rate - 0.003125, magnitude of gradient - 1.3428356771651173\n",
      "Step - 4698, Loss - 0.6632497478211489, Learning Rate - 0.003125, magnitude of gradient - 1.280804944575207\n",
      "Step - 4699, Loss - 0.5695375344336884, Learning Rate - 0.003125, magnitude of gradient - 0.8987444022942234\n",
      "Step - 4700, Loss - 0.6135349159726454, Learning Rate - 0.003125, magnitude of gradient - 1.13906483234747\n",
      "Step - 4701, Loss - 0.49169572147388285, Learning Rate - 0.003125, magnitude of gradient - 0.8065974010386944\n",
      "Step - 4702, Loss - 0.8491577911996602, Learning Rate - 0.003125, magnitude of gradient - 2.45674666129551\n",
      "Step - 4703, Loss - 0.6802672967472323, Learning Rate - 0.003125, magnitude of gradient - 1.1118438057156852\n",
      "Step - 4704, Loss - 0.631885636004217, Learning Rate - 0.003125, magnitude of gradient - 1.620844413537103\n",
      "Step - 4705, Loss - 0.5771445364863113, Learning Rate - 0.003125, magnitude of gradient - 0.8743682624106772\n",
      "Step - 4706, Loss - 0.6883912223087238, Learning Rate - 0.003125, magnitude of gradient - 1.187763381687694\n",
      "Step - 4707, Loss - 0.6956146033942732, Learning Rate - 0.003125, magnitude of gradient - 1.406427857765215\n",
      "Step - 4708, Loss - 0.7720255259770878, Learning Rate - 0.003125, magnitude of gradient - 1.5037203322805872\n",
      "Step - 4709, Loss - 0.49501084906805315, Learning Rate - 0.003125, magnitude of gradient - 2.383163721410788\n",
      "Step - 4710, Loss - 0.6944809562103376, Learning Rate - 0.003125, magnitude of gradient - 1.5171816631115742\n",
      "Step - 4711, Loss - 0.737461829981058, Learning Rate - 0.003125, magnitude of gradient - 0.821030182589248\n",
      "Step - 4712, Loss - 0.5673986036275301, Learning Rate - 0.003125, magnitude of gradient - 0.9551521674934261\n",
      "Step - 4713, Loss - 0.46094523079541194, Learning Rate - 0.003125, magnitude of gradient - 0.8798783661779161\n",
      "Step - 4714, Loss - 0.8156924030993499, Learning Rate - 0.003125, magnitude of gradient - 1.4036452037266698\n",
      "Step - 4715, Loss - 0.7840309183235235, Learning Rate - 0.003125, magnitude of gradient - 2.742825229995792\n",
      "Step - 4716, Loss - 0.4906242700722115, Learning Rate - 0.003125, magnitude of gradient - 1.133397170940644\n",
      "Step - 4717, Loss - 0.8107657069544165, Learning Rate - 0.003125, magnitude of gradient - 1.1941749111535844\n",
      "Step - 4718, Loss - 0.7502658486741371, Learning Rate - 0.003125, magnitude of gradient - 0.6555471829769528\n",
      "Step - 4719, Loss - 0.7029782051474178, Learning Rate - 0.003125, magnitude of gradient - 0.5544969104869458\n",
      "Step - 4720, Loss - 0.7480382758487827, Learning Rate - 0.003125, magnitude of gradient - 1.8235108076727118\n",
      "Step - 4721, Loss - 0.8332201120599012, Learning Rate - 0.003125, magnitude of gradient - 1.3264155645932134\n",
      "Step - 4722, Loss - 0.6602636405067395, Learning Rate - 0.003125, magnitude of gradient - 1.7232568635347731\n",
      "Step - 4723, Loss - 0.7232692481179236, Learning Rate - 0.003125, magnitude of gradient - 1.3794695853050918\n",
      "Step - 4724, Loss - 0.6700812842633712, Learning Rate - 0.003125, magnitude of gradient - 0.9357556393313756\n",
      "Step - 4725, Loss - 0.7701927645811296, Learning Rate - 0.003125, magnitude of gradient - 0.7501451300863738\n",
      "Step - 4726, Loss - 0.6145003586905605, Learning Rate - 0.003125, magnitude of gradient - 1.4840009727961554\n",
      "Step - 4727, Loss - 0.8093093318026138, Learning Rate - 0.003125, magnitude of gradient - 2.437995284702311\n",
      "Step - 4728, Loss - 0.5792519892108804, Learning Rate - 0.003125, magnitude of gradient - 1.1957158154244458\n",
      "Step - 4729, Loss - 0.6286643682195335, Learning Rate - 0.003125, magnitude of gradient - 1.0208706713126174\n",
      "Step - 4730, Loss - 0.5718718204989692, Learning Rate - 0.003125, magnitude of gradient - 1.1804647361465062\n",
      "Step - 4731, Loss - 0.7835075402592273, Learning Rate - 0.003125, magnitude of gradient - 0.8014836828886136\n",
      "Step - 4732, Loss - 0.704387676377063, Learning Rate - 0.003125, magnitude of gradient - 1.0832294999646939\n",
      "Step - 4733, Loss - 0.8695820611733484, Learning Rate - 0.003125, magnitude of gradient - 1.2841369868682533\n",
      "Step - 4734, Loss - 0.5082846279910822, Learning Rate - 0.003125, magnitude of gradient - 1.5763261091586258\n",
      "Step - 4735, Loss - 0.7818055321784719, Learning Rate - 0.003125, magnitude of gradient - 1.6493870454394104\n",
      "Step - 4736, Loss - 0.5326870497547275, Learning Rate - 0.003125, magnitude of gradient - 1.8265476261948135\n",
      "Step - 4737, Loss - 0.6348068446208701, Learning Rate - 0.003125, magnitude of gradient - 1.0820712978027653\n",
      "Step - 4738, Loss - 0.6363618896997687, Learning Rate - 0.003125, magnitude of gradient - 1.3000230164208795\n",
      "Step - 4739, Loss - 0.9661903962863773, Learning Rate - 0.003125, magnitude of gradient - 1.0071682796359864\n",
      "Step - 4740, Loss - 0.6587877179106031, Learning Rate - 0.003125, magnitude of gradient - 1.3735341033392392\n",
      "Step - 4741, Loss - 0.7270421977920744, Learning Rate - 0.003125, magnitude of gradient - 1.7630659757563911\n",
      "Step - 4742, Loss - 0.7882691238491126, Learning Rate - 0.003125, magnitude of gradient - 1.2706217656542773\n",
      "Step - 4743, Loss - 0.5727669489602677, Learning Rate - 0.003125, magnitude of gradient - 1.2531570734725475\n",
      "Step - 4744, Loss - 0.5806619630436853, Learning Rate - 0.003125, magnitude of gradient - 2.0336764528287037\n",
      "Step - 4745, Loss - 0.6967921041205938, Learning Rate - 0.003125, magnitude of gradient - 1.234722969992768\n",
      "Step - 4746, Loss - 0.910017886269587, Learning Rate - 0.003125, magnitude of gradient - 1.4469875918947595\n",
      "Step - 4747, Loss - 0.8256337363667071, Learning Rate - 0.003125, magnitude of gradient - 1.351124411659014\n",
      "Step - 4748, Loss - 0.5981947129059496, Learning Rate - 0.003125, magnitude of gradient - 1.3829390256063385\n",
      "Step - 4749, Loss - 0.866883468096305, Learning Rate - 0.003125, magnitude of gradient - 0.8135779559416185\n",
      "Step - 4750, Loss - 0.6179619190637651, Learning Rate - 0.003125, magnitude of gradient - 0.6328513740119218\n",
      "Step - 4751, Loss - 0.6800770909130731, Learning Rate - 0.003125, magnitude of gradient - 1.3033790806204297\n",
      "Step - 4752, Loss - 0.802089900618562, Learning Rate - 0.003125, magnitude of gradient - 0.5690699022587261\n",
      "Step - 4753, Loss - 0.9323847667980679, Learning Rate - 0.003125, magnitude of gradient - 1.0677460051228638\n",
      "Step - 4754, Loss - 0.8073119632037332, Learning Rate - 0.003125, magnitude of gradient - 1.2850934975562227\n",
      "Step - 4755, Loss - 0.7391476045220289, Learning Rate - 0.003125, magnitude of gradient - 0.7355621072481616\n",
      "Step - 4756, Loss - 0.8458005950104474, Learning Rate - 0.003125, magnitude of gradient - 0.9453197395424768\n",
      "Step - 4757, Loss - 0.7553485741087782, Learning Rate - 0.003125, magnitude of gradient - 2.234119273335839\n",
      "Step - 4758, Loss - 0.7657867167036326, Learning Rate - 0.003125, magnitude of gradient - 1.538815377984001\n",
      "Step - 4759, Loss - 0.6827330364234037, Learning Rate - 0.003125, magnitude of gradient - 1.345644434322545\n",
      "Step - 4760, Loss - 0.5531711294490171, Learning Rate - 0.003125, magnitude of gradient - 1.619249413593989\n",
      "Step - 4761, Loss - 0.5851814253915744, Learning Rate - 0.003125, magnitude of gradient - 1.0063180735742934\n",
      "Step - 4762, Loss - 0.7870720600274002, Learning Rate - 0.003125, magnitude of gradient - 1.037534697366673\n",
      "Step - 4763, Loss - 0.6567616607966159, Learning Rate - 0.003125, magnitude of gradient - 1.1871802383534986\n",
      "Step - 4764, Loss - 0.7466391033136015, Learning Rate - 0.003125, magnitude of gradient - 1.2342606953930566\n",
      "Step - 4765, Loss - 0.9662320494063689, Learning Rate - 0.003125, magnitude of gradient - 2.0850060831993678\n",
      "Step - 4766, Loss - 0.7452708318975911, Learning Rate - 0.003125, magnitude of gradient - 1.5664632463442645\n",
      "Step - 4767, Loss - 0.6169628123881246, Learning Rate - 0.003125, magnitude of gradient - 0.906663249996702\n",
      "Step - 4768, Loss - 0.554268779797121, Learning Rate - 0.003125, magnitude of gradient - 2.459382609191895\n",
      "Step - 4769, Loss - 0.6968512348775601, Learning Rate - 0.003125, magnitude of gradient - 1.0068168572325227\n",
      "Step - 4770, Loss - 0.7694203153398261, Learning Rate - 0.003125, magnitude of gradient - 0.5551370656114587\n",
      "Step - 4771, Loss - 0.46107439697170005, Learning Rate - 0.003125, magnitude of gradient - 1.116345479276261\n",
      "Step - 4772, Loss - 0.7342172407397104, Learning Rate - 0.003125, magnitude of gradient - 0.9712380456343449\n",
      "Step - 4773, Loss - 0.7411115837571688, Learning Rate - 0.003125, magnitude of gradient - 1.1485741884030252\n",
      "Step - 4774, Loss - 0.5295857991446139, Learning Rate - 0.003125, magnitude of gradient - 0.4788517400866904\n",
      "Step - 4775, Loss - 0.6104156009867687, Learning Rate - 0.003125, magnitude of gradient - 0.9245277595730417\n",
      "Step - 4776, Loss - 0.6084713163684842, Learning Rate - 0.003125, magnitude of gradient - 0.9120599371747677\n",
      "Step - 4777, Loss - 0.3339262248314976, Learning Rate - 0.003125, magnitude of gradient - 0.8471884677370249\n",
      "Step - 4778, Loss - 0.7805093104424456, Learning Rate - 0.003125, magnitude of gradient - 1.35384660840252\n",
      "Step - 4779, Loss - 0.7517895750771282, Learning Rate - 0.003125, magnitude of gradient - 1.2156633811806599\n",
      "Step - 4780, Loss - 0.7223876405991945, Learning Rate - 0.003125, magnitude of gradient - 0.4492157879112157\n",
      "Step - 4781, Loss - 0.5551283337099874, Learning Rate - 0.003125, magnitude of gradient - 1.0331085829756947\n",
      "Step - 4782, Loss - 0.6781182269775408, Learning Rate - 0.003125, magnitude of gradient - 1.0575062737544498\n",
      "Step - 4783, Loss - 0.605677544212532, Learning Rate - 0.003125, magnitude of gradient - 1.9805389624548952\n",
      "Step - 4784, Loss - 0.7293878480831958, Learning Rate - 0.003125, magnitude of gradient - 0.8760736390711817\n",
      "Step - 4785, Loss - 0.699764910066587, Learning Rate - 0.003125, magnitude of gradient - 2.1210104032518506\n",
      "Step - 4786, Loss - 0.5378726492670309, Learning Rate - 0.003125, magnitude of gradient - 0.744461025396909\n",
      "Step - 4787, Loss - 0.6342903553787678, Learning Rate - 0.003125, magnitude of gradient - 3.786560778702277\n",
      "Step - 4788, Loss - 0.6508758706457429, Learning Rate - 0.003125, magnitude of gradient - 1.5244397489394066\n",
      "Step - 4789, Loss - 0.6550645749092125, Learning Rate - 0.003125, magnitude of gradient - 0.6031846370704099\n",
      "Step - 4790, Loss - 0.7836140344596643, Learning Rate - 0.003125, magnitude of gradient - 1.8381446888427995\n",
      "Step - 4791, Loss - 0.5960498845570857, Learning Rate - 0.003125, magnitude of gradient - 0.5056359104224125\n",
      "Step - 4792, Loss - 0.8833593275026262, Learning Rate - 0.003125, magnitude of gradient - 1.678092458793595\n",
      "Step - 4793, Loss - 0.6823126791254084, Learning Rate - 0.003125, magnitude of gradient - 1.603784605062968\n",
      "Step - 4794, Loss - 0.7866014989075325, Learning Rate - 0.003125, magnitude of gradient - 2.652045707830938\n",
      "Step - 4795, Loss - 0.8137274932425671, Learning Rate - 0.003125, magnitude of gradient - 0.7235197589920959\n",
      "Step - 4796, Loss - 0.7952071948523114, Learning Rate - 0.003125, magnitude of gradient - 1.0637148816373558\n",
      "Step - 4797, Loss - 0.7654161514662415, Learning Rate - 0.003125, magnitude of gradient - 1.5526012793418802\n",
      "Step - 4798, Loss - 0.7776796216364538, Learning Rate - 0.003125, magnitude of gradient - 1.9635325682274267\n",
      "Step - 4799, Loss - 0.6207190469426859, Learning Rate - 0.003125, magnitude of gradient - 1.2523691687645004\n",
      "Step - 4800, Loss - 0.5799374473412551, Learning Rate - 0.003125, magnitude of gradient - 1.095051091701703\n",
      "Step - 4801, Loss - 0.6737063345664439, Learning Rate - 0.003125, magnitude of gradient - 1.8820816946630508\n",
      "Step - 4802, Loss - 0.8377557987261, Learning Rate - 0.003125, magnitude of gradient - 2.3030100174817028\n",
      "Step - 4803, Loss - 0.6770438024414895, Learning Rate - 0.003125, magnitude of gradient - 1.5108928528566976\n",
      "Step - 4804, Loss - 0.5882882979247473, Learning Rate - 0.003125, magnitude of gradient - 0.9983990647534177\n",
      "Step - 4805, Loss - 0.4907502509301441, Learning Rate - 0.003125, magnitude of gradient - 0.8134980147246377\n",
      "Step - 4806, Loss - 0.6819167107689241, Learning Rate - 0.003125, magnitude of gradient - 0.7083665151124113\n",
      "Step - 4807, Loss - 0.7399931181094033, Learning Rate - 0.003125, magnitude of gradient - 1.4048610035238014\n",
      "Step - 4808, Loss - 0.6560900515234855, Learning Rate - 0.003125, magnitude of gradient - 1.598954256528175\n",
      "Step - 4809, Loss - 0.6300819768391994, Learning Rate - 0.003125, magnitude of gradient - 1.1747127015690138\n",
      "Step - 4810, Loss - 0.6563143075615376, Learning Rate - 0.003125, magnitude of gradient - 1.406504874369358\n",
      "Step - 4811, Loss - 0.6872951169072039, Learning Rate - 0.003125, magnitude of gradient - 1.8754257193216384\n",
      "Step - 4812, Loss - 0.42841811493354814, Learning Rate - 0.003125, magnitude of gradient - 0.8251690455741757\n",
      "Step - 4813, Loss - 0.7165238875044535, Learning Rate - 0.003125, magnitude of gradient - 2.2153193433213993\n",
      "Step - 4814, Loss - 0.8476048613869621, Learning Rate - 0.003125, magnitude of gradient - 1.1287989841507946\n",
      "Step - 4815, Loss - 0.5941461410137163, Learning Rate - 0.003125, magnitude of gradient - 2.0173615806848657\n",
      "Step - 4816, Loss - 0.646972983187128, Learning Rate - 0.003125, magnitude of gradient - 0.8463680220425717\n",
      "Step - 4817, Loss - 0.7191922865865225, Learning Rate - 0.003125, magnitude of gradient - 1.168939656906478\n",
      "Step - 4818, Loss - 0.8051722506083039, Learning Rate - 0.003125, magnitude of gradient - 2.1083636967587887\n",
      "Step - 4819, Loss - 0.7991592788325761, Learning Rate - 0.003125, magnitude of gradient - 1.2990019479698633\n",
      "Step - 4820, Loss - 0.6052241769909275, Learning Rate - 0.003125, magnitude of gradient - 1.7592098536866556\n",
      "Step - 4821, Loss - 0.8057871850181892, Learning Rate - 0.003125, magnitude of gradient - 1.0866211936954433\n",
      "Step - 4822, Loss - 0.5129125246451597, Learning Rate - 0.003125, magnitude of gradient - 1.9621693681176704\n",
      "Step - 4823, Loss - 0.82885763890329, Learning Rate - 0.003125, magnitude of gradient - 0.8795396958561302\n",
      "Step - 4824, Loss - 0.5079789146848573, Learning Rate - 0.003125, magnitude of gradient - 2.1559795352937123\n",
      "Step - 4825, Loss - 0.8170258962456332, Learning Rate - 0.003125, magnitude of gradient - 1.1604662198380948\n",
      "Step - 4826, Loss - 0.7044271116798462, Learning Rate - 0.003125, magnitude of gradient - 1.5040177990199117\n",
      "Step - 4827, Loss - 0.5826877970656428, Learning Rate - 0.003125, magnitude of gradient - 2.04272683174011\n",
      "Step - 4828, Loss - 0.5760302061956851, Learning Rate - 0.003125, magnitude of gradient - 1.5164881450286405\n",
      "Step - 4829, Loss - 0.8036346919198947, Learning Rate - 0.003125, magnitude of gradient - 1.7228418765667153\n",
      "Step - 4830, Loss - 0.8019554468641293, Learning Rate - 0.003125, magnitude of gradient - 1.995997308414786\n",
      "Step - 4831, Loss - 0.7618377607088126, Learning Rate - 0.003125, magnitude of gradient - 1.0118661829935371\n",
      "Step - 4832, Loss - 0.5590164918537436, Learning Rate - 0.003125, magnitude of gradient - 1.9940122669676432\n",
      "Step - 4833, Loss - 0.6452102030095453, Learning Rate - 0.003125, magnitude of gradient - 1.6381969505055693\n",
      "Step - 4834, Loss - 0.6867860588180936, Learning Rate - 0.003125, magnitude of gradient - 1.2180458064399227\n",
      "Step - 4835, Loss - 0.654610792512492, Learning Rate - 0.003125, magnitude of gradient - 1.6983998033066188\n",
      "Step - 4836, Loss - 0.6898326592061996, Learning Rate - 0.003125, magnitude of gradient - 2.5316358807770696\n",
      "Step - 4837, Loss - 0.5621754711278588, Learning Rate - 0.003125, magnitude of gradient - 2.2130873358839627\n",
      "Step - 4838, Loss - 0.7095603038782659, Learning Rate - 0.003125, magnitude of gradient - 2.4584219064814303\n",
      "Step - 4839, Loss - 0.75011477128289, Learning Rate - 0.003125, magnitude of gradient - 1.0467182768723928\n",
      "Step - 4840, Loss - 0.7965446391504855, Learning Rate - 0.003125, magnitude of gradient - 1.5674416271652927\n",
      "Step - 4841, Loss - 0.6225710807322684, Learning Rate - 0.003125, magnitude of gradient - 1.7527768778897432\n",
      "Step - 4842, Loss - 0.9608332399841473, Learning Rate - 0.003125, magnitude of gradient - 1.3652493233468044\n",
      "Step - 4843, Loss - 0.7371852859279754, Learning Rate - 0.003125, magnitude of gradient - 1.8193308351316417\n",
      "Step - 4844, Loss - 0.7511300225452732, Learning Rate - 0.003125, magnitude of gradient - 0.7193207934675836\n",
      "Step - 4845, Loss - 0.6911524248252889, Learning Rate - 0.003125, magnitude of gradient - 0.9124300907960435\n",
      "Step - 4846, Loss - 0.5570347852859316, Learning Rate - 0.003125, magnitude of gradient - 1.8634784766720458\n",
      "Step - 4847, Loss - 0.630162233190682, Learning Rate - 0.003125, magnitude of gradient - 1.0924800483550767\n",
      "Step - 4848, Loss - 0.6102510398627109, Learning Rate - 0.003125, magnitude of gradient - 1.1563397188671338\n",
      "Step - 4849, Loss - 0.6772893370709304, Learning Rate - 0.003125, magnitude of gradient - 1.8338934507706728\n",
      "Step - 4850, Loss - 0.7609911047597427, Learning Rate - 0.003125, magnitude of gradient - 1.2595422954911786\n",
      "Step - 4851, Loss - 0.8759039400415571, Learning Rate - 0.003125, magnitude of gradient - 0.5433921515845421\n",
      "Step - 4852, Loss - 0.7295590172265259, Learning Rate - 0.003125, magnitude of gradient - 0.5319442099236996\n",
      "Step - 4853, Loss - 0.6913885711601122, Learning Rate - 0.003125, magnitude of gradient - 1.7185044566381988\n",
      "Step - 4854, Loss - 0.5602288819899521, Learning Rate - 0.003125, magnitude of gradient - 0.3803270409197464\n",
      "Step - 4855, Loss - 0.630127666585039, Learning Rate - 0.003125, magnitude of gradient - 1.5578885480649605\n",
      "Step - 4856, Loss - 0.9177563737053036, Learning Rate - 0.003125, magnitude of gradient - 2.114269290001265\n",
      "Step - 4857, Loss - 0.8995603970481886, Learning Rate - 0.003125, magnitude of gradient - 1.8828562661509656\n",
      "Step - 4858, Loss - 0.7080076807555787, Learning Rate - 0.003125, magnitude of gradient - 0.5843699718308537\n",
      "Step - 4859, Loss - 0.7721007482809964, Learning Rate - 0.003125, magnitude of gradient - 0.620436440038641\n",
      "Step - 4860, Loss - 0.6485699673613515, Learning Rate - 0.003125, magnitude of gradient - 1.0200193384198906\n",
      "Step - 4861, Loss - 0.7856630546543724, Learning Rate - 0.003125, magnitude of gradient - 1.1926341420274944\n",
      "Step - 4862, Loss - 0.748358555931012, Learning Rate - 0.003125, magnitude of gradient - 1.6012248137049923\n",
      "Step - 4863, Loss - 0.898209297225341, Learning Rate - 0.003125, magnitude of gradient - 0.6310240973050927\n",
      "Step - 4864, Loss - 0.7005823634305894, Learning Rate - 0.003125, magnitude of gradient - 2.1140123124491716\n",
      "Step - 4865, Loss - 0.7688523634070646, Learning Rate - 0.003125, magnitude of gradient - 2.1456030484351025\n",
      "Step - 4866, Loss - 0.6846407603267628, Learning Rate - 0.003125, magnitude of gradient - 1.8373218980183408\n",
      "Step - 4867, Loss - 0.771239752126243, Learning Rate - 0.003125, magnitude of gradient - 1.2306677302860523\n",
      "Step - 4868, Loss - 0.7783279634728913, Learning Rate - 0.003125, magnitude of gradient - 0.8429653452943842\n",
      "Step - 4869, Loss - 0.6041178941479648, Learning Rate - 0.003125, magnitude of gradient - 1.3623493876628228\n",
      "Step - 4870, Loss - 0.7739374501077878, Learning Rate - 0.003125, magnitude of gradient - 0.8199669684848079\n",
      "Step - 4871, Loss - 0.7128796044757879, Learning Rate - 0.003125, magnitude of gradient - 1.4595679695138672\n",
      "Step - 4872, Loss - 0.6829555633662941, Learning Rate - 0.003125, magnitude of gradient - 1.570747694564493\n",
      "Step - 4873, Loss - 0.8101509362330769, Learning Rate - 0.003125, magnitude of gradient - 0.9784296980547983\n",
      "Step - 4874, Loss - 0.6325670032344188, Learning Rate - 0.003125, magnitude of gradient - 0.690941913410444\n",
      "Step - 4875, Loss - 0.6829987290780082, Learning Rate - 0.003125, magnitude of gradient - 1.8332825140313178\n",
      "Step - 4876, Loss - 0.8577385252588, Learning Rate - 0.003125, magnitude of gradient - 1.9325820154241447\n",
      "Step - 4877, Loss - 0.7392866366271053, Learning Rate - 0.003125, magnitude of gradient - 0.7457477425191493\n",
      "Step - 4878, Loss - 0.6924111853829681, Learning Rate - 0.003125, magnitude of gradient - 0.9642550974868155\n",
      "Step - 4879, Loss - 0.58135437342416, Learning Rate - 0.003125, magnitude of gradient - 0.6264765649143086\n",
      "Step - 4880, Loss - 0.5715793647943759, Learning Rate - 0.003125, magnitude of gradient - 2.6059112481269366\n",
      "Step - 4881, Loss - 0.6440291529296647, Learning Rate - 0.003125, magnitude of gradient - 1.454735952688777\n",
      "Step - 4882, Loss - 0.8033850486841282, Learning Rate - 0.003125, magnitude of gradient - 0.865137801266273\n",
      "Step - 4883, Loss - 0.6469279938783302, Learning Rate - 0.003125, magnitude of gradient - 1.2635054021185976\n",
      "Step - 4884, Loss - 0.9241806245774005, Learning Rate - 0.003125, magnitude of gradient - 0.8862347357801206\n",
      "Step - 4885, Loss - 0.6070838477518777, Learning Rate - 0.003125, magnitude of gradient - 0.6263038774747993\n",
      "Step - 4886, Loss - 0.8013798756714828, Learning Rate - 0.003125, magnitude of gradient - 0.33906413623687176\n",
      "Step - 4887, Loss - 0.622073935049733, Learning Rate - 0.003125, magnitude of gradient - 0.5704259886803631\n",
      "Step - 4888, Loss - 0.5759030642005978, Learning Rate - 0.003125, magnitude of gradient - 1.544775377019948\n",
      "Step - 4889, Loss - 0.5814491244487, Learning Rate - 0.003125, magnitude of gradient - 0.45717196443045083\n",
      "Step - 4890, Loss - 0.5894062886639275, Learning Rate - 0.003125, magnitude of gradient - 0.9071542597715613\n",
      "Step - 4891, Loss - 0.6715691379066929, Learning Rate - 0.003125, magnitude of gradient - 0.8011496503121279\n",
      "Step - 4892, Loss - 0.674620671120798, Learning Rate - 0.003125, magnitude of gradient - 1.7186683081733916\n",
      "Step - 4893, Loss - 0.8856740752060703, Learning Rate - 0.003125, magnitude of gradient - 1.1856547672012168\n",
      "Step - 4894, Loss - 0.6066719954817182, Learning Rate - 0.003125, magnitude of gradient - 1.1080547848509639\n",
      "Step - 4895, Loss - 0.6909247418241571, Learning Rate - 0.003125, magnitude of gradient - 0.7161639309900191\n",
      "Step - 4896, Loss - 0.7262371418673641, Learning Rate - 0.003125, magnitude of gradient - 0.8276179690557915\n",
      "Step - 4897, Loss - 0.5980705921337279, Learning Rate - 0.003125, magnitude of gradient - 1.7274973033428396\n",
      "Step - 4898, Loss - 0.7158239698495961, Learning Rate - 0.003125, magnitude of gradient - 0.614555582415747\n",
      "Step - 4899, Loss - 0.9011804874157624, Learning Rate - 0.003125, magnitude of gradient - 0.8049383478581728\n",
      "Step - 4900, Loss - 0.6930912000098304, Learning Rate - 0.003125, magnitude of gradient - 1.964249244507435\n",
      "Step - 4901, Loss - 0.6435602394491724, Learning Rate - 0.003125, magnitude of gradient - 0.4132019084790855\n",
      "Step - 4902, Loss - 0.6783090592187155, Learning Rate - 0.003125, magnitude of gradient - 2.4782612110704165\n",
      "Step - 4903, Loss - 0.6995985676646493, Learning Rate - 0.003125, magnitude of gradient - 0.7507356096418478\n",
      "Step - 4904, Loss - 0.7678586553865472, Learning Rate - 0.003125, magnitude of gradient - 1.3558636779126632\n",
      "Step - 4905, Loss - 0.7411597175007376, Learning Rate - 0.003125, magnitude of gradient - 1.7751606087811405\n",
      "Step - 4906, Loss - 0.719176526962111, Learning Rate - 0.003125, magnitude of gradient - 1.1053912385441094\n",
      "Step - 4907, Loss - 0.6269749081714108, Learning Rate - 0.003125, magnitude of gradient - 1.5655253577511457\n",
      "Step - 4908, Loss - 0.6323744920845618, Learning Rate - 0.003125, magnitude of gradient - 1.1266872067249707\n",
      "Step - 4909, Loss - 0.5561522604794911, Learning Rate - 0.003125, magnitude of gradient - 1.2182196825598328\n",
      "Step - 4910, Loss - 0.592156527359735, Learning Rate - 0.003125, magnitude of gradient - 1.3992835109391173\n",
      "Step - 4911, Loss - 0.7550734816277286, Learning Rate - 0.003125, magnitude of gradient - 1.530907581145282\n",
      "Step - 4912, Loss - 0.7792891350231749, Learning Rate - 0.003125, magnitude of gradient - 1.6461838644490634\n",
      "Step - 4913, Loss - 0.7391447880578152, Learning Rate - 0.003125, magnitude of gradient - 1.2500638736847454\n",
      "Step - 4914, Loss - 0.783812873070484, Learning Rate - 0.003125, magnitude of gradient - 0.49739299382317914\n",
      "Step - 4915, Loss - 0.7497844988487986, Learning Rate - 0.003125, magnitude of gradient - 2.164050997789915\n",
      "Step - 4916, Loss - 0.7881887424857189, Learning Rate - 0.003125, magnitude of gradient - 0.6905627021631774\n",
      "Step - 4917, Loss - 0.7820674395386226, Learning Rate - 0.003125, magnitude of gradient - 0.738825566532602\n",
      "Step - 4918, Loss - 0.6390373592921573, Learning Rate - 0.003125, magnitude of gradient - 1.0281745086744445\n",
      "Step - 4919, Loss - 0.7566164369338216, Learning Rate - 0.003125, magnitude of gradient - 0.2225667402735745\n",
      "Step - 4920, Loss - 0.45624357183826636, Learning Rate - 0.003125, magnitude of gradient - 2.022847212895102\n",
      "Step - 4921, Loss - 0.838649248650619, Learning Rate - 0.003125, magnitude of gradient - 0.8822315360362771\n",
      "Step - 4922, Loss - 0.9356854464387997, Learning Rate - 0.003125, magnitude of gradient - 2.2822304802997433\n",
      "Step - 4923, Loss - 0.7123805491096372, Learning Rate - 0.003125, magnitude of gradient - 1.2669582089467526\n",
      "Step - 4924, Loss - 0.9106621889821662, Learning Rate - 0.003125, magnitude of gradient - 3.100421363908678\n",
      "Step - 4925, Loss - 0.8849666250697226, Learning Rate - 0.003125, magnitude of gradient - 1.4897133448537627\n",
      "Step - 4926, Loss - 0.5489678099435429, Learning Rate - 0.003125, magnitude of gradient - 0.7520667260623567\n",
      "Step - 4927, Loss - 0.7355072627625996, Learning Rate - 0.003125, magnitude of gradient - 1.6050291533030048\n",
      "Step - 4928, Loss - 0.563206066894989, Learning Rate - 0.003125, magnitude of gradient - 1.8946820934557072\n",
      "Step - 4929, Loss - 0.6967149095635312, Learning Rate - 0.003125, magnitude of gradient - 1.299473413055137\n",
      "Step - 4930, Loss - 0.6921968090253617, Learning Rate - 0.003125, magnitude of gradient - 0.505245720394568\n",
      "Step - 4931, Loss - 0.7481469166126887, Learning Rate - 0.003125, magnitude of gradient - 2.459413644025977\n",
      "Step - 4932, Loss - 0.8739865214865519, Learning Rate - 0.003125, magnitude of gradient - 1.7738507983483527\n",
      "Step - 4933, Loss - 0.933416750968137, Learning Rate - 0.003125, magnitude of gradient - 0.7170194622588935\n",
      "Step - 4934, Loss - 0.6531035375309746, Learning Rate - 0.003125, magnitude of gradient - 0.775928138895001\n",
      "Step - 4935, Loss - 0.5964808844436142, Learning Rate - 0.003125, magnitude of gradient - 1.0301358571029648\n",
      "Step - 4936, Loss - 0.6243957705631575, Learning Rate - 0.003125, magnitude of gradient - 3.4004802075034872\n",
      "Step - 4937, Loss - 0.621474786352999, Learning Rate - 0.003125, magnitude of gradient - 1.3937901646523378\n",
      "Step - 4938, Loss - 0.7887356669495713, Learning Rate - 0.003125, magnitude of gradient - 1.0997316003249897\n",
      "Step - 4939, Loss - 0.712738571246197, Learning Rate - 0.003125, magnitude of gradient - 0.7732970131800541\n",
      "Step - 4940, Loss - 0.6252414641482117, Learning Rate - 0.003125, magnitude of gradient - 1.1498131792943467\n",
      "Step - 4941, Loss - 0.7294356861786313, Learning Rate - 0.003125, magnitude of gradient - 0.9762944617449762\n",
      "Step - 4942, Loss - 0.5781215020807398, Learning Rate - 0.003125, magnitude of gradient - 1.8066992497220453\n",
      "Step - 4943, Loss - 0.5440271729260544, Learning Rate - 0.003125, magnitude of gradient - 0.4757805591531702\n",
      "Step - 4944, Loss - 0.6935124150119861, Learning Rate - 0.003125, magnitude of gradient - 1.1364807161535821\n",
      "Step - 4945, Loss - 0.730139335080995, Learning Rate - 0.003125, magnitude of gradient - 0.9290880007183747\n",
      "Step - 4946, Loss - 0.6772735263992047, Learning Rate - 0.003125, magnitude of gradient - 1.0124363051466454\n",
      "Step - 4947, Loss - 0.7109118048679068, Learning Rate - 0.003125, magnitude of gradient - 1.1868805533966114\n",
      "Step - 4948, Loss - 0.6534223990543304, Learning Rate - 0.003125, magnitude of gradient - 1.2867274809324878\n",
      "Step - 4949, Loss - 0.6648211355250044, Learning Rate - 0.003125, magnitude of gradient - 1.277346189715566\n",
      "Step - 4950, Loss - 0.9138741894794605, Learning Rate - 0.003125, magnitude of gradient - 2.086144435638564\n",
      "Step - 4951, Loss - 0.5298075270224885, Learning Rate - 0.003125, magnitude of gradient - 2.037788454358088\n",
      "Step - 4952, Loss - 0.7735277712310753, Learning Rate - 0.003125, magnitude of gradient - 0.6325176133565019\n",
      "Step - 4953, Loss - 0.6641790364812109, Learning Rate - 0.003125, magnitude of gradient - 1.193328308912917\n",
      "Step - 4954, Loss - 0.7234120475165757, Learning Rate - 0.003125, magnitude of gradient - 2.4600827708376616\n",
      "Step - 4955, Loss - 0.7285422801536141, Learning Rate - 0.003125, magnitude of gradient - 0.9630207954106016\n",
      "Step - 4956, Loss - 0.6042102847678973, Learning Rate - 0.003125, magnitude of gradient - 2.5386110968311173\n",
      "Step - 4957, Loss - 0.8088446781828386, Learning Rate - 0.003125, magnitude of gradient - 1.7301983633226141\n",
      "Step - 4958, Loss - 0.5655822352521676, Learning Rate - 0.003125, magnitude of gradient - 0.6454121952834962\n",
      "Step - 4959, Loss - 0.4936990802160611, Learning Rate - 0.003125, magnitude of gradient - 1.9014374500964502\n",
      "Step - 4960, Loss - 0.7803268761262682, Learning Rate - 0.003125, magnitude of gradient - 2.231775149131858\n",
      "Step - 4961, Loss - 0.6136042514247264, Learning Rate - 0.003125, magnitude of gradient - 0.3184724950375026\n",
      "Step - 4962, Loss - 0.7630543549833104, Learning Rate - 0.003125, magnitude of gradient - 1.562532238487613\n",
      "Step - 4963, Loss - 0.5263858315983004, Learning Rate - 0.003125, magnitude of gradient - 0.9732553738653573\n",
      "Step - 4964, Loss - 0.6150685295469329, Learning Rate - 0.003125, magnitude of gradient - 0.6948832233009815\n",
      "Step - 4965, Loss - 0.6585766366957212, Learning Rate - 0.003125, magnitude of gradient - 0.8036308458786173\n",
      "Step - 4966, Loss - 0.7589270550847433, Learning Rate - 0.003125, magnitude of gradient - 0.8475622816583496\n",
      "Step - 4967, Loss - 0.6145940502496576, Learning Rate - 0.003125, magnitude of gradient - 0.4317940538984586\n",
      "Step - 4968, Loss - 0.576577478174336, Learning Rate - 0.003125, magnitude of gradient - 0.5535503849359743\n",
      "Step - 4969, Loss - 0.6353010174174382, Learning Rate - 0.003125, magnitude of gradient - 0.6132647055015392\n",
      "Step - 4970, Loss - 0.8262162751375991, Learning Rate - 0.003125, magnitude of gradient - 1.8130390997071553\n",
      "Step - 4971, Loss - 0.6257588823771841, Learning Rate - 0.003125, magnitude of gradient - 0.45910879841683383\n",
      "Step - 4972, Loss - 0.6741825620270625, Learning Rate - 0.003125, magnitude of gradient - 1.412160132448136\n",
      "Step - 4973, Loss - 0.7411816686931318, Learning Rate - 0.003125, magnitude of gradient - 0.4008293719664716\n",
      "Step - 4974, Loss - 0.6419702377295471, Learning Rate - 0.003125, magnitude of gradient - 1.753731045065665\n",
      "Step - 4975, Loss - 0.5831950916355025, Learning Rate - 0.003125, magnitude of gradient - 0.5564156692540636\n",
      "Step - 4976, Loss - 0.7197552307305355, Learning Rate - 0.003125, magnitude of gradient - 0.5976438454293213\n",
      "Step - 4977, Loss - 0.6437933194014386, Learning Rate - 0.003125, magnitude of gradient - 0.4859845093141405\n",
      "Step - 4978, Loss - 0.5598284817270331, Learning Rate - 0.003125, magnitude of gradient - 1.1027384458227405\n",
      "Step - 4979, Loss - 0.7674888003418948, Learning Rate - 0.003125, magnitude of gradient - 0.890124289389899\n",
      "Step - 4980, Loss - 0.6238090036054821, Learning Rate - 0.003125, magnitude of gradient - 2.541137146242316\n",
      "Step - 4981, Loss - 0.5166238505379617, Learning Rate - 0.003125, magnitude of gradient - 2.073417512028666\n",
      "Step - 4982, Loss - 0.7823203548206124, Learning Rate - 0.003125, magnitude of gradient - 1.7577600276311836\n",
      "Step - 4983, Loss - 0.8177763993677133, Learning Rate - 0.003125, magnitude of gradient - 0.807181918908024\n",
      "Step - 4984, Loss - 0.7742360535390796, Learning Rate - 0.003125, magnitude of gradient - 0.8329974920314177\n",
      "Step - 4985, Loss - 0.7163398570093908, Learning Rate - 0.003125, magnitude of gradient - 1.3620911108667368\n",
      "Step - 4986, Loss - 0.5414869273731766, Learning Rate - 0.003125, magnitude of gradient - 1.5368769986509703\n",
      "Step - 4987, Loss - 0.5365771770611378, Learning Rate - 0.003125, magnitude of gradient - 0.634993663725989\n",
      "Step - 4988, Loss - 0.7129350330978765, Learning Rate - 0.003125, magnitude of gradient - 1.5012794510867171\n",
      "Step - 4989, Loss - 0.7931459446149616, Learning Rate - 0.003125, magnitude of gradient - 2.1271540801622395\n",
      "Step - 4990, Loss - 0.9266162960376263, Learning Rate - 0.003125, magnitude of gradient - 1.2636539019011301\n",
      "Step - 4991, Loss - 0.8695305090710445, Learning Rate - 0.003125, magnitude of gradient - 0.7775290116654457\n",
      "Step - 4992, Loss - 0.556282131564084, Learning Rate - 0.003125, magnitude of gradient - 1.0607804521774147\n",
      "Step - 4993, Loss - 0.598181032662444, Learning Rate - 0.003125, magnitude of gradient - 1.4359839400950971\n",
      "Step - 4994, Loss - 0.8156202593196566, Learning Rate - 0.003125, magnitude of gradient - 1.1747396198815323\n",
      "Step - 4995, Loss - 0.6489359678620588, Learning Rate - 0.003125, magnitude of gradient - 0.9262866168371194\n",
      "Step - 4996, Loss - 0.7001176055125631, Learning Rate - 0.003125, magnitude of gradient - 1.4574795681125616\n",
      "Step - 4997, Loss - 0.7773531693651266, Learning Rate - 0.003125, magnitude of gradient - 0.9656148773432285\n",
      "Step - 4998, Loss - 0.7061449069331754, Learning Rate - 0.003125, magnitude of gradient - 1.8011936587978523\n",
      "Step - 4999, Loss - 0.6842899683774103, Learning Rate - 0.003125, magnitude of gradient - 1.6956125761461185\n",
      "Step - 5000, Loss - 0.5534878274550382, Learning Rate - 0.003125, magnitude of gradient - 0.45941367683282036\n",
      "Step - 5001, Loss - 0.8485787048628844, Learning Rate - 0.0015625, magnitude of gradient - 0.5521430581904255\n",
      "Step - 5002, Loss - 0.5594346084631979, Learning Rate - 0.0015625, magnitude of gradient - 1.2235608385212295\n",
      "Step - 5003, Loss - 0.6694390904401966, Learning Rate - 0.0015625, magnitude of gradient - 1.1962227070891303\n",
      "Step - 5004, Loss - 0.699391463009349, Learning Rate - 0.0015625, magnitude of gradient - 1.411169236817787\n",
      "Step - 5005, Loss - 0.7781176105303675, Learning Rate - 0.0015625, magnitude of gradient - 0.7754402928866584\n",
      "Step - 5006, Loss - 0.7868637042286994, Learning Rate - 0.0015625, magnitude of gradient - 1.2147987511578713\n",
      "Step - 5007, Loss - 0.6161083736456504, Learning Rate - 0.0015625, magnitude of gradient - 1.6328863974154322\n",
      "Step - 5008, Loss - 0.7113351151693112, Learning Rate - 0.0015625, magnitude of gradient - 1.6856404967080958\n",
      "Step - 5009, Loss - 0.8775088954475517, Learning Rate - 0.0015625, magnitude of gradient - 1.056175132276642\n",
      "Step - 5010, Loss - 0.7272021030181183, Learning Rate - 0.0015625, magnitude of gradient - 0.8850722922220873\n",
      "Step - 5011, Loss - 0.5246060194659307, Learning Rate - 0.0015625, magnitude of gradient - 1.2553125166417958\n",
      "Step - 5012, Loss - 0.7473563893664866, Learning Rate - 0.0015625, magnitude of gradient - 2.7294380112984586\n",
      "Step - 5013, Loss - 0.7252020043421556, Learning Rate - 0.0015625, magnitude of gradient - 0.765325287494688\n",
      "Step - 5014, Loss - 0.6463472143567762, Learning Rate - 0.0015625, magnitude of gradient - 0.5238742049508384\n",
      "Step - 5015, Loss - 0.6298200958042975, Learning Rate - 0.0015625, magnitude of gradient - 0.8991009672946886\n",
      "Step - 5016, Loss - 0.6687225818557376, Learning Rate - 0.0015625, magnitude of gradient - 0.9136446213861376\n",
      "Step - 5017, Loss - 0.5180749291375872, Learning Rate - 0.0015625, magnitude of gradient - 2.7066966423624863\n",
      "Step - 5018, Loss - 0.7420926345381909, Learning Rate - 0.0015625, magnitude of gradient - 2.129323795744099\n",
      "Step - 5019, Loss - 0.7100938789586807, Learning Rate - 0.0015625, magnitude of gradient - 1.4271510455790266\n",
      "Step - 5020, Loss - 0.859884254578538, Learning Rate - 0.0015625, magnitude of gradient - 1.6517219379214367\n",
      "Step - 5021, Loss - 0.763493740076339, Learning Rate - 0.0015625, magnitude of gradient - 1.7662404251008421\n",
      "Step - 5022, Loss - 0.7974896463453837, Learning Rate - 0.0015625, magnitude of gradient - 1.4080430813593219\n",
      "Step - 5023, Loss - 0.6832692053907643, Learning Rate - 0.0015625, magnitude of gradient - 1.5103968293544228\n",
      "Step - 5024, Loss - 0.7309587644575779, Learning Rate - 0.0015625, magnitude of gradient - 1.5971261539890285\n",
      "Step - 5025, Loss - 0.6791868024618338, Learning Rate - 0.0015625, magnitude of gradient - 1.169221020609549\n",
      "Step - 5026, Loss - 0.6552447594761373, Learning Rate - 0.0015625, magnitude of gradient - 1.964573517533909\n",
      "Step - 5027, Loss - 0.6294252803434442, Learning Rate - 0.0015625, magnitude of gradient - 2.332361349817015\n",
      "Step - 5028, Loss - 0.4936800805764207, Learning Rate - 0.0015625, magnitude of gradient - 1.3592254558238268\n",
      "Step - 5029, Loss - 0.647190191919444, Learning Rate - 0.0015625, magnitude of gradient - 1.677929611303034\n",
      "Step - 5030, Loss - 0.6751067610593989, Learning Rate - 0.0015625, magnitude of gradient - 1.61624501698092\n",
      "Step - 5031, Loss - 0.4269429470554528, Learning Rate - 0.0015625, magnitude of gradient - 0.5475508354357932\n",
      "Step - 5032, Loss - 0.6846920151645827, Learning Rate - 0.0015625, magnitude of gradient - 1.0971614156798997\n",
      "Step - 5033, Loss - 0.6607899704932131, Learning Rate - 0.0015625, magnitude of gradient - 0.5831038499887233\n",
      "Step - 5034, Loss - 0.933083201315189, Learning Rate - 0.0015625, magnitude of gradient - 2.1158262959421434\n",
      "Step - 5035, Loss - 0.8238807976871204, Learning Rate - 0.0015625, magnitude of gradient - 2.3635401960551405\n",
      "Step - 5036, Loss - 0.6994342169779592, Learning Rate - 0.0015625, magnitude of gradient - 2.1840809292895464\n",
      "Step - 5037, Loss - 0.9607060546006038, Learning Rate - 0.0015625, magnitude of gradient - 1.2722956749028995\n",
      "Step - 5038, Loss - 0.7143515049210588, Learning Rate - 0.0015625, magnitude of gradient - 0.9966308084663072\n",
      "Step - 5039, Loss - 0.7383154595211834, Learning Rate - 0.0015625, magnitude of gradient - 1.1902197176742677\n",
      "Step - 5040, Loss - 0.6085261894607599, Learning Rate - 0.0015625, magnitude of gradient - 1.6637408237437838\n",
      "Step - 5041, Loss - 0.5895389914902267, Learning Rate - 0.0015625, magnitude of gradient - 0.5613040925147852\n",
      "Step - 5042, Loss - 0.981750483008171, Learning Rate - 0.0015625, magnitude of gradient - 1.4132927312832566\n",
      "Step - 5043, Loss - 0.7482291650892756, Learning Rate - 0.0015625, magnitude of gradient - 1.1066807594440842\n",
      "Step - 5044, Loss - 0.7646358803464716, Learning Rate - 0.0015625, magnitude of gradient - 0.8520020166979193\n",
      "Step - 5045, Loss - 0.6812595593136417, Learning Rate - 0.0015625, magnitude of gradient - 1.098917052725674\n",
      "Step - 5046, Loss - 0.6925486749704135, Learning Rate - 0.0015625, magnitude of gradient - 0.4129502617975399\n",
      "Step - 5047, Loss - 0.698927067921715, Learning Rate - 0.0015625, magnitude of gradient - 0.8641126183401401\n",
      "Step - 5048, Loss - 0.6775055573988329, Learning Rate - 0.0015625, magnitude of gradient - 0.9446156926442828\n",
      "Step - 5049, Loss - 0.8576620558576465, Learning Rate - 0.0015625, magnitude of gradient - 1.3424333713718877\n",
      "Step - 5050, Loss - 0.612079239711397, Learning Rate - 0.0015625, magnitude of gradient - 1.09036412970534\n",
      "Step - 5051, Loss - 0.5878227624322566, Learning Rate - 0.0015625, magnitude of gradient - 1.60073304075731\n",
      "Step - 5052, Loss - 0.6155073565953549, Learning Rate - 0.0015625, magnitude of gradient - 2.6521641864435637\n",
      "Step - 5053, Loss - 0.7038505033040902, Learning Rate - 0.0015625, magnitude of gradient - 0.8237491076787206\n",
      "Step - 5054, Loss - 0.7027613435192633, Learning Rate - 0.0015625, magnitude of gradient - 0.8512937982043489\n",
      "Step - 5055, Loss - 0.9525110620428223, Learning Rate - 0.0015625, magnitude of gradient - 1.6406439563318789\n",
      "Step - 5056, Loss - 0.8275853587564995, Learning Rate - 0.0015625, magnitude of gradient - 0.7623529023623312\n",
      "Step - 5057, Loss - 0.8619309178265102, Learning Rate - 0.0015625, magnitude of gradient - 0.705081686005463\n",
      "Step - 5058, Loss - 0.6662129757276771, Learning Rate - 0.0015625, magnitude of gradient - 0.4906545916902746\n",
      "Step - 5059, Loss - 0.5925994099380311, Learning Rate - 0.0015625, magnitude of gradient - 1.790303962980962\n",
      "Step - 5060, Loss - 0.7055396374705052, Learning Rate - 0.0015625, magnitude of gradient - 1.479155361036066\n",
      "Step - 5061, Loss - 0.6622983137510889, Learning Rate - 0.0015625, magnitude of gradient - 1.286586350435892\n",
      "Step - 5062, Loss - 0.8427601052299282, Learning Rate - 0.0015625, magnitude of gradient - 0.4700259994501215\n",
      "Step - 5063, Loss - 0.8394049646309224, Learning Rate - 0.0015625, magnitude of gradient - 1.2537835567499822\n",
      "Step - 5064, Loss - 0.6828527492559371, Learning Rate - 0.0015625, magnitude of gradient - 1.0292289064013036\n",
      "Step - 5065, Loss - 0.5749330632620347, Learning Rate - 0.0015625, magnitude of gradient - 1.8740245847987567\n",
      "Step - 5066, Loss - 0.6108196566956889, Learning Rate - 0.0015625, magnitude of gradient - 1.4725903024575253\n",
      "Step - 5067, Loss - 0.6727427567051496, Learning Rate - 0.0015625, magnitude of gradient - 1.3140700398538192\n",
      "Step - 5068, Loss - 0.6011585016072043, Learning Rate - 0.0015625, magnitude of gradient - 1.115225301818167\n",
      "Step - 5069, Loss - 0.6281814735820838, Learning Rate - 0.0015625, magnitude of gradient - 0.42504816665763084\n",
      "Step - 5070, Loss - 0.7021943594414876, Learning Rate - 0.0015625, magnitude of gradient - 1.1420391347118903\n",
      "Step - 5071, Loss - 0.8381990770073058, Learning Rate - 0.0015625, magnitude of gradient - 2.2984558199706826\n",
      "Step - 5072, Loss - 0.6772427994265051, Learning Rate - 0.0015625, magnitude of gradient - 1.534202308305774\n",
      "Step - 5073, Loss - 0.840755342400374, Learning Rate - 0.0015625, magnitude of gradient - 2.482465305227854\n",
      "Step - 5074, Loss - 0.5224003036258806, Learning Rate - 0.0015625, magnitude of gradient - 1.2579138824274387\n",
      "Step - 5075, Loss - 0.8788533245909758, Learning Rate - 0.0015625, magnitude of gradient - 1.4674485546410112\n",
      "Step - 5076, Loss - 0.6535058097856181, Learning Rate - 0.0015625, magnitude of gradient - 1.497136816526994\n",
      "Step - 5077, Loss - 0.6072426786973706, Learning Rate - 0.0015625, magnitude of gradient - 1.3594721972075745\n",
      "Step - 5078, Loss - 0.7594348517574905, Learning Rate - 0.0015625, magnitude of gradient - 1.2337112058235145\n",
      "Step - 5079, Loss - 0.4337664003217067, Learning Rate - 0.0015625, magnitude of gradient - 0.6645797506450988\n",
      "Step - 5080, Loss - 0.6100877027354554, Learning Rate - 0.0015625, magnitude of gradient - 0.4128663802708465\n",
      "Step - 5081, Loss - 0.7641291433499642, Learning Rate - 0.0015625, magnitude of gradient - 1.4991827636533797\n",
      "Step - 5082, Loss - 0.7232215888328204, Learning Rate - 0.0015625, magnitude of gradient - 1.404039960335314\n",
      "Step - 5083, Loss - 0.6196518839373475, Learning Rate - 0.0015625, magnitude of gradient - 1.3821023456992478\n",
      "Step - 5084, Loss - 0.5347297160336638, Learning Rate - 0.0015625, magnitude of gradient - 1.582051486282858\n",
      "Step - 5085, Loss - 0.9641506084807536, Learning Rate - 0.0015625, magnitude of gradient - 1.4674306062288678\n",
      "Step - 5086, Loss - 0.7314531886138265, Learning Rate - 0.0015625, magnitude of gradient - 1.0329689202010786\n",
      "Step - 5087, Loss - 0.9740986911941472, Learning Rate - 0.0015625, magnitude of gradient - 0.5272668213273648\n",
      "Step - 5088, Loss - 0.6537540682107088, Learning Rate - 0.0015625, magnitude of gradient - 0.9262153239434806\n",
      "Step - 5089, Loss - 0.8142168381225706, Learning Rate - 0.0015625, magnitude of gradient - 2.3532648484268828\n",
      "Step - 5090, Loss - 0.6426640594894476, Learning Rate - 0.0015625, magnitude of gradient - 1.1635552166110046\n",
      "Step - 5091, Loss - 0.44482970839415636, Learning Rate - 0.0015625, magnitude of gradient - 1.0704609850527909\n",
      "Step - 5092, Loss - 0.7735815553509776, Learning Rate - 0.0015625, magnitude of gradient - 1.370880022997526\n",
      "Step - 5093, Loss - 0.7730363779654115, Learning Rate - 0.0015625, magnitude of gradient - 1.5231549453746709\n",
      "Step - 5094, Loss - 0.749909286413975, Learning Rate - 0.0015625, magnitude of gradient - 2.4140508506761846\n",
      "Step - 5095, Loss - 0.6068632818790618, Learning Rate - 0.0015625, magnitude of gradient - 1.9432819654939435\n",
      "Step - 5096, Loss - 0.5873091631697307, Learning Rate - 0.0015625, magnitude of gradient - 1.6521989466418152\n",
      "Step - 5097, Loss - 0.6258277649541264, Learning Rate - 0.0015625, magnitude of gradient - 1.7568204240863188\n",
      "Step - 5098, Loss - 0.8425879010730091, Learning Rate - 0.0015625, magnitude of gradient - 0.9964156138911267\n",
      "Step - 5099, Loss - 0.5791646983294695, Learning Rate - 0.0015625, magnitude of gradient - 0.8418334734346699\n",
      "Step - 5100, Loss - 0.5694864643871086, Learning Rate - 0.0015625, magnitude of gradient - 0.35115287963211456\n",
      "Step - 5101, Loss - 0.6766366217917947, Learning Rate - 0.0015625, magnitude of gradient - 1.6396175025772826\n",
      "Step - 5102, Loss - 0.5987413240173445, Learning Rate - 0.0015625, magnitude of gradient - 1.5561099359508592\n",
      "Step - 5103, Loss - 0.7366971695236911, Learning Rate - 0.0015625, magnitude of gradient - 0.9763705822070372\n",
      "Step - 5104, Loss - 0.6853297913695654, Learning Rate - 0.0015625, magnitude of gradient - 1.4249953489501652\n",
      "Step - 5105, Loss - 0.663073709100942, Learning Rate - 0.0015625, magnitude of gradient - 0.25508678477875923\n",
      "Step - 5106, Loss - 0.7095581249040213, Learning Rate - 0.0015625, magnitude of gradient - 0.8479523152585944\n",
      "Step - 5107, Loss - 0.8439151458799984, Learning Rate - 0.0015625, magnitude of gradient - 0.6085579232258166\n",
      "Step - 5108, Loss - 0.6818181417530744, Learning Rate - 0.0015625, magnitude of gradient - 0.44731859248927636\n",
      "Step - 5109, Loss - 0.83099813573223, Learning Rate - 0.0015625, magnitude of gradient - 1.1948399627624628\n",
      "Step - 5110, Loss - 0.6710732016254704, Learning Rate - 0.0015625, magnitude of gradient - 1.5502015794649877\n",
      "Step - 5111, Loss - 0.6517722452248522, Learning Rate - 0.0015625, magnitude of gradient - 0.8862863381284739\n",
      "Step - 5112, Loss - 0.6761178395906224, Learning Rate - 0.0015625, magnitude of gradient - 0.6705499462326475\n",
      "Step - 5113, Loss - 0.7706270049983108, Learning Rate - 0.0015625, magnitude of gradient - 0.8243790455861708\n",
      "Step - 5114, Loss - 0.5456355521623155, Learning Rate - 0.0015625, magnitude of gradient - 2.019240566341938\n",
      "Step - 5115, Loss - 0.839766550670402, Learning Rate - 0.0015625, magnitude of gradient - 1.6563437520140876\n",
      "Step - 5116, Loss - 0.6640199952463897, Learning Rate - 0.0015625, magnitude of gradient - 1.358135961093562\n",
      "Step - 5117, Loss - 0.591740434721636, Learning Rate - 0.0015625, magnitude of gradient - 0.5038134177312801\n",
      "Step - 5118, Loss - 0.7673161154035302, Learning Rate - 0.0015625, magnitude of gradient - 1.073551260207409\n",
      "Step - 5119, Loss - 0.6409094797731234, Learning Rate - 0.0015625, magnitude of gradient - 1.788972192184259\n",
      "Step - 5120, Loss - 0.7998365765261014, Learning Rate - 0.0015625, magnitude of gradient - 0.5123615081909874\n",
      "Step - 5121, Loss - 0.73917466339415, Learning Rate - 0.0015625, magnitude of gradient - 1.087031034408285\n",
      "Step - 5122, Loss - 0.7260727030430585, Learning Rate - 0.0015625, magnitude of gradient - 1.2109299749815374\n",
      "Step - 5123, Loss - 0.6212009028841736, Learning Rate - 0.0015625, magnitude of gradient - 0.8357591072113196\n",
      "Step - 5124, Loss - 0.6042262078447511, Learning Rate - 0.0015625, magnitude of gradient - 1.128938751742465\n",
      "Step - 5125, Loss - 0.493305254669532, Learning Rate - 0.0015625, magnitude of gradient - 0.765541030070194\n",
      "Step - 5126, Loss - 0.6004624904197144, Learning Rate - 0.0015625, magnitude of gradient - 1.1878432281113467\n",
      "Step - 5127, Loss - 0.7768881542049645, Learning Rate - 0.0015625, magnitude of gradient - 0.3799452448349505\n",
      "Step - 5128, Loss - 0.3872436690221132, Learning Rate - 0.0015625, magnitude of gradient - 1.7185635934503571\n",
      "Step - 5129, Loss - 0.6999697989401419, Learning Rate - 0.0015625, magnitude of gradient - 1.158845121283072\n",
      "Step - 5130, Loss - 0.6247520907811468, Learning Rate - 0.0015625, magnitude of gradient - 0.8511465248526764\n",
      "Step - 5131, Loss - 0.8436357707432164, Learning Rate - 0.0015625, magnitude of gradient - 1.948617835655131\n",
      "Step - 5132, Loss - 0.662699602267328, Learning Rate - 0.0015625, magnitude of gradient - 1.7994616993387813\n",
      "Step - 5133, Loss - 0.778763820300218, Learning Rate - 0.0015625, magnitude of gradient - 0.7036350561877303\n",
      "Step - 5134, Loss - 0.7319565048543012, Learning Rate - 0.0015625, magnitude of gradient - 1.3666071800644275\n",
      "Step - 5135, Loss - 0.8212510929864787, Learning Rate - 0.0015625, magnitude of gradient - 0.6646817625934808\n",
      "Step - 5136, Loss - 0.7753926270050701, Learning Rate - 0.0015625, magnitude of gradient - 1.9622626786784088\n",
      "Step - 5137, Loss - 0.6472619874678174, Learning Rate - 0.0015625, magnitude of gradient - 1.92564932051143\n",
      "Step - 5138, Loss - 0.6254026658462447, Learning Rate - 0.0015625, magnitude of gradient - 1.4111874834863891\n",
      "Step - 5139, Loss - 0.4816279188879288, Learning Rate - 0.0015625, magnitude of gradient - 0.9621197487047689\n",
      "Step - 5140, Loss - 0.7481849521831672, Learning Rate - 0.0015625, magnitude of gradient - 1.9136850412057307\n",
      "Step - 5141, Loss - 0.6329311782880888, Learning Rate - 0.0015625, magnitude of gradient - 0.7255186434285154\n",
      "Step - 5142, Loss - 0.6707038024350129, Learning Rate - 0.0015625, magnitude of gradient - 2.682232689319433\n",
      "Step - 5143, Loss - 0.5549429595438438, Learning Rate - 0.0015625, magnitude of gradient - 0.9905287865083299\n",
      "Step - 5144, Loss - 0.6507262455495448, Learning Rate - 0.0015625, magnitude of gradient - 0.9578022039813632\n",
      "Step - 5145, Loss - 0.6024699720842268, Learning Rate - 0.0015625, magnitude of gradient - 1.619894119796764\n",
      "Step - 5146, Loss - 0.7381517446057854, Learning Rate - 0.0015625, magnitude of gradient - 2.222080968424829\n",
      "Step - 5147, Loss - 0.7305566436004894, Learning Rate - 0.0015625, magnitude of gradient - 0.9733202551468472\n",
      "Step - 5148, Loss - 0.5893655633725806, Learning Rate - 0.0015625, magnitude of gradient - 1.3558494773127867\n",
      "Step - 5149, Loss - 0.7505454032169634, Learning Rate - 0.0015625, magnitude of gradient - 0.4269773061069316\n",
      "Step - 5150, Loss - 0.7682375875731605, Learning Rate - 0.0015625, magnitude of gradient - 1.3086181519551756\n",
      "Step - 5151, Loss - 0.7755683449094996, Learning Rate - 0.0015625, magnitude of gradient - 0.5138509933132653\n",
      "Step - 5152, Loss - 0.6996334976111862, Learning Rate - 0.0015625, magnitude of gradient - 2.1096179170827236\n",
      "Step - 5153, Loss - 0.8282363403689575, Learning Rate - 0.0015625, magnitude of gradient - 1.8127248949552222\n",
      "Step - 5154, Loss - 0.6038244983937411, Learning Rate - 0.0015625, magnitude of gradient - 0.44423509898683583\n",
      "Step - 5155, Loss - 0.5870224135046146, Learning Rate - 0.0015625, magnitude of gradient - 0.557638473344905\n",
      "Step - 5156, Loss - 0.7871145811927723, Learning Rate - 0.0015625, magnitude of gradient - 0.5948512094333971\n",
      "Step - 5157, Loss - 0.532568585270004, Learning Rate - 0.0015625, magnitude of gradient - 0.301991919603536\n",
      "Step - 5158, Loss - 0.7487183425078331, Learning Rate - 0.0015625, magnitude of gradient - 1.0693906304140182\n",
      "Step - 5159, Loss - 0.6383285678734766, Learning Rate - 0.0015625, magnitude of gradient - 0.795807514587329\n",
      "Step - 5160, Loss - 0.5494772463716344, Learning Rate - 0.0015625, magnitude of gradient - 0.5833666483574577\n",
      "Step - 5161, Loss - 0.7716089523892397, Learning Rate - 0.0015625, magnitude of gradient - 0.8737920652917636\n",
      "Step - 5162, Loss - 0.7625555492141409, Learning Rate - 0.0015625, magnitude of gradient - 1.5793637026179\n",
      "Step - 5163, Loss - 0.6910912467328972, Learning Rate - 0.0015625, magnitude of gradient - 0.752034465462283\n",
      "Step - 5164, Loss - 0.6237439799034352, Learning Rate - 0.0015625, magnitude of gradient - 1.5483831557718202\n",
      "Step - 5165, Loss - 0.5653297218707448, Learning Rate - 0.0015625, magnitude of gradient - 1.5820784023106191\n",
      "Step - 5166, Loss - 0.7885581347404702, Learning Rate - 0.0015625, magnitude of gradient - 1.3624370137143207\n",
      "Step - 5167, Loss - 0.5889479081100139, Learning Rate - 0.0015625, magnitude of gradient - 1.3380996359206316\n",
      "Step - 5168, Loss - 0.6460443354032857, Learning Rate - 0.0015625, magnitude of gradient - 1.6953124332703882\n",
      "Step - 5169, Loss - 0.5036682828362306, Learning Rate - 0.0015625, magnitude of gradient - 0.9386755493960675\n",
      "Step - 5170, Loss - 0.6729617979161879, Learning Rate - 0.0015625, magnitude of gradient - 0.25756857347638545\n",
      "Step - 5171, Loss - 0.728688133940582, Learning Rate - 0.0015625, magnitude of gradient - 0.7078040554963843\n",
      "Step - 5172, Loss - 0.535127324174745, Learning Rate - 0.0015625, magnitude of gradient - 1.8137707614596301\n",
      "Step - 5173, Loss - 0.6519169537013139, Learning Rate - 0.0015625, magnitude of gradient - 0.5401878027817174\n",
      "Step - 5174, Loss - 0.6611095011491028, Learning Rate - 0.0015625, magnitude of gradient - 0.9408693944666497\n",
      "Step - 5175, Loss - 0.5237021368877535, Learning Rate - 0.0015625, magnitude of gradient - 1.9786651696966235\n",
      "Step - 5176, Loss - 0.5395804419055046, Learning Rate - 0.0015625, magnitude of gradient - 1.5174842286265702\n",
      "Step - 5177, Loss - 0.6881646005579605, Learning Rate - 0.0015625, magnitude of gradient - 1.276563670441107\n",
      "Step - 5178, Loss - 0.7332035952075008, Learning Rate - 0.0015625, magnitude of gradient - 0.8044936631086393\n",
      "Step - 5179, Loss - 0.8656539154748366, Learning Rate - 0.0015625, magnitude of gradient - 1.5402737559958353\n",
      "Step - 5180, Loss - 0.618023473518933, Learning Rate - 0.0015625, magnitude of gradient - 2.5067302522382624\n",
      "Step - 5181, Loss - 0.6232497166020258, Learning Rate - 0.0015625, magnitude of gradient - 0.7841594953781049\n",
      "Step - 5182, Loss - 0.5292550052992263, Learning Rate - 0.0015625, magnitude of gradient - 1.2712644078168114\n",
      "Step - 5183, Loss - 0.8005627433450615, Learning Rate - 0.0015625, magnitude of gradient - 2.2578839547386984\n",
      "Step - 5184, Loss - 0.6422566797726154, Learning Rate - 0.0015625, magnitude of gradient - 0.6565509714324175\n",
      "Step - 5185, Loss - 0.6717572311251381, Learning Rate - 0.0015625, magnitude of gradient - 0.5269895241043118\n",
      "Step - 5186, Loss - 0.72961269318581, Learning Rate - 0.0015625, magnitude of gradient - 1.1095367631538564\n",
      "Step - 5187, Loss - 0.5338419202035732, Learning Rate - 0.0015625, magnitude of gradient - 0.8770556291515682\n",
      "Step - 5188, Loss - 0.686057442791761, Learning Rate - 0.0015625, magnitude of gradient - 1.91391072316182\n",
      "Step - 5189, Loss - 0.6377903147516053, Learning Rate - 0.0015625, magnitude of gradient - 0.7467516776873959\n",
      "Step - 5190, Loss - 0.9076837948507588, Learning Rate - 0.0015625, magnitude of gradient - 1.7779155715422599\n",
      "Step - 5191, Loss - 0.718789742644754, Learning Rate - 0.0015625, magnitude of gradient - 0.6892470106667755\n",
      "Step - 5192, Loss - 0.8119408866763347, Learning Rate - 0.0015625, magnitude of gradient - 1.393741468526101\n",
      "Step - 5193, Loss - 0.5340447990744591, Learning Rate - 0.0015625, magnitude of gradient - 0.24883496394335444\n",
      "Step - 5194, Loss - 0.638473193406951, Learning Rate - 0.0015625, magnitude of gradient - 0.7502447188580227\n",
      "Step - 5195, Loss - 0.565728034513606, Learning Rate - 0.0015625, magnitude of gradient - 0.7334872324196053\n",
      "Step - 5196, Loss - 0.9615130869215097, Learning Rate - 0.0015625, magnitude of gradient - 2.142163460182443\n",
      "Step - 5197, Loss - 0.6820204626301278, Learning Rate - 0.0015625, magnitude of gradient - 2.713352739285807\n",
      "Step - 5198, Loss - 0.4713730532077297, Learning Rate - 0.0015625, magnitude of gradient - 1.207909163528575\n",
      "Step - 5199, Loss - 0.8410721077336935, Learning Rate - 0.0015625, magnitude of gradient - 0.6373708146244818\n",
      "Step - 5200, Loss - 0.7410867747658664, Learning Rate - 0.0015625, magnitude of gradient - 0.7673104401832712\n",
      "Step - 5201, Loss - 0.8420931461004184, Learning Rate - 0.0015625, magnitude of gradient - 0.774783482254263\n",
      "Step - 5202, Loss - 0.5825044748127786, Learning Rate - 0.0015625, magnitude of gradient - 1.4806555989994306\n",
      "Step - 5203, Loss - 0.6288158084302532, Learning Rate - 0.0015625, magnitude of gradient - 0.5569552377802616\n",
      "Step - 5204, Loss - 0.5337927328035859, Learning Rate - 0.0015625, magnitude of gradient - 1.158467288675319\n",
      "Step - 5205, Loss - 0.6620434656683949, Learning Rate - 0.0015625, magnitude of gradient - 0.4935862612648448\n",
      "Step - 5206, Loss - 0.7477302166262996, Learning Rate - 0.0015625, magnitude of gradient - 0.6850336268726758\n",
      "Step - 5207, Loss - 0.6769896128946526, Learning Rate - 0.0015625, magnitude of gradient - 1.6280438374120978\n",
      "Step - 5208, Loss - 0.7189513444594711, Learning Rate - 0.0015625, magnitude of gradient - 0.832915578517529\n",
      "Step - 5209, Loss - 0.9153310421269768, Learning Rate - 0.0015625, magnitude of gradient - 1.350928482828137\n",
      "Step - 5210, Loss - 0.7144321952131947, Learning Rate - 0.0015625, magnitude of gradient - 1.6268937751890407\n",
      "Step - 5211, Loss - 0.6962834304092816, Learning Rate - 0.0015625, magnitude of gradient - 0.4535563364236762\n",
      "Step - 5212, Loss - 0.5307496245586049, Learning Rate - 0.0015625, magnitude of gradient - 0.7135661997438135\n",
      "Step - 5213, Loss - 0.9487079698440604, Learning Rate - 0.0015625, magnitude of gradient - 0.5414464179946844\n",
      "Step - 5214, Loss - 0.7796892516948061, Learning Rate - 0.0015625, magnitude of gradient - 1.57210223287102\n",
      "Step - 5215, Loss - 0.6867594873517181, Learning Rate - 0.0015625, magnitude of gradient - 1.897715151571078\n",
      "Step - 5216, Loss - 0.7393744791438233, Learning Rate - 0.0015625, magnitude of gradient - 1.47980850542372\n",
      "Step - 5217, Loss - 0.6124864241549076, Learning Rate - 0.0015625, magnitude of gradient - 1.5976494589213965\n",
      "Step - 5218, Loss - 0.901520924453369, Learning Rate - 0.0015625, magnitude of gradient - 0.935229572924393\n",
      "Step - 5219, Loss - 0.6734770284751908, Learning Rate - 0.0015625, magnitude of gradient - 1.031765930197603\n",
      "Step - 5220, Loss - 0.8650235721545124, Learning Rate - 0.0015625, magnitude of gradient - 1.2184727415618797\n",
      "Step - 5221, Loss - 0.6464474552920731, Learning Rate - 0.0015625, magnitude of gradient - 1.4811796672081332\n",
      "Step - 5222, Loss - 0.6179303228131201, Learning Rate - 0.0015625, magnitude of gradient - 1.3327882472144055\n",
      "Step - 5223, Loss - 0.6862387481295789, Learning Rate - 0.0015625, magnitude of gradient - 2.440580938101717\n",
      "Step - 5224, Loss - 0.7619214266274068, Learning Rate - 0.0015625, magnitude of gradient - 0.6811176414418211\n",
      "Step - 5225, Loss - 0.4071939623507504, Learning Rate - 0.0015625, magnitude of gradient - 3.2635170925575983\n",
      "Step - 5226, Loss - 0.6034087568811223, Learning Rate - 0.0015625, magnitude of gradient - 1.882788239281857\n",
      "Step - 5227, Loss - 0.5725008732637125, Learning Rate - 0.0015625, magnitude of gradient - 0.5390607375774636\n",
      "Step - 5228, Loss - 0.696270444066015, Learning Rate - 0.0015625, magnitude of gradient - 1.057834616389787\n",
      "Step - 5229, Loss - 0.7363610048095324, Learning Rate - 0.0015625, magnitude of gradient - 1.2997490693960325\n",
      "Step - 5230, Loss - 0.603785263472408, Learning Rate - 0.0015625, magnitude of gradient - 0.536464039184312\n",
      "Step - 5231, Loss - 0.7134521104219798, Learning Rate - 0.0015625, magnitude of gradient - 2.1613259211258837\n",
      "Step - 5232, Loss - 0.5873998610566283, Learning Rate - 0.0015625, magnitude of gradient - 1.0126175771916266\n",
      "Step - 5233, Loss - 0.6553831036364223, Learning Rate - 0.0015625, magnitude of gradient - 1.2495174535739322\n",
      "Step - 5234, Loss - 0.7065311417637028, Learning Rate - 0.0015625, magnitude of gradient - 0.6563945155006704\n",
      "Step - 5235, Loss - 0.6918819787788353, Learning Rate - 0.0015625, magnitude of gradient - 1.6514553712673008\n",
      "Step - 5236, Loss - 0.5905890934284523, Learning Rate - 0.0015625, magnitude of gradient - 2.1617440118681612\n",
      "Step - 5237, Loss - 0.7585127595001484, Learning Rate - 0.0015625, magnitude of gradient - 1.4221754533121607\n",
      "Step - 5238, Loss - 0.8009150600276094, Learning Rate - 0.0015625, magnitude of gradient - 0.46873992325551495\n",
      "Step - 5239, Loss - 0.9021202698116036, Learning Rate - 0.0015625, magnitude of gradient - 1.9884303907240528\n",
      "Step - 5240, Loss - 1.1099520531195695, Learning Rate - 0.0015625, magnitude of gradient - 2.8219834171214693\n",
      "Step - 5241, Loss - 0.6699826161171869, Learning Rate - 0.0015625, magnitude of gradient - 1.5888307521750777\n",
      "Step - 5242, Loss - 0.8275535813653513, Learning Rate - 0.0015625, magnitude of gradient - 1.5769923041992013\n",
      "Step - 5243, Loss - 0.7806746345202796, Learning Rate - 0.0015625, magnitude of gradient - 1.9257534392837927\n",
      "Step - 5244, Loss - 0.6503922630090033, Learning Rate - 0.0015625, magnitude of gradient - 1.3409407749860698\n",
      "Step - 5245, Loss - 0.8326455709020447, Learning Rate - 0.0015625, magnitude of gradient - 2.292982023039015\n",
      "Step - 5246, Loss - 0.8244623850481294, Learning Rate - 0.0015625, magnitude of gradient - 2.1853007339085275\n",
      "Step - 5247, Loss - 0.6428877714117018, Learning Rate - 0.0015625, magnitude of gradient - 0.8657594699880075\n",
      "Step - 5248, Loss - 0.6068071292596263, Learning Rate - 0.0015625, magnitude of gradient - 0.5500773117397793\n",
      "Step - 5249, Loss - 0.7541362113789937, Learning Rate - 0.0015625, magnitude of gradient - 1.5759706592066616\n",
      "Step - 5250, Loss - 0.5860989591429976, Learning Rate - 0.0015625, magnitude of gradient - 0.37101407532545166\n",
      "Step - 5251, Loss - 0.6452305276139823, Learning Rate - 0.0015625, magnitude of gradient - 0.5220197767598408\n",
      "Step - 5252, Loss - 0.9507823980287725, Learning Rate - 0.0015625, magnitude of gradient - 1.6913075403731195\n",
      "Step - 5253, Loss - 0.8053123061511455, Learning Rate - 0.0015625, magnitude of gradient - 1.5049916005132251\n",
      "Step - 5254, Loss - 0.7935449332112339, Learning Rate - 0.0015625, magnitude of gradient - 0.9425591288868173\n",
      "Step - 5255, Loss - 0.6748373138997152, Learning Rate - 0.0015625, magnitude of gradient - 0.3173826299888907\n",
      "Step - 5256, Loss - 0.5536658122180954, Learning Rate - 0.0015625, magnitude of gradient - 1.1285338546783366\n",
      "Step - 5257, Loss - 0.6720738495671013, Learning Rate - 0.0015625, magnitude of gradient - 2.42534802792098\n",
      "Step - 5258, Loss - 0.6967865373078592, Learning Rate - 0.0015625, magnitude of gradient - 1.9910115979919145\n",
      "Step - 5259, Loss - 0.7982441380872357, Learning Rate - 0.0015625, magnitude of gradient - 0.4861254414335917\n",
      "Step - 5260, Loss - 0.7344315275111503, Learning Rate - 0.0015625, magnitude of gradient - 0.7034275824911664\n",
      "Step - 5261, Loss - 0.6597980875457665, Learning Rate - 0.0015625, magnitude of gradient - 1.3473304295077335\n",
      "Step - 5262, Loss - 0.7995355769644117, Learning Rate - 0.0015625, magnitude of gradient - 0.2170664392850701\n",
      "Step - 5263, Loss - 0.650497754343381, Learning Rate - 0.0015625, magnitude of gradient - 1.4497864905426296\n",
      "Step - 5264, Loss - 0.6951114481071804, Learning Rate - 0.0015625, magnitude of gradient - 2.008457458274959\n",
      "Step - 5265, Loss - 0.6077648532184116, Learning Rate - 0.0015625, magnitude of gradient - 0.9757213739308731\n",
      "Step - 5266, Loss - 0.5958818723752116, Learning Rate - 0.0015625, magnitude of gradient - 1.4667235813723685\n",
      "Step - 5267, Loss - 0.695521771265394, Learning Rate - 0.0015625, magnitude of gradient - 0.35033738436651896\n",
      "Step - 5268, Loss - 0.7150096418848535, Learning Rate - 0.0015625, magnitude of gradient - 0.9119984270631718\n",
      "Step - 5269, Loss - 0.9961574034989045, Learning Rate - 0.0015625, magnitude of gradient - 1.5423274054028089\n",
      "Step - 5270, Loss - 0.7498732082450414, Learning Rate - 0.0015625, magnitude of gradient - 1.8601077626302067\n",
      "Step - 5271, Loss - 0.6822731987318751, Learning Rate - 0.0015625, magnitude of gradient - 1.1187057956229265\n",
      "Step - 5272, Loss - 0.6587338369883236, Learning Rate - 0.0015625, magnitude of gradient - 1.671191339108017\n",
      "Step - 5273, Loss - 0.6065267412662341, Learning Rate - 0.0015625, magnitude of gradient - 1.188008663529563\n",
      "Step - 5274, Loss - 0.7970202562932087, Learning Rate - 0.0015625, magnitude of gradient - 1.0137239124659347\n",
      "Step - 5275, Loss - 0.6686165635084205, Learning Rate - 0.0015625, magnitude of gradient - 1.8704031471634046\n",
      "Step - 5276, Loss - 0.9445024804343725, Learning Rate - 0.0015625, magnitude of gradient - 0.4436062046205134\n",
      "Step - 5277, Loss - 0.7861606833277757, Learning Rate - 0.0015625, magnitude of gradient - 1.179873519694887\n",
      "Step - 5278, Loss - 0.6921484978964336, Learning Rate - 0.0015625, magnitude of gradient - 1.218580409203478\n",
      "Step - 5279, Loss - 0.5935901432141177, Learning Rate - 0.0015625, magnitude of gradient - 0.702804469812747\n",
      "Step - 5280, Loss - 0.7443130556115879, Learning Rate - 0.0015625, magnitude of gradient - 0.6470838532899726\n",
      "Step - 5281, Loss - 0.6713298587732307, Learning Rate - 0.0015625, magnitude of gradient - 0.7820495520553079\n",
      "Step - 5282, Loss - 0.6930699529680028, Learning Rate - 0.0015625, magnitude of gradient - 2.765603681655725\n",
      "Step - 5283, Loss - 0.5595792580996379, Learning Rate - 0.0015625, magnitude of gradient - 0.6297734204918003\n",
      "Step - 5284, Loss - 1.0116915975615925, Learning Rate - 0.0015625, magnitude of gradient - 1.820673675274899\n",
      "Step - 5285, Loss - 0.575487369626545, Learning Rate - 0.0015625, magnitude of gradient - 1.663211264933913\n",
      "Step - 5286, Loss - 0.685139445543157, Learning Rate - 0.0015625, magnitude of gradient - 1.0160408676267494\n",
      "Step - 5287, Loss - 0.6418980425659137, Learning Rate - 0.0015625, magnitude of gradient - 1.0900285519143857\n",
      "Step - 5288, Loss - 0.539945762354994, Learning Rate - 0.0015625, magnitude of gradient - 1.0375500877624482\n",
      "Step - 5289, Loss - 0.7919812377818838, Learning Rate - 0.0015625, magnitude of gradient - 1.5229480594750329\n",
      "Step - 5290, Loss - 0.7384415483929573, Learning Rate - 0.0015625, magnitude of gradient - 1.0904334015253958\n",
      "Step - 5291, Loss - 0.7536467169661691, Learning Rate - 0.0015625, magnitude of gradient - 0.92018000660889\n",
      "Step - 5292, Loss - 0.714974043225579, Learning Rate - 0.0015625, magnitude of gradient - 1.0285524298507032\n",
      "Step - 5293, Loss - 0.6465085858032237, Learning Rate - 0.0015625, magnitude of gradient - 1.0997032394357869\n",
      "Step - 5294, Loss - 0.645107552475509, Learning Rate - 0.0015625, magnitude of gradient - 0.836293361571601\n",
      "Step - 5295, Loss - 0.6588258501745772, Learning Rate - 0.0015625, magnitude of gradient - 0.3898367034830132\n",
      "Step - 5296, Loss - 0.5754744543088396, Learning Rate - 0.0015625, magnitude of gradient - 1.563178197538599\n",
      "Step - 5297, Loss - 0.6538262654659834, Learning Rate - 0.0015625, magnitude of gradient - 2.6277849308142653\n",
      "Step - 5298, Loss - 0.6996800370075211, Learning Rate - 0.0015625, magnitude of gradient - 2.380711268414577\n",
      "Step - 5299, Loss - 0.7558166505659896, Learning Rate - 0.0015625, magnitude of gradient - 2.007391876260949\n",
      "Step - 5300, Loss - 0.8227488008162965, Learning Rate - 0.0015625, magnitude of gradient - 1.3702144158244034\n",
      "Step - 5301, Loss - 0.6295559797474918, Learning Rate - 0.0015625, magnitude of gradient - 1.0009550542207395\n",
      "Step - 5302, Loss - 0.5595323174068073, Learning Rate - 0.0015625, magnitude of gradient - 0.6877618341203883\n",
      "Step - 5303, Loss - 0.49721554329051987, Learning Rate - 0.0015625, magnitude of gradient - 1.3441529378370891\n",
      "Step - 5304, Loss - 0.7508258547341798, Learning Rate - 0.0015625, magnitude of gradient - 1.7227261296411456\n",
      "Step - 5305, Loss - 0.6393488029984882, Learning Rate - 0.0015625, magnitude of gradient - 1.5299703799646034\n",
      "Step - 5306, Loss - 0.7242314354195609, Learning Rate - 0.0015625, magnitude of gradient - 1.1011159090618374\n",
      "Step - 5307, Loss - 0.7580074631934339, Learning Rate - 0.0015625, magnitude of gradient - 1.4425453511429767\n",
      "Step - 5308, Loss - 0.7571312360317233, Learning Rate - 0.0015625, magnitude of gradient - 1.7909177354069818\n",
      "Step - 5309, Loss - 0.6218994434566361, Learning Rate - 0.0015625, magnitude of gradient - 0.5377693488312191\n",
      "Step - 5310, Loss - 0.8399713210943538, Learning Rate - 0.0015625, magnitude of gradient - 1.4460949509394125\n",
      "Step - 5311, Loss - 0.7265893822441524, Learning Rate - 0.0015625, magnitude of gradient - 1.1016950770575764\n",
      "Step - 5312, Loss - 0.7708586766398299, Learning Rate - 0.0015625, magnitude of gradient - 1.1254299951867444\n",
      "Step - 5313, Loss - 0.6491147568065299, Learning Rate - 0.0015625, magnitude of gradient - 1.689706100006656\n",
      "Step - 5314, Loss - 0.7832335537031707, Learning Rate - 0.0015625, magnitude of gradient - 2.8763822378584525\n",
      "Step - 5315, Loss - 0.5363828661269373, Learning Rate - 0.0015625, magnitude of gradient - 0.942424528134701\n",
      "Step - 5316, Loss - 0.6719211395296428, Learning Rate - 0.0015625, magnitude of gradient - 1.4752814472428584\n",
      "Step - 5317, Loss - 0.6628463743633894, Learning Rate - 0.0015625, magnitude of gradient - 0.4760153355618246\n",
      "Step - 5318, Loss - 0.6354134974929997, Learning Rate - 0.0015625, magnitude of gradient - 2.5752597896358376\n",
      "Step - 5319, Loss - 0.6149322643638615, Learning Rate - 0.0015625, magnitude of gradient - 0.34355904825940736\n",
      "Step - 5320, Loss - 0.6495795078360919, Learning Rate - 0.0015625, magnitude of gradient - 0.7520386558020506\n",
      "Step - 5321, Loss - 0.5528856601481467, Learning Rate - 0.0015625, magnitude of gradient - 1.6094180169780552\n",
      "Step - 5322, Loss - 0.652602892345904, Learning Rate - 0.0015625, magnitude of gradient - 2.420343693816413\n",
      "Step - 5323, Loss - 0.6844666259277086, Learning Rate - 0.0015625, magnitude of gradient - 1.2743542466442819\n",
      "Step - 5324, Loss - 0.6999097218239829, Learning Rate - 0.0015625, magnitude of gradient - 1.2321565258797402\n",
      "Step - 5325, Loss - 0.7758405707860061, Learning Rate - 0.0015625, magnitude of gradient - 2.015598929726681\n",
      "Step - 5326, Loss - 0.791962292211293, Learning Rate - 0.0015625, magnitude of gradient - 1.2747118578495167\n",
      "Step - 5327, Loss - 0.8603269314941846, Learning Rate - 0.0015625, magnitude of gradient - 1.6066180771791763\n",
      "Step - 5328, Loss - 0.7363458644075503, Learning Rate - 0.0015625, magnitude of gradient - 1.9956504535336865\n",
      "Step - 5329, Loss - 0.8836661632112182, Learning Rate - 0.0015625, magnitude of gradient - 1.8057952603907605\n",
      "Step - 5330, Loss - 0.7693335975826314, Learning Rate - 0.0015625, magnitude of gradient - 1.0217783859397036\n",
      "Step - 5331, Loss - 0.5679225318154966, Learning Rate - 0.0015625, magnitude of gradient - 1.7223994078975933\n",
      "Step - 5332, Loss - 0.668441959988795, Learning Rate - 0.0015625, magnitude of gradient - 0.7512837670771045\n",
      "Step - 5333, Loss - 0.5673694701181329, Learning Rate - 0.0015625, magnitude of gradient - 1.1126213810940624\n",
      "Step - 5334, Loss - 0.8064184468077111, Learning Rate - 0.0015625, magnitude of gradient - 0.921308398011646\n",
      "Step - 5335, Loss - 0.781135702254496, Learning Rate - 0.0015625, magnitude of gradient - 2.127929697736748\n",
      "Step - 5336, Loss - 0.6587532851885278, Learning Rate - 0.0015625, magnitude of gradient - 1.9897572451321708\n",
      "Step - 5337, Loss - 0.7127467042493653, Learning Rate - 0.0015625, magnitude of gradient - 0.8184673358809557\n",
      "Step - 5338, Loss - 0.6105825660517966, Learning Rate - 0.0015625, magnitude of gradient - 0.9736074928223413\n",
      "Step - 5339, Loss - 0.6856137797327844, Learning Rate - 0.0015625, magnitude of gradient - 1.9564634544279373\n",
      "Step - 5340, Loss - 0.7366923797313049, Learning Rate - 0.0015625, magnitude of gradient - 0.7234051421988542\n",
      "Step - 5341, Loss - 0.6669177835454592, Learning Rate - 0.0015625, magnitude of gradient - 1.6411771053099298\n",
      "Step - 5342, Loss - 0.6615166668935651, Learning Rate - 0.0015625, magnitude of gradient - 0.547697804799531\n",
      "Step - 5343, Loss - 0.7888153257064506, Learning Rate - 0.0015625, magnitude of gradient - 0.9441595527106713\n",
      "Step - 5344, Loss - 0.7684476098262597, Learning Rate - 0.0015625, magnitude of gradient - 1.685946374657789\n",
      "Step - 5345, Loss - 0.7226875112037265, Learning Rate - 0.0015625, magnitude of gradient - 0.8571952989259484\n",
      "Step - 5346, Loss - 0.8656538255100446, Learning Rate - 0.0015625, magnitude of gradient - 2.005275233492441\n",
      "Step - 5347, Loss - 0.780862121483625, Learning Rate - 0.0015625, magnitude of gradient - 1.224963245897784\n",
      "Step - 5348, Loss - 0.701664276458747, Learning Rate - 0.0015625, magnitude of gradient - 0.621876496804681\n",
      "Step - 5349, Loss - 0.6364879255349466, Learning Rate - 0.0015625, magnitude of gradient - 1.5767171379911662\n",
      "Step - 5350, Loss - 0.7015307511528088, Learning Rate - 0.0015625, magnitude of gradient - 0.961695612761826\n",
      "Step - 5351, Loss - 0.7042437303297399, Learning Rate - 0.0015625, magnitude of gradient - 1.7266839414965276\n",
      "Step - 5352, Loss - 0.6943264368696029, Learning Rate - 0.0015625, magnitude of gradient - 1.6445059144988026\n",
      "Step - 5353, Loss - 0.7325887420379493, Learning Rate - 0.0015625, magnitude of gradient - 2.2301466251050845\n",
      "Step - 5354, Loss - 0.7241000040171282, Learning Rate - 0.0015625, magnitude of gradient - 1.5387803913285942\n",
      "Step - 5355, Loss - 0.6142007149026335, Learning Rate - 0.0015625, magnitude of gradient - 1.266358704882982\n",
      "Step - 5356, Loss - 0.7112085556025325, Learning Rate - 0.0015625, magnitude of gradient - 3.01271935606387\n",
      "Step - 5357, Loss - 0.7441060167778137, Learning Rate - 0.0015625, magnitude of gradient - 0.8900077985934608\n",
      "Step - 5358, Loss - 0.8910858382559097, Learning Rate - 0.0015625, magnitude of gradient - 2.3789883889396344\n",
      "Step - 5359, Loss - 0.7062000251413962, Learning Rate - 0.0015625, magnitude of gradient - 0.5173118796571594\n",
      "Step - 5360, Loss - 0.7708797341130235, Learning Rate - 0.0015625, magnitude of gradient - 1.802545728391453\n",
      "Step - 5361, Loss - 0.6998678396141236, Learning Rate - 0.0015625, magnitude of gradient - 0.7041019115477697\n",
      "Step - 5362, Loss - 0.6842190968863504, Learning Rate - 0.0015625, magnitude of gradient - 1.1116078160223337\n",
      "Step - 5363, Loss - 0.5759412027624311, Learning Rate - 0.0015625, magnitude of gradient - 0.9439234376640953\n",
      "Step - 5364, Loss - 0.6555736263460094, Learning Rate - 0.0015625, magnitude of gradient - 2.042116414214885\n",
      "Step - 5365, Loss - 0.7743765229553461, Learning Rate - 0.0015625, magnitude of gradient - 1.2020057666294162\n",
      "Step - 5366, Loss - 0.7056194120672177, Learning Rate - 0.0015625, magnitude of gradient - 1.0007412423855542\n",
      "Step - 5367, Loss - 0.5128845854645677, Learning Rate - 0.0015625, magnitude of gradient - 0.9762289161497072\n",
      "Step - 5368, Loss - 0.6001765422357221, Learning Rate - 0.0015625, magnitude of gradient - 1.1426150139746976\n",
      "Step - 5369, Loss - 0.7437429055174551, Learning Rate - 0.0015625, magnitude of gradient - 1.6226203867268618\n",
      "Step - 5370, Loss - 0.5626348069419774, Learning Rate - 0.0015625, magnitude of gradient - 0.9505137251411293\n",
      "Step - 5371, Loss - 0.5951567322241944, Learning Rate - 0.0015625, magnitude of gradient - 1.8875656687318638\n",
      "Step - 5372, Loss - 0.8403374480726159, Learning Rate - 0.0015625, magnitude of gradient - 2.3416761588240527\n",
      "Step - 5373, Loss - 0.7383427248465388, Learning Rate - 0.0015625, magnitude of gradient - 0.7972826857125158\n",
      "Step - 5374, Loss - 0.7817042412631748, Learning Rate - 0.0015625, magnitude of gradient - 0.8506428186215221\n",
      "Step - 5375, Loss - 0.7423931429544928, Learning Rate - 0.0015625, magnitude of gradient - 0.8088828634812212\n",
      "Step - 5376, Loss - 0.5630005602691406, Learning Rate - 0.0015625, magnitude of gradient - 2.7007977527224813\n",
      "Step - 5377, Loss - 0.7370448270481743, Learning Rate - 0.0015625, magnitude of gradient - 1.251153614158816\n",
      "Step - 5378, Loss - 0.637899149827114, Learning Rate - 0.0015625, magnitude of gradient - 1.6089084404245189\n",
      "Step - 5379, Loss - 0.6962836551646404, Learning Rate - 0.0015625, magnitude of gradient - 1.6509275348157626\n",
      "Step - 5380, Loss - 0.7956407967703077, Learning Rate - 0.0015625, magnitude of gradient - 1.0717042476517513\n",
      "Step - 5381, Loss - 0.875332646169972, Learning Rate - 0.0015625, magnitude of gradient - 1.438284131337483\n",
      "Step - 5382, Loss - 0.8237139578643914, Learning Rate - 0.0015625, magnitude of gradient - 1.4537277336221748\n",
      "Step - 5383, Loss - 0.7205406207633648, Learning Rate - 0.0015625, magnitude of gradient - 1.245653192967055\n",
      "Step - 5384, Loss - 0.8493425760233911, Learning Rate - 0.0015625, magnitude of gradient - 1.688134545201699\n",
      "Step - 5385, Loss - 0.7251488257838979, Learning Rate - 0.0015625, magnitude of gradient - 0.9870272798269338\n",
      "Step - 5386, Loss - 0.8043375886209229, Learning Rate - 0.0015625, magnitude of gradient - 0.6012054556590565\n",
      "Step - 5387, Loss - 0.5041459769549539, Learning Rate - 0.0015625, magnitude of gradient - 1.6115218169167769\n",
      "Step - 5388, Loss - 0.7905967530888052, Learning Rate - 0.0015625, magnitude of gradient - 2.068251396860436\n",
      "Step - 5389, Loss - 0.733055206647506, Learning Rate - 0.0015625, magnitude of gradient - 0.7883524329706479\n",
      "Step - 5390, Loss - 0.8431741008499412, Learning Rate - 0.0015625, magnitude of gradient - 0.8386570177512666\n",
      "Step - 5391, Loss - 0.7582445975196143, Learning Rate - 0.0015625, magnitude of gradient - 0.46142602175945574\n",
      "Step - 5392, Loss - 0.7419632876507829, Learning Rate - 0.0015625, magnitude of gradient - 0.9141736545823987\n",
      "Step - 5393, Loss - 0.5910521269264157, Learning Rate - 0.0015625, magnitude of gradient - 2.340685151104922\n",
      "Step - 5394, Loss - 0.6336470293267205, Learning Rate - 0.0015625, magnitude of gradient - 0.7423905262164638\n",
      "Step - 5395, Loss - 0.7505906335847782, Learning Rate - 0.0015625, magnitude of gradient - 0.9299230501659113\n",
      "Step - 5396, Loss - 0.5785802599980885, Learning Rate - 0.0015625, magnitude of gradient - 0.8785091239499683\n",
      "Step - 5397, Loss - 0.6708611953392241, Learning Rate - 0.0015625, magnitude of gradient - 2.263803129411676\n",
      "Step - 5398, Loss - 0.7183674986674548, Learning Rate - 0.0015625, magnitude of gradient - 0.6854750534690036\n",
      "Step - 5399, Loss - 0.5289726805435305, Learning Rate - 0.0015625, magnitude of gradient - 2.031324558270682\n",
      "Step - 5400, Loss - 0.9292300107088232, Learning Rate - 0.0015625, magnitude of gradient - 1.0265609010274275\n",
      "Step - 5401, Loss - 0.7551187856672219, Learning Rate - 0.0015625, magnitude of gradient - 1.2447761639202333\n",
      "Step - 5402, Loss - 0.8580229007520539, Learning Rate - 0.0015625, magnitude of gradient - 2.4084637714853843\n",
      "Step - 5403, Loss - 0.8231809391788911, Learning Rate - 0.0015625, magnitude of gradient - 1.8070078462178187\n",
      "Step - 5404, Loss - 0.6680390372871302, Learning Rate - 0.0015625, magnitude of gradient - 0.8450915332613054\n",
      "Step - 5405, Loss - 0.6894762838767352, Learning Rate - 0.0015625, magnitude of gradient - 1.0541067457948405\n",
      "Step - 5406, Loss - 0.6854950295864317, Learning Rate - 0.0015625, magnitude of gradient - 0.9307954590572636\n",
      "Step - 5407, Loss - 0.7434691045948713, Learning Rate - 0.0015625, magnitude of gradient - 1.7150705862751503\n",
      "Step - 5408, Loss - 0.8409880150299368, Learning Rate - 0.0015625, magnitude of gradient - 1.6261574246153032\n",
      "Step - 5409, Loss - 0.654592187687959, Learning Rate - 0.0015625, magnitude of gradient - 1.1691658889938983\n",
      "Step - 5410, Loss - 0.5243245132630406, Learning Rate - 0.0015625, magnitude of gradient - 1.8448480932808553\n",
      "Step - 5411, Loss - 0.7311662120102725, Learning Rate - 0.0015625, magnitude of gradient - 0.7166909638989416\n",
      "Step - 5412, Loss - 0.8240737112367633, Learning Rate - 0.0015625, magnitude of gradient - 1.1447121859337728\n",
      "Step - 5413, Loss - 0.6164097369076211, Learning Rate - 0.0015625, magnitude of gradient - 1.418104918913931\n",
      "Step - 5414, Loss - 0.7079445634858403, Learning Rate - 0.0015625, magnitude of gradient - 1.3367666922398453\n",
      "Step - 5415, Loss - 0.6370806155327708, Learning Rate - 0.0015625, magnitude of gradient - 1.2850324747472373\n",
      "Step - 5416, Loss - 0.7517538060863656, Learning Rate - 0.0015625, magnitude of gradient - 0.8821517148534439\n",
      "Step - 5417, Loss - 0.610227891970224, Learning Rate - 0.0015625, magnitude of gradient - 1.6967588157072135\n",
      "Step - 5418, Loss - 0.76274396452223, Learning Rate - 0.0015625, magnitude of gradient - 0.6164044074794487\n",
      "Step - 5419, Loss - 0.7685555797852931, Learning Rate - 0.0015625, magnitude of gradient - 1.737597652775739\n",
      "Step - 5420, Loss - 0.7147638142702923, Learning Rate - 0.0015625, magnitude of gradient - 1.7377000312340083\n",
      "Step - 5421, Loss - 0.6711765486474498, Learning Rate - 0.0015625, magnitude of gradient - 0.39356003630395864\n",
      "Step - 5422, Loss - 0.677771781079105, Learning Rate - 0.0015625, magnitude of gradient - 1.0725226409517308\n",
      "Step - 5423, Loss - 1.046109889865393, Learning Rate - 0.0015625, magnitude of gradient - 1.3520070010875636\n",
      "Step - 5424, Loss - 0.6600176813614553, Learning Rate - 0.0015625, magnitude of gradient - 1.3173094563054624\n",
      "Step - 5425, Loss - 0.7413561589512083, Learning Rate - 0.0015625, magnitude of gradient - 1.0735313820538857\n",
      "Step - 5426, Loss - 0.715390698409969, Learning Rate - 0.0015625, magnitude of gradient - 3.5303248544281103\n",
      "Step - 5427, Loss - 0.6272107737954811, Learning Rate - 0.0015625, magnitude of gradient - 0.49571494227750873\n",
      "Step - 5428, Loss - 0.6712424960024345, Learning Rate - 0.0015625, magnitude of gradient - 2.7409722943280954\n",
      "Step - 5429, Loss - 0.6112598812259535, Learning Rate - 0.0015625, magnitude of gradient - 1.377778927691427\n",
      "Step - 5430, Loss - 0.9156611669148456, Learning Rate - 0.0015625, magnitude of gradient - 0.9746420896850169\n",
      "Step - 5431, Loss - 0.5904355509436351, Learning Rate - 0.0015625, magnitude of gradient - 1.3158129533355898\n",
      "Step - 5432, Loss - 0.6137908538399519, Learning Rate - 0.0015625, magnitude of gradient - 1.3861723147238074\n",
      "Step - 5433, Loss - 0.6482015020675467, Learning Rate - 0.0015625, magnitude of gradient - 1.3600689156315289\n",
      "Step - 5434, Loss - 0.7110454231515273, Learning Rate - 0.0015625, magnitude of gradient - 0.7449457544810439\n",
      "Step - 5435, Loss - 0.6911063884962004, Learning Rate - 0.0015625, magnitude of gradient - 1.3301810301435621\n",
      "Step - 5436, Loss - 0.5344298909939674, Learning Rate - 0.0015625, magnitude of gradient - 1.062758970329569\n",
      "Step - 5437, Loss - 0.6955325826889618, Learning Rate - 0.0015625, magnitude of gradient - 1.0487012852551885\n",
      "Step - 5438, Loss - 0.7148528041818538, Learning Rate - 0.0015625, magnitude of gradient - 0.9471485658701939\n",
      "Step - 5439, Loss - 0.45962699501291315, Learning Rate - 0.0015625, magnitude of gradient - 2.30972884181872\n",
      "Step - 5440, Loss - 0.8299154717799215, Learning Rate - 0.0015625, magnitude of gradient - 0.6838532012396829\n",
      "Step - 5441, Loss - 0.7707583248517482, Learning Rate - 0.0015625, magnitude of gradient - 0.09590631987932742\n",
      "Step - 5442, Loss - 0.6346698477118051, Learning Rate - 0.0015625, magnitude of gradient - 0.6666166398569972\n",
      "Step - 5443, Loss - 0.667101634846667, Learning Rate - 0.0015625, magnitude of gradient - 2.7853081370889456\n",
      "Step - 5444, Loss - 0.8643989688579377, Learning Rate - 0.0015625, magnitude of gradient - 0.8581931118272047\n",
      "Step - 5445, Loss - 0.7665238164401811, Learning Rate - 0.0015625, magnitude of gradient - 1.0615109358605992\n",
      "Step - 5446, Loss - 0.802981880247342, Learning Rate - 0.0015625, magnitude of gradient - 1.6402501420217648\n",
      "Step - 5447, Loss - 0.6608985175653236, Learning Rate - 0.0015625, magnitude of gradient - 1.3172289145685285\n",
      "Step - 5448, Loss - 0.8089109110465413, Learning Rate - 0.0015625, magnitude of gradient - 1.4961197118371523\n",
      "Step - 5449, Loss - 0.7083581616240998, Learning Rate - 0.0015625, magnitude of gradient - 2.0153469002269144\n",
      "Step - 5450, Loss - 0.9732365771815347, Learning Rate - 0.0015625, magnitude of gradient - 1.6379244043889103\n",
      "Step - 5451, Loss - 0.7274581382352956, Learning Rate - 0.0015625, magnitude of gradient - 1.1755748261938601\n",
      "Step - 5452, Loss - 0.6223765830528661, Learning Rate - 0.0015625, magnitude of gradient - 1.3025557388410545\n",
      "Step - 5453, Loss - 0.8165441729495095, Learning Rate - 0.0015625, magnitude of gradient - 1.4572271596346031\n",
      "Step - 5454, Loss - 0.7443437299625881, Learning Rate - 0.0015625, magnitude of gradient - 2.4891680516366033\n",
      "Step - 5455, Loss - 0.67424329117194, Learning Rate - 0.0015625, magnitude of gradient - 0.5681455358014049\n",
      "Step - 5456, Loss - 0.6952724333985649, Learning Rate - 0.0015625, magnitude of gradient - 1.3750370396824017\n",
      "Step - 5457, Loss - 0.5411724787747777, Learning Rate - 0.0015625, magnitude of gradient - 1.934360049022265\n",
      "Step - 5458, Loss - 0.7009831342091734, Learning Rate - 0.0015625, magnitude of gradient - 1.3981980142613546\n",
      "Step - 5459, Loss - 0.7549004195809679, Learning Rate - 0.0015625, magnitude of gradient - 1.2593662523275677\n",
      "Step - 5460, Loss - 0.6124201641381353, Learning Rate - 0.0015625, magnitude of gradient - 1.3875174281552127\n",
      "Step - 5461, Loss - 0.7371004011256513, Learning Rate - 0.0015625, magnitude of gradient - 0.7453310219447309\n",
      "Step - 5462, Loss - 0.755435236783291, Learning Rate - 0.0015625, magnitude of gradient - 1.4571132678743457\n",
      "Step - 5463, Loss - 0.6631396156200202, Learning Rate - 0.0015625, magnitude of gradient - 1.1574005827940188\n",
      "Step - 5464, Loss - 0.6457409420855701, Learning Rate - 0.0015625, magnitude of gradient - 1.632397528489084\n",
      "Step - 5465, Loss - 0.6410190802075266, Learning Rate - 0.0015625, magnitude of gradient - 1.2777461525196239\n",
      "Step - 5466, Loss - 0.7296347944787454, Learning Rate - 0.0015625, magnitude of gradient - 2.0154185203584616\n",
      "Step - 5467, Loss - 0.6549924072925689, Learning Rate - 0.0015625, magnitude of gradient - 2.556228695671982\n",
      "Step - 5468, Loss - 0.7990970014451417, Learning Rate - 0.0015625, magnitude of gradient - 1.518496000735029\n",
      "Step - 5469, Loss - 0.4993120331981158, Learning Rate - 0.0015625, magnitude of gradient - 1.0100339843654262\n",
      "Step - 5470, Loss - 0.6838451668911197, Learning Rate - 0.0015625, magnitude of gradient - 1.5757732213020532\n",
      "Step - 5471, Loss - 0.6847981558164181, Learning Rate - 0.0015625, magnitude of gradient - 1.4768416323145106\n",
      "Step - 5472, Loss - 0.7472014453422563, Learning Rate - 0.0015625, magnitude of gradient - 2.552652092686893\n",
      "Step - 5473, Loss - 0.8415289413676632, Learning Rate - 0.0015625, magnitude of gradient - 1.99114478653782\n",
      "Step - 5474, Loss - 0.6437059800885668, Learning Rate - 0.0015625, magnitude of gradient - 1.0244888498354694\n",
      "Step - 5475, Loss - 0.7714628859758634, Learning Rate - 0.0015625, magnitude of gradient - 0.4932352914676404\n",
      "Step - 5476, Loss - 0.7232146867736057, Learning Rate - 0.0015625, magnitude of gradient - 0.9268636157176907\n",
      "Step - 5477, Loss - 0.7609930067223797, Learning Rate - 0.0015625, magnitude of gradient - 1.6548510929757418\n",
      "Step - 5478, Loss - 0.7391247579602588, Learning Rate - 0.0015625, magnitude of gradient - 0.6736860859948084\n",
      "Step - 5479, Loss - 0.4878251370277907, Learning Rate - 0.0015625, magnitude of gradient - 3.8207883177082347\n",
      "Step - 5480, Loss - 0.6557112495301155, Learning Rate - 0.0015625, magnitude of gradient - 2.3133371284153745\n",
      "Step - 5481, Loss - 0.48582915911933083, Learning Rate - 0.0015625, magnitude of gradient - 1.1205227226216226\n",
      "Step - 5482, Loss - 0.6037257863579868, Learning Rate - 0.0015625, magnitude of gradient - 0.26352211649268925\n",
      "Step - 5483, Loss - 0.6290360396598718, Learning Rate - 0.0015625, magnitude of gradient - 1.0401537132674188\n",
      "Step - 5484, Loss - 0.6503603618617441, Learning Rate - 0.0015625, magnitude of gradient - 0.9004320116111089\n",
      "Step - 5485, Loss - 0.6864093388409692, Learning Rate - 0.0015625, magnitude of gradient - 1.9202149533208854\n",
      "Step - 5486, Loss - 0.7824823248021351, Learning Rate - 0.0015625, magnitude of gradient - 1.2547622964546052\n",
      "Step - 5487, Loss - 0.6179677848505853, Learning Rate - 0.0015625, magnitude of gradient - 0.5803231925330686\n",
      "Step - 5488, Loss - 0.6572059761211695, Learning Rate - 0.0015625, magnitude of gradient - 0.7335794635051404\n",
      "Step - 5489, Loss - 0.6141617618438893, Learning Rate - 0.0015625, magnitude of gradient - 1.4168531029889457\n",
      "Step - 5490, Loss - 0.6302241714576295, Learning Rate - 0.0015625, magnitude of gradient - 2.4635229483959256\n",
      "Step - 5491, Loss - 0.7315587320578641, Learning Rate - 0.0015625, magnitude of gradient - 1.7718213020504485\n",
      "Step - 5492, Loss - 0.7340772384097373, Learning Rate - 0.0015625, magnitude of gradient - 0.8087880938804526\n",
      "Step - 5493, Loss - 0.7516892615555426, Learning Rate - 0.0015625, magnitude of gradient - 1.4447760221848376\n",
      "Step - 5494, Loss - 0.8708342771904964, Learning Rate - 0.0015625, magnitude of gradient - 1.100993144312867\n",
      "Step - 5495, Loss - 0.8217119648406682, Learning Rate - 0.0015625, magnitude of gradient - 1.427592809999796\n",
      "Step - 5496, Loss - 0.8376466681471073, Learning Rate - 0.0015625, magnitude of gradient - 1.8801753071581855\n",
      "Step - 5497, Loss - 0.5762233994920264, Learning Rate - 0.0015625, magnitude of gradient - 1.9134788330143415\n",
      "Step - 5498, Loss - 0.591627025051882, Learning Rate - 0.0015625, magnitude of gradient - 0.6028500561741632\n",
      "Step - 5499, Loss - 0.9208049284375432, Learning Rate - 0.0015625, magnitude of gradient - 0.9535205528443519\n",
      "Step - 5500, Loss - 0.7940434944274052, Learning Rate - 0.0015625, magnitude of gradient - 0.8352251087939266\n",
      "Step - 5501, Loss - 0.835511556900322, Learning Rate - 0.0015625, magnitude of gradient - 1.5323316514027017\n",
      "Step - 5502, Loss - 0.6250975265752275, Learning Rate - 0.0015625, magnitude of gradient - 0.45481746379452337\n",
      "Step - 5503, Loss - 0.8006860043306603, Learning Rate - 0.0015625, magnitude of gradient - 1.607548826956026\n",
      "Step - 5504, Loss - 0.736156779252281, Learning Rate - 0.0015625, magnitude of gradient - 2.173725196460774\n",
      "Step - 5505, Loss - 0.7652973270447768, Learning Rate - 0.0015625, magnitude of gradient - 2.1424455784350216\n",
      "Step - 5506, Loss - 0.7872765095947297, Learning Rate - 0.0015625, magnitude of gradient - 1.4807227087479147\n",
      "Step - 5507, Loss - 0.708665379616564, Learning Rate - 0.0015625, magnitude of gradient - 0.916329373638801\n",
      "Step - 5508, Loss - 0.7406001129660487, Learning Rate - 0.0015625, magnitude of gradient - 1.5225547133633812\n",
      "Step - 5509, Loss - 0.6420322990881587, Learning Rate - 0.0015625, magnitude of gradient - 1.4633990169283542\n",
      "Step - 5510, Loss - 0.6151657251791587, Learning Rate - 0.0015625, magnitude of gradient - 1.9452347248275121\n",
      "Step - 5511, Loss - 0.6694252567746256, Learning Rate - 0.0015625, magnitude of gradient - 0.5631160294732547\n",
      "Step - 5512, Loss - 0.6443846503127812, Learning Rate - 0.0015625, magnitude of gradient - 1.4442971929885382\n",
      "Step - 5513, Loss - 0.7086292782159292, Learning Rate - 0.0015625, magnitude of gradient - 1.5831708355805094\n",
      "Step - 5514, Loss - 0.866509706548819, Learning Rate - 0.0015625, magnitude of gradient - 0.8176098772529405\n",
      "Step - 5515, Loss - 0.7483749872595007, Learning Rate - 0.0015625, magnitude of gradient - 0.7370203184445597\n",
      "Step - 5516, Loss - 0.6943789765594157, Learning Rate - 0.0015625, magnitude of gradient - 0.9847339136590646\n",
      "Step - 5517, Loss - 0.5561118977517013, Learning Rate - 0.0015625, magnitude of gradient - 1.2923369872539041\n",
      "Step - 5518, Loss - 0.5813366113021753, Learning Rate - 0.0015625, magnitude of gradient - 0.863659856641633\n",
      "Step - 5519, Loss - 0.7475801199079484, Learning Rate - 0.0015625, magnitude of gradient - 1.7338580369087726\n",
      "Step - 5520, Loss - 0.8544504667310946, Learning Rate - 0.0015625, magnitude of gradient - 0.7107731876913893\n",
      "Step - 5521, Loss - 0.7059566547708115, Learning Rate - 0.0015625, magnitude of gradient - 0.5647635260019899\n",
      "Step - 5522, Loss - 0.6529060837795984, Learning Rate - 0.0015625, magnitude of gradient - 1.3720118084377393\n",
      "Step - 5523, Loss - 0.7660575058103491, Learning Rate - 0.0015625, magnitude of gradient - 0.5665597516991641\n",
      "Step - 5524, Loss - 0.8829995890430932, Learning Rate - 0.0015625, magnitude of gradient - 1.0052144416067872\n",
      "Step - 5525, Loss - 0.7816150573185232, Learning Rate - 0.0015625, magnitude of gradient - 2.8653918791509247\n",
      "Step - 5526, Loss - 0.6040355470279547, Learning Rate - 0.0015625, magnitude of gradient - 0.7892015025993737\n",
      "Step - 5527, Loss - 0.9073229290142113, Learning Rate - 0.0015625, magnitude of gradient - 1.049646807295603\n",
      "Step - 5528, Loss - 0.5739406293074073, Learning Rate - 0.0015625, magnitude of gradient - 1.2318897766073449\n",
      "Step - 5529, Loss - 0.6646985988206787, Learning Rate - 0.0015625, magnitude of gradient - 1.090079913142876\n",
      "Step - 5530, Loss - 0.7664893027275754, Learning Rate - 0.0015625, magnitude of gradient - 0.8695432226369446\n",
      "Step - 5531, Loss - 0.8123675419822693, Learning Rate - 0.0015625, magnitude of gradient - 1.1012706139729502\n",
      "Step - 5532, Loss - 0.6421810141912256, Learning Rate - 0.0015625, magnitude of gradient - 2.246201926745398\n",
      "Step - 5533, Loss - 0.6062283722302778, Learning Rate - 0.0015625, magnitude of gradient - 1.1175006295495804\n",
      "Step - 5534, Loss - 0.6342088921096999, Learning Rate - 0.0015625, magnitude of gradient - 1.4063979805177043\n",
      "Step - 5535, Loss - 0.8736867056213234, Learning Rate - 0.0015625, magnitude of gradient - 1.5955623419438987\n",
      "Step - 5536, Loss - 0.570531485951606, Learning Rate - 0.0015625, magnitude of gradient - 1.8803105438729777\n",
      "Step - 5537, Loss - 0.7487618365506876, Learning Rate - 0.0015625, magnitude of gradient - 1.5425803717913804\n",
      "Step - 5538, Loss - 0.4725572639561809, Learning Rate - 0.0015625, magnitude of gradient - 0.9991675826420862\n",
      "Step - 5539, Loss - 0.5785341780598376, Learning Rate - 0.0015625, magnitude of gradient - 1.174798287676519\n",
      "Step - 5540, Loss - 0.9719858932062077, Learning Rate - 0.0015625, magnitude of gradient - 1.850081434033737\n",
      "Step - 5541, Loss - 0.9151916299221389, Learning Rate - 0.0015625, magnitude of gradient - 1.1714846989804664\n",
      "Step - 5542, Loss - 0.7606646486927848, Learning Rate - 0.0015625, magnitude of gradient - 1.9417614678190012\n",
      "Step - 5543, Loss - 0.7009359538345732, Learning Rate - 0.0015625, magnitude of gradient - 0.6091198149046836\n",
      "Step - 5544, Loss - 0.6720365880132276, Learning Rate - 0.0015625, magnitude of gradient - 1.3267000920804617\n",
      "Step - 5545, Loss - 0.6659614329646537, Learning Rate - 0.0015625, magnitude of gradient - 0.7218012478310507\n",
      "Step - 5546, Loss - 0.8232435236227205, Learning Rate - 0.0015625, magnitude of gradient - 1.593627553006558\n",
      "Step - 5547, Loss - 0.6522288625398448, Learning Rate - 0.0015625, magnitude of gradient - 0.9613662540744167\n",
      "Step - 5548, Loss - 0.8579890699828293, Learning Rate - 0.0015625, magnitude of gradient - 0.597831780976366\n",
      "Step - 5549, Loss - 0.6176257211951517, Learning Rate - 0.0015625, magnitude of gradient - 1.1804978923976082\n",
      "Step - 5550, Loss - 0.6115937316706841, Learning Rate - 0.0015625, magnitude of gradient - 1.343801378601705\n",
      "Step - 5551, Loss - 0.77269378075842, Learning Rate - 0.0015625, magnitude of gradient - 1.9576956856891985\n",
      "Step - 5552, Loss - 0.7742240767083753, Learning Rate - 0.0015625, magnitude of gradient - 0.939344176100924\n",
      "Step - 5553, Loss - 0.7245825152065256, Learning Rate - 0.0015625, magnitude of gradient - 2.602970833080341\n",
      "Step - 5554, Loss - 0.6938564681674241, Learning Rate - 0.0015625, magnitude of gradient - 1.3125340569152237\n",
      "Step - 5555, Loss - 0.6586914839012565, Learning Rate - 0.0015625, magnitude of gradient - 1.4724456397682313\n",
      "Step - 5556, Loss - 0.6764365172318286, Learning Rate - 0.0015625, magnitude of gradient - 0.7142780441784518\n",
      "Step - 5557, Loss - 0.6291256552469354, Learning Rate - 0.0015625, magnitude of gradient - 0.557800013201084\n",
      "Step - 5558, Loss - 0.6754955056927243, Learning Rate - 0.0015625, magnitude of gradient - 1.3778013395515534\n",
      "Step - 5559, Loss - 0.6208170369030388, Learning Rate - 0.0015625, magnitude of gradient - 1.0098129087934005\n",
      "Step - 5560, Loss - 0.5969844721953851, Learning Rate - 0.0015625, magnitude of gradient - 0.8627915269810684\n",
      "Step - 5561, Loss - 0.7586448528756125, Learning Rate - 0.0015625, magnitude of gradient - 2.3432157553097506\n",
      "Step - 5562, Loss - 0.5019706204712024, Learning Rate - 0.0015625, magnitude of gradient - 1.3112542335562056\n",
      "Step - 5563, Loss - 0.607180340274762, Learning Rate - 0.0015625, magnitude of gradient - 1.0507092894121643\n",
      "Step - 5564, Loss - 0.630602148546386, Learning Rate - 0.0015625, magnitude of gradient - 1.0599341021948572\n",
      "Step - 5565, Loss - 0.6717454557451636, Learning Rate - 0.0015625, magnitude of gradient - 0.9493480532569247\n",
      "Step - 5566, Loss - 0.723050399857242, Learning Rate - 0.0015625, magnitude of gradient - 1.3770741181090724\n",
      "Step - 5567, Loss - 0.5376326627041235, Learning Rate - 0.0015625, magnitude of gradient - 0.8517724918942139\n",
      "Step - 5568, Loss - 0.6576633910744174, Learning Rate - 0.0015625, magnitude of gradient - 1.2613454006776037\n",
      "Step - 5569, Loss - 0.6376678147538539, Learning Rate - 0.0015625, magnitude of gradient - 1.0373417783831569\n",
      "Step - 5570, Loss - 0.7516282262645855, Learning Rate - 0.0015625, magnitude of gradient - 0.937088487222398\n",
      "Step - 5571, Loss - 0.5101531511726747, Learning Rate - 0.0015625, magnitude of gradient - 1.2205497791048008\n",
      "Step - 5572, Loss - 0.5167420636777232, Learning Rate - 0.0015625, magnitude of gradient - 1.930213010541872\n",
      "Step - 5573, Loss - 0.7483064443765669, Learning Rate - 0.0015625, magnitude of gradient - 0.6300001566592307\n",
      "Step - 5574, Loss - 0.6548390899976353, Learning Rate - 0.0015625, magnitude of gradient - 1.8426238777967923\n",
      "Step - 5575, Loss - 0.5354180595693963, Learning Rate - 0.0015625, magnitude of gradient - 1.1698816165658836\n",
      "Step - 5576, Loss - 0.5991480293069876, Learning Rate - 0.0015625, magnitude of gradient - 1.077390961713417\n",
      "Step - 5577, Loss - 0.7974668898284016, Learning Rate - 0.0015625, magnitude of gradient - 1.3957520281885443\n",
      "Step - 5578, Loss - 0.8186626096223083, Learning Rate - 0.0015625, magnitude of gradient - 1.0522895012653048\n",
      "Step - 5579, Loss - 0.7365401654247592, Learning Rate - 0.0015625, magnitude of gradient - 0.7018874344129333\n",
      "Step - 5580, Loss - 0.6467129585594593, Learning Rate - 0.0015625, magnitude of gradient - 0.8833378018245951\n",
      "Step - 5581, Loss - 0.4149574918718035, Learning Rate - 0.0015625, magnitude of gradient - 1.4595774919756321\n",
      "Step - 5582, Loss - 0.771567529434631, Learning Rate - 0.0015625, magnitude of gradient - 1.6009447326776092\n",
      "Step - 5583, Loss - 0.7617194123679881, Learning Rate - 0.0015625, magnitude of gradient - 1.6592772307788297\n",
      "Step - 5584, Loss - 0.617823548741544, Learning Rate - 0.0015625, magnitude of gradient - 1.3648795366502924\n",
      "Step - 5585, Loss - 0.6724700180687181, Learning Rate - 0.0015625, magnitude of gradient - 0.7718612666548776\n",
      "Step - 5586, Loss - 0.759353981802307, Learning Rate - 0.0015625, magnitude of gradient - 0.8655971303669191\n",
      "Step - 5587, Loss - 0.7226551698232643, Learning Rate - 0.0015625, magnitude of gradient - 1.8931287845374494\n",
      "Step - 5588, Loss - 0.6690326482733041, Learning Rate - 0.0015625, magnitude of gradient - 1.6981903138880607\n",
      "Step - 5589, Loss - 0.6080188087315481, Learning Rate - 0.0015625, magnitude of gradient - 1.1990630157117825\n",
      "Step - 5590, Loss - 0.8321183300865763, Learning Rate - 0.0015625, magnitude of gradient - 1.0533590350474702\n",
      "Step - 5591, Loss - 0.6750931735892727, Learning Rate - 0.0015625, magnitude of gradient - 0.8878960440446799\n",
      "Step - 5592, Loss - 0.6098524358325663, Learning Rate - 0.0015625, magnitude of gradient - 0.8581192949983066\n",
      "Step - 5593, Loss - 0.7175516350675736, Learning Rate - 0.0015625, magnitude of gradient - 1.616897247579372\n",
      "Step - 5594, Loss - 0.7000376497929385, Learning Rate - 0.0015625, magnitude of gradient - 1.2202114170331935\n",
      "Step - 5595, Loss - 0.8242345464619225, Learning Rate - 0.0015625, magnitude of gradient - 2.120715807093591\n",
      "Step - 5596, Loss - 0.6927012175188009, Learning Rate - 0.0015625, magnitude of gradient - 1.0592680126544496\n",
      "Step - 5597, Loss - 0.6729066749217938, Learning Rate - 0.0015625, magnitude of gradient - 1.418627191228825\n",
      "Step - 5598, Loss - 0.8136059760176695, Learning Rate - 0.0015625, magnitude of gradient - 0.4987920443195192\n",
      "Step - 5599, Loss - 0.7149819570675372, Learning Rate - 0.0015625, magnitude of gradient - 0.505253609122424\n",
      "Step - 5600, Loss - 0.8825745106232863, Learning Rate - 0.0015625, magnitude of gradient - 1.1180605007645463\n",
      "Step - 5601, Loss - 0.7057909864792566, Learning Rate - 0.0015625, magnitude of gradient - 1.8254415467360192\n",
      "Step - 5602, Loss - 0.737640900373274, Learning Rate - 0.0015625, magnitude of gradient - 0.9837832483859981\n",
      "Step - 5603, Loss - 0.6595280115913751, Learning Rate - 0.0015625, magnitude of gradient - 0.7789166909434629\n",
      "Step - 5604, Loss - 0.5211814453661818, Learning Rate - 0.0015625, magnitude of gradient - 0.7357871424368299\n",
      "Step - 5605, Loss - 0.6620950938902077, Learning Rate - 0.0015625, magnitude of gradient - 1.0828396430270635\n",
      "Step - 5606, Loss - 0.745492344775954, Learning Rate - 0.0015625, magnitude of gradient - 1.8664465493729607\n",
      "Step - 5607, Loss - 0.8251528720168584, Learning Rate - 0.0015625, magnitude of gradient - 1.1626168937368877\n",
      "Step - 5608, Loss - 0.9122157330565824, Learning Rate - 0.0015625, magnitude of gradient - 1.0131493612156122\n",
      "Step - 5609, Loss - 0.6460991158255192, Learning Rate - 0.0015625, magnitude of gradient - 1.1090811346842335\n",
      "Step - 5610, Loss - 0.7854830885916853, Learning Rate - 0.0015625, magnitude of gradient - 0.8922949780822799\n",
      "Step - 5611, Loss - 0.6937126628781527, Learning Rate - 0.0015625, magnitude of gradient - 1.1514872602407717\n",
      "Step - 5612, Loss - 0.7763479318955201, Learning Rate - 0.0015625, magnitude of gradient - 0.9880237290915552\n",
      "Step - 5613, Loss - 0.7015009929565792, Learning Rate - 0.0015625, magnitude of gradient - 2.3433566541067656\n",
      "Step - 5614, Loss - 0.534502777849262, Learning Rate - 0.0015625, magnitude of gradient - 1.3578478125742737\n",
      "Step - 5615, Loss - 0.7185040822523071, Learning Rate - 0.0015625, magnitude of gradient - 1.1241020666225394\n",
      "Step - 5616, Loss - 0.7468720071939415, Learning Rate - 0.0015625, magnitude of gradient - 1.041233694271286\n",
      "Step - 5617, Loss - 0.5821574716215141, Learning Rate - 0.0015625, magnitude of gradient - 0.6131294799378804\n",
      "Step - 5618, Loss - 0.6136817412181426, Learning Rate - 0.0015625, magnitude of gradient - 1.7506092558079784\n",
      "Step - 5619, Loss - 0.5884597322190446, Learning Rate - 0.0015625, magnitude of gradient - 1.0959887456101487\n",
      "Step - 5620, Loss - 0.6962271049402169, Learning Rate - 0.0015625, magnitude of gradient - 1.3767240815693178\n",
      "Step - 5621, Loss - 0.7707709637226122, Learning Rate - 0.0015625, magnitude of gradient - 1.121307158813003\n",
      "Step - 5622, Loss - 0.6755495334067912, Learning Rate - 0.0015625, magnitude of gradient - 0.94720515043742\n",
      "Step - 5623, Loss - 0.7850548102394062, Learning Rate - 0.0015625, magnitude of gradient - 1.758920691075183\n",
      "Step - 5624, Loss - 0.5970894526292262, Learning Rate - 0.0015625, magnitude of gradient - 0.5274128455172642\n",
      "Step - 5625, Loss - 0.7836267220112015, Learning Rate - 0.0015625, magnitude of gradient - 0.4888251431735824\n",
      "Step - 5626, Loss - 0.6505603326383969, Learning Rate - 0.0015625, magnitude of gradient - 2.333351245002495\n",
      "Step - 5627, Loss - 0.5545284254937811, Learning Rate - 0.0015625, magnitude of gradient - 1.3168975208123803\n",
      "Step - 5628, Loss - 0.6349202601895392, Learning Rate - 0.0015625, magnitude of gradient - 0.5907924384338741\n",
      "Step - 5629, Loss - 0.7965462888007259, Learning Rate - 0.0015625, magnitude of gradient - 1.7513365459001526\n",
      "Step - 5630, Loss - 0.6107477050760682, Learning Rate - 0.0015625, magnitude of gradient - 0.7679937146154558\n",
      "Step - 5631, Loss - 0.8537153863116566, Learning Rate - 0.0015625, magnitude of gradient - 2.0537954305831656\n",
      "Step - 5632, Loss - 0.7038671020675522, Learning Rate - 0.0015625, magnitude of gradient - 0.6459469656884346\n",
      "Step - 5633, Loss - 0.6562420040587464, Learning Rate - 0.0015625, magnitude of gradient - 1.1412466210472898\n",
      "Step - 5634, Loss - 0.5858159539078021, Learning Rate - 0.0015625, magnitude of gradient - 0.42330367848679135\n",
      "Step - 5635, Loss - 0.7754864409574468, Learning Rate - 0.0015625, magnitude of gradient - 1.6172658931892656\n",
      "Step - 5636, Loss - 0.6879962160174328, Learning Rate - 0.0015625, magnitude of gradient - 1.3192614492523849\n",
      "Step - 5637, Loss - 0.48723315316103344, Learning Rate - 0.0015625, magnitude of gradient - 0.49800054080788486\n",
      "Step - 5638, Loss - 0.4658894644598592, Learning Rate - 0.0015625, magnitude of gradient - 1.9704988819927771\n",
      "Step - 5639, Loss - 0.9105178686795696, Learning Rate - 0.0015625, magnitude of gradient - 2.0182996022678252\n",
      "Step - 5640, Loss - 0.7331736840999693, Learning Rate - 0.0015625, magnitude of gradient - 1.4503042171791585\n",
      "Step - 5641, Loss - 0.8124800910813366, Learning Rate - 0.0015625, magnitude of gradient - 0.6446320021223858\n",
      "Step - 5642, Loss - 0.6864032936708616, Learning Rate - 0.0015625, magnitude of gradient - 0.6190632233044886\n",
      "Step - 5643, Loss - 0.8805497500840354, Learning Rate - 0.0015625, magnitude of gradient - 0.5324673950769763\n",
      "Step - 5644, Loss - 0.7088185897734577, Learning Rate - 0.0015625, magnitude of gradient - 0.5759737867795724\n",
      "Step - 5645, Loss - 0.7463616820738168, Learning Rate - 0.0015625, magnitude of gradient - 1.7405616401283543\n",
      "Step - 5646, Loss - 0.7445291716303672, Learning Rate - 0.0015625, magnitude of gradient - 1.6060121509721625\n",
      "Step - 5647, Loss - 0.8049076761045821, Learning Rate - 0.0015625, magnitude of gradient - 0.862649810234707\n",
      "Step - 5648, Loss - 0.901245703815972, Learning Rate - 0.0015625, magnitude of gradient - 1.2780629044345417\n",
      "Step - 5649, Loss - 0.7183993377565386, Learning Rate - 0.0015625, magnitude of gradient - 1.1026774068068617\n",
      "Step - 5650, Loss - 0.7031705434550545, Learning Rate - 0.0015625, magnitude of gradient - 1.5267789386374617\n",
      "Step - 5651, Loss - 0.5482143975465422, Learning Rate - 0.0015625, magnitude of gradient - 0.6751040032928567\n",
      "Step - 5652, Loss - 0.8685213310813076, Learning Rate - 0.0015625, magnitude of gradient - 1.286458864283748\n",
      "Step - 5653, Loss - 0.6365565621895952, Learning Rate - 0.0015625, magnitude of gradient - 1.103915622185562\n",
      "Step - 5654, Loss - 0.5789222759553095, Learning Rate - 0.0015625, magnitude of gradient - 2.0156131140525706\n",
      "Step - 5655, Loss - 0.7251969248440521, Learning Rate - 0.0015625, magnitude of gradient - 0.7942589441517858\n",
      "Step - 5656, Loss - 0.4157408131093685, Learning Rate - 0.0015625, magnitude of gradient - 1.7689226153280742\n",
      "Step - 5657, Loss - 0.6826052753100089, Learning Rate - 0.0015625, magnitude of gradient - 1.6140517694069478\n",
      "Step - 5658, Loss - 0.5965135532322317, Learning Rate - 0.0015625, magnitude of gradient - 0.5010679986680058\n",
      "Step - 5659, Loss - 0.5736314587033584, Learning Rate - 0.0015625, magnitude of gradient - 0.5195235763285487\n",
      "Step - 5660, Loss - 0.772211216323675, Learning Rate - 0.0015625, magnitude of gradient - 1.0225608902625871\n",
      "Step - 5661, Loss - 0.5376851707686967, Learning Rate - 0.0015625, magnitude of gradient - 3.2494549729227025\n",
      "Step - 5662, Loss - 0.6983844526533638, Learning Rate - 0.0015625, magnitude of gradient - 1.7646918058781158\n",
      "Step - 5663, Loss - 0.5858153085390505, Learning Rate - 0.0015625, magnitude of gradient - 1.1659193496206148\n",
      "Step - 5664, Loss - 0.5786090199689417, Learning Rate - 0.0015625, magnitude of gradient - 1.6026377921778237\n",
      "Step - 5665, Loss - 0.6861764126740816, Learning Rate - 0.0015625, magnitude of gradient - 1.6835373089923251\n",
      "Step - 5666, Loss - 0.8633500596205224, Learning Rate - 0.0015625, magnitude of gradient - 0.42437059321884235\n",
      "Step - 5667, Loss - 1.0235113859512497, Learning Rate - 0.0015625, magnitude of gradient - 2.256383907943688\n",
      "Step - 5668, Loss - 0.7725975041628546, Learning Rate - 0.0015625, magnitude of gradient - 2.968512731460844\n",
      "Step - 5669, Loss - 0.5776813625622971, Learning Rate - 0.0015625, magnitude of gradient - 1.2911391563284889\n",
      "Step - 5670, Loss - 0.6863690229314022, Learning Rate - 0.0015625, magnitude of gradient - 1.0022645207449752\n",
      "Step - 5671, Loss - 0.8172116698433181, Learning Rate - 0.0015625, magnitude of gradient - 0.7086294999320919\n",
      "Step - 5672, Loss - 0.6465713523876147, Learning Rate - 0.0015625, magnitude of gradient - 1.1555224693256558\n",
      "Step - 5673, Loss - 0.6231688653865366, Learning Rate - 0.0015625, magnitude of gradient - 0.9456000957068701\n",
      "Step - 5674, Loss - 0.8923099414852234, Learning Rate - 0.0015625, magnitude of gradient - 1.2630884295922993\n",
      "Step - 5675, Loss - 0.8638412828880605, Learning Rate - 0.0015625, magnitude of gradient - 1.4748439912413638\n",
      "Step - 5676, Loss - 0.5584805091554852, Learning Rate - 0.0015625, magnitude of gradient - 1.8656104071811013\n",
      "Step - 5677, Loss - 0.5733791667113732, Learning Rate - 0.0015625, magnitude of gradient - 1.8566196052318322\n",
      "Step - 5678, Loss - 0.590101972053204, Learning Rate - 0.0015625, magnitude of gradient - 1.1249766509122523\n",
      "Step - 5679, Loss - 0.5008904843154071, Learning Rate - 0.0015625, magnitude of gradient - 1.0183755247608117\n",
      "Step - 5680, Loss - 0.6639991524205413, Learning Rate - 0.0015625, magnitude of gradient - 1.1705737154743558\n",
      "Step - 5681, Loss - 0.4930253667706416, Learning Rate - 0.0015625, magnitude of gradient - 0.9942463691843232\n",
      "Step - 5682, Loss - 0.8251489167092085, Learning Rate - 0.0015625, magnitude of gradient - 2.298403556885797\n",
      "Step - 5683, Loss - 0.6191362635623797, Learning Rate - 0.0015625, magnitude of gradient - 2.4314324542014583\n",
      "Step - 5684, Loss - 0.7750899486231861, Learning Rate - 0.0015625, magnitude of gradient - 0.7923629793181934\n",
      "Step - 5685, Loss - 0.8171582559791928, Learning Rate - 0.0015625, magnitude of gradient - 0.8165254277547088\n",
      "Step - 5686, Loss - 0.6676438248074119, Learning Rate - 0.0015625, magnitude of gradient - 0.5120370536646734\n",
      "Step - 5687, Loss - 0.7806314567241591, Learning Rate - 0.0015625, magnitude of gradient - 1.658770795552121\n",
      "Step - 5688, Loss - 0.6061008203246754, Learning Rate - 0.0015625, magnitude of gradient - 2.755653083981286\n",
      "Step - 5689, Loss - 0.7148617841649717, Learning Rate - 0.0015625, magnitude of gradient - 1.4815527965481123\n",
      "Step - 5690, Loss - 0.6400084937276302, Learning Rate - 0.0015625, magnitude of gradient - 1.9616492118141136\n",
      "Step - 5691, Loss - 0.5201761768221128, Learning Rate - 0.0015625, magnitude of gradient - 1.0081387302430012\n",
      "Step - 5692, Loss - 0.8318669600472692, Learning Rate - 0.0015625, magnitude of gradient - 1.389335671360131\n",
      "Step - 5693, Loss - 0.45906142899905295, Learning Rate - 0.0015625, magnitude of gradient - 0.8204327900236383\n",
      "Step - 5694, Loss - 0.670633459046756, Learning Rate - 0.0015625, magnitude of gradient - 1.0826363232838228\n",
      "Step - 5695, Loss - 0.5960984618722747, Learning Rate - 0.0015625, magnitude of gradient - 0.8668582808878704\n",
      "Step - 5696, Loss - 0.8134520383956307, Learning Rate - 0.0015625, magnitude of gradient - 1.3120807564093548\n",
      "Step - 5697, Loss - 0.5451690771992452, Learning Rate - 0.0015625, magnitude of gradient - 2.185308682345078\n",
      "Step - 5698, Loss - 0.6735369198935928, Learning Rate - 0.0015625, magnitude of gradient - 1.3371591326571277\n",
      "Step - 5699, Loss - 0.7612728439348774, Learning Rate - 0.0015625, magnitude of gradient - 1.564329529628247\n",
      "Step - 5700, Loss - 0.5374278656995379, Learning Rate - 0.0015625, magnitude of gradient - 1.3124364885348956\n",
      "Step - 5701, Loss - 0.6047310765894821, Learning Rate - 0.0015625, magnitude of gradient - 1.194854268763953\n",
      "Step - 5702, Loss - 0.8089291745460457, Learning Rate - 0.0015625, magnitude of gradient - 1.3611399404700077\n",
      "Step - 5703, Loss - 0.7235460116000114, Learning Rate - 0.0015625, magnitude of gradient - 1.4698036459254455\n",
      "Step - 5704, Loss - 1.0082026246194846, Learning Rate - 0.0015625, magnitude of gradient - 2.0515421221665293\n",
      "Step - 5705, Loss - 0.8984555568503363, Learning Rate - 0.0015625, magnitude of gradient - 2.340241829561454\n",
      "Step - 5706, Loss - 0.712095597743892, Learning Rate - 0.0015625, magnitude of gradient - 0.7973947943062623\n",
      "Step - 5707, Loss - 0.7372379698512375, Learning Rate - 0.0015625, magnitude of gradient - 0.9623479126142994\n",
      "Step - 5708, Loss - 0.6925284657016246, Learning Rate - 0.0015625, magnitude of gradient - 0.9955288945401237\n",
      "Step - 5709, Loss - 0.6692734278255079, Learning Rate - 0.0015625, magnitude of gradient - 1.5601680654812438\n",
      "Step - 5710, Loss - 0.9982711132133253, Learning Rate - 0.0015625, magnitude of gradient - 2.4665150104755953\n",
      "Step - 5711, Loss - 0.7097538504605697, Learning Rate - 0.0015625, magnitude of gradient - 0.7480310972077832\n",
      "Step - 5712, Loss - 1.0111268174938608, Learning Rate - 0.0015625, magnitude of gradient - 1.3754711273122593\n",
      "Step - 5713, Loss - 0.6348776868279612, Learning Rate - 0.0015625, magnitude of gradient - 0.31184642383052236\n",
      "Step - 5714, Loss - 0.8122304176339047, Learning Rate - 0.0015625, magnitude of gradient - 2.265845096754395\n",
      "Step - 5715, Loss - 0.6852728458545555, Learning Rate - 0.0015625, magnitude of gradient - 0.5104636087535793\n",
      "Step - 5716, Loss - 0.6920713621330432, Learning Rate - 0.0015625, magnitude of gradient - 1.5063425910799004\n",
      "Step - 5717, Loss - 0.6381681669531428, Learning Rate - 0.0015625, magnitude of gradient - 1.1743538339822361\n",
      "Step - 5718, Loss - 0.6938219739572968, Learning Rate - 0.0015625, magnitude of gradient - 1.033150416342118\n",
      "Step - 5719, Loss - 0.6339528998306752, Learning Rate - 0.0015625, magnitude of gradient - 1.4218488924338633\n",
      "Step - 5720, Loss - 0.5710851514004033, Learning Rate - 0.0015625, magnitude of gradient - 1.34361431765446\n",
      "Step - 5721, Loss - 0.6556264496876565, Learning Rate - 0.0015625, magnitude of gradient - 1.4569440610504494\n",
      "Step - 5722, Loss - 0.7114954347669173, Learning Rate - 0.0015625, magnitude of gradient - 1.0162217562489932\n",
      "Step - 5723, Loss - 0.7855635022098998, Learning Rate - 0.0015625, magnitude of gradient - 1.330625591765784\n",
      "Step - 5724, Loss - 0.6776973726998112, Learning Rate - 0.0015625, magnitude of gradient - 0.46312202000703023\n",
      "Step - 5725, Loss - 0.7268457750232271, Learning Rate - 0.0015625, magnitude of gradient - 2.1361922521113224\n",
      "Step - 5726, Loss - 0.6127674276686081, Learning Rate - 0.0015625, magnitude of gradient - 1.181449120573982\n",
      "Step - 5727, Loss - 0.6755375151338168, Learning Rate - 0.0015625, magnitude of gradient - 2.0680995553635766\n",
      "Step - 5728, Loss - 0.6299185890284702, Learning Rate - 0.0015625, magnitude of gradient - 1.4481146993040357\n",
      "Step - 5729, Loss - 0.7317935570240834, Learning Rate - 0.0015625, magnitude of gradient - 1.3118308405195642\n",
      "Step - 5730, Loss - 0.753646370495661, Learning Rate - 0.0015625, magnitude of gradient - 1.6753137321848992\n",
      "Step - 5731, Loss - 0.720063289836456, Learning Rate - 0.0015625, magnitude of gradient - 1.569532196209568\n",
      "Step - 5732, Loss - 0.5840306495409103, Learning Rate - 0.0015625, magnitude of gradient - 1.1351368237049253\n",
      "Step - 5733, Loss - 0.5557578709834904, Learning Rate - 0.0015625, magnitude of gradient - 0.926319576055533\n",
      "Step - 5734, Loss - 0.6925140983826475, Learning Rate - 0.0015625, magnitude of gradient - 0.28055010986255097\n",
      "Step - 5735, Loss - 0.8328728959690314, Learning Rate - 0.0015625, magnitude of gradient - 0.9727394228006492\n",
      "Step - 5736, Loss - 0.604111974718642, Learning Rate - 0.0015625, magnitude of gradient - 0.6248772320058845\n",
      "Step - 5737, Loss - 0.6815911803640352, Learning Rate - 0.0015625, magnitude of gradient - 0.6718863912458806\n",
      "Step - 5738, Loss - 0.6788050216865126, Learning Rate - 0.0015625, magnitude of gradient - 0.4614575449605881\n",
      "Step - 5739, Loss - 0.5908266395388088, Learning Rate - 0.0015625, magnitude of gradient - 3.467998492825291\n",
      "Step - 5740, Loss - 0.7578843364500079, Learning Rate - 0.0015625, magnitude of gradient - 1.7508046549410956\n",
      "Step - 5741, Loss - 0.7216733561231038, Learning Rate - 0.0015625, magnitude of gradient - 0.663679865625358\n",
      "Step - 5742, Loss - 0.6374341618391948, Learning Rate - 0.0015625, magnitude of gradient - 1.7748544227002543\n",
      "Step - 5743, Loss - 0.7246825941559081, Learning Rate - 0.0015625, magnitude of gradient - 1.0005924058615239\n",
      "Step - 5744, Loss - 0.6837320235372624, Learning Rate - 0.0015625, magnitude of gradient - 0.5608526710147195\n",
      "Step - 5745, Loss - 0.5478649408632491, Learning Rate - 0.0015625, magnitude of gradient - 1.6481586876922125\n",
      "Step - 5746, Loss - 0.6756902993697238, Learning Rate - 0.0015625, magnitude of gradient - 0.40572479193986005\n",
      "Step - 5747, Loss - 0.7019935577583215, Learning Rate - 0.0015625, magnitude of gradient - 1.1118809140903618\n",
      "Step - 5748, Loss - 0.6147882617440151, Learning Rate - 0.0015625, magnitude of gradient - 0.8089019092157452\n",
      "Step - 5749, Loss - 0.694570903903161, Learning Rate - 0.0015625, magnitude of gradient - 1.012544937436936\n",
      "Step - 5750, Loss - 0.716964352976718, Learning Rate - 0.0015625, magnitude of gradient - 0.7887669291776331\n",
      "Step - 5751, Loss - 0.5644774843445135, Learning Rate - 0.0015625, magnitude of gradient - 2.0716451688637765\n",
      "Step - 5752, Loss - 0.6370736368241224, Learning Rate - 0.0015625, magnitude of gradient - 1.9086761457648538\n",
      "Step - 5753, Loss - 0.8403652929055249, Learning Rate - 0.0015625, magnitude of gradient - 1.7404518565664615\n",
      "Step - 5754, Loss - 0.8929339898959922, Learning Rate - 0.0015625, magnitude of gradient - 0.9373209280561455\n",
      "Step - 5755, Loss - 0.6864090792945164, Learning Rate - 0.0015625, magnitude of gradient - 2.0655045664305676\n",
      "Step - 5756, Loss - 0.7755218921918687, Learning Rate - 0.0015625, magnitude of gradient - 1.0783627495060244\n",
      "Step - 5757, Loss - 0.5466145780598852, Learning Rate - 0.0015625, magnitude of gradient - 1.3245169597064892\n",
      "Step - 5758, Loss - 0.6436814796430428, Learning Rate - 0.0015625, magnitude of gradient - 0.6201995461946841\n",
      "Step - 5759, Loss - 0.5982261641206215, Learning Rate - 0.0015625, magnitude of gradient - 1.2606056532826668\n",
      "Step - 5760, Loss - 0.6974500329125343, Learning Rate - 0.0015625, magnitude of gradient - 1.278524095405955\n",
      "Step - 5761, Loss - 0.9488927843733569, Learning Rate - 0.0015625, magnitude of gradient - 1.0317566004293441\n",
      "Step - 5762, Loss - 0.7037549861105852, Learning Rate - 0.0015625, magnitude of gradient - 1.1461929270943323\n",
      "Step - 5763, Loss - 0.644170678947879, Learning Rate - 0.0015625, magnitude of gradient - 3.6807318345309055\n",
      "Step - 5764, Loss - 0.8445489662192395, Learning Rate - 0.0015625, magnitude of gradient - 1.0970836077911996\n",
      "Step - 5765, Loss - 0.7427302952276782, Learning Rate - 0.0015625, magnitude of gradient - 1.1383417237861313\n",
      "Step - 5766, Loss - 0.755937944815508, Learning Rate - 0.0015625, magnitude of gradient - 1.1479382071397735\n",
      "Step - 5767, Loss - 0.8402023599264883, Learning Rate - 0.0015625, magnitude of gradient - 1.1442052307499666\n",
      "Step - 5768, Loss - 0.5607474010582876, Learning Rate - 0.0015625, magnitude of gradient - 1.086311162125299\n",
      "Step - 5769, Loss - 0.44518145274358983, Learning Rate - 0.0015625, magnitude of gradient - 0.39230888354529025\n",
      "Step - 5770, Loss - 0.6875752362596582, Learning Rate - 0.0015625, magnitude of gradient - 0.7752364636846628\n",
      "Step - 5771, Loss - 0.7201539999081339, Learning Rate - 0.0015625, magnitude of gradient - 1.5104566962298667\n",
      "Step - 5772, Loss - 0.6247892168382033, Learning Rate - 0.0015625, magnitude of gradient - 1.0434257422600157\n",
      "Step - 5773, Loss - 0.6429809961841114, Learning Rate - 0.0015625, magnitude of gradient - 1.1143378664207477\n",
      "Step - 5774, Loss - 0.4168716512640486, Learning Rate - 0.0015625, magnitude of gradient - 1.7473663020324928\n",
      "Step - 5775, Loss - 0.826217518741625, Learning Rate - 0.0015625, magnitude of gradient - 1.270052340975378\n",
      "Step - 5776, Loss - 0.7627241589101543, Learning Rate - 0.0015625, magnitude of gradient - 0.825966404557224\n",
      "Step - 5777, Loss - 0.7070496008380013, Learning Rate - 0.0015625, magnitude of gradient - 1.863537502157523\n",
      "Step - 5778, Loss - 0.9911433335722959, Learning Rate - 0.0015625, magnitude of gradient - 2.537031346672621\n",
      "Step - 5779, Loss - 0.5504971791417255, Learning Rate - 0.0015625, magnitude of gradient - 0.5679007445502265\n",
      "Step - 5780, Loss - 0.9504899000976454, Learning Rate - 0.0015625, magnitude of gradient - 1.81298141596617\n",
      "Step - 5781, Loss - 0.6241079336565902, Learning Rate - 0.0015625, magnitude of gradient - 0.7910056882575663\n",
      "Step - 5782, Loss - 0.8703013785063181, Learning Rate - 0.0015625, magnitude of gradient - 0.9155907417154404\n",
      "Step - 5783, Loss - 0.4923723330298787, Learning Rate - 0.0015625, magnitude of gradient - 2.4226477212142443\n",
      "Step - 5784, Loss - 0.6378580711949462, Learning Rate - 0.0015625, magnitude of gradient - 1.7766451274200006\n",
      "Step - 5785, Loss - 0.5648745474766621, Learning Rate - 0.0015625, magnitude of gradient - 0.9477167454411688\n",
      "Step - 5786, Loss - 0.7449756212655282, Learning Rate - 0.0015625, magnitude of gradient - 1.4408933435966391\n",
      "Step - 5787, Loss - 0.7646569579078054, Learning Rate - 0.0015625, magnitude of gradient - 1.591760964862198\n",
      "Step - 5788, Loss - 0.8777249015345723, Learning Rate - 0.0015625, magnitude of gradient - 0.7244479672155946\n",
      "Step - 5789, Loss - 0.7233412997499872, Learning Rate - 0.0015625, magnitude of gradient - 0.6431904179872012\n",
      "Step - 5790, Loss - 0.7321203820128646, Learning Rate - 0.0015625, magnitude of gradient - 1.752657651224613\n",
      "Step - 5791, Loss - 0.554118057063553, Learning Rate - 0.0015625, magnitude of gradient - 1.815326899373185\n",
      "Step - 5792, Loss - 0.8084637017605454, Learning Rate - 0.0015625, magnitude of gradient - 1.2312777873740834\n",
      "Step - 5793, Loss - 0.6467988788872845, Learning Rate - 0.0015625, magnitude of gradient - 0.39294352566912855\n",
      "Step - 5794, Loss - 0.679311020440644, Learning Rate - 0.0015625, magnitude of gradient - 0.4771809490514866\n",
      "Step - 5795, Loss - 0.6447384481378121, Learning Rate - 0.0015625, magnitude of gradient - 2.2265270146609133\n",
      "Step - 5796, Loss - 0.6702176120618403, Learning Rate - 0.0015625, magnitude of gradient - 1.6960248274097411\n",
      "Step - 5797, Loss - 0.5277714206702524, Learning Rate - 0.0015625, magnitude of gradient - 0.8624598689459647\n",
      "Step - 5798, Loss - 0.6180932650516302, Learning Rate - 0.0015625, magnitude of gradient - 2.4538879219929575\n",
      "Step - 5799, Loss - 0.7283277221277712, Learning Rate - 0.0015625, magnitude of gradient - 2.0687014872372838\n",
      "Step - 5800, Loss - 0.5079434049988131, Learning Rate - 0.0015625, magnitude of gradient - 1.017921368151105\n",
      "Step - 5801, Loss - 0.5721061017905956, Learning Rate - 0.0015625, magnitude of gradient - 1.9740949168096653\n",
      "Step - 5802, Loss - 0.724595668645125, Learning Rate - 0.0015625, magnitude of gradient - 0.38704188245916676\n",
      "Step - 5803, Loss - 0.7747784416504949, Learning Rate - 0.0015625, magnitude of gradient - 1.4083162475680449\n",
      "Step - 5804, Loss - 0.6718437326887685, Learning Rate - 0.0015625, magnitude of gradient - 1.0205873377582713\n",
      "Step - 5805, Loss - 0.630137302464816, Learning Rate - 0.0015625, magnitude of gradient - 1.061947734393002\n",
      "Step - 5806, Loss - 0.7386655396497764, Learning Rate - 0.0015625, magnitude of gradient - 0.8762643072639882\n",
      "Step - 5807, Loss - 0.6532082082669497, Learning Rate - 0.0015625, magnitude of gradient - 2.2357927251374967\n",
      "Step - 5808, Loss - 0.8745208834727375, Learning Rate - 0.0015625, magnitude of gradient - 2.1782846303541445\n",
      "Step - 5809, Loss - 0.9103974781686597, Learning Rate - 0.0015625, magnitude of gradient - 0.5979021842118406\n",
      "Step - 5810, Loss - 0.7341871384995363, Learning Rate - 0.0015625, magnitude of gradient - 0.7852374370595872\n",
      "Step - 5811, Loss - 0.8095712500455123, Learning Rate - 0.0015625, magnitude of gradient - 1.4826405655290589\n",
      "Step - 5812, Loss - 0.7398098231692504, Learning Rate - 0.0015625, magnitude of gradient - 3.647432073496279\n",
      "Step - 5813, Loss - 0.8548851029823268, Learning Rate - 0.0015625, magnitude of gradient - 0.7655478137296895\n",
      "Step - 5814, Loss - 0.47882942000603634, Learning Rate - 0.0015625, magnitude of gradient - 0.8097389088239672\n",
      "Step - 5815, Loss - 0.7017147261154941, Learning Rate - 0.0015625, magnitude of gradient - 1.2124956509067755\n",
      "Step - 5816, Loss - 0.7079561506415268, Learning Rate - 0.0015625, magnitude of gradient - 0.8711583794797817\n",
      "Step - 5817, Loss - 0.7657833627507721, Learning Rate - 0.0015625, magnitude of gradient - 1.695846628531721\n",
      "Step - 5818, Loss - 0.7052366420169112, Learning Rate - 0.0015625, magnitude of gradient - 1.0176502780625518\n",
      "Step - 5819, Loss - 0.6276735368325521, Learning Rate - 0.0015625, magnitude of gradient - 1.2176249788559634\n",
      "Step - 5820, Loss - 0.7903970759811914, Learning Rate - 0.0015625, magnitude of gradient - 1.8696367308218156\n",
      "Step - 5821, Loss - 0.6111713642370393, Learning Rate - 0.0015625, magnitude of gradient - 1.1544334119991684\n",
      "Step - 5822, Loss - 0.7697766947423366, Learning Rate - 0.0015625, magnitude of gradient - 2.053708002225202\n",
      "Step - 5823, Loss - 0.5899691444666046, Learning Rate - 0.0015625, magnitude of gradient - 0.6565931663970532\n",
      "Step - 5824, Loss - 0.5390324826629402, Learning Rate - 0.0015625, magnitude of gradient - 0.743648279298922\n",
      "Step - 5825, Loss - 0.5135583354470272, Learning Rate - 0.0015625, magnitude of gradient - 0.9191909159017059\n",
      "Step - 5826, Loss - 0.6538876468169321, Learning Rate - 0.0015625, magnitude of gradient - 1.731509312373213\n",
      "Step - 5827, Loss - 0.7276460898170347, Learning Rate - 0.0015625, magnitude of gradient - 1.1075098446517677\n",
      "Step - 5828, Loss - 0.6576660351429492, Learning Rate - 0.0015625, magnitude of gradient - 0.68269183145853\n",
      "Step - 5829, Loss - 0.5806567481970135, Learning Rate - 0.0015625, magnitude of gradient - 0.7254596717964876\n",
      "Step - 5830, Loss - 0.6752931529597382, Learning Rate - 0.0015625, magnitude of gradient - 0.7011083741881361\n",
      "Step - 5831, Loss - 0.5704724989851314, Learning Rate - 0.0015625, magnitude of gradient - 0.39625940505981255\n",
      "Step - 5832, Loss - 0.6181093058767687, Learning Rate - 0.0015625, magnitude of gradient - 1.1757377122057853\n",
      "Step - 5833, Loss - 0.6707831457502533, Learning Rate - 0.0015625, magnitude of gradient - 0.7036375950330194\n",
      "Step - 5834, Loss - 0.7740623860027582, Learning Rate - 0.0015625, magnitude of gradient - 0.9339512787714599\n",
      "Step - 5835, Loss - 0.7085658863698925, Learning Rate - 0.0015625, magnitude of gradient - 1.6822489185838192\n",
      "Step - 5836, Loss - 0.6647814948404689, Learning Rate - 0.0015625, magnitude of gradient - 1.1023537516172133\n",
      "Step - 5837, Loss - 0.5872811605328649, Learning Rate - 0.0015625, magnitude of gradient - 1.1238641401312992\n",
      "Step - 5838, Loss - 0.9257394485634001, Learning Rate - 0.0015625, magnitude of gradient - 1.9920244436966523\n",
      "Step - 5839, Loss - 0.7440148708447193, Learning Rate - 0.0015625, magnitude of gradient - 1.0454056318338227\n",
      "Step - 5840, Loss - 0.6042814643233354, Learning Rate - 0.0015625, magnitude of gradient - 0.5641252275936229\n",
      "Step - 5841, Loss - 0.6430413451229238, Learning Rate - 0.0015625, magnitude of gradient - 1.1540945071035047\n",
      "Step - 5842, Loss - 0.7123041289735759, Learning Rate - 0.0015625, magnitude of gradient - 0.993986196327593\n",
      "Step - 5843, Loss - 0.9335373793465416, Learning Rate - 0.0015625, magnitude of gradient - 0.4967992462748924\n",
      "Step - 5844, Loss - 0.6041037151677877, Learning Rate - 0.0015625, magnitude of gradient - 1.9739955405094365\n",
      "Step - 5845, Loss - 0.8779942297856458, Learning Rate - 0.0015625, magnitude of gradient - 1.042883135355078\n",
      "Step - 5846, Loss - 0.6150896489873081, Learning Rate - 0.0015625, magnitude of gradient - 0.8723969808119062\n",
      "Step - 5847, Loss - 0.640801687974881, Learning Rate - 0.0015625, magnitude of gradient - 1.4355683244826276\n",
      "Step - 5848, Loss - 0.7116774624980745, Learning Rate - 0.0015625, magnitude of gradient - 3.233671391913451\n",
      "Step - 5849, Loss - 0.8064671739145179, Learning Rate - 0.0015625, magnitude of gradient - 0.6975467956169041\n",
      "Step - 5850, Loss - 0.6994108048309668, Learning Rate - 0.0015625, magnitude of gradient - 2.932761149635557\n",
      "Step - 5851, Loss - 0.7199059106435544, Learning Rate - 0.0015625, magnitude of gradient - 1.04681653742146\n",
      "Step - 5852, Loss - 0.6228680607418149, Learning Rate - 0.0015625, magnitude of gradient - 2.6053273542619553\n",
      "Step - 5853, Loss - 0.6946607024167565, Learning Rate - 0.0015625, magnitude of gradient - 0.7320366051102046\n",
      "Step - 5854, Loss - 0.6029376067765616, Learning Rate - 0.0015625, magnitude of gradient - 1.9884003531460406\n",
      "Step - 5855, Loss - 0.5300031990925891, Learning Rate - 0.0015625, magnitude of gradient - 1.9608182483453678\n",
      "Step - 5856, Loss - 0.7224072310192382, Learning Rate - 0.0015625, magnitude of gradient - 0.8303164782878657\n",
      "Step - 5857, Loss - 0.6712506093180489, Learning Rate - 0.0015625, magnitude of gradient - 2.2785306165916466\n",
      "Step - 5858, Loss - 0.6578739346162294, Learning Rate - 0.0015625, magnitude of gradient - 1.1540230195077068\n",
      "Step - 5859, Loss - 0.7858974189340391, Learning Rate - 0.0015625, magnitude of gradient - 0.6563461711613641\n",
      "Step - 5860, Loss - 0.7464851431175847, Learning Rate - 0.0015625, magnitude of gradient - 2.814006597515865\n",
      "Step - 5861, Loss - 0.7318311547218355, Learning Rate - 0.0015625, magnitude of gradient - 1.6486581987045565\n",
      "Step - 5862, Loss - 0.7177724890114461, Learning Rate - 0.0015625, magnitude of gradient - 1.6205858992589655\n",
      "Step - 5863, Loss - 0.7045250432821704, Learning Rate - 0.0015625, magnitude of gradient - 1.0045856034793903\n",
      "Step - 5864, Loss - 0.7035553758391994, Learning Rate - 0.0015625, magnitude of gradient - 1.6214056871722438\n",
      "Step - 5865, Loss - 0.7238698660692044, Learning Rate - 0.0015625, magnitude of gradient - 2.560605765814482\n",
      "Step - 5866, Loss - 0.7555398962617805, Learning Rate - 0.0015625, magnitude of gradient - 1.4361913995032112\n",
      "Step - 5867, Loss - 0.741251337402478, Learning Rate - 0.0015625, magnitude of gradient - 1.4895839007492329\n",
      "Step - 5868, Loss - 0.7024893533716385, Learning Rate - 0.0015625, magnitude of gradient - 0.2542022186500701\n",
      "Step - 5869, Loss - 0.989695044525981, Learning Rate - 0.0015625, magnitude of gradient - 0.6703272409354251\n",
      "Step - 5870, Loss - 0.6582451941875409, Learning Rate - 0.0015625, magnitude of gradient - 0.5058447909325321\n",
      "Step - 5871, Loss - 0.7698275828680438, Learning Rate - 0.0015625, magnitude of gradient - 1.80465430580945\n",
      "Step - 5872, Loss - 0.700992861942371, Learning Rate - 0.0015625, magnitude of gradient - 2.012510712462967\n",
      "Step - 5873, Loss - 0.5862227469226293, Learning Rate - 0.0015625, magnitude of gradient - 1.9046161869779465\n",
      "Step - 5874, Loss - 0.6567883184406385, Learning Rate - 0.0015625, magnitude of gradient - 0.6901656473235026\n",
      "Step - 5875, Loss - 0.9262868453916058, Learning Rate - 0.0015625, magnitude of gradient - 1.2202930674784906\n",
      "Step - 5876, Loss - 0.5531283808271832, Learning Rate - 0.0015625, magnitude of gradient - 1.2033115907545244\n",
      "Step - 5877, Loss - 0.7756232896987669, Learning Rate - 0.0015625, magnitude of gradient - 1.2295310480164114\n",
      "Step - 5878, Loss - 0.703532770803504, Learning Rate - 0.0015625, magnitude of gradient - 1.1566481748228088\n",
      "Step - 5879, Loss - 0.6488323881034156, Learning Rate - 0.0015625, magnitude of gradient - 1.8953914654035642\n",
      "Step - 5880, Loss - 0.9806133832621055, Learning Rate - 0.0015625, magnitude of gradient - 1.7377143322848678\n",
      "Step - 5881, Loss - 0.7532219125266493, Learning Rate - 0.0015625, magnitude of gradient - 3.4407975092458574\n",
      "Step - 5882, Loss - 0.7665829599251941, Learning Rate - 0.0015625, magnitude of gradient - 1.8077709386832177\n",
      "Step - 5883, Loss - 0.6857166683464588, Learning Rate - 0.0015625, magnitude of gradient - 1.102275281616511\n",
      "Step - 5884, Loss - 0.5671870549258934, Learning Rate - 0.0015625, magnitude of gradient - 1.745239016284009\n",
      "Step - 5885, Loss - 0.6790273749647071, Learning Rate - 0.0015625, magnitude of gradient - 1.168533497286379\n",
      "Step - 5886, Loss - 0.7689048806149564, Learning Rate - 0.0015625, magnitude of gradient - 0.9212278038183938\n",
      "Step - 5887, Loss - 0.6013268198619122, Learning Rate - 0.0015625, magnitude of gradient - 2.1149107104358817\n",
      "Step - 5888, Loss - 0.8350719328472729, Learning Rate - 0.0015625, magnitude of gradient - 2.367222629543593\n",
      "Step - 5889, Loss - 0.7207668839567625, Learning Rate - 0.0015625, magnitude of gradient - 0.7930983065350058\n",
      "Step - 5890, Loss - 0.5034619799054286, Learning Rate - 0.0015625, magnitude of gradient - 0.5000448364107659\n",
      "Step - 5891, Loss - 0.5522704329091813, Learning Rate - 0.0015625, magnitude of gradient - 1.0302961071121215\n",
      "Step - 5892, Loss - 0.786246391706812, Learning Rate - 0.0015625, magnitude of gradient - 0.6824999181594091\n",
      "Step - 5893, Loss - 0.5994660494574504, Learning Rate - 0.0015625, magnitude of gradient - 1.60859800948553\n",
      "Step - 5894, Loss - 0.8021372410246039, Learning Rate - 0.0015625, magnitude of gradient - 1.4654736066921694\n",
      "Step - 5895, Loss - 0.56968514629845, Learning Rate - 0.0015625, magnitude of gradient - 0.3428089531170359\n",
      "Step - 5896, Loss - 0.7013061897744173, Learning Rate - 0.0015625, magnitude of gradient - 1.9383929682149652\n",
      "Step - 5897, Loss - 0.6573634957181892, Learning Rate - 0.0015625, magnitude of gradient - 0.6653576948385935\n",
      "Step - 5898, Loss - 0.7954237780182541, Learning Rate - 0.0015625, magnitude of gradient - 2.017726103391734\n",
      "Step - 5899, Loss - 0.6579556484553823, Learning Rate - 0.0015625, magnitude of gradient - 1.9312422973469878\n",
      "Step - 5900, Loss - 0.5838332840183103, Learning Rate - 0.0015625, magnitude of gradient - 1.7311916348867096\n",
      "Step - 5901, Loss - 0.8499992967956059, Learning Rate - 0.0015625, magnitude of gradient - 1.0563719528598243\n",
      "Step - 5902, Loss - 0.53352694415856, Learning Rate - 0.0015625, magnitude of gradient - 0.3434791210425647\n",
      "Step - 5903, Loss - 0.6901712917660311, Learning Rate - 0.0015625, magnitude of gradient - 1.2480505307782135\n",
      "Step - 5904, Loss - 0.6255981850491106, Learning Rate - 0.0015625, magnitude of gradient - 1.7532917032889899\n",
      "Step - 5905, Loss - 0.7336631051261048, Learning Rate - 0.0015625, magnitude of gradient - 1.0578358702791097\n",
      "Step - 5906, Loss - 0.9339102233264137, Learning Rate - 0.0015625, magnitude of gradient - 2.506469785180665\n",
      "Step - 5907, Loss - 0.652612036617438, Learning Rate - 0.0015625, magnitude of gradient - 1.338588585867704\n",
      "Step - 5908, Loss - 0.8307581799942252, Learning Rate - 0.0015625, magnitude of gradient - 1.4274676222647373\n",
      "Step - 5909, Loss - 0.5849256295437617, Learning Rate - 0.0015625, magnitude of gradient - 0.46594676788952455\n",
      "Step - 5910, Loss - 0.7457733135683955, Learning Rate - 0.0015625, magnitude of gradient - 0.9792219298841861\n",
      "Step - 5911, Loss - 0.7582564778874669, Learning Rate - 0.0015625, magnitude of gradient - 1.1344641799239497\n",
      "Step - 5912, Loss - 0.8102570183102353, Learning Rate - 0.0015625, magnitude of gradient - 1.0786626477405685\n",
      "Step - 5913, Loss - 0.6551425104571348, Learning Rate - 0.0015625, magnitude of gradient - 0.8709613760686221\n",
      "Step - 5914, Loss - 0.4751614695811681, Learning Rate - 0.0015625, magnitude of gradient - 0.9020514616512206\n",
      "Step - 5915, Loss - 0.5694565465012057, Learning Rate - 0.0015625, magnitude of gradient - 1.0054785539145754\n",
      "Step - 5916, Loss - 0.5492552915680239, Learning Rate - 0.0015625, magnitude of gradient - 0.4615559373663206\n",
      "Step - 5917, Loss - 0.7929855559909056, Learning Rate - 0.0015625, magnitude of gradient - 1.8519064696272138\n",
      "Step - 5918, Loss - 0.8333908308600343, Learning Rate - 0.0015625, magnitude of gradient - 1.0245987922786397\n",
      "Step - 5919, Loss - 0.5559940997019758, Learning Rate - 0.0015625, magnitude of gradient - 0.4840556216425897\n",
      "Step - 5920, Loss - 0.5831837813024617, Learning Rate - 0.0015625, magnitude of gradient - 0.8144097372328706\n",
      "Step - 5921, Loss - 0.773328323582761, Learning Rate - 0.0015625, magnitude of gradient - 0.9867875322737242\n",
      "Step - 5922, Loss - 0.7692031814535816, Learning Rate - 0.0015625, magnitude of gradient - 1.1993037587887696\n",
      "Step - 5923, Loss - 0.7990139334756017, Learning Rate - 0.0015625, magnitude of gradient - 0.16315629672540416\n",
      "Step - 5924, Loss - 0.6647980091484312, Learning Rate - 0.0015625, magnitude of gradient - 1.5494583996558935\n",
      "Step - 5925, Loss - 0.5487000786744815, Learning Rate - 0.0015625, magnitude of gradient - 3.918481246367294\n",
      "Step - 5926, Loss - 0.7010743188660173, Learning Rate - 0.0015625, magnitude of gradient - 1.4800078179153626\n",
      "Step - 5927, Loss - 0.7043928640456327, Learning Rate - 0.0015625, magnitude of gradient - 1.3979863235339076\n",
      "Step - 5928, Loss - 0.7726979453588285, Learning Rate - 0.0015625, magnitude of gradient - 2.354069990632241\n",
      "Step - 5929, Loss - 0.5442248313516813, Learning Rate - 0.0015625, magnitude of gradient - 0.45241968419696993\n",
      "Step - 5930, Loss - 0.7284582363095733, Learning Rate - 0.0015625, magnitude of gradient - 1.4170750430000172\n",
      "Step - 5931, Loss - 0.705186714672654, Learning Rate - 0.0015625, magnitude of gradient - 0.6500759277703404\n",
      "Step - 5932, Loss - 0.7801752945280257, Learning Rate - 0.0015625, magnitude of gradient - 1.7261721907942864\n",
      "Step - 5933, Loss - 0.7440377667333349, Learning Rate - 0.0015625, magnitude of gradient - 0.745196258019415\n",
      "Step - 5934, Loss - 0.5067388917515155, Learning Rate - 0.0015625, magnitude of gradient - 2.1815638045641244\n",
      "Step - 5935, Loss - 0.7055153442414579, Learning Rate - 0.0015625, magnitude of gradient - 0.3391949328757737\n",
      "Step - 5936, Loss - 0.8850139618938948, Learning Rate - 0.0015625, magnitude of gradient - 1.6233876755307117\n",
      "Step - 5937, Loss - 0.6656789372583265, Learning Rate - 0.0015625, magnitude of gradient - 1.1375524339390013\n",
      "Step - 5938, Loss - 0.8084922867474809, Learning Rate - 0.0015625, magnitude of gradient - 0.7547692354284457\n",
      "Step - 5939, Loss - 0.6879918820146202, Learning Rate - 0.0015625, magnitude of gradient - 0.4357382626398992\n",
      "Step - 5940, Loss - 0.6768381421401432, Learning Rate - 0.0015625, magnitude of gradient - 1.559894236310965\n",
      "Step - 5941, Loss - 0.5822216740600642, Learning Rate - 0.0015625, magnitude of gradient - 0.8404642287479661\n",
      "Step - 5942, Loss - 0.6895852729864357, Learning Rate - 0.0015625, magnitude of gradient - 1.354822434480619\n",
      "Step - 5943, Loss - 0.8065937106505223, Learning Rate - 0.0015625, magnitude of gradient - 1.2296500379572775\n",
      "Step - 5944, Loss - 0.6903113925132691, Learning Rate - 0.0015625, magnitude of gradient - 0.925200260311637\n",
      "Step - 5945, Loss - 0.8267466071652462, Learning Rate - 0.0015625, magnitude of gradient - 1.044297514588749\n",
      "Step - 5946, Loss - 0.5286099274129301, Learning Rate - 0.0015625, magnitude of gradient - 0.9988067346644455\n",
      "Step - 5947, Loss - 0.7872416161609745, Learning Rate - 0.0015625, magnitude of gradient - 2.396260355245267\n",
      "Step - 5948, Loss - 0.7442652329023136, Learning Rate - 0.0015625, magnitude of gradient - 1.0619292038925425\n",
      "Step - 5949, Loss - 0.6299850215227915, Learning Rate - 0.0015625, magnitude of gradient - 2.140381868639192\n",
      "Step - 5950, Loss - 0.6086384241382832, Learning Rate - 0.0015625, magnitude of gradient - 1.4232446691869756\n",
      "Step - 5951, Loss - 0.6124575004544288, Learning Rate - 0.0015625, magnitude of gradient - 1.2678452546081156\n",
      "Step - 5952, Loss - 0.6054854909449832, Learning Rate - 0.0015625, magnitude of gradient - 0.5119713377064765\n",
      "Step - 5953, Loss - 0.6206477468017096, Learning Rate - 0.0015625, magnitude of gradient - 0.983896437002064\n",
      "Step - 5954, Loss - 0.6124225450623271, Learning Rate - 0.0015625, magnitude of gradient - 1.299517688195539\n",
      "Step - 5955, Loss - 0.8409378544726994, Learning Rate - 0.0015625, magnitude of gradient - 0.7099661425645177\n",
      "Step - 5956, Loss - 0.607080298454312, Learning Rate - 0.0015625, magnitude of gradient - 1.5671390564477787\n",
      "Step - 5957, Loss - 0.6791422474253062, Learning Rate - 0.0015625, magnitude of gradient - 0.8722686266194596\n",
      "Step - 5958, Loss - 0.8873903432201415, Learning Rate - 0.0015625, magnitude of gradient - 1.6280857464190046\n",
      "Step - 5959, Loss - 0.9966560760563081, Learning Rate - 0.0015625, magnitude of gradient - 2.974136694757665\n",
      "Step - 5960, Loss - 0.6319940410228809, Learning Rate - 0.0015625, magnitude of gradient - 0.774727002732427\n",
      "Step - 5961, Loss - 0.5574757960755437, Learning Rate - 0.0015625, magnitude of gradient - 1.2317224778796145\n",
      "Step - 5962, Loss - 0.610010007429154, Learning Rate - 0.0015625, magnitude of gradient - 1.2253444724927167\n",
      "Step - 5963, Loss - 0.7756608023824563, Learning Rate - 0.0015625, magnitude of gradient - 2.5419763889906957\n",
      "Step - 5964, Loss - 0.4856958096096344, Learning Rate - 0.0015625, magnitude of gradient - 1.911002562566508\n",
      "Step - 5965, Loss - 0.5768980121668315, Learning Rate - 0.0015625, magnitude of gradient - 1.787484415590422\n",
      "Step - 5966, Loss - 0.7207413737860444, Learning Rate - 0.0015625, magnitude of gradient - 1.229637489538147\n",
      "Step - 5967, Loss - 0.5957100180852353, Learning Rate - 0.0015625, magnitude of gradient - 1.0540954124622344\n",
      "Step - 5968, Loss - 0.6508261819162645, Learning Rate - 0.0015625, magnitude of gradient - 0.29543122170867475\n",
      "Step - 5969, Loss - 0.656189934000586, Learning Rate - 0.0015625, magnitude of gradient - 1.4153055888248354\n",
      "Step - 5970, Loss - 0.7627459634773892, Learning Rate - 0.0015625, magnitude of gradient - 1.6416422442231497\n",
      "Step - 5971, Loss - 0.7421730344544585, Learning Rate - 0.0015625, magnitude of gradient - 0.8860700563771191\n",
      "Step - 5972, Loss - 0.5400409020569175, Learning Rate - 0.0015625, magnitude of gradient - 0.834501153106648\n",
      "Step - 5973, Loss - 0.5818926329375514, Learning Rate - 0.0015625, magnitude of gradient - 1.230956263801575\n",
      "Step - 5974, Loss - 0.6687340931665995, Learning Rate - 0.0015625, magnitude of gradient - 1.3465733613734385\n",
      "Step - 5975, Loss - 0.9796181801235911, Learning Rate - 0.0015625, magnitude of gradient - 1.3378845833392128\n",
      "Step - 5976, Loss - 0.7049336267339923, Learning Rate - 0.0015625, magnitude of gradient - 1.9559697480862732\n",
      "Step - 5977, Loss - 0.8388366649805671, Learning Rate - 0.0015625, magnitude of gradient - 1.9224229569815605\n",
      "Step - 5978, Loss - 0.7372498773326878, Learning Rate - 0.0015625, magnitude of gradient - 1.8635461784385463\n",
      "Step - 5979, Loss - 0.6146817765142021, Learning Rate - 0.0015625, magnitude of gradient - 1.4523918503289217\n",
      "Step - 5980, Loss - 0.8407329493217806, Learning Rate - 0.0015625, magnitude of gradient - 1.3475026686071938\n",
      "Step - 5981, Loss - 0.8183796067257011, Learning Rate - 0.0015625, magnitude of gradient - 1.0083687742014753\n",
      "Step - 5982, Loss - 0.6576304828189488, Learning Rate - 0.0015625, magnitude of gradient - 1.4807503034263916\n",
      "Step - 5983, Loss - 0.7808299309549205, Learning Rate - 0.0015625, magnitude of gradient - 1.0870848471192542\n",
      "Step - 5984, Loss - 0.7921114993960168, Learning Rate - 0.0015625, magnitude of gradient - 0.8777158283730316\n",
      "Step - 5985, Loss - 0.7262949503646795, Learning Rate - 0.0015625, magnitude of gradient - 2.1156932434861644\n",
      "Step - 5986, Loss - 0.6542497893767025, Learning Rate - 0.0015625, magnitude of gradient - 1.7090705008551472\n",
      "Step - 5987, Loss - 0.576977434403929, Learning Rate - 0.0015625, magnitude of gradient - 0.6747126354612321\n",
      "Step - 5988, Loss - 0.6868393240650369, Learning Rate - 0.0015625, magnitude of gradient - 0.7278971210074167\n",
      "Step - 5989, Loss - 0.7076479049459568, Learning Rate - 0.0015625, magnitude of gradient - 1.212831821164599\n",
      "Step - 5990, Loss - 0.6482624107410132, Learning Rate - 0.0015625, magnitude of gradient - 0.8270594993867662\n",
      "Step - 5991, Loss - 0.8159156712978648, Learning Rate - 0.0015625, magnitude of gradient - 0.434420172820507\n",
      "Step - 5992, Loss - 0.4080596861761344, Learning Rate - 0.0015625, magnitude of gradient - 1.2571274899823106\n",
      "Step - 5993, Loss - 0.5551478012272534, Learning Rate - 0.0015625, magnitude of gradient - 0.6360046731414521\n",
      "Step - 5994, Loss - 0.4949835806012801, Learning Rate - 0.0015625, magnitude of gradient - 0.9925418308353652\n",
      "Step - 5995, Loss - 0.6463824519834984, Learning Rate - 0.0015625, magnitude of gradient - 2.403939962240502\n",
      "Step - 5996, Loss - 0.7609657787850694, Learning Rate - 0.0015625, magnitude of gradient - 2.342690121047149\n",
      "Step - 5997, Loss - 0.6655496579486011, Learning Rate - 0.0015625, magnitude of gradient - 0.9656878375693888\n",
      "Step - 5998, Loss - 0.7342930530623846, Learning Rate - 0.0015625, magnitude of gradient - 0.3980895922862222\n",
      "Step - 5999, Loss - 0.7437109617742516, Learning Rate - 0.0015625, magnitude of gradient - 1.1278559811760676\n",
      "Step - 6000, Loss - 0.661656970458629, Learning Rate - 0.0015625, magnitude of gradient - 1.7631412521301235\n",
      "Step - 6001, Loss - 0.5729364720434367, Learning Rate - 0.00078125, magnitude of gradient - 2.056821703453911\n",
      "Step - 6002, Loss - 0.6781165143334438, Learning Rate - 0.00078125, magnitude of gradient - 1.2990705038527883\n",
      "Step - 6003, Loss - 0.6278071026152906, Learning Rate - 0.00078125, magnitude of gradient - 0.988671317484989\n",
      "Step - 6004, Loss - 0.5740580379871496, Learning Rate - 0.00078125, magnitude of gradient - 0.7274406688373343\n",
      "Step - 6005, Loss - 0.7719880782457523, Learning Rate - 0.00078125, magnitude of gradient - 1.1921601554272125\n",
      "Step - 6006, Loss - 0.7822854243218089, Learning Rate - 0.00078125, magnitude of gradient - 2.2920744121400958\n",
      "Step - 6007, Loss - 0.690634297247176, Learning Rate - 0.00078125, magnitude of gradient - 0.59624172743069\n",
      "Step - 6008, Loss - 0.6313736818151926, Learning Rate - 0.00078125, magnitude of gradient - 2.2288586752292745\n",
      "Step - 6009, Loss - 0.7462167374049284, Learning Rate - 0.00078125, magnitude of gradient - 1.5597548133706494\n",
      "Step - 6010, Loss - 0.5210310101250281, Learning Rate - 0.00078125, magnitude of gradient - 0.5235182158043459\n",
      "Step - 6011, Loss - 0.6500549072131134, Learning Rate - 0.00078125, magnitude of gradient - 0.7315271778888699\n",
      "Step - 6012, Loss - 0.5086344690299033, Learning Rate - 0.00078125, magnitude of gradient - 0.8623494850441894\n",
      "Step - 6013, Loss - 0.714502217761497, Learning Rate - 0.00078125, magnitude of gradient - 0.6375806648985045\n",
      "Step - 6014, Loss - 0.7204638985834181, Learning Rate - 0.00078125, magnitude of gradient - 1.3985581851187416\n",
      "Step - 6015, Loss - 0.6903093051442921, Learning Rate - 0.00078125, magnitude of gradient - 1.261669455234593\n",
      "Step - 6016, Loss - 0.7079521609540756, Learning Rate - 0.00078125, magnitude of gradient - 0.9702886587689094\n",
      "Step - 6017, Loss - 0.7019530663153624, Learning Rate - 0.00078125, magnitude of gradient - 1.0090166169315942\n",
      "Step - 6018, Loss - 0.798704420614505, Learning Rate - 0.00078125, magnitude of gradient - 0.9124598730815784\n",
      "Step - 6019, Loss - 0.6468633414217504, Learning Rate - 0.00078125, magnitude of gradient - 1.0503778332795233\n",
      "Step - 6020, Loss - 0.7893780535483266, Learning Rate - 0.00078125, magnitude of gradient - 1.0888210423983766\n",
      "Step - 6021, Loss - 0.6730964742275718, Learning Rate - 0.00078125, magnitude of gradient - 0.5569559727907295\n",
      "Step - 6022, Loss - 0.6857460586348822, Learning Rate - 0.00078125, magnitude of gradient - 2.0121371891421305\n",
      "Step - 6023, Loss - 0.9119292022290899, Learning Rate - 0.00078125, magnitude of gradient - 1.0217556571645112\n",
      "Step - 6024, Loss - 0.638155486150384, Learning Rate - 0.00078125, magnitude of gradient - 0.6393417965111842\n",
      "Step - 6025, Loss - 0.6890448554088403, Learning Rate - 0.00078125, magnitude of gradient - 1.7751005287030959\n",
      "Step - 6026, Loss - 0.7078005501095369, Learning Rate - 0.00078125, magnitude of gradient - 0.8254869654787437\n",
      "Step - 6027, Loss - 0.5721632853833234, Learning Rate - 0.00078125, magnitude of gradient - 1.3767073572119586\n",
      "Step - 6028, Loss - 0.5521202644771361, Learning Rate - 0.00078125, magnitude of gradient - 1.549141642119033\n",
      "Step - 6029, Loss - 0.6833618859933404, Learning Rate - 0.00078125, magnitude of gradient - 1.4678943304191074\n",
      "Step - 6030, Loss - 0.6625197918955813, Learning Rate - 0.00078125, magnitude of gradient - 1.7476775530124375\n",
      "Step - 6031, Loss - 0.5811882418933281, Learning Rate - 0.00078125, magnitude of gradient - 0.6323060718664498\n",
      "Step - 6032, Loss - 0.6040140209612146, Learning Rate - 0.00078125, magnitude of gradient - 1.0724675847092922\n",
      "Step - 6033, Loss - 0.7399622438536482, Learning Rate - 0.00078125, magnitude of gradient - 1.3146936315155036\n",
      "Step - 6034, Loss - 0.7249092563542964, Learning Rate - 0.00078125, magnitude of gradient - 1.7925410837310058\n",
      "Step - 6035, Loss - 0.659424511996195, Learning Rate - 0.00078125, magnitude of gradient - 0.6840093128492896\n",
      "Step - 6036, Loss - 0.8331209178528998, Learning Rate - 0.00078125, magnitude of gradient - 2.5291415358318363\n",
      "Step - 6037, Loss - 0.501149420679351, Learning Rate - 0.00078125, magnitude of gradient - 0.4866209701169923\n",
      "Step - 6038, Loss - 0.7293302546432786, Learning Rate - 0.00078125, magnitude of gradient - 1.311073497670476\n",
      "Step - 6039, Loss - 0.6199217466525675, Learning Rate - 0.00078125, magnitude of gradient - 2.6859794752405963\n",
      "Step - 6040, Loss - 0.9501732916800333, Learning Rate - 0.00078125, magnitude of gradient - 2.2573730286324625\n",
      "Step - 6041, Loss - 0.7290982085689666, Learning Rate - 0.00078125, magnitude of gradient - 1.3773696162258948\n",
      "Step - 6042, Loss - 0.7142607473151116, Learning Rate - 0.00078125, magnitude of gradient - 0.9867273379676982\n",
      "Step - 6043, Loss - 0.5882887971500915, Learning Rate - 0.00078125, magnitude of gradient - 1.1521193368157798\n",
      "Step - 6044, Loss - 0.5378840457175468, Learning Rate - 0.00078125, magnitude of gradient - 1.3400952451925268\n",
      "Step - 6045, Loss - 0.5872161653971799, Learning Rate - 0.00078125, magnitude of gradient - 1.4652723174060014\n",
      "Step - 6046, Loss - 0.8333002880077516, Learning Rate - 0.00078125, magnitude of gradient - 0.49356059929073076\n",
      "Step - 6047, Loss - 0.6055775365917279, Learning Rate - 0.00078125, magnitude of gradient - 0.9561128882975666\n",
      "Step - 6048, Loss - 0.8157609666609642, Learning Rate - 0.00078125, magnitude of gradient - 2.6612965099793393\n",
      "Step - 6049, Loss - 0.5821158538753395, Learning Rate - 0.00078125, magnitude of gradient - 0.892184208906589\n",
      "Step - 6050, Loss - 0.7314917673813466, Learning Rate - 0.00078125, magnitude of gradient - 0.8833715309702354\n",
      "Step - 6051, Loss - 0.483874036837862, Learning Rate - 0.00078125, magnitude of gradient - 1.4646872907815964\n",
      "Step - 6052, Loss - 0.6841860215575455, Learning Rate - 0.00078125, magnitude of gradient - 1.001305506453132\n",
      "Step - 6053, Loss - 0.8343666628550765, Learning Rate - 0.00078125, magnitude of gradient - 2.265345894217949\n",
      "Step - 6054, Loss - 0.6974350780937051, Learning Rate - 0.00078125, magnitude of gradient - 0.382635052477637\n",
      "Step - 6055, Loss - 0.770954312097963, Learning Rate - 0.00078125, magnitude of gradient - 1.6594469733633355\n",
      "Step - 6056, Loss - 0.728579298547377, Learning Rate - 0.00078125, magnitude of gradient - 0.8140716484390519\n",
      "Step - 6057, Loss - 0.7074050243410513, Learning Rate - 0.00078125, magnitude of gradient - 1.7925292296485151\n",
      "Step - 6058, Loss - 0.6507223314293143, Learning Rate - 0.00078125, magnitude of gradient - 0.7586425273085855\n",
      "Step - 6059, Loss - 0.9044498064771288, Learning Rate - 0.00078125, magnitude of gradient - 0.9125886310361269\n",
      "Step - 6060, Loss - 0.598536983400869, Learning Rate - 0.00078125, magnitude of gradient - 1.2728940367083452\n",
      "Step - 6061, Loss - 0.7923362163727073, Learning Rate - 0.00078125, magnitude of gradient - 1.9144861211511366\n",
      "Step - 6062, Loss - 0.6812869255805016, Learning Rate - 0.00078125, magnitude of gradient - 0.9243847508487442\n",
      "Step - 6063, Loss - 0.5656529387895355, Learning Rate - 0.00078125, magnitude of gradient - 0.6285891451238439\n",
      "Step - 6064, Loss - 0.8848830011078744, Learning Rate - 0.00078125, magnitude of gradient - 1.094874678888557\n",
      "Step - 6065, Loss - 0.693042086071342, Learning Rate - 0.00078125, magnitude of gradient - 0.964584631564527\n",
      "Step - 6066, Loss - 0.6945874176736824, Learning Rate - 0.00078125, magnitude of gradient - 0.5313009448095423\n",
      "Step - 6067, Loss - 0.8077676131768496, Learning Rate - 0.00078125, magnitude of gradient - 1.267606782899424\n",
      "Step - 6068, Loss - 0.5697190935672823, Learning Rate - 0.00078125, magnitude of gradient - 1.402591613921385\n",
      "Step - 6069, Loss - 0.6317827709725047, Learning Rate - 0.00078125, magnitude of gradient - 2.391512785047402\n",
      "Step - 6070, Loss - 0.6943503752739337, Learning Rate - 0.00078125, magnitude of gradient - 0.7467375385213694\n",
      "Step - 6071, Loss - 0.7834670407690713, Learning Rate - 0.00078125, magnitude of gradient - 0.7546802641140105\n",
      "Step - 6072, Loss - 0.608169468163419, Learning Rate - 0.00078125, magnitude of gradient - 1.0393442214420294\n",
      "Step - 6073, Loss - 0.7244222054173326, Learning Rate - 0.00078125, magnitude of gradient - 2.1735232849420085\n",
      "Step - 6074, Loss - 0.7498412627106567, Learning Rate - 0.00078125, magnitude of gradient - 1.215684400978318\n",
      "Step - 6075, Loss - 0.8122308474507978, Learning Rate - 0.00078125, magnitude of gradient - 2.0451983020979294\n",
      "Step - 6076, Loss - 0.6797880152865099, Learning Rate - 0.00078125, magnitude of gradient - 0.8712249284993362\n",
      "Step - 6077, Loss - 0.6805045751519642, Learning Rate - 0.00078125, magnitude of gradient - 0.772975091876544\n",
      "Step - 6078, Loss - 0.562313724241001, Learning Rate - 0.00078125, magnitude of gradient - 1.8742259241089472\n",
      "Step - 6079, Loss - 0.7524751810012019, Learning Rate - 0.00078125, magnitude of gradient - 0.6167152131473127\n",
      "Step - 6080, Loss - 0.5929192719239565, Learning Rate - 0.00078125, magnitude of gradient - 1.4975258588081835\n",
      "Step - 6081, Loss - 0.6856132670214514, Learning Rate - 0.00078125, magnitude of gradient - 0.6661500058137517\n",
      "Step - 6082, Loss - 0.8087579948433554, Learning Rate - 0.00078125, magnitude of gradient - 1.0576996523403528\n",
      "Step - 6083, Loss - 0.8252227506832541, Learning Rate - 0.00078125, magnitude of gradient - 0.7264774190293309\n",
      "Step - 6084, Loss - 0.7997631134627139, Learning Rate - 0.00078125, magnitude of gradient - 1.9356384340391346\n",
      "Step - 6085, Loss - 0.6421910004318288, Learning Rate - 0.00078125, magnitude of gradient - 0.8083927689502215\n",
      "Step - 6086, Loss - 0.9016103354700195, Learning Rate - 0.00078125, magnitude of gradient - 1.1973100867424566\n",
      "Step - 6087, Loss - 0.6484795525970315, Learning Rate - 0.00078125, magnitude of gradient - 2.219518822365509\n",
      "Step - 6088, Loss - 0.6613046096362547, Learning Rate - 0.00078125, magnitude of gradient - 0.8769777414470589\n",
      "Step - 6089, Loss - 0.592335908398966, Learning Rate - 0.00078125, magnitude of gradient - 1.0510303818356266\n",
      "Step - 6090, Loss - 0.6627837784576063, Learning Rate - 0.00078125, magnitude of gradient - 2.291455410532945\n",
      "Step - 6091, Loss - 0.5953209060240059, Learning Rate - 0.00078125, magnitude of gradient - 0.889206132988857\n",
      "Step - 6092, Loss - 0.6789373243059035, Learning Rate - 0.00078125, magnitude of gradient - 0.9727492201418906\n",
      "Step - 6093, Loss - 0.7288966582885299, Learning Rate - 0.00078125, magnitude of gradient - 0.8994285743281328\n",
      "Step - 6094, Loss - 0.6465073680773232, Learning Rate - 0.00078125, magnitude of gradient - 0.7839213036804707\n",
      "Step - 6095, Loss - 0.8120067443916756, Learning Rate - 0.00078125, magnitude of gradient - 1.1123305772352674\n",
      "Step - 6096, Loss - 0.585765841796891, Learning Rate - 0.00078125, magnitude of gradient - 0.8230198331301969\n",
      "Step - 6097, Loss - 0.713328872603733, Learning Rate - 0.00078125, magnitude of gradient - 0.5960969383877213\n",
      "Step - 6098, Loss - 0.7014652901402694, Learning Rate - 0.00078125, magnitude of gradient - 1.8837448470744735\n",
      "Step - 6099, Loss - 0.8292393720598017, Learning Rate - 0.00078125, magnitude of gradient - 0.9570630740586814\n",
      "Step - 6100, Loss - 0.6388026209009903, Learning Rate - 0.00078125, magnitude of gradient - 0.68689714072062\n",
      "Step - 6101, Loss - 0.8445678169543882, Learning Rate - 0.00078125, magnitude of gradient - 0.5968963555471546\n",
      "Step - 6102, Loss - 0.6063731437860685, Learning Rate - 0.00078125, magnitude of gradient - 0.7191472867589872\n",
      "Step - 6103, Loss - 0.6756679485446846, Learning Rate - 0.00078125, magnitude of gradient - 0.622453241037113\n",
      "Step - 6104, Loss - 0.6396038217480264, Learning Rate - 0.00078125, magnitude of gradient - 0.9433788954944041\n",
      "Step - 6105, Loss - 0.7853930560190916, Learning Rate - 0.00078125, magnitude of gradient - 1.7156461921581245\n",
      "Step - 6106, Loss - 0.8070198721065704, Learning Rate - 0.00078125, magnitude of gradient - 1.2227774958623239\n",
      "Step - 6107, Loss - 0.6285077722738533, Learning Rate - 0.00078125, magnitude of gradient - 1.984016654342605\n",
      "Step - 6108, Loss - 0.7943591610019416, Learning Rate - 0.00078125, magnitude of gradient - 0.5948902444787889\n",
      "Step - 6109, Loss - 0.6820411727394818, Learning Rate - 0.00078125, magnitude of gradient - 0.7182506723060139\n",
      "Step - 6110, Loss - 0.7717900231728075, Learning Rate - 0.00078125, magnitude of gradient - 1.7361719259231867\n",
      "Step - 6111, Loss - 0.6921058629105991, Learning Rate - 0.00078125, magnitude of gradient - 1.9226504027838334\n",
      "Step - 6112, Loss - 0.529504159495317, Learning Rate - 0.00078125, magnitude of gradient - 1.0793949271575\n",
      "Step - 6113, Loss - 0.5825201717455375, Learning Rate - 0.00078125, magnitude of gradient - 2.50817677900388\n",
      "Step - 6114, Loss - 0.6795355609397296, Learning Rate - 0.00078125, magnitude of gradient - 0.3731446826396905\n",
      "Step - 6115, Loss - 0.7733132436295252, Learning Rate - 0.00078125, magnitude of gradient - 1.547609100178807\n",
      "Step - 6116, Loss - 0.6705806935123109, Learning Rate - 0.00078125, magnitude of gradient - 0.9067043655076796\n",
      "Step - 6117, Loss - 0.530124659557037, Learning Rate - 0.00078125, magnitude of gradient - 0.934413035606199\n",
      "Step - 6118, Loss - 0.6102401620971363, Learning Rate - 0.00078125, magnitude of gradient - 0.818850638127178\n",
      "Step - 6119, Loss - 0.5190280365803706, Learning Rate - 0.00078125, magnitude of gradient - 1.5195999036361634\n",
      "Step - 6120, Loss - 0.6075166611051768, Learning Rate - 0.00078125, magnitude of gradient - 1.2298711999447003\n",
      "Step - 6121, Loss - 0.684750250313928, Learning Rate - 0.00078125, magnitude of gradient - 1.0561434340355447\n",
      "Step - 6122, Loss - 0.8017907969796314, Learning Rate - 0.00078125, magnitude of gradient - 1.6948812865219907\n",
      "Step - 6123, Loss - 0.7370765726343642, Learning Rate - 0.00078125, magnitude of gradient - 0.7464969095980548\n",
      "Step - 6124, Loss - 0.7195133290094204, Learning Rate - 0.00078125, magnitude of gradient - 1.9145672402982175\n",
      "Step - 6125, Loss - 0.821444836432557, Learning Rate - 0.00078125, magnitude of gradient - 1.5595471326237749\n",
      "Step - 6126, Loss - 0.6074209521113311, Learning Rate - 0.00078125, magnitude of gradient - 1.2795127718072166\n",
      "Step - 6127, Loss - 0.7443797930217423, Learning Rate - 0.00078125, magnitude of gradient - 1.8575740686822646\n",
      "Step - 6128, Loss - 0.6741930739808931, Learning Rate - 0.00078125, magnitude of gradient - 0.8053958227574222\n",
      "Step - 6129, Loss - 0.6950583895836489, Learning Rate - 0.00078125, magnitude of gradient - 0.8539448865616248\n",
      "Step - 6130, Loss - 0.7183702171743636, Learning Rate - 0.00078125, magnitude of gradient - 0.9980382661185756\n",
      "Step - 6131, Loss - 0.6580890185656922, Learning Rate - 0.00078125, magnitude of gradient - 1.2257587913815216\n",
      "Step - 6132, Loss - 0.6492488674099391, Learning Rate - 0.00078125, magnitude of gradient - 1.0957575089948348\n",
      "Step - 6133, Loss - 0.7035116631092286, Learning Rate - 0.00078125, magnitude of gradient - 1.8677465064958156\n",
      "Step - 6134, Loss - 0.5882710682109484, Learning Rate - 0.00078125, magnitude of gradient - 1.3518363609547943\n",
      "Step - 6135, Loss - 0.7064765827536671, Learning Rate - 0.00078125, magnitude of gradient - 0.7128087336016773\n",
      "Step - 6136, Loss - 0.6821944741288116, Learning Rate - 0.00078125, magnitude of gradient - 0.6461314965414755\n",
      "Step - 6137, Loss - 0.8307132183527338, Learning Rate - 0.00078125, magnitude of gradient - 1.1886903405884721\n",
      "Step - 6138, Loss - 0.7206965934878253, Learning Rate - 0.00078125, magnitude of gradient - 1.2428271601872092\n",
      "Step - 6139, Loss - 0.8097008541806702, Learning Rate - 0.00078125, magnitude of gradient - 1.8394815341901256\n",
      "Step - 6140, Loss - 0.9637720906749782, Learning Rate - 0.00078125, magnitude of gradient - 0.8547052510529791\n",
      "Step - 6141, Loss - 0.606810105382268, Learning Rate - 0.00078125, magnitude of gradient - 2.7034591491166\n",
      "Step - 6142, Loss - 0.8583615018442086, Learning Rate - 0.00078125, magnitude of gradient - 0.9141629203676327\n",
      "Step - 6143, Loss - 0.6664411583244214, Learning Rate - 0.00078125, magnitude of gradient - 3.6798844094100125\n",
      "Step - 6144, Loss - 0.6010912072762744, Learning Rate - 0.00078125, magnitude of gradient - 1.6267174787944565\n",
      "Step - 6145, Loss - 0.616135327501674, Learning Rate - 0.00078125, magnitude of gradient - 1.0851541741053485\n",
      "Step - 6146, Loss - 0.5640364823721901, Learning Rate - 0.00078125, magnitude of gradient - 0.9545530101646533\n",
      "Step - 6147, Loss - 0.6440059303460315, Learning Rate - 0.00078125, magnitude of gradient - 1.2975126941949022\n",
      "Step - 6148, Loss - 0.8122015517076926, Learning Rate - 0.00078125, magnitude of gradient - 1.9621461721022335\n",
      "Step - 6149, Loss - 0.7918960883192041, Learning Rate - 0.00078125, magnitude of gradient - 2.1209486682616214\n",
      "Step - 6150, Loss - 0.7639017026527966, Learning Rate - 0.00078125, magnitude of gradient - 1.3027550129695595\n",
      "Step - 6151, Loss - 0.8519959293688015, Learning Rate - 0.00078125, magnitude of gradient - 1.6585355887982323\n",
      "Step - 6152, Loss - 0.6275049484578823, Learning Rate - 0.00078125, magnitude of gradient - 1.1548465548639422\n",
      "Step - 6153, Loss - 0.7927091787081235, Learning Rate - 0.00078125, magnitude of gradient - 2.014689257880342\n",
      "Step - 6154, Loss - 0.842838760829905, Learning Rate - 0.00078125, magnitude of gradient - 1.1247486190672165\n",
      "Step - 6155, Loss - 0.7124058865551279, Learning Rate - 0.00078125, magnitude of gradient - 1.0473558853630989\n",
      "Step - 6156, Loss - 0.6146622771132453, Learning Rate - 0.00078125, magnitude of gradient - 0.7704608298765516\n",
      "Step - 6157, Loss - 0.7216544361366802, Learning Rate - 0.00078125, magnitude of gradient - 1.215209853652736\n",
      "Step - 6158, Loss - 0.674167244605804, Learning Rate - 0.00078125, magnitude of gradient - 0.6083227856345363\n",
      "Step - 6159, Loss - 0.5154806451209004, Learning Rate - 0.00078125, magnitude of gradient - 0.308779986946402\n",
      "Step - 6160, Loss - 0.7851582150305336, Learning Rate - 0.00078125, magnitude of gradient - 0.6371490304568014\n",
      "Step - 6161, Loss - 0.6772319277937896, Learning Rate - 0.00078125, magnitude of gradient - 0.6774622937331612\n",
      "Step - 6162, Loss - 0.4297901313985599, Learning Rate - 0.00078125, magnitude of gradient - 1.2060415399604067\n",
      "Step - 6163, Loss - 0.664469026054203, Learning Rate - 0.00078125, magnitude of gradient - 1.4938750795355427\n",
      "Step - 6164, Loss - 0.6809913057197745, Learning Rate - 0.00078125, magnitude of gradient - 2.155488379432952\n",
      "Step - 6165, Loss - 0.7025298925884781, Learning Rate - 0.00078125, magnitude of gradient - 0.7967460522264644\n",
      "Step - 6166, Loss - 0.7224037686598476, Learning Rate - 0.00078125, magnitude of gradient - 1.146821557388465\n",
      "Step - 6167, Loss - 0.8411826563931208, Learning Rate - 0.00078125, magnitude of gradient - 1.2469455862966654\n",
      "Step - 6168, Loss - 0.6538496602486346, Learning Rate - 0.00078125, magnitude of gradient - 1.0504467850932595\n",
      "Step - 6169, Loss - 0.8581634230139487, Learning Rate - 0.00078125, magnitude of gradient - 1.4676225430467806\n",
      "Step - 6170, Loss - 0.6587240831949657, Learning Rate - 0.00078125, magnitude of gradient - 0.6346151060679929\n",
      "Step - 6171, Loss - 0.9552313506400081, Learning Rate - 0.00078125, magnitude of gradient - 1.8811395123822499\n",
      "Step - 6172, Loss - 0.5413933110551764, Learning Rate - 0.00078125, magnitude of gradient - 2.4720127146091\n",
      "Step - 6173, Loss - 1.070394560813603, Learning Rate - 0.00078125, magnitude of gradient - 2.522896122267292\n",
      "Step - 6174, Loss - 0.6643936197133979, Learning Rate - 0.00078125, magnitude of gradient - 1.7146729018740758\n",
      "Step - 6175, Loss - 0.753995426567927, Learning Rate - 0.00078125, magnitude of gradient - 1.2372295540932523\n",
      "Step - 6176, Loss - 0.6433742557538659, Learning Rate - 0.00078125, magnitude of gradient - 2.725080392384053\n",
      "Step - 6177, Loss - 0.7144568959423059, Learning Rate - 0.00078125, magnitude of gradient - 1.5936632057529623\n",
      "Step - 6178, Loss - 0.8121722144610783, Learning Rate - 0.00078125, magnitude of gradient - 1.761823707608019\n",
      "Step - 6179, Loss - 0.8013144567962617, Learning Rate - 0.00078125, magnitude of gradient - 1.013767475857347\n",
      "Step - 6180, Loss - 0.6468856725209376, Learning Rate - 0.00078125, magnitude of gradient - 0.9826050051852471\n",
      "Step - 6181, Loss - 0.7480111023375948, Learning Rate - 0.00078125, magnitude of gradient - 1.285871002582093\n",
      "Step - 6182, Loss - 0.7730802224894604, Learning Rate - 0.00078125, magnitude of gradient - 1.277787861266842\n",
      "Step - 6183, Loss - 0.6966686768828194, Learning Rate - 0.00078125, magnitude of gradient - 1.2059713144958502\n",
      "Step - 6184, Loss - 0.5357161377456716, Learning Rate - 0.00078125, magnitude of gradient - 1.6689457895162385\n",
      "Step - 6185, Loss - 0.8598791585465136, Learning Rate - 0.00078125, magnitude of gradient - 1.0852553754232641\n",
      "Step - 6186, Loss - 0.5645371006398788, Learning Rate - 0.00078125, magnitude of gradient - 0.7942592144768921\n",
      "Step - 6187, Loss - 0.6509628386171347, Learning Rate - 0.00078125, magnitude of gradient - 2.0782455251171603\n",
      "Step - 6188, Loss - 0.698690432760788, Learning Rate - 0.00078125, magnitude of gradient - 1.1703128840578694\n",
      "Step - 6189, Loss - 0.8053549358956467, Learning Rate - 0.00078125, magnitude of gradient - 1.2027739642735986\n",
      "Step - 6190, Loss - 0.7526548055383516, Learning Rate - 0.00078125, magnitude of gradient - 1.2490966784575264\n",
      "Step - 6191, Loss - 0.8213122652242015, Learning Rate - 0.00078125, magnitude of gradient - 0.8648795320091941\n",
      "Step - 6192, Loss - 0.5899383610703162, Learning Rate - 0.00078125, magnitude of gradient - 0.8580214724178735\n",
      "Step - 6193, Loss - 0.6584436155334801, Learning Rate - 0.00078125, magnitude of gradient - 0.9423760208974211\n",
      "Step - 6194, Loss - 0.6545670930800329, Learning Rate - 0.00078125, magnitude of gradient - 1.0223279397074159\n",
      "Step - 6195, Loss - 0.9058906288741244, Learning Rate - 0.00078125, magnitude of gradient - 1.9861554022035592\n",
      "Step - 6196, Loss - 0.6347538548411318, Learning Rate - 0.00078125, magnitude of gradient - 1.4483098954065488\n",
      "Step - 6197, Loss - 0.6363886382197578, Learning Rate - 0.00078125, magnitude of gradient - 1.1163832112368417\n",
      "Step - 6198, Loss - 0.8730287805495476, Learning Rate - 0.00078125, magnitude of gradient - 1.6856983354440072\n",
      "Step - 6199, Loss - 0.84480708413324, Learning Rate - 0.00078125, magnitude of gradient - 0.5421244467612714\n",
      "Step - 6200, Loss - 0.6879067868603654, Learning Rate - 0.00078125, magnitude of gradient - 1.9080951753937427\n",
      "Step - 6201, Loss - 0.6844683162410732, Learning Rate - 0.00078125, magnitude of gradient - 0.34562968056364557\n",
      "Step - 6202, Loss - 0.673299499460884, Learning Rate - 0.00078125, magnitude of gradient - 1.6915564957375173\n",
      "Step - 6203, Loss - 0.6928152229022846, Learning Rate - 0.00078125, magnitude of gradient - 0.8254802520949978\n",
      "Step - 6204, Loss - 0.7781500803770824, Learning Rate - 0.00078125, magnitude of gradient - 1.229622001577548\n",
      "Step - 6205, Loss - 0.6090298142607122, Learning Rate - 0.00078125, magnitude of gradient - 1.3551312041236707\n",
      "Step - 6206, Loss - 0.6477481984937691, Learning Rate - 0.00078125, magnitude of gradient - 1.0756953023479083\n",
      "Step - 6207, Loss - 0.6955160442221089, Learning Rate - 0.00078125, magnitude of gradient - 1.5842908270988392\n",
      "Step - 6208, Loss - 0.8515783506814518, Learning Rate - 0.00078125, magnitude of gradient - 1.0450647693415855\n",
      "Step - 6209, Loss - 0.6454911717764377, Learning Rate - 0.00078125, magnitude of gradient - 0.271215456239893\n",
      "Step - 6210, Loss - 0.6065690263696794, Learning Rate - 0.00078125, magnitude of gradient - 0.4846033438709601\n",
      "Step - 6211, Loss - 0.6630421157565918, Learning Rate - 0.00078125, magnitude of gradient - 0.9586696750871624\n",
      "Step - 6212, Loss - 0.5468243259330734, Learning Rate - 0.00078125, magnitude of gradient - 1.0824384933792892\n",
      "Step - 6213, Loss - 0.7117793895173431, Learning Rate - 0.00078125, magnitude of gradient - 1.0934103081629996\n",
      "Step - 6214, Loss - 0.7397843164045821, Learning Rate - 0.00078125, magnitude of gradient - 1.1296152107234134\n",
      "Step - 6215, Loss - 0.8649346757329878, Learning Rate - 0.00078125, magnitude of gradient - 1.349714758932779\n",
      "Step - 6216, Loss - 0.5202776320194613, Learning Rate - 0.00078125, magnitude of gradient - 1.8406855127355826\n",
      "Step - 6217, Loss - 0.7300505737849582, Learning Rate - 0.00078125, magnitude of gradient - 1.376336183918726\n",
      "Step - 6218, Loss - 0.7311899014377379, Learning Rate - 0.00078125, magnitude of gradient - 1.373851013454782\n",
      "Step - 6219, Loss - 0.6539512832302412, Learning Rate - 0.00078125, magnitude of gradient - 1.4750052073661788\n",
      "Step - 6220, Loss - 0.7557881491472092, Learning Rate - 0.00078125, magnitude of gradient - 1.2727286564599174\n",
      "Step - 6221, Loss - 0.6829723992267672, Learning Rate - 0.00078125, magnitude of gradient - 1.5328716613351283\n",
      "Step - 6222, Loss - 0.5489388737934277, Learning Rate - 0.00078125, magnitude of gradient - 3.3399796309433127\n",
      "Step - 6223, Loss - 0.7164851729762547, Learning Rate - 0.00078125, magnitude of gradient - 2.723361525527077\n",
      "Step - 6224, Loss - 0.6594466169391602, Learning Rate - 0.00078125, magnitude of gradient - 1.3921485587806899\n",
      "Step - 6225, Loss - 0.7977517877430388, Learning Rate - 0.00078125, magnitude of gradient - 0.5162918124878386\n",
      "Step - 6226, Loss - 0.5821619345945912, Learning Rate - 0.00078125, magnitude of gradient - 0.6581169800706039\n",
      "Step - 6227, Loss - 0.5012409911773541, Learning Rate - 0.00078125, magnitude of gradient - 0.7080779501392563\n",
      "Step - 6228, Loss - 0.5305210315819178, Learning Rate - 0.00078125, magnitude of gradient - 0.9602668437840144\n",
      "Step - 6229, Loss - 0.6363596003765927, Learning Rate - 0.00078125, magnitude of gradient - 0.8365130003029658\n",
      "Step - 6230, Loss - 0.7558488072006074, Learning Rate - 0.00078125, magnitude of gradient - 1.5940262391731201\n",
      "Step - 6231, Loss - 0.8299012438844094, Learning Rate - 0.00078125, magnitude of gradient - 1.0739282844237723\n",
      "Step - 6232, Loss - 0.8769268890793351, Learning Rate - 0.00078125, magnitude of gradient - 0.8826699716198106\n",
      "Step - 6233, Loss - 0.6244799608334284, Learning Rate - 0.00078125, magnitude of gradient - 2.20237969723843\n",
      "Step - 6234, Loss - 0.893039645721232, Learning Rate - 0.00078125, magnitude of gradient - 1.074483939378402\n",
      "Step - 6235, Loss - 0.7456680430158933, Learning Rate - 0.00078125, magnitude of gradient - 1.6315833779989235\n",
      "Step - 6236, Loss - 0.7085553563176364, Learning Rate - 0.00078125, magnitude of gradient - 0.5965384709863173\n",
      "Step - 6237, Loss - 0.721193955278165, Learning Rate - 0.00078125, magnitude of gradient - 0.5325396683724325\n",
      "Step - 6238, Loss - 0.6282819597036625, Learning Rate - 0.00078125, magnitude of gradient - 1.1103998734493057\n",
      "Step - 6239, Loss - 0.6374515965470833, Learning Rate - 0.00078125, magnitude of gradient - 0.5681847792126744\n",
      "Step - 6240, Loss - 0.6612352714721195, Learning Rate - 0.00078125, magnitude of gradient - 2.05769951856021\n",
      "Step - 6241, Loss - 0.8441351990759633, Learning Rate - 0.00078125, magnitude of gradient - 1.0825591103008843\n",
      "Step - 6242, Loss - 0.7135999897969304, Learning Rate - 0.00078125, magnitude of gradient - 1.7647521726328987\n",
      "Step - 6243, Loss - 0.5772437166422175, Learning Rate - 0.00078125, magnitude of gradient - 0.915532611458597\n",
      "Step - 6244, Loss - 0.7858528263801542, Learning Rate - 0.00078125, magnitude of gradient - 0.543394007878325\n",
      "Step - 6245, Loss - 0.5893916047263086, Learning Rate - 0.00078125, magnitude of gradient - 0.7961711367737825\n",
      "Step - 6246, Loss - 0.5350855537454209, Learning Rate - 0.00078125, magnitude of gradient - 0.5514845303931815\n",
      "Step - 6247, Loss - 0.8152670578120843, Learning Rate - 0.00078125, magnitude of gradient - 1.8905535456582625\n",
      "Step - 6248, Loss - 0.6467231582605986, Learning Rate - 0.00078125, magnitude of gradient - 1.6036659617152471\n",
      "Step - 6249, Loss - 0.781996709625975, Learning Rate - 0.00078125, magnitude of gradient - 0.3077837935006443\n",
      "Step - 6250, Loss - 0.8139787058234443, Learning Rate - 0.00078125, magnitude of gradient - 0.9469316998726741\n",
      "Step - 6251, Loss - 0.9024785132505457, Learning Rate - 0.00078125, magnitude of gradient - 0.7535075483783944\n",
      "Step - 6252, Loss - 0.8533465399132774, Learning Rate - 0.00078125, magnitude of gradient - 0.8336690122460315\n",
      "Step - 6253, Loss - 0.5048386604278075, Learning Rate - 0.00078125, magnitude of gradient - 2.6299479902632044\n",
      "Step - 6254, Loss - 0.8366071688252465, Learning Rate - 0.00078125, magnitude of gradient - 1.0369141153166845\n",
      "Step - 6255, Loss - 0.7645373072887661, Learning Rate - 0.00078125, magnitude of gradient - 1.0234936627106568\n",
      "Step - 6256, Loss - 0.8020350532990054, Learning Rate - 0.00078125, magnitude of gradient - 0.6106034792709077\n",
      "Step - 6257, Loss - 0.7581188570268655, Learning Rate - 0.00078125, magnitude of gradient - 1.0083828473340692\n",
      "Step - 6258, Loss - 0.6865540506023627, Learning Rate - 0.00078125, magnitude of gradient - 1.6963245869379022\n",
      "Step - 6259, Loss - 0.6200663567740892, Learning Rate - 0.00078125, magnitude of gradient - 0.7312540432095234\n",
      "Step - 6260, Loss - 0.8621894712868198, Learning Rate - 0.00078125, magnitude of gradient - 1.5252323026923909\n",
      "Step - 6261, Loss - 0.7316398267957216, Learning Rate - 0.00078125, magnitude of gradient - 1.1624180201772472\n",
      "Step - 6262, Loss - 0.8407664048708001, Learning Rate - 0.00078125, magnitude of gradient - 0.7025961719239919\n",
      "Step - 6263, Loss - 0.8334862019309336, Learning Rate - 0.00078125, magnitude of gradient - 0.5835757651802806\n",
      "Step - 6264, Loss - 0.7470223610514763, Learning Rate - 0.00078125, magnitude of gradient - 1.2994764094651765\n",
      "Step - 6265, Loss - 0.6708899483029467, Learning Rate - 0.00078125, magnitude of gradient - 0.3896054138436733\n",
      "Step - 6266, Loss - 0.5911374254563072, Learning Rate - 0.00078125, magnitude of gradient - 1.496463739439355\n",
      "Step - 6267, Loss - 0.796867527939553, Learning Rate - 0.00078125, magnitude of gradient - 0.9944299683341918\n",
      "Step - 6268, Loss - 0.7644469022871089, Learning Rate - 0.00078125, magnitude of gradient - 1.410477777645467\n",
      "Step - 6269, Loss - 0.7232443167097041, Learning Rate - 0.00078125, magnitude of gradient - 1.1557686560754759\n",
      "Step - 6270, Loss - 0.6717468306822814, Learning Rate - 0.00078125, magnitude of gradient - 0.580254711971205\n",
      "Step - 6271, Loss - 0.7755641971067031, Learning Rate - 0.00078125, magnitude of gradient - 1.6906963952566938\n",
      "Step - 6272, Loss - 0.5731655221314441, Learning Rate - 0.00078125, magnitude of gradient - 0.9876995133538735\n",
      "Step - 6273, Loss - 0.5888629366663906, Learning Rate - 0.00078125, magnitude of gradient - 0.5199674175508681\n",
      "Step - 6274, Loss - 0.7002772823552439, Learning Rate - 0.00078125, magnitude of gradient - 0.4336642233801271\n",
      "Step - 6275, Loss - 0.7788427286039998, Learning Rate - 0.00078125, magnitude of gradient - 0.8908261189842012\n",
      "Step - 6276, Loss - 0.8327339003637678, Learning Rate - 0.00078125, magnitude of gradient - 1.5667924976806433\n",
      "Step - 6277, Loss - 0.75226427161895, Learning Rate - 0.00078125, magnitude of gradient - 1.2422150277481394\n",
      "Step - 6278, Loss - 0.7973077519118721, Learning Rate - 0.00078125, magnitude of gradient - 1.7376178454701585\n",
      "Step - 6279, Loss - 0.6335129043633003, Learning Rate - 0.00078125, magnitude of gradient - 0.867818710090292\n",
      "Step - 6280, Loss - 0.8384834903335919, Learning Rate - 0.00078125, magnitude of gradient - 1.4308918234103367\n",
      "Step - 6281, Loss - 0.6779153740154409, Learning Rate - 0.00078125, magnitude of gradient - 1.2539903227213398\n",
      "Step - 6282, Loss - 0.8187240239016488, Learning Rate - 0.00078125, magnitude of gradient - 2.767244699648512\n",
      "Step - 6283, Loss - 0.6886221504407967, Learning Rate - 0.00078125, magnitude of gradient - 0.7367449923034446\n",
      "Step - 6284, Loss - 0.6564793984382813, Learning Rate - 0.00078125, magnitude of gradient - 1.5757049765425228\n",
      "Step - 6285, Loss - 0.6129143112138481, Learning Rate - 0.00078125, magnitude of gradient - 1.5126756065545608\n",
      "Step - 6286, Loss - 0.6630944904873266, Learning Rate - 0.00078125, magnitude of gradient - 0.5425224241453582\n",
      "Step - 6287, Loss - 0.6975700337605357, Learning Rate - 0.00078125, magnitude of gradient - 0.8230505672856937\n",
      "Step - 6288, Loss - 0.7637293044897961, Learning Rate - 0.00078125, magnitude of gradient - 1.9876135040732579\n",
      "Step - 6289, Loss - 0.813400174372865, Learning Rate - 0.00078125, magnitude of gradient - 0.9850285483800615\n",
      "Step - 6290, Loss - 0.7124341716360136, Learning Rate - 0.00078125, magnitude of gradient - 2.2224860964481157\n",
      "Step - 6291, Loss - 0.7363576089301184, Learning Rate - 0.00078125, magnitude of gradient - 1.6293793649942776\n",
      "Step - 6292, Loss - 0.6722070709998973, Learning Rate - 0.00078125, magnitude of gradient - 0.4404480226724713\n",
      "Step - 6293, Loss - 0.7974002688758043, Learning Rate - 0.00078125, magnitude of gradient - 1.7533968792674828\n",
      "Step - 6294, Loss - 0.8326026661648135, Learning Rate - 0.00078125, magnitude of gradient - 0.654735759407359\n",
      "Step - 6295, Loss - 0.7552204558657165, Learning Rate - 0.00078125, magnitude of gradient - 1.0233541808176942\n",
      "Step - 6296, Loss - 0.5856693342911521, Learning Rate - 0.00078125, magnitude of gradient - 1.4308480705335502\n",
      "Step - 6297, Loss - 0.6437493490947743, Learning Rate - 0.00078125, magnitude of gradient - 2.075997541941692\n",
      "Step - 6298, Loss - 0.864472673207566, Learning Rate - 0.00078125, magnitude of gradient - 1.3312634783380453\n",
      "Step - 6299, Loss - 0.6796298565261756, Learning Rate - 0.00078125, magnitude of gradient - 1.8051867039607643\n",
      "Step - 6300, Loss - 0.8368148070071262, Learning Rate - 0.00078125, magnitude of gradient - 1.373963950150351\n",
      "Step - 6301, Loss - 0.6918663268787778, Learning Rate - 0.00078125, magnitude of gradient - 1.8357329738108183\n",
      "Step - 6302, Loss - 0.6382373650686786, Learning Rate - 0.00078125, magnitude of gradient - 2.2321201980143384\n",
      "Step - 6303, Loss - 0.808502169312675, Learning Rate - 0.00078125, magnitude of gradient - 1.6946732502403574\n",
      "Step - 6304, Loss - 0.517966028053026, Learning Rate - 0.00078125, magnitude of gradient - 0.8427417918381457\n",
      "Step - 6305, Loss - 0.6776867500105124, Learning Rate - 0.00078125, magnitude of gradient - 0.7196776870047366\n",
      "Step - 6306, Loss - 0.5328818104618102, Learning Rate - 0.00078125, magnitude of gradient - 0.5667849094615157\n",
      "Step - 6307, Loss - 0.7839662799741263, Learning Rate - 0.00078125, magnitude of gradient - 0.7404455053677889\n",
      "Step - 6308, Loss - 0.6146328973999395, Learning Rate - 0.00078125, magnitude of gradient - 2.1277301429207056\n",
      "Step - 6309, Loss - 0.6467255807857648, Learning Rate - 0.00078125, magnitude of gradient - 1.788038474493753\n",
      "Step - 6310, Loss - 0.7259700635753155, Learning Rate - 0.00078125, magnitude of gradient - 1.6450761200840505\n",
      "Step - 6311, Loss - 0.5642333790652164, Learning Rate - 0.00078125, magnitude of gradient - 0.9868681576652151\n",
      "Step - 6312, Loss - 0.7794509659069857, Learning Rate - 0.00078125, magnitude of gradient - 0.7400751122446495\n",
      "Step - 6313, Loss - 0.7969284700581336, Learning Rate - 0.00078125, magnitude of gradient - 1.452379618420806\n",
      "Step - 6314, Loss - 0.9114756636664113, Learning Rate - 0.00078125, magnitude of gradient - 1.4099810592742976\n",
      "Step - 6315, Loss - 0.6484557834910261, Learning Rate - 0.00078125, magnitude of gradient - 0.8972624747391645\n",
      "Step - 6316, Loss - 0.6220873593139702, Learning Rate - 0.00078125, magnitude of gradient - 1.2156074021796115\n",
      "Step - 6317, Loss - 0.8193631958454668, Learning Rate - 0.00078125, magnitude of gradient - 0.8798307044214935\n",
      "Step - 6318, Loss - 0.7767043215421956, Learning Rate - 0.00078125, magnitude of gradient - 0.6199919192233658\n",
      "Step - 6319, Loss - 0.6714076751966178, Learning Rate - 0.00078125, magnitude of gradient - 0.5724960332776895\n",
      "Step - 6320, Loss - 0.7087483382251926, Learning Rate - 0.00078125, magnitude of gradient - 0.9937858330886338\n",
      "Step - 6321, Loss - 0.5728518273817803, Learning Rate - 0.00078125, magnitude of gradient - 1.4863695680099336\n",
      "Step - 6322, Loss - 0.8014786326132612, Learning Rate - 0.00078125, magnitude of gradient - 1.0860662510758325\n",
      "Step - 6323, Loss - 0.7003864293377722, Learning Rate - 0.00078125, magnitude of gradient - 1.1562485947158907\n",
      "Step - 6324, Loss - 0.7523464176694468, Learning Rate - 0.00078125, magnitude of gradient - 0.8054493337723929\n",
      "Step - 6325, Loss - 0.64704557412527, Learning Rate - 0.00078125, magnitude of gradient - 0.6820441823964883\n",
      "Step - 6326, Loss - 0.6394622246214087, Learning Rate - 0.00078125, magnitude of gradient - 0.8272413120886463\n",
      "Step - 6327, Loss - 0.7086815438276142, Learning Rate - 0.00078125, magnitude of gradient - 0.40522686507989764\n",
      "Step - 6328, Loss - 0.6695394008917239, Learning Rate - 0.00078125, magnitude of gradient - 2.0870320221773255\n",
      "Step - 6329, Loss - 0.7659371468418217, Learning Rate - 0.00078125, magnitude of gradient - 0.578333373704214\n",
      "Step - 6330, Loss - 0.6029882020356405, Learning Rate - 0.00078125, magnitude of gradient - 1.1473704696210583\n",
      "Step - 6331, Loss - 0.5054096340753544, Learning Rate - 0.00078125, magnitude of gradient - 0.8240714688943522\n",
      "Step - 6332, Loss - 0.6866264153808349, Learning Rate - 0.00078125, magnitude of gradient - 1.7319023188061766\n",
      "Step - 6333, Loss - 0.6313559093182026, Learning Rate - 0.00078125, magnitude of gradient - 0.5922916276268297\n",
      "Step - 6334, Loss - 0.7120361995540457, Learning Rate - 0.00078125, magnitude of gradient - 1.8778981868219982\n",
      "Step - 6335, Loss - 0.7798020776721867, Learning Rate - 0.00078125, magnitude of gradient - 1.2559636322576972\n",
      "Step - 6336, Loss - 0.577078470555545, Learning Rate - 0.00078125, magnitude of gradient - 1.3194196429712293\n",
      "Step - 6337, Loss - 0.8315469297614511, Learning Rate - 0.00078125, magnitude of gradient - 1.9022171762900817\n",
      "Step - 6338, Loss - 0.8848811083327982, Learning Rate - 0.00078125, magnitude of gradient - 2.203929237074766\n",
      "Step - 6339, Loss - 0.6547334668200671, Learning Rate - 0.00078125, magnitude of gradient - 1.9622223363362024\n",
      "Step - 6340, Loss - 0.6769699736154329, Learning Rate - 0.00078125, magnitude of gradient - 1.0719867645303323\n",
      "Step - 6341, Loss - 0.5806499193333517, Learning Rate - 0.00078125, magnitude of gradient - 1.422230298596373\n",
      "Step - 6342, Loss - 0.8326447267432521, Learning Rate - 0.00078125, magnitude of gradient - 0.425664903571304\n",
      "Step - 6343, Loss - 0.826970839831716, Learning Rate - 0.00078125, magnitude of gradient - 1.7084517439541034\n",
      "Step - 6344, Loss - 0.6864095629585689, Learning Rate - 0.00078125, magnitude of gradient - 1.691250704296791\n",
      "Step - 6345, Loss - 0.6880228580880772, Learning Rate - 0.00078125, magnitude of gradient - 1.8717479468396112\n",
      "Step - 6346, Loss - 0.695611162584365, Learning Rate - 0.00078125, magnitude of gradient - 0.6738522707413835\n",
      "Step - 6347, Loss - 0.6731303036038989, Learning Rate - 0.00078125, magnitude of gradient - 2.385492915574267\n",
      "Step - 6348, Loss - 0.7427649757851756, Learning Rate - 0.00078125, magnitude of gradient - 1.6170301469524353\n",
      "Step - 6349, Loss - 0.7048139005099161, Learning Rate - 0.00078125, magnitude of gradient - 1.7504244447022241\n",
      "Step - 6350, Loss - 0.636698827121602, Learning Rate - 0.00078125, magnitude of gradient - 1.4272789808010307\n",
      "Step - 6351, Loss - 0.5871437594780262, Learning Rate - 0.00078125, magnitude of gradient - 0.19931690657688933\n",
      "Step - 6352, Loss - 0.6723050746726984, Learning Rate - 0.00078125, magnitude of gradient - 1.1877388714353934\n",
      "Step - 6353, Loss - 0.6703070959156003, Learning Rate - 0.00078125, magnitude of gradient - 0.5306714578583713\n",
      "Step - 6354, Loss - 0.7879569633106708, Learning Rate - 0.00078125, magnitude of gradient - 1.738610777907928\n",
      "Step - 6355, Loss - 0.6881767125666092, Learning Rate - 0.00078125, magnitude of gradient - 0.8392090372985361\n",
      "Step - 6356, Loss - 0.5645938326480001, Learning Rate - 0.00078125, magnitude of gradient - 0.9747008529160205\n",
      "Step - 6357, Loss - 0.7098157922673645, Learning Rate - 0.00078125, magnitude of gradient - 1.6338790475613274\n",
      "Step - 6358, Loss - 0.7303908638103991, Learning Rate - 0.00078125, magnitude of gradient - 1.1158160044748633\n",
      "Step - 6359, Loss - 0.7030115131607112, Learning Rate - 0.00078125, magnitude of gradient - 1.6791095727692937\n",
      "Step - 6360, Loss - 0.7949147963436498, Learning Rate - 0.00078125, magnitude of gradient - 1.2957281886721361\n",
      "Step - 6361, Loss - 0.7939047004215107, Learning Rate - 0.00078125, magnitude of gradient - 1.7927098310556056\n",
      "Step - 6362, Loss - 0.5441597028190618, Learning Rate - 0.00078125, magnitude of gradient - 1.4684281955019327\n",
      "Step - 6363, Loss - 0.4562448612161473, Learning Rate - 0.00078125, magnitude of gradient - 1.1583177712200783\n",
      "Step - 6364, Loss - 0.6127712085594911, Learning Rate - 0.00078125, magnitude of gradient - 1.471219489117876\n",
      "Step - 6365, Loss - 0.5237238830438333, Learning Rate - 0.00078125, magnitude of gradient - 2.312592216543682\n",
      "Step - 6366, Loss - 0.6375917327148367, Learning Rate - 0.00078125, magnitude of gradient - 1.1501013187356983\n",
      "Step - 6367, Loss - 0.6882572655360851, Learning Rate - 0.00078125, magnitude of gradient - 0.612576252774447\n",
      "Step - 6368, Loss - 0.5870911192620111, Learning Rate - 0.00078125, magnitude of gradient - 0.7835453045601601\n",
      "Step - 6369, Loss - 0.7090907623867453, Learning Rate - 0.00078125, magnitude of gradient - 1.6255117716642065\n",
      "Step - 6370, Loss - 0.7582982347054598, Learning Rate - 0.00078125, magnitude of gradient - 1.2484458736718236\n",
      "Step - 6371, Loss - 0.9266090224894804, Learning Rate - 0.00078125, magnitude of gradient - 1.5726961257749181\n",
      "Step - 6372, Loss - 0.5892653056218649, Learning Rate - 0.00078125, magnitude of gradient - 1.3526378877874383\n",
      "Step - 6373, Loss - 0.6534458645702997, Learning Rate - 0.00078125, magnitude of gradient - 0.78465197792511\n",
      "Step - 6374, Loss - 0.6711846647097369, Learning Rate - 0.00078125, magnitude of gradient - 1.3378898318440544\n",
      "Step - 6375, Loss - 0.697750529494082, Learning Rate - 0.00078125, magnitude of gradient - 1.272611445414459\n",
      "Step - 6376, Loss - 0.47223256497187455, Learning Rate - 0.00078125, magnitude of gradient - 1.7843492736355053\n",
      "Step - 6377, Loss - 0.634569673547418, Learning Rate - 0.00078125, magnitude of gradient - 3.2168392263426853\n",
      "Step - 6378, Loss - 0.7048896756376137, Learning Rate - 0.00078125, magnitude of gradient - 2.145907522493526\n",
      "Step - 6379, Loss - 0.7247855177293555, Learning Rate - 0.00078125, magnitude of gradient - 0.6557265532336221\n",
      "Step - 6380, Loss - 0.6421507571319011, Learning Rate - 0.00078125, magnitude of gradient - 0.7575485361997785\n",
      "Step - 6381, Loss - 0.6518035285417969, Learning Rate - 0.00078125, magnitude of gradient - 1.6063341926958583\n",
      "Step - 6382, Loss - 0.4762982217151598, Learning Rate - 0.00078125, magnitude of gradient - 0.7693247504133193\n",
      "Step - 6383, Loss - 0.6753055650441295, Learning Rate - 0.00078125, magnitude of gradient - 0.5576941079034996\n",
      "Step - 6384, Loss - 0.6731928433492949, Learning Rate - 0.00078125, magnitude of gradient - 0.4986154588151546\n",
      "Step - 6385, Loss - 0.6625012555072346, Learning Rate - 0.00078125, magnitude of gradient - 1.4548032073948054\n",
      "Step - 6386, Loss - 0.7332126282260789, Learning Rate - 0.00078125, magnitude of gradient - 1.830044406775122\n",
      "Step - 6387, Loss - 0.593846852961041, Learning Rate - 0.00078125, magnitude of gradient - 0.5262681192333647\n",
      "Step - 6388, Loss - 0.6392315168561818, Learning Rate - 0.00078125, magnitude of gradient - 0.48284956519488165\n",
      "Step - 6389, Loss - 0.6704160399913713, Learning Rate - 0.00078125, magnitude of gradient - 0.9839207086676929\n",
      "Step - 6390, Loss - 0.7768861097366514, Learning Rate - 0.00078125, magnitude of gradient - 1.0297058455726733\n",
      "Step - 6391, Loss - 0.6224778032432474, Learning Rate - 0.00078125, magnitude of gradient - 1.1984841417486105\n",
      "Step - 6392, Loss - 0.7061499314765124, Learning Rate - 0.00078125, magnitude of gradient - 0.9055156584094615\n",
      "Step - 6393, Loss - 0.7519590100801042, Learning Rate - 0.00078125, magnitude of gradient - 1.0745518763032917\n",
      "Step - 6394, Loss - 0.47275196413724174, Learning Rate - 0.00078125, magnitude of gradient - 0.4264325073668315\n",
      "Step - 6395, Loss - 0.7737104881829528, Learning Rate - 0.00078125, magnitude of gradient - 1.4134694522826643\n",
      "Step - 6396, Loss - 0.6896298271244293, Learning Rate - 0.00078125, magnitude of gradient - 0.9451383859405704\n",
      "Step - 6397, Loss - 0.7699778186305716, Learning Rate - 0.00078125, magnitude of gradient - 1.3952088473335758\n",
      "Step - 6398, Loss - 0.7775599453123718, Learning Rate - 0.00078125, magnitude of gradient - 1.114210480211037\n",
      "Step - 6399, Loss - 0.8495229400800529, Learning Rate - 0.00078125, magnitude of gradient - 0.8929424339207469\n",
      "Step - 6400, Loss - 0.8847360206708712, Learning Rate - 0.00078125, magnitude of gradient - 2.2895630042662822\n",
      "Step - 6401, Loss - 0.6614280060163805, Learning Rate - 0.00078125, magnitude of gradient - 0.9096261984707869\n",
      "Step - 6402, Loss - 0.3717820762959759, Learning Rate - 0.00078125, magnitude of gradient - 1.6497098922261983\n",
      "Step - 6403, Loss - 0.8208780585752201, Learning Rate - 0.00078125, magnitude of gradient - 1.6815300551435504\n",
      "Step - 6404, Loss - 0.5406704290221392, Learning Rate - 0.00078125, magnitude of gradient - 1.3939771221125925\n",
      "Step - 6405, Loss - 0.7136456956853291, Learning Rate - 0.00078125, magnitude of gradient - 1.1407344992250503\n",
      "Step - 6406, Loss - 0.7928725894829173, Learning Rate - 0.00078125, magnitude of gradient - 1.1429928631562565\n",
      "Step - 6407, Loss - 0.7340532322725127, Learning Rate - 0.00078125, magnitude of gradient - 1.1584998598469658\n",
      "Step - 6408, Loss - 0.7139597218142859, Learning Rate - 0.00078125, magnitude of gradient - 0.6577847047438642\n",
      "Step - 6409, Loss - 0.8482874124644189, Learning Rate - 0.00078125, magnitude of gradient - 0.9811320120012631\n",
      "Step - 6410, Loss - 0.6045548532934724, Learning Rate - 0.00078125, magnitude of gradient - 2.0261589193837164\n",
      "Step - 6411, Loss - 0.7604577319609532, Learning Rate - 0.00078125, magnitude of gradient - 0.5732515884932025\n",
      "Step - 6412, Loss - 0.7922382442214078, Learning Rate - 0.00078125, magnitude of gradient - 0.8237869226441666\n",
      "Step - 6413, Loss - 0.5245605417544419, Learning Rate - 0.00078125, magnitude of gradient - 0.776270159619604\n",
      "Step - 6414, Loss - 0.6756036960618365, Learning Rate - 0.00078125, magnitude of gradient - 0.5402293821197891\n",
      "Step - 6415, Loss - 0.7677751433440501, Learning Rate - 0.00078125, magnitude of gradient - 0.7856470465347785\n",
      "Step - 6416, Loss - 0.6090200230367615, Learning Rate - 0.00078125, magnitude of gradient - 0.38431923405363416\n",
      "Step - 6417, Loss - 0.7728933415789807, Learning Rate - 0.00078125, magnitude of gradient - 1.2425090636523726\n",
      "Step - 6418, Loss - 0.7993404854809151, Learning Rate - 0.00078125, magnitude of gradient - 1.3918437079094046\n",
      "Step - 6419, Loss - 0.9037254883014474, Learning Rate - 0.00078125, magnitude of gradient - 1.3388043502310727\n",
      "Step - 6420, Loss - 0.7627703023459671, Learning Rate - 0.00078125, magnitude of gradient - 0.250520552864535\n",
      "Step - 6421, Loss - 0.6073804319892536, Learning Rate - 0.00078125, magnitude of gradient - 1.1673067407026145\n",
      "Step - 6422, Loss - 0.6245628356374482, Learning Rate - 0.00078125, magnitude of gradient - 1.6486989296176644\n",
      "Step - 6423, Loss - 0.682475660011361, Learning Rate - 0.00078125, magnitude of gradient - 1.2909892466214785\n",
      "Step - 6424, Loss - 0.899739564800747, Learning Rate - 0.00078125, magnitude of gradient - 1.8583021892321459\n",
      "Step - 6425, Loss - 0.6376948725310161, Learning Rate - 0.00078125, magnitude of gradient - 0.9415044523909664\n",
      "Step - 6426, Loss - 0.7328010922868067, Learning Rate - 0.00078125, magnitude of gradient - 1.163497554456607\n",
      "Step - 6427, Loss - 0.7885023851544782, Learning Rate - 0.00078125, magnitude of gradient - 0.8167272698134846\n",
      "Step - 6428, Loss - 0.737863929235208, Learning Rate - 0.00078125, magnitude of gradient - 2.501452279327378\n",
      "Step - 6429, Loss - 0.7036334940912192, Learning Rate - 0.00078125, magnitude of gradient - 1.1471413903775476\n",
      "Step - 6430, Loss - 0.7671399007431842, Learning Rate - 0.00078125, magnitude of gradient - 1.0413952357795115\n",
      "Step - 6431, Loss - 0.6003444627536938, Learning Rate - 0.00078125, magnitude of gradient - 0.5506296563406082\n",
      "Step - 6432, Loss - 0.8016408845933661, Learning Rate - 0.00078125, magnitude of gradient - 1.6869638627960877\n",
      "Step - 6433, Loss - 0.7184480693541853, Learning Rate - 0.00078125, magnitude of gradient - 1.0345630186964583\n",
      "Step - 6434, Loss - 0.6309712498603621, Learning Rate - 0.00078125, magnitude of gradient - 1.225185710309426\n",
      "Step - 6435, Loss - 0.6121234334136538, Learning Rate - 0.00078125, magnitude of gradient - 1.5103636924599124\n",
      "Step - 6436, Loss - 0.5604491973147477, Learning Rate - 0.00078125, magnitude of gradient - 0.637394962056097\n",
      "Step - 6437, Loss - 0.6859717812813226, Learning Rate - 0.00078125, magnitude of gradient - 0.6905230422497005\n",
      "Step - 6438, Loss - 0.7435217666727032, Learning Rate - 0.00078125, magnitude of gradient - 0.41865651293658396\n",
      "Step - 6439, Loss - 0.7125642438187411, Learning Rate - 0.00078125, magnitude of gradient - 1.5094599557957575\n",
      "Step - 6440, Loss - 0.6690222441592097, Learning Rate - 0.00078125, magnitude of gradient - 1.3728313853433352\n",
      "Step - 6441, Loss - 0.4793467583870546, Learning Rate - 0.00078125, magnitude of gradient - 1.2707263603784136\n",
      "Step - 6442, Loss - 0.5526625070424719, Learning Rate - 0.00078125, magnitude of gradient - 2.193483112136398\n",
      "Step - 6443, Loss - 0.6665010995599456, Learning Rate - 0.00078125, magnitude of gradient - 1.1781222038639532\n",
      "Step - 6444, Loss - 0.5929728334488703, Learning Rate - 0.00078125, magnitude of gradient - 1.5289291358868526\n",
      "Step - 6445, Loss - 0.5592107288452739, Learning Rate - 0.00078125, magnitude of gradient - 0.532114433838357\n",
      "Step - 6446, Loss - 0.6833535403219915, Learning Rate - 0.00078125, magnitude of gradient - 1.9563798183176595\n",
      "Step - 6447, Loss - 0.7047768619245111, Learning Rate - 0.00078125, magnitude of gradient - 0.6954127044352401\n",
      "Step - 6448, Loss - 0.7596115011694142, Learning Rate - 0.00078125, magnitude of gradient - 1.2203226654999368\n",
      "Step - 6449, Loss - 0.6836018332645362, Learning Rate - 0.00078125, magnitude of gradient - 1.591600422349981\n",
      "Step - 6450, Loss - 0.7328820401796976, Learning Rate - 0.00078125, magnitude of gradient - 0.6637161605219255\n",
      "Step - 6451, Loss - 0.6089225551300586, Learning Rate - 0.00078125, magnitude of gradient - 1.5990604465546898\n",
      "Step - 6452, Loss - 0.6092436837028476, Learning Rate - 0.00078125, magnitude of gradient - 0.7686880167794223\n",
      "Step - 6453, Loss - 0.5465963477732959, Learning Rate - 0.00078125, magnitude of gradient - 1.0525439701918002\n",
      "Step - 6454, Loss - 0.49094417468558627, Learning Rate - 0.00078125, magnitude of gradient - 1.5191101296539746\n",
      "Step - 6455, Loss - 0.5845259365280564, Learning Rate - 0.00078125, magnitude of gradient - 1.5907006312044913\n",
      "Step - 6456, Loss - 0.7736979595327724, Learning Rate - 0.00078125, magnitude of gradient - 1.5441035980608038\n",
      "Step - 6457, Loss - 0.5312371290178388, Learning Rate - 0.00078125, magnitude of gradient - 1.3925441877705735\n",
      "Step - 6458, Loss - 0.5933562043945521, Learning Rate - 0.00078125, magnitude of gradient - 0.6776379492925312\n",
      "Step - 6459, Loss - 0.565395491278113, Learning Rate - 0.00078125, magnitude of gradient - 0.37366223003633164\n",
      "Step - 6460, Loss - 0.6320624160137751, Learning Rate - 0.00078125, magnitude of gradient - 0.5964927073457251\n",
      "Step - 6461, Loss - 0.8686775998000531, Learning Rate - 0.00078125, magnitude of gradient - 2.1065417978613095\n",
      "Step - 6462, Loss - 0.6567770119710169, Learning Rate - 0.00078125, magnitude of gradient - 0.5242765970546577\n",
      "Step - 6463, Loss - 0.7960800802170279, Learning Rate - 0.00078125, magnitude of gradient - 2.229220676269357\n",
      "Step - 6464, Loss - 0.8423524587899353, Learning Rate - 0.00078125, magnitude of gradient - 1.0783975683309464\n",
      "Step - 6465, Loss - 0.4343632588968887, Learning Rate - 0.00078125, magnitude of gradient - 0.971585044632591\n",
      "Step - 6466, Loss - 0.5511416839125547, Learning Rate - 0.00078125, magnitude of gradient - 0.5502637181987403\n",
      "Step - 6467, Loss - 0.7340505440228511, Learning Rate - 0.00078125, magnitude of gradient - 1.9847428601870587\n",
      "Step - 6468, Loss - 0.6827564131022229, Learning Rate - 0.00078125, magnitude of gradient - 1.3956866166719415\n",
      "Step - 6469, Loss - 0.8407516404478855, Learning Rate - 0.00078125, magnitude of gradient - 1.3558636824548722\n",
      "Step - 6470, Loss - 0.713125551592, Learning Rate - 0.00078125, magnitude of gradient - 0.9140623743435167\n",
      "Step - 6471, Loss - 0.7243657704403289, Learning Rate - 0.00078125, magnitude of gradient - 0.8765009535833139\n",
      "Step - 6472, Loss - 0.5403099424530271, Learning Rate - 0.00078125, magnitude of gradient - 0.7523361283940093\n",
      "Step - 6473, Loss - 0.6877577561952297, Learning Rate - 0.00078125, magnitude of gradient - 0.957202109040267\n",
      "Step - 6474, Loss - 0.5453790081138125, Learning Rate - 0.00078125, magnitude of gradient - 1.8109735597760486\n",
      "Step - 6475, Loss - 0.6988083767220991, Learning Rate - 0.00078125, magnitude of gradient - 0.6316757622281529\n",
      "Step - 6476, Loss - 0.8919390409476854, Learning Rate - 0.00078125, magnitude of gradient - 0.9269392199762202\n",
      "Step - 6477, Loss - 0.6437046451108971, Learning Rate - 0.00078125, magnitude of gradient - 1.8176015024208232\n",
      "Step - 6478, Loss - 0.5110236745729908, Learning Rate - 0.00078125, magnitude of gradient - 0.6724733706546918\n",
      "Step - 6479, Loss - 0.606476464659679, Learning Rate - 0.00078125, magnitude of gradient - 0.8309243365776741\n",
      "Step - 6480, Loss - 0.7030602789605064, Learning Rate - 0.00078125, magnitude of gradient - 1.502634711982052\n",
      "Step - 6481, Loss - 0.7034842593373067, Learning Rate - 0.00078125, magnitude of gradient - 1.342801462512383\n",
      "Step - 6482, Loss - 0.5495655863769935, Learning Rate - 0.00078125, magnitude of gradient - 1.207560161315208\n",
      "Step - 6483, Loss - 0.7280962106278275, Learning Rate - 0.00078125, magnitude of gradient - 0.6037817438612954\n",
      "Step - 6484, Loss - 0.5858677894746125, Learning Rate - 0.00078125, magnitude of gradient - 1.1907708154295973\n",
      "Step - 6485, Loss - 0.7467588007204731, Learning Rate - 0.00078125, magnitude of gradient - 1.6527458132130104\n",
      "Step - 6486, Loss - 0.6051105839469936, Learning Rate - 0.00078125, magnitude of gradient - 2.1228382446749605\n",
      "Step - 6487, Loss - 0.5865359509415202, Learning Rate - 0.00078125, magnitude of gradient - 1.2593265145152475\n",
      "Step - 6488, Loss - 0.6516272371900418, Learning Rate - 0.00078125, magnitude of gradient - 1.138130119561013\n",
      "Step - 6489, Loss - 0.5675937963681843, Learning Rate - 0.00078125, magnitude of gradient - 1.1617554069439244\n",
      "Step - 6490, Loss - 0.8767491368392235, Learning Rate - 0.00078125, magnitude of gradient - 1.6556508517801114\n",
      "Step - 6491, Loss - 0.6147715616432458, Learning Rate - 0.00078125, magnitude of gradient - 0.6236060574741858\n",
      "Step - 6492, Loss - 0.6582741610617691, Learning Rate - 0.00078125, magnitude of gradient - 0.7270189796173877\n",
      "Step - 6493, Loss - 0.8521772970225971, Learning Rate - 0.00078125, magnitude of gradient - 1.2226512096803477\n",
      "Step - 6494, Loss - 0.5463976407360307, Learning Rate - 0.00078125, magnitude of gradient - 1.064041643509563\n",
      "Step - 6495, Loss - 0.6960749642773187, Learning Rate - 0.00078125, magnitude of gradient - 1.4407958318953502\n",
      "Step - 6496, Loss - 0.8507409697471824, Learning Rate - 0.00078125, magnitude of gradient - 1.5533423502717856\n",
      "Step - 6497, Loss - 0.4333215026521059, Learning Rate - 0.00078125, magnitude of gradient - 1.263490263372278\n",
      "Step - 6498, Loss - 0.6528750660962849, Learning Rate - 0.00078125, magnitude of gradient - 1.5077545270849766\n",
      "Step - 6499, Loss - 0.7170242513449481, Learning Rate - 0.00078125, magnitude of gradient - 0.7087673610016113\n",
      "Step - 6500, Loss - 0.7333626236902635, Learning Rate - 0.00078125, magnitude of gradient - 1.1646999151403188\n",
      "Step - 6501, Loss - 0.7824492243036061, Learning Rate - 0.00078125, magnitude of gradient - 1.21874179967554\n",
      "Step - 6502, Loss - 0.6820384446745941, Learning Rate - 0.00078125, magnitude of gradient - 0.9654240109441257\n",
      "Step - 6503, Loss - 0.7250136392130524, Learning Rate - 0.00078125, magnitude of gradient - 0.7272228238154833\n",
      "Step - 6504, Loss - 0.8278541265289616, Learning Rate - 0.00078125, magnitude of gradient - 1.126426702993072\n",
      "Step - 6505, Loss - 0.7557020215009533, Learning Rate - 0.00078125, magnitude of gradient - 1.0369877738098825\n",
      "Step - 6506, Loss - 0.9001709958789905, Learning Rate - 0.00078125, magnitude of gradient - 2.727885273564356\n",
      "Step - 6507, Loss - 0.583340004027456, Learning Rate - 0.00078125, magnitude of gradient - 1.8723887478544723\n",
      "Step - 6508, Loss - 0.7111808772446233, Learning Rate - 0.00078125, magnitude of gradient - 1.3310649616691397\n",
      "Step - 6509, Loss - 0.7198405748346487, Learning Rate - 0.00078125, magnitude of gradient - 1.2811398778459897\n",
      "Step - 6510, Loss - 0.7006855908820033, Learning Rate - 0.00078125, magnitude of gradient - 1.2848771924019315\n",
      "Step - 6511, Loss - 0.7216050715013047, Learning Rate - 0.00078125, magnitude of gradient - 1.1707182159594895\n",
      "Step - 6512, Loss - 0.6883557282252724, Learning Rate - 0.00078125, magnitude of gradient - 1.407873073445888\n",
      "Step - 6513, Loss - 0.8176923956910073, Learning Rate - 0.00078125, magnitude of gradient - 0.8811905640976438\n",
      "Step - 6514, Loss - 0.7255088131493459, Learning Rate - 0.00078125, magnitude of gradient - 1.7266642795194147\n",
      "Step - 6515, Loss - 0.6425054131214168, Learning Rate - 0.00078125, magnitude of gradient - 1.4714790871973333\n",
      "Step - 6516, Loss - 0.6194686483992455, Learning Rate - 0.00078125, magnitude of gradient - 1.5877997807307689\n",
      "Step - 6517, Loss - 0.7313134383110027, Learning Rate - 0.00078125, magnitude of gradient - 0.7529807165229997\n",
      "Step - 6518, Loss - 0.939793713513873, Learning Rate - 0.00078125, magnitude of gradient - 1.3208876965829455\n",
      "Step - 6519, Loss - 0.7398890003381959, Learning Rate - 0.00078125, magnitude of gradient - 0.8696139624024496\n",
      "Step - 6520, Loss - 0.7370112572200053, Learning Rate - 0.00078125, magnitude of gradient - 0.9426661600720762\n",
      "Step - 6521, Loss - 0.7732174237577891, Learning Rate - 0.00078125, magnitude of gradient - 1.4801839464330913\n",
      "Step - 6522, Loss - 0.5898034541371934, Learning Rate - 0.00078125, magnitude of gradient - 1.6868160486106123\n",
      "Step - 6523, Loss - 0.7289048504397465, Learning Rate - 0.00078125, magnitude of gradient - 1.0655966662919913\n",
      "Step - 6524, Loss - 0.5758451709329279, Learning Rate - 0.00078125, magnitude of gradient - 1.0624590305516648\n",
      "Step - 6525, Loss - 0.5682715700835033, Learning Rate - 0.00078125, magnitude of gradient - 1.0993787247417703\n",
      "Step - 6526, Loss - 0.7918339100207312, Learning Rate - 0.00078125, magnitude of gradient - 2.4019153009396383\n",
      "Step - 6527, Loss - 0.6807961308166086, Learning Rate - 0.00078125, magnitude of gradient - 0.9657898234457302\n",
      "Step - 6528, Loss - 0.7304425384369978, Learning Rate - 0.00078125, magnitude of gradient - 1.0008241775856281\n",
      "Step - 6529, Loss - 0.5497087959441522, Learning Rate - 0.00078125, magnitude of gradient - 1.7203652805997702\n",
      "Step - 6530, Loss - 0.6609327107764998, Learning Rate - 0.00078125, magnitude of gradient - 1.6375800571339056\n",
      "Step - 6531, Loss - 0.6531828704654423, Learning Rate - 0.00078125, magnitude of gradient - 0.6256222387178305\n",
      "Step - 6532, Loss - 0.7193134193548715, Learning Rate - 0.00078125, magnitude of gradient - 1.333181940658951\n",
      "Step - 6533, Loss - 0.7438528025532329, Learning Rate - 0.00078125, magnitude of gradient - 1.9968574955473273\n",
      "Step - 6534, Loss - 0.7455112061615937, Learning Rate - 0.00078125, magnitude of gradient - 2.2699571185793266\n",
      "Step - 6535, Loss - 0.822445371889215, Learning Rate - 0.00078125, magnitude of gradient - 1.6077489434297123\n",
      "Step - 6536, Loss - 0.7304242807045709, Learning Rate - 0.00078125, magnitude of gradient - 1.626481543684342\n",
      "Step - 6537, Loss - 0.6681785897469088, Learning Rate - 0.00078125, magnitude of gradient - 1.7517517624451147\n",
      "Step - 6538, Loss - 0.750521850800873, Learning Rate - 0.00078125, magnitude of gradient - 1.221155620993686\n",
      "Step - 6539, Loss - 0.6376173431718857, Learning Rate - 0.00078125, magnitude of gradient - 0.8297934525022861\n",
      "Step - 6540, Loss - 0.7034198009188721, Learning Rate - 0.00078125, magnitude of gradient - 1.1557040261165143\n",
      "Step - 6541, Loss - 0.7106932849875565, Learning Rate - 0.00078125, magnitude of gradient - 0.7585713656972123\n",
      "Step - 6542, Loss - 0.7573280975711654, Learning Rate - 0.00078125, magnitude of gradient - 2.065647803391095\n",
      "Step - 6543, Loss - 0.9725325052395237, Learning Rate - 0.00078125, magnitude of gradient - 1.8382452471771342\n",
      "Step - 6544, Loss - 0.7059387173968076, Learning Rate - 0.00078125, magnitude of gradient - 1.3273537686911423\n",
      "Step - 6545, Loss - 0.7228117139955447, Learning Rate - 0.00078125, magnitude of gradient - 1.8109418074480945\n",
      "Step - 6546, Loss - 0.44395918610138485, Learning Rate - 0.00078125, magnitude of gradient - 2.4167652675949305\n",
      "Step - 6547, Loss - 0.8302944681000358, Learning Rate - 0.00078125, magnitude of gradient - 0.8024502514722921\n",
      "Step - 6548, Loss - 0.8886614302708731, Learning Rate - 0.00078125, magnitude of gradient - 1.3739777238848865\n",
      "Step - 6549, Loss - 0.6767690340716603, Learning Rate - 0.00078125, magnitude of gradient - 1.1762619160973367\n",
      "Step - 6550, Loss - 0.6169513344120283, Learning Rate - 0.00078125, magnitude of gradient - 1.4644496827763882\n",
      "Step - 6551, Loss - 0.717345106263806, Learning Rate - 0.00078125, magnitude of gradient - 1.094631673842147\n",
      "Step - 6552, Loss - 0.569310774023018, Learning Rate - 0.00078125, magnitude of gradient - 1.5078354890931138\n",
      "Step - 6553, Loss - 0.7086093880782941, Learning Rate - 0.00078125, magnitude of gradient - 1.0650210996664302\n",
      "Step - 6554, Loss - 0.7142007072202647, Learning Rate - 0.00078125, magnitude of gradient - 0.9758511558220917\n",
      "Step - 6555, Loss - 0.4812269527329588, Learning Rate - 0.00078125, magnitude of gradient - 1.7610291599168135\n",
      "Step - 6556, Loss - 0.7578746917156605, Learning Rate - 0.00078125, magnitude of gradient - 1.1759795196052312\n",
      "Step - 6557, Loss - 0.5367055037849998, Learning Rate - 0.00078125, magnitude of gradient - 1.8003484660585871\n",
      "Step - 6558, Loss - 0.8225236237944781, Learning Rate - 0.00078125, magnitude of gradient - 1.1386104277054776\n",
      "Step - 6559, Loss - 0.7012687400747117, Learning Rate - 0.00078125, magnitude of gradient - 0.6977159162633167\n",
      "Step - 6560, Loss - 0.6861696892687619, Learning Rate - 0.00078125, magnitude of gradient - 0.22777409334609866\n",
      "Step - 6561, Loss - 0.7549279249741325, Learning Rate - 0.00078125, magnitude of gradient - 1.6217460241274761\n",
      "Step - 6562, Loss - 0.5529557585565781, Learning Rate - 0.00078125, magnitude of gradient - 1.4530001189599577\n",
      "Step - 6563, Loss - 0.6034640927781694, Learning Rate - 0.00078125, magnitude of gradient - 1.5961790697600922\n",
      "Step - 6564, Loss - 0.6582714099124185, Learning Rate - 0.00078125, magnitude of gradient - 0.6148828052092006\n",
      "Step - 6565, Loss - 0.5327524567440876, Learning Rate - 0.00078125, magnitude of gradient - 0.9375053608937162\n",
      "Step - 6566, Loss - 0.5960632922239183, Learning Rate - 0.00078125, magnitude of gradient - 1.3331034122288328\n",
      "Step - 6567, Loss - 0.6568289340177085, Learning Rate - 0.00078125, magnitude of gradient - 1.5237036283218985\n",
      "Step - 6568, Loss - 0.8884466231562878, Learning Rate - 0.00078125, magnitude of gradient - 1.6743775838142017\n",
      "Step - 6569, Loss - 0.8218052160237086, Learning Rate - 0.00078125, magnitude of gradient - 0.9805964460201154\n",
      "Step - 6570, Loss - 0.8434422077638899, Learning Rate - 0.00078125, magnitude of gradient - 1.2347289186102293\n",
      "Step - 6571, Loss - 0.5135759201079182, Learning Rate - 0.00078125, magnitude of gradient - 0.8470451051744606\n",
      "Step - 6572, Loss - 0.7181785211026848, Learning Rate - 0.00078125, magnitude of gradient - 1.9850445240594625\n",
      "Step - 6573, Loss - 0.7297763302490142, Learning Rate - 0.00078125, magnitude of gradient - 0.7157983538145765\n",
      "Step - 6574, Loss - 0.7278420826923673, Learning Rate - 0.00078125, magnitude of gradient - 1.2394619414571793\n",
      "Step - 6575, Loss - 0.6565251459573824, Learning Rate - 0.00078125, magnitude of gradient - 0.9746068076830284\n",
      "Step - 6576, Loss - 0.448262102605898, Learning Rate - 0.00078125, magnitude of gradient - 0.6380936090534548\n",
      "Step - 6577, Loss - 0.669650925517926, Learning Rate - 0.00078125, magnitude of gradient - 1.5277075872765824\n",
      "Step - 6578, Loss - 0.8508153343066727, Learning Rate - 0.00078125, magnitude of gradient - 0.7481276791278502\n",
      "Step - 6579, Loss - 0.5466524768796098, Learning Rate - 0.00078125, magnitude of gradient - 1.6702706445512208\n",
      "Step - 6580, Loss - 0.7867222219436477, Learning Rate - 0.00078125, magnitude of gradient - 1.3427793209828243\n",
      "Step - 6581, Loss - 0.7507971675102701, Learning Rate - 0.00078125, magnitude of gradient - 1.1284986311510958\n",
      "Step - 6582, Loss - 0.4435676518758452, Learning Rate - 0.00078125, magnitude of gradient - 1.052630871250538\n",
      "Step - 6583, Loss - 0.7118732769412873, Learning Rate - 0.00078125, magnitude of gradient - 1.1767419594623283\n",
      "Step - 6584, Loss - 0.7410562071301426, Learning Rate - 0.00078125, magnitude of gradient - 0.5693195023137732\n",
      "Step - 6585, Loss - 0.6216716914891919, Learning Rate - 0.00078125, magnitude of gradient - 0.7041217636987791\n",
      "Step - 6586, Loss - 0.7047502488458054, Learning Rate - 0.00078125, magnitude of gradient - 1.7124374362659995\n",
      "Step - 6587, Loss - 0.729351905349571, Learning Rate - 0.00078125, magnitude of gradient - 0.7784758955350893\n",
      "Step - 6588, Loss - 0.7630658195647979, Learning Rate - 0.00078125, magnitude of gradient - 2.5277636911291363\n",
      "Step - 6589, Loss - 0.685608702288643, Learning Rate - 0.00078125, magnitude of gradient - 1.5564387102722588\n",
      "Step - 6590, Loss - 0.7750478534829198, Learning Rate - 0.00078125, magnitude of gradient - 2.6801709960407316\n",
      "Step - 6591, Loss - 0.8009645108927351, Learning Rate - 0.00078125, magnitude of gradient - 0.7053511622811575\n",
      "Step - 6592, Loss - 0.7372693378111972, Learning Rate - 0.00078125, magnitude of gradient - 1.0203606837124617\n",
      "Step - 6593, Loss - 0.8826231339973739, Learning Rate - 0.00078125, magnitude of gradient - 1.4497274510452896\n",
      "Step - 6594, Loss - 0.6198270669494422, Learning Rate - 0.00078125, magnitude of gradient - 2.0867070032864388\n",
      "Step - 6595, Loss - 0.7281804437533745, Learning Rate - 0.00078125, magnitude of gradient - 1.236739408607984\n",
      "Step - 6596, Loss - 0.6624448790556856, Learning Rate - 0.00078125, magnitude of gradient - 1.8042697235937386\n",
      "Step - 6597, Loss - 0.8015461950524658, Learning Rate - 0.00078125, magnitude of gradient - 1.0563133969008\n",
      "Step - 6598, Loss - 0.54130284840432, Learning Rate - 0.00078125, magnitude of gradient - 2.4938283030067203\n",
      "Step - 6599, Loss - 0.7607324975677261, Learning Rate - 0.00078125, magnitude of gradient - 1.121095571834227\n",
      "Step - 6600, Loss - 0.7557254498770218, Learning Rate - 0.00078125, magnitude of gradient - 0.6929269712983824\n",
      "Step - 6601, Loss - 0.768814746225683, Learning Rate - 0.00078125, magnitude of gradient - 1.144973209093408\n",
      "Step - 6602, Loss - 0.602105832535603, Learning Rate - 0.00078125, magnitude of gradient - 1.1467584814899707\n",
      "Step - 6603, Loss - 0.5776139568753608, Learning Rate - 0.00078125, magnitude of gradient - 0.9425454288033035\n",
      "Step - 6604, Loss - 0.8157991062013402, Learning Rate - 0.00078125, magnitude of gradient - 0.4510497448313256\n",
      "Step - 6605, Loss - 0.6070297625846369, Learning Rate - 0.00078125, magnitude of gradient - 0.788220542867966\n",
      "Step - 6606, Loss - 0.6487355984198611, Learning Rate - 0.00078125, magnitude of gradient - 0.8179064735455832\n",
      "Step - 6607, Loss - 0.6062892604962574, Learning Rate - 0.00078125, magnitude of gradient - 1.1969617911878012\n",
      "Step - 6608, Loss - 0.6361107274491904, Learning Rate - 0.00078125, magnitude of gradient - 1.0379424470006413\n",
      "Step - 6609, Loss - 0.560008591642633, Learning Rate - 0.00078125, magnitude of gradient - 0.5520793664397207\n",
      "Step - 6610, Loss - 0.73522448393137, Learning Rate - 0.00078125, magnitude of gradient - 0.8642878274187321\n",
      "Step - 6611, Loss - 0.6979708381388141, Learning Rate - 0.00078125, magnitude of gradient - 1.2192722688493636\n",
      "Step - 6612, Loss - 0.5614381463513487, Learning Rate - 0.00078125, magnitude of gradient - 1.29213596639266\n",
      "Step - 6613, Loss - 0.5803354231290041, Learning Rate - 0.00078125, magnitude of gradient - 0.6781881566589691\n",
      "Step - 6614, Loss - 0.6666875243081047, Learning Rate - 0.00078125, magnitude of gradient - 0.9058038122882587\n",
      "Step - 6615, Loss - 0.6609115758456542, Learning Rate - 0.00078125, magnitude of gradient - 1.561096837471048\n",
      "Step - 6616, Loss - 0.5984625824850791, Learning Rate - 0.00078125, magnitude of gradient - 0.6533128514623008\n",
      "Step - 6617, Loss - 0.7575558603274912, Learning Rate - 0.00078125, magnitude of gradient - 1.4009511982542848\n",
      "Step - 6618, Loss - 0.7933542554352302, Learning Rate - 0.00078125, magnitude of gradient - 1.390678980443065\n",
      "Step - 6619, Loss - 0.5790788127121759, Learning Rate - 0.00078125, magnitude of gradient - 1.001551105217293\n",
      "Step - 6620, Loss - 0.7034430472551304, Learning Rate - 0.00078125, magnitude of gradient - 1.7994330365134679\n",
      "Step - 6621, Loss - 0.6520162718021403, Learning Rate - 0.00078125, magnitude of gradient - 0.864512549082736\n",
      "Step - 6622, Loss - 0.6720321010595542, Learning Rate - 0.00078125, magnitude of gradient - 1.3050487672876347\n",
      "Step - 6623, Loss - 0.5516376520534205, Learning Rate - 0.00078125, magnitude of gradient - 2.442084017839965\n",
      "Step - 6624, Loss - 0.5477073758492297, Learning Rate - 0.00078125, magnitude of gradient - 1.590969351872189\n",
      "Step - 6625, Loss - 0.7759477580435121, Learning Rate - 0.00078125, magnitude of gradient - 1.625006056854654\n",
      "Step - 6626, Loss - 0.7264986665691691, Learning Rate - 0.00078125, magnitude of gradient - 0.7609769979203903\n",
      "Step - 6627, Loss - 0.6185158762717925, Learning Rate - 0.00078125, magnitude of gradient - 0.8546055588938412\n",
      "Step - 6628, Loss - 0.451007028663898, Learning Rate - 0.00078125, magnitude of gradient - 0.8057056964093526\n",
      "Step - 6629, Loss - 0.4695437639580472, Learning Rate - 0.00078125, magnitude of gradient - 1.944707183008597\n",
      "Step - 6630, Loss - 0.7154426112970553, Learning Rate - 0.00078125, magnitude of gradient - 1.837042727032853\n",
      "Step - 6631, Loss - 0.9177396896988774, Learning Rate - 0.00078125, magnitude of gradient - 0.9583855629854787\n",
      "Step - 6632, Loss - 0.8997596435708889, Learning Rate - 0.00078125, magnitude of gradient - 1.1564194675178643\n",
      "Step - 6633, Loss - 0.9014665491352164, Learning Rate - 0.00078125, magnitude of gradient - 1.7451604683034863\n",
      "Step - 6634, Loss - 0.6288817311135302, Learning Rate - 0.00078125, magnitude of gradient - 1.5663166906070918\n",
      "Step - 6635, Loss - 0.9087324529926418, Learning Rate - 0.00078125, magnitude of gradient - 1.4259009287211402\n",
      "Step - 6636, Loss - 0.7073569651087964, Learning Rate - 0.00078125, magnitude of gradient - 0.634454744657574\n",
      "Step - 6637, Loss - 0.652074540473728, Learning Rate - 0.00078125, magnitude of gradient - 0.7526696114125906\n",
      "Step - 6638, Loss - 0.8541874543418138, Learning Rate - 0.00078125, magnitude of gradient - 1.7947429105854664\n",
      "Step - 6639, Loss - 0.6276547373111214, Learning Rate - 0.00078125, magnitude of gradient - 1.6893193828830626\n",
      "Step - 6640, Loss - 0.6209270877006741, Learning Rate - 0.00078125, magnitude of gradient - 0.8694598550773711\n",
      "Step - 6641, Loss - 0.6375431209893944, Learning Rate - 0.00078125, magnitude of gradient - 1.4019776080816262\n",
      "Step - 6642, Loss - 0.5309884232203262, Learning Rate - 0.00078125, magnitude of gradient - 0.9786257999185762\n",
      "Step - 6643, Loss - 0.6269748188515909, Learning Rate - 0.00078125, magnitude of gradient - 1.0775804931624957\n",
      "Step - 6644, Loss - 0.7024198524070772, Learning Rate - 0.00078125, magnitude of gradient - 1.3621473170044964\n",
      "Step - 6645, Loss - 0.5607774152337828, Learning Rate - 0.00078125, magnitude of gradient - 2.829611087777883\n",
      "Step - 6646, Loss - 0.8788144897060866, Learning Rate - 0.00078125, magnitude of gradient - 1.4551517937190883\n",
      "Step - 6647, Loss - 0.7602385014861559, Learning Rate - 0.00078125, magnitude of gradient - 0.7542710696432539\n",
      "Step - 6648, Loss - 0.5470900133962466, Learning Rate - 0.00078125, magnitude of gradient - 1.8471768983658015\n",
      "Step - 6649, Loss - 0.8318032124758792, Learning Rate - 0.00078125, magnitude of gradient - 1.521585368297212\n",
      "Step - 6650, Loss - 0.8261313459493658, Learning Rate - 0.00078125, magnitude of gradient - 1.106433323484346\n",
      "Step - 6651, Loss - 0.6630468962691773, Learning Rate - 0.00078125, magnitude of gradient - 0.6060261657972879\n",
      "Step - 6652, Loss - 0.39799398065287583, Learning Rate - 0.00078125, magnitude of gradient - 1.5362205574212904\n",
      "Step - 6653, Loss - 0.6023534644790572, Learning Rate - 0.00078125, magnitude of gradient - 1.3128807741931976\n",
      "Step - 6654, Loss - 0.6303071716542823, Learning Rate - 0.00078125, magnitude of gradient - 0.5156762088327906\n",
      "Step - 6655, Loss - 0.7020387780695185, Learning Rate - 0.00078125, magnitude of gradient - 0.8268179949066549\n",
      "Step - 6656, Loss - 0.593427680958639, Learning Rate - 0.00078125, magnitude of gradient - 0.8176711316739237\n",
      "Step - 6657, Loss - 0.6662333855542434, Learning Rate - 0.00078125, magnitude of gradient - 0.7647815485596835\n",
      "Step - 6658, Loss - 0.6622455798366675, Learning Rate - 0.00078125, magnitude of gradient - 0.529473279846389\n",
      "Step - 6659, Loss - 0.7716216084418819, Learning Rate - 0.00078125, magnitude of gradient - 1.255170221730777\n",
      "Step - 6660, Loss - 0.559569344121098, Learning Rate - 0.00078125, magnitude of gradient - 0.502753692693152\n",
      "Step - 6661, Loss - 0.754959082839529, Learning Rate - 0.00078125, magnitude of gradient - 0.9473202764164065\n",
      "Step - 6662, Loss - 0.5834817459694407, Learning Rate - 0.00078125, magnitude of gradient - 1.3008046874709347\n",
      "Step - 6663, Loss - 0.6159260456220739, Learning Rate - 0.00078125, magnitude of gradient - 0.4399299469432479\n",
      "Step - 6664, Loss - 0.7672651701788843, Learning Rate - 0.00078125, magnitude of gradient - 0.5715548617230813\n",
      "Step - 6665, Loss - 0.6584476233505175, Learning Rate - 0.00078125, magnitude of gradient - 2.622460805289345\n",
      "Step - 6666, Loss - 0.7966414972844214, Learning Rate - 0.00078125, magnitude of gradient - 1.3457154591656966\n",
      "Step - 6667, Loss - 1.0311462828851892, Learning Rate - 0.00078125, magnitude of gradient - 2.0450382034407277\n",
      "Step - 6668, Loss - 0.7701615408248462, Learning Rate - 0.00078125, magnitude of gradient - 1.5202771302664855\n",
      "Step - 6669, Loss - 0.7072340223500725, Learning Rate - 0.00078125, magnitude of gradient - 1.5689268301874035\n",
      "Step - 6670, Loss - 0.616056301311391, Learning Rate - 0.00078125, magnitude of gradient - 0.8383108609684897\n",
      "Step - 6671, Loss - 0.7804764229746655, Learning Rate - 0.00078125, magnitude of gradient - 1.2253417975176053\n",
      "Step - 6672, Loss - 0.6210505178609993, Learning Rate - 0.00078125, magnitude of gradient - 2.3077054655266465\n",
      "Step - 6673, Loss - 0.5976726310297412, Learning Rate - 0.00078125, magnitude of gradient - 0.34920271936417935\n",
      "Step - 6674, Loss - 0.6376645527987436, Learning Rate - 0.00078125, magnitude of gradient - 0.6559142318159804\n",
      "Step - 6675, Loss - 0.6263420049952299, Learning Rate - 0.00078125, magnitude of gradient - 1.3091177086336245\n",
      "Step - 6676, Loss - 0.6823960241502364, Learning Rate - 0.00078125, magnitude of gradient - 1.455900422047364\n",
      "Step - 6677, Loss - 0.9097192247027709, Learning Rate - 0.00078125, magnitude of gradient - 1.7879321903990062\n",
      "Step - 6678, Loss - 0.80215712490994, Learning Rate - 0.00078125, magnitude of gradient - 0.9444933352285957\n",
      "Step - 6679, Loss - 0.71679002454761, Learning Rate - 0.00078125, magnitude of gradient - 0.5846941787411214\n",
      "Step - 6680, Loss - 0.6775906794015814, Learning Rate - 0.00078125, magnitude of gradient - 0.9842924278196642\n",
      "Step - 6681, Loss - 0.6068895283203746, Learning Rate - 0.00078125, magnitude of gradient - 1.2512768844758684\n",
      "Step - 6682, Loss - 0.7478613262637924, Learning Rate - 0.00078125, magnitude of gradient - 1.0414813233202425\n",
      "Step - 6683, Loss - 0.7157482036182177, Learning Rate - 0.00078125, magnitude of gradient - 1.1777613183690083\n",
      "Step - 6684, Loss - 0.6849933795723445, Learning Rate - 0.00078125, magnitude of gradient - 0.2781032126903421\n",
      "Step - 6685, Loss - 0.562872244625171, Learning Rate - 0.00078125, magnitude of gradient - 1.3549349351395457\n",
      "Step - 6686, Loss - 0.7949472104277834, Learning Rate - 0.00078125, magnitude of gradient - 1.6658723938979016\n",
      "Step - 6687, Loss - 0.717630247260752, Learning Rate - 0.00078125, magnitude of gradient - 1.568807702135441\n",
      "Step - 6688, Loss - 0.7784406378410078, Learning Rate - 0.00078125, magnitude of gradient - 1.4090879195832435\n",
      "Step - 6689, Loss - 0.8679328384867324, Learning Rate - 0.00078125, magnitude of gradient - 1.7675875213545935\n",
      "Step - 6690, Loss - 0.5332207734563836, Learning Rate - 0.00078125, magnitude of gradient - 1.3480944812225784\n",
      "Step - 6691, Loss - 0.7839962837711542, Learning Rate - 0.00078125, magnitude of gradient - 0.9767340512011059\n",
      "Step - 6692, Loss - 0.6874219985711534, Learning Rate - 0.00078125, magnitude of gradient - 0.9363341483055279\n",
      "Step - 6693, Loss - 0.7487439624463023, Learning Rate - 0.00078125, magnitude of gradient - 0.7522421443568497\n",
      "Step - 6694, Loss - 0.6310631141599018, Learning Rate - 0.00078125, magnitude of gradient - 1.3174866115775656\n",
      "Step - 6695, Loss - 0.736001803565206, Learning Rate - 0.00078125, magnitude of gradient - 1.8220670662710676\n",
      "Step - 6696, Loss - 0.807391237164928, Learning Rate - 0.00078125, magnitude of gradient - 1.0450746638119806\n",
      "Step - 6697, Loss - 0.7531947200025251, Learning Rate - 0.00078125, magnitude of gradient - 1.0458384008202577\n",
      "Step - 6698, Loss - 0.617971571033527, Learning Rate - 0.00078125, magnitude of gradient - 1.49160883057273\n",
      "Step - 6699, Loss - 0.7062620614904741, Learning Rate - 0.00078125, magnitude of gradient - 0.9343767520396545\n",
      "Step - 6700, Loss - 0.767502637121233, Learning Rate - 0.00078125, magnitude of gradient - 1.7369083548534032\n",
      "Step - 6701, Loss - 0.8644995454399511, Learning Rate - 0.00078125, magnitude of gradient - 1.3270000188848317\n",
      "Step - 6702, Loss - 0.7982242705431553, Learning Rate - 0.00078125, magnitude of gradient - 1.5991967694546823\n",
      "Step - 6703, Loss - 0.6873670030708301, Learning Rate - 0.00078125, magnitude of gradient - 1.396343612787109\n",
      "Step - 6704, Loss - 0.8079823674120575, Learning Rate - 0.00078125, magnitude of gradient - 1.5107098198150237\n",
      "Step - 6705, Loss - 0.8603847153846857, Learning Rate - 0.00078125, magnitude of gradient - 0.8283083670772922\n",
      "Step - 6706, Loss - 0.8892660803471455, Learning Rate - 0.00078125, magnitude of gradient - 1.4341510082228162\n",
      "Step - 6707, Loss - 0.6977377879835144, Learning Rate - 0.00078125, magnitude of gradient - 2.1027046968394707\n",
      "Step - 6708, Loss - 0.7117448222880544, Learning Rate - 0.00078125, magnitude of gradient - 2.0201419542152568\n",
      "Step - 6709, Loss - 0.6722055848683769, Learning Rate - 0.00078125, magnitude of gradient - 0.6272835226415805\n",
      "Step - 6710, Loss - 0.5857659079289853, Learning Rate - 0.00078125, magnitude of gradient - 1.2207999901082227\n",
      "Step - 6711, Loss - 0.9441406315574196, Learning Rate - 0.00078125, magnitude of gradient - 0.8985924895532104\n",
      "Step - 6712, Loss - 0.7629271311903417, Learning Rate - 0.00078125, magnitude of gradient - 1.1295706033573139\n",
      "Step - 6713, Loss - 0.6340045904258212, Learning Rate - 0.00078125, magnitude of gradient - 1.1882420826588322\n",
      "Step - 6714, Loss - 0.6448188793411272, Learning Rate - 0.00078125, magnitude of gradient - 0.8061619807517794\n",
      "Step - 6715, Loss - 0.672413316937613, Learning Rate - 0.00078125, magnitude of gradient - 3.335134262222913\n",
      "Step - 6716, Loss - 0.4880791956637201, Learning Rate - 0.00078125, magnitude of gradient - 1.1894563786653574\n",
      "Step - 6717, Loss - 0.7934897734844848, Learning Rate - 0.00078125, magnitude of gradient - 1.0816959195639841\n",
      "Step - 6718, Loss - 0.6238669434643275, Learning Rate - 0.00078125, magnitude of gradient - 0.8653960685154928\n",
      "Step - 6719, Loss - 0.7778192674104786, Learning Rate - 0.00078125, magnitude of gradient - 1.3490607282631468\n",
      "Step - 6720, Loss - 0.7972739182931685, Learning Rate - 0.00078125, magnitude of gradient - 1.6755928293512623\n",
      "Step - 6721, Loss - 0.6799639819718909, Learning Rate - 0.00078125, magnitude of gradient - 2.0509854376377934\n",
      "Step - 6722, Loss - 0.6771732064349127, Learning Rate - 0.00078125, magnitude of gradient - 1.6331441401328575\n",
      "Step - 6723, Loss - 0.7704840607696773, Learning Rate - 0.00078125, magnitude of gradient - 1.6138304198679776\n",
      "Step - 6724, Loss - 0.6364652599864707, Learning Rate - 0.00078125, magnitude of gradient - 1.5852259363105223\n",
      "Step - 6725, Loss - 0.5495522284947134, Learning Rate - 0.00078125, magnitude of gradient - 1.6511490637890145\n",
      "Step - 6726, Loss - 0.6263401194197198, Learning Rate - 0.00078125, magnitude of gradient - 1.7872338608511014\n",
      "Step - 6727, Loss - 0.8315050412130398, Learning Rate - 0.00078125, magnitude of gradient - 1.1020006091084562\n",
      "Step - 6728, Loss - 0.7313309117044408, Learning Rate - 0.00078125, magnitude of gradient - 0.8641597396398689\n",
      "Step - 6729, Loss - 0.9935031248133884, Learning Rate - 0.00078125, magnitude of gradient - 2.4481749972671047\n",
      "Step - 6730, Loss - 0.710311493344853, Learning Rate - 0.00078125, magnitude of gradient - 1.5973059413655935\n",
      "Step - 6731, Loss - 0.6949904454064677, Learning Rate - 0.00078125, magnitude of gradient - 1.1352069145162944\n",
      "Step - 6732, Loss - 0.7017471176086634, Learning Rate - 0.00078125, magnitude of gradient - 0.8569494982536802\n",
      "Step - 6733, Loss - 0.8911306875998216, Learning Rate - 0.00078125, magnitude of gradient - 0.5972435784263365\n",
      "Step - 6734, Loss - 0.7582628959214394, Learning Rate - 0.00078125, magnitude of gradient - 0.3739555953314204\n",
      "Step - 6735, Loss - 0.7396761866367382, Learning Rate - 0.00078125, magnitude of gradient - 1.7705830837181478\n",
      "Step - 6736, Loss - 0.7207730072841696, Learning Rate - 0.00078125, magnitude of gradient - 1.4260253792879864\n",
      "Step - 6737, Loss - 0.6291952435521934, Learning Rate - 0.00078125, magnitude of gradient - 0.6157727088698166\n",
      "Step - 6738, Loss - 0.7189927457346809, Learning Rate - 0.00078125, magnitude of gradient - 1.5120968331573448\n",
      "Step - 6739, Loss - 0.5336648007586713, Learning Rate - 0.00078125, magnitude of gradient - 1.8153178452206473\n",
      "Step - 6740, Loss - 0.569487925286624, Learning Rate - 0.00078125, magnitude of gradient - 0.889939472380755\n",
      "Step - 6741, Loss - 0.6853025282623483, Learning Rate - 0.00078125, magnitude of gradient - 1.5644989005435364\n",
      "Step - 6742, Loss - 0.6602738515718549, Learning Rate - 0.00078125, magnitude of gradient - 0.4618969065400674\n",
      "Step - 6743, Loss - 0.6923540736382419, Learning Rate - 0.00078125, magnitude of gradient - 0.6842430431674484\n",
      "Step - 6744, Loss - 0.6657100363876284, Learning Rate - 0.00078125, magnitude of gradient - 2.7922786294083477\n",
      "Step - 6745, Loss - 0.7703187791207273, Learning Rate - 0.00078125, magnitude of gradient - 1.5759229382171465\n",
      "Step - 6746, Loss - 0.7564922418382055, Learning Rate - 0.00078125, magnitude of gradient - 0.4133467302370718\n",
      "Step - 6747, Loss - 0.8031191241129377, Learning Rate - 0.00078125, magnitude of gradient - 1.4331616469708113\n",
      "Step - 6748, Loss - 0.6808854069592297, Learning Rate - 0.00078125, magnitude of gradient - 2.090994391338343\n",
      "Step - 6749, Loss - 0.7548359186239432, Learning Rate - 0.00078125, magnitude of gradient - 1.4235406633053234\n",
      "Step - 6750, Loss - 0.5299397446052998, Learning Rate - 0.00078125, magnitude of gradient - 0.8208759538712311\n",
      "Step - 6751, Loss - 0.6755795513648916, Learning Rate - 0.00078125, magnitude of gradient - 1.4405735475411985\n",
      "Step - 6752, Loss - 0.5704557474777517, Learning Rate - 0.00078125, magnitude of gradient - 0.9457587403687716\n",
      "Step - 6753, Loss - 0.6143553664184593, Learning Rate - 0.00078125, magnitude of gradient - 0.7192190813146103\n",
      "Step - 6754, Loss - 0.6687180982354709, Learning Rate - 0.00078125, magnitude of gradient - 1.4746792241709465\n",
      "Step - 6755, Loss - 0.49635299068254246, Learning Rate - 0.00078125, magnitude of gradient - 1.9772684451612161\n",
      "Step - 6756, Loss - 0.8750913903569216, Learning Rate - 0.00078125, magnitude of gradient - 2.5187576777174767\n",
      "Step - 6757, Loss - 0.7198947399535455, Learning Rate - 0.00078125, magnitude of gradient - 1.639971181771803\n",
      "Step - 6758, Loss - 0.4254508513975288, Learning Rate - 0.00078125, magnitude of gradient - 0.49355810809394685\n",
      "Step - 6759, Loss - 0.9806732377297982, Learning Rate - 0.00078125, magnitude of gradient - 1.912781379970499\n",
      "Step - 6760, Loss - 0.7228932424515186, Learning Rate - 0.00078125, magnitude of gradient - 1.5669518942647394\n",
      "Step - 6761, Loss - 0.6199298878372008, Learning Rate - 0.00078125, magnitude of gradient - 0.2707001789171261\n",
      "Step - 6762, Loss - 0.9319741999350457, Learning Rate - 0.00078125, magnitude of gradient - 1.6613682320534413\n",
      "Step - 6763, Loss - 0.6591902841802046, Learning Rate - 0.00078125, magnitude of gradient - 0.37958724799016924\n",
      "Step - 6764, Loss - 0.6086923849285907, Learning Rate - 0.00078125, magnitude of gradient - 1.0578560834786097\n",
      "Step - 6765, Loss - 0.6177661854256054, Learning Rate - 0.00078125, magnitude of gradient - 0.7564030841602003\n",
      "Step - 6766, Loss - 0.6244513624544354, Learning Rate - 0.00078125, magnitude of gradient - 0.7320784896794578\n",
      "Step - 6767, Loss - 0.821413342338839, Learning Rate - 0.00078125, magnitude of gradient - 2.696108303293762\n",
      "Step - 6768, Loss - 0.6931895939384751, Learning Rate - 0.00078125, magnitude of gradient - 1.8482644396471242\n",
      "Step - 6769, Loss - 0.7136659949389671, Learning Rate - 0.00078125, magnitude of gradient - 1.0273708935411945\n",
      "Step - 6770, Loss - 0.723634994883523, Learning Rate - 0.00078125, magnitude of gradient - 0.9088023778205947\n",
      "Step - 6771, Loss - 0.6671642475903099, Learning Rate - 0.00078125, magnitude of gradient - 0.8829262867664445\n",
      "Step - 6772, Loss - 0.6191220782362203, Learning Rate - 0.00078125, magnitude of gradient - 2.669613014909898\n",
      "Step - 6773, Loss - 0.7394921356988504, Learning Rate - 0.00078125, magnitude of gradient - 1.416490740798831\n",
      "Step - 6774, Loss - 0.6076680014922564, Learning Rate - 0.00078125, magnitude of gradient - 1.2265273516685078\n",
      "Step - 6775, Loss - 0.5810062511416266, Learning Rate - 0.00078125, magnitude of gradient - 1.5823515233169136\n",
      "Step - 6776, Loss - 0.6888734928117953, Learning Rate - 0.00078125, magnitude of gradient - 0.622360442804233\n",
      "Step - 6777, Loss - 0.8660154515497425, Learning Rate - 0.00078125, magnitude of gradient - 1.0577176546575733\n",
      "Step - 6778, Loss - 0.8660195798626379, Learning Rate - 0.00078125, magnitude of gradient - 0.5929764397815891\n",
      "Step - 6779, Loss - 0.8731832428915874, Learning Rate - 0.00078125, magnitude of gradient - 0.2704463745044632\n",
      "Step - 6780, Loss - 0.80466465633921, Learning Rate - 0.00078125, magnitude of gradient - 1.304283230242437\n",
      "Step - 6781, Loss - 0.6597308361635907, Learning Rate - 0.00078125, magnitude of gradient - 1.2037857214263485\n",
      "Step - 6782, Loss - 0.6028627020606836, Learning Rate - 0.00078125, magnitude of gradient - 1.7598207263088474\n",
      "Step - 6783, Loss - 0.6630330878381466, Learning Rate - 0.00078125, magnitude of gradient - 0.9660340805859767\n",
      "Step - 6784, Loss - 0.7509388772228841, Learning Rate - 0.00078125, magnitude of gradient - 1.7108440963117506\n",
      "Step - 6785, Loss - 0.6473512259374492, Learning Rate - 0.00078125, magnitude of gradient - 1.473722602125207\n",
      "Step - 6786, Loss - 0.7573484600481533, Learning Rate - 0.00078125, magnitude of gradient - 1.5181757925828672\n",
      "Step - 6787, Loss - 0.7015864359132596, Learning Rate - 0.00078125, magnitude of gradient - 0.9876737282206167\n",
      "Step - 6788, Loss - 0.6076033530938405, Learning Rate - 0.00078125, magnitude of gradient - 1.4923344944868728\n",
      "Step - 6789, Loss - 0.5529718181224056, Learning Rate - 0.00078125, magnitude of gradient - 2.1226366903312117\n",
      "Step - 6790, Loss - 0.5649018481803119, Learning Rate - 0.00078125, magnitude of gradient - 1.0245934969855524\n",
      "Step - 6791, Loss - 0.9498973002049698, Learning Rate - 0.00078125, magnitude of gradient - 2.9224314556142144\n",
      "Step - 6792, Loss - 0.6588101634376627, Learning Rate - 0.00078125, magnitude of gradient - 1.3421672646857594\n",
      "Step - 6793, Loss - 0.7033101137117133, Learning Rate - 0.00078125, magnitude of gradient - 0.8880214951772183\n",
      "Step - 6794, Loss - 0.8019998888156366, Learning Rate - 0.00078125, magnitude of gradient - 0.3012965597409864\n",
      "Step - 6795, Loss - 0.6321602764835812, Learning Rate - 0.00078125, magnitude of gradient - 1.1990945682963843\n",
      "Step - 6796, Loss - 0.691078748872335, Learning Rate - 0.00078125, magnitude of gradient - 1.7161119199817747\n",
      "Step - 6797, Loss - 0.5576117074664919, Learning Rate - 0.00078125, magnitude of gradient - 0.674126438979868\n",
      "Step - 6798, Loss - 0.9449993361063506, Learning Rate - 0.00078125, magnitude of gradient - 0.9542945736720496\n",
      "Step - 6799, Loss - 0.5005743780441362, Learning Rate - 0.00078125, magnitude of gradient - 1.4679620799743536\n",
      "Step - 6800, Loss - 0.7160738778987963, Learning Rate - 0.00078125, magnitude of gradient - 1.6562186414024156\n",
      "Step - 6801, Loss - 0.6762217816354531, Learning Rate - 0.00078125, magnitude of gradient - 1.1974178608152277\n",
      "Step - 6802, Loss - 1.0022301149681516, Learning Rate - 0.00078125, magnitude of gradient - 1.455243831838723\n",
      "Step - 6803, Loss - 0.7005300716458598, Learning Rate - 0.00078125, magnitude of gradient - 1.280137102292164\n",
      "Step - 6804, Loss - 0.7600972030435706, Learning Rate - 0.00078125, magnitude of gradient - 0.5632286664594965\n",
      "Step - 6805, Loss - 0.897612718082261, Learning Rate - 0.00078125, magnitude of gradient - 1.0116695725562757\n",
      "Step - 6806, Loss - 0.7950195456030402, Learning Rate - 0.00078125, magnitude of gradient - 0.7372975088562267\n",
      "Step - 6807, Loss - 0.6345268077028365, Learning Rate - 0.00078125, magnitude of gradient - 1.9138482749244232\n",
      "Step - 6808, Loss - 0.29494783632396493, Learning Rate - 0.00078125, magnitude of gradient - 0.7457848593217482\n",
      "Step - 6809, Loss - 0.6100141728983353, Learning Rate - 0.00078125, magnitude of gradient - 0.9408418446871635\n",
      "Step - 6810, Loss - 0.7840909730287285, Learning Rate - 0.00078125, magnitude of gradient - 0.8345493662256614\n",
      "Step - 6811, Loss - 0.8776207392769392, Learning Rate - 0.00078125, magnitude of gradient - 0.873010601840981\n",
      "Step - 6812, Loss - 0.649892167060978, Learning Rate - 0.00078125, magnitude of gradient - 0.572952436650564\n",
      "Step - 6813, Loss - 0.5928146234794334, Learning Rate - 0.00078125, magnitude of gradient - 1.4763154107303689\n",
      "Step - 6814, Loss - 0.7572553693978309, Learning Rate - 0.00078125, magnitude of gradient - 1.7859632182651808\n",
      "Step - 6815, Loss - 0.5347907849088382, Learning Rate - 0.00078125, magnitude of gradient - 1.8597510139677191\n",
      "Step - 6816, Loss - 0.8031333246889268, Learning Rate - 0.00078125, magnitude of gradient - 0.7822556504418909\n",
      "Step - 6817, Loss - 0.7422893190588653, Learning Rate - 0.00078125, magnitude of gradient - 0.7773606598428447\n",
      "Step - 6818, Loss - 0.7411729365505489, Learning Rate - 0.00078125, magnitude of gradient - 0.34758663021480796\n",
      "Step - 6819, Loss - 0.6872300443564959, Learning Rate - 0.00078125, magnitude of gradient - 1.3630514612213505\n",
      "Step - 6820, Loss - 0.6083829334812426, Learning Rate - 0.00078125, magnitude of gradient - 0.9388780543668683\n",
      "Step - 6821, Loss - 0.7477384249706809, Learning Rate - 0.00078125, magnitude of gradient - 1.5991249668327203\n",
      "Step - 6822, Loss - 0.7250405248584406, Learning Rate - 0.00078125, magnitude of gradient - 2.306709539782408\n",
      "Step - 6823, Loss - 0.6274913855180873, Learning Rate - 0.00078125, magnitude of gradient - 1.5210098672202212\n",
      "Step - 6824, Loss - 0.7359831146707012, Learning Rate - 0.00078125, magnitude of gradient - 0.606763606857363\n",
      "Step - 6825, Loss - 0.6638274208833788, Learning Rate - 0.00078125, magnitude of gradient - 2.0420730510826015\n",
      "Step - 6826, Loss - 0.6874272941662036, Learning Rate - 0.00078125, magnitude of gradient - 1.3297582689084615\n",
      "Step - 6827, Loss - 0.6057910476104676, Learning Rate - 0.00078125, magnitude of gradient - 0.9396077074964544\n",
      "Step - 6828, Loss - 0.6752532643831275, Learning Rate - 0.00078125, magnitude of gradient - 0.8499881358606558\n",
      "Step - 6829, Loss - 0.6592971300603525, Learning Rate - 0.00078125, magnitude of gradient - 1.133443853889214\n",
      "Step - 6830, Loss - 0.7430161909802735, Learning Rate - 0.00078125, magnitude of gradient - 0.8093128251640392\n",
      "Step - 6831, Loss - 0.7199863764338046, Learning Rate - 0.00078125, magnitude of gradient - 0.9437577350986656\n",
      "Step - 6832, Loss - 0.6251999441262486, Learning Rate - 0.00078125, magnitude of gradient - 2.00083012198\n",
      "Step - 6833, Loss - 0.7532017654352486, Learning Rate - 0.00078125, magnitude of gradient - 1.2713634554960211\n",
      "Step - 6834, Loss - 0.8230165761355196, Learning Rate - 0.00078125, magnitude of gradient - 1.5744440934266208\n",
      "Step - 6835, Loss - 0.8532032048879102, Learning Rate - 0.00078125, magnitude of gradient - 1.2830765951990593\n",
      "Step - 6836, Loss - 0.9748344711587025, Learning Rate - 0.00078125, magnitude of gradient - 2.1178239233544702\n",
      "Step - 6837, Loss - 0.5764533101287568, Learning Rate - 0.00078125, magnitude of gradient - 0.9190847596709337\n",
      "Step - 6838, Loss - 0.5735751919369065, Learning Rate - 0.00078125, magnitude of gradient - 1.4546701266684374\n",
      "Step - 6839, Loss - 0.8990404214953864, Learning Rate - 0.00078125, magnitude of gradient - 2.157137616622907\n",
      "Step - 6840, Loss - 0.6106797941702891, Learning Rate - 0.00078125, magnitude of gradient - 0.40699156585192603\n",
      "Step - 6841, Loss - 0.6620089150704463, Learning Rate - 0.00078125, magnitude of gradient - 1.1600291978558428\n",
      "Step - 6842, Loss - 0.8782491620481083, Learning Rate - 0.00078125, magnitude of gradient - 2.380962782224675\n",
      "Step - 6843, Loss - 0.6315378322415732, Learning Rate - 0.00078125, magnitude of gradient - 1.2902609684947122\n",
      "Step - 6844, Loss - 0.8103746832798725, Learning Rate - 0.00078125, magnitude of gradient - 1.6244799841351274\n",
      "Step - 6845, Loss - 0.7462367741079277, Learning Rate - 0.00078125, magnitude of gradient - 1.0125842660279576\n",
      "Step - 6846, Loss - 0.8512130470834403, Learning Rate - 0.00078125, magnitude of gradient - 0.8692000762539411\n",
      "Step - 6847, Loss - 0.7113082453718824, Learning Rate - 0.00078125, magnitude of gradient - 1.5035110889736485\n",
      "Step - 6848, Loss - 0.8474762827585599, Learning Rate - 0.00078125, magnitude of gradient - 2.4180176655697694\n",
      "Step - 6849, Loss - 0.7170688373606753, Learning Rate - 0.00078125, magnitude of gradient - 1.2900313755267405\n",
      "Step - 6850, Loss - 0.791427108714868, Learning Rate - 0.00078125, magnitude of gradient - 1.417789263299557\n",
      "Step - 6851, Loss - 0.7528201882531139, Learning Rate - 0.00078125, magnitude of gradient - 1.8191181883738128\n",
      "Step - 6852, Loss - 0.5038884269995403, Learning Rate - 0.00078125, magnitude of gradient - 2.4269059417564494\n",
      "Step - 6853, Loss - 0.6931870394264714, Learning Rate - 0.00078125, magnitude of gradient - 1.6689410622007388\n",
      "Step - 6854, Loss - 0.7420406251009408, Learning Rate - 0.00078125, magnitude of gradient - 0.7655828406861029\n",
      "Step - 6855, Loss - 0.6819784020715888, Learning Rate - 0.00078125, magnitude of gradient - 2.1870198095522513\n",
      "Step - 6856, Loss - 0.520756232588464, Learning Rate - 0.00078125, magnitude of gradient - 0.7971605943898756\n",
      "Step - 6857, Loss - 0.7026837581313033, Learning Rate - 0.00078125, magnitude of gradient - 1.3696262497175957\n",
      "Step - 6858, Loss - 0.7014706988338608, Learning Rate - 0.00078125, magnitude of gradient - 0.9317079694680938\n",
      "Step - 6859, Loss - 0.8054649635447797, Learning Rate - 0.00078125, magnitude of gradient - 0.8936501778475443\n",
      "Step - 6860, Loss - 0.626451774623106, Learning Rate - 0.00078125, magnitude of gradient - 1.083795382763999\n",
      "Step - 6861, Loss - 0.6407764666220659, Learning Rate - 0.00078125, magnitude of gradient - 3.168502917281201\n",
      "Step - 6862, Loss - 0.6743630763187478, Learning Rate - 0.00078125, magnitude of gradient - 1.113434798086057\n",
      "Step - 6863, Loss - 0.7926283286157318, Learning Rate - 0.00078125, magnitude of gradient - 1.2680959976132422\n",
      "Step - 6864, Loss - 0.727840052412458, Learning Rate - 0.00078125, magnitude of gradient - 2.098875167119715\n",
      "Step - 6865, Loss - 0.7123853295087706, Learning Rate - 0.00078125, magnitude of gradient - 1.0284479623412295\n",
      "Step - 6866, Loss - 0.6898615115628408, Learning Rate - 0.00078125, magnitude of gradient - 0.8899745180755448\n",
      "Step - 6867, Loss - 0.6781286061888352, Learning Rate - 0.00078125, magnitude of gradient - 0.9938281592270256\n",
      "Step - 6868, Loss - 0.5372029902118426, Learning Rate - 0.00078125, magnitude of gradient - 0.4875460032498567\n",
      "Step - 6869, Loss - 0.6478622685979558, Learning Rate - 0.00078125, magnitude of gradient - 1.8864836695320777\n",
      "Step - 6870, Loss - 0.6922654756852065, Learning Rate - 0.00078125, magnitude of gradient - 0.5390769668124352\n",
      "Step - 6871, Loss - 0.7528468256522461, Learning Rate - 0.00078125, magnitude of gradient - 1.6344839098536488\n",
      "Step - 6872, Loss - 0.49319906069786823, Learning Rate - 0.00078125, magnitude of gradient - 1.6675527387856086\n",
      "Step - 6873, Loss - 0.5609745056471769, Learning Rate - 0.00078125, magnitude of gradient - 1.491896306108666\n",
      "Step - 6874, Loss - 0.7993825791895686, Learning Rate - 0.00078125, magnitude of gradient - 0.7908944315186052\n",
      "Step - 6875, Loss - 0.7562563475817804, Learning Rate - 0.00078125, magnitude of gradient - 0.9121110648050389\n",
      "Step - 6876, Loss - 0.7412619730752044, Learning Rate - 0.00078125, magnitude of gradient - 0.5460770464789984\n",
      "Step - 6877, Loss - 0.6348552802363147, Learning Rate - 0.00078125, magnitude of gradient - 0.48438668843904337\n",
      "Step - 6878, Loss - 0.7120982995268309, Learning Rate - 0.00078125, magnitude of gradient - 2.5240639301484102\n",
      "Step - 6879, Loss - 0.6121391914794747, Learning Rate - 0.00078125, magnitude of gradient - 1.07450112264267\n",
      "Step - 6880, Loss - 0.6537399708145062, Learning Rate - 0.00078125, magnitude of gradient - 1.3048087782399957\n",
      "Step - 6881, Loss - 0.7276325896361617, Learning Rate - 0.00078125, magnitude of gradient - 0.6365296300030494\n",
      "Step - 6882, Loss - 0.7649622668247927, Learning Rate - 0.00078125, magnitude of gradient - 1.8423507071134517\n",
      "Step - 6883, Loss - 0.7077276648479579, Learning Rate - 0.00078125, magnitude of gradient - 0.4293926616284244\n",
      "Step - 6884, Loss - 0.97205013683915, Learning Rate - 0.00078125, magnitude of gradient - 1.5349781696223224\n",
      "Step - 6885, Loss - 0.9086721275025242, Learning Rate - 0.00078125, magnitude of gradient - 1.8638180893405905\n",
      "Step - 6886, Loss - 0.7304738867892965, Learning Rate - 0.00078125, magnitude of gradient - 1.249154358235733\n",
      "Step - 6887, Loss - 0.7641699072874877, Learning Rate - 0.00078125, magnitude of gradient - 1.7968454859939553\n",
      "Step - 6888, Loss - 0.7091022841371162, Learning Rate - 0.00078125, magnitude of gradient - 1.3555104751276934\n",
      "Step - 6889, Loss - 0.6415783072044418, Learning Rate - 0.00078125, magnitude of gradient - 2.0124815079791105\n",
      "Step - 6890, Loss - 0.8805450518843534, Learning Rate - 0.00078125, magnitude of gradient - 1.450376762273916\n",
      "Step - 6891, Loss - 0.6750025087253816, Learning Rate - 0.00078125, magnitude of gradient - 2.15252393548798\n",
      "Step - 6892, Loss - 0.5904251540945876, Learning Rate - 0.00078125, magnitude of gradient - 1.0945773180301175\n",
      "Step - 6893, Loss - 0.643709855565175, Learning Rate - 0.00078125, magnitude of gradient - 1.8710988709703995\n",
      "Step - 6894, Loss - 0.6203956837261447, Learning Rate - 0.00078125, magnitude of gradient - 1.140097280217339\n",
      "Step - 6895, Loss - 0.5426719193604436, Learning Rate - 0.00078125, magnitude of gradient - 2.8163124505625245\n",
      "Step - 6896, Loss - 0.594552078191999, Learning Rate - 0.00078125, magnitude of gradient - 2.2787428124250195\n",
      "Step - 6897, Loss - 0.7268286707717919, Learning Rate - 0.00078125, magnitude of gradient - 1.3658934106779796\n",
      "Step - 6898, Loss - 0.6132289544407482, Learning Rate - 0.00078125, magnitude of gradient - 0.5079927523350779\n",
      "Step - 6899, Loss - 0.48310763218947644, Learning Rate - 0.00078125, magnitude of gradient - 1.406657744854454\n",
      "Step - 6900, Loss - 0.6447886123968141, Learning Rate - 0.00078125, magnitude of gradient - 0.8926697427791019\n",
      "Step - 6901, Loss - 0.6782190853664183, Learning Rate - 0.00078125, magnitude of gradient - 0.5715452356482611\n",
      "Step - 6902, Loss - 0.738971353496059, Learning Rate - 0.00078125, magnitude of gradient - 0.5797455070312486\n",
      "Step - 6903, Loss - 0.6931728588006878, Learning Rate - 0.00078125, magnitude of gradient - 1.141115487257272\n",
      "Step - 6904, Loss - 0.6409608444179499, Learning Rate - 0.00078125, magnitude of gradient - 0.8122340377683298\n",
      "Step - 6905, Loss - 0.9315143064454463, Learning Rate - 0.00078125, magnitude of gradient - 1.1733384761084305\n",
      "Step - 6906, Loss - 0.7505462040239695, Learning Rate - 0.00078125, magnitude of gradient - 1.6442722708542503\n",
      "Step - 6907, Loss - 0.6090713944358576, Learning Rate - 0.00078125, magnitude of gradient - 1.2880063498197736\n",
      "Step - 6908, Loss - 0.6195133581604098, Learning Rate - 0.00078125, magnitude of gradient - 0.8568202736963444\n",
      "Step - 6909, Loss - 0.6470839385239064, Learning Rate - 0.00078125, magnitude of gradient - 2.391167957790941\n",
      "Step - 6910, Loss - 0.8139978570887823, Learning Rate - 0.00078125, magnitude of gradient - 1.1832970112960342\n",
      "Step - 6911, Loss - 0.6574941532480265, Learning Rate - 0.00078125, magnitude of gradient - 0.7271918604152474\n",
      "Step - 6912, Loss - 0.9971758187097404, Learning Rate - 0.00078125, magnitude of gradient - 1.634300604809855\n",
      "Step - 6913, Loss - 0.982909913743683, Learning Rate - 0.00078125, magnitude of gradient - 0.7799478553446336\n",
      "Step - 6914, Loss - 0.6092870964876469, Learning Rate - 0.00078125, magnitude of gradient - 1.338889969706396\n",
      "Step - 6915, Loss - 0.5619391792187045, Learning Rate - 0.00078125, magnitude of gradient - 1.088396421580974\n",
      "Step - 6916, Loss - 0.5142343264005409, Learning Rate - 0.00078125, magnitude of gradient - 1.0607145981802197\n",
      "Step - 6917, Loss - 0.5642149425122762, Learning Rate - 0.00078125, magnitude of gradient - 0.9909203434680877\n",
      "Step - 6918, Loss - 0.579362900292808, Learning Rate - 0.00078125, magnitude of gradient - 1.004208970437717\n",
      "Step - 6919, Loss - 0.629795721177911, Learning Rate - 0.00078125, magnitude of gradient - 1.1081747438987948\n",
      "Step - 6920, Loss - 0.5372202415630007, Learning Rate - 0.00078125, magnitude of gradient - 1.4774179745832992\n",
      "Step - 6921, Loss - 0.6816186115211201, Learning Rate - 0.00078125, magnitude of gradient - 0.43526276564173394\n",
      "Step - 6922, Loss - 0.5716717839354222, Learning Rate - 0.00078125, magnitude of gradient - 1.3083319921729888\n",
      "Step - 6923, Loss - 0.6705653179541293, Learning Rate - 0.00078125, magnitude of gradient - 0.48620690622696644\n",
      "Step - 6924, Loss - 0.7518530469799025, Learning Rate - 0.00078125, magnitude of gradient - 1.1924702143318766\n",
      "Step - 6925, Loss - 0.7618673127066401, Learning Rate - 0.00078125, magnitude of gradient - 2.41385876631301\n",
      "Step - 6926, Loss - 0.4988562924692055, Learning Rate - 0.00078125, magnitude of gradient - 1.1107516034017515\n",
      "Step - 6927, Loss - 0.5864062609658798, Learning Rate - 0.00078125, magnitude of gradient - 1.1113693867606915\n",
      "Step - 6928, Loss - 0.868088133570037, Learning Rate - 0.00078125, magnitude of gradient - 2.0955815492491485\n",
      "Step - 6929, Loss - 0.6986833227269317, Learning Rate - 0.00078125, magnitude of gradient - 0.6899228996046031\n",
      "Step - 6930, Loss - 0.8072891625391849, Learning Rate - 0.00078125, magnitude of gradient - 2.0113746308381595\n",
      "Step - 6931, Loss - 0.6497519990117506, Learning Rate - 0.00078125, magnitude of gradient - 1.8075864565688162\n",
      "Step - 6932, Loss - 0.8656417511521496, Learning Rate - 0.00078125, magnitude of gradient - 1.263593459380363\n",
      "Step - 6933, Loss - 0.5217418604134978, Learning Rate - 0.00078125, magnitude of gradient - 0.560858730799861\n",
      "Step - 6934, Loss - 0.7140489879694318, Learning Rate - 0.00078125, magnitude of gradient - 1.495581377112816\n",
      "Step - 6935, Loss - 0.7982301823017569, Learning Rate - 0.00078125, magnitude of gradient - 2.773030361879415\n",
      "Step - 6936, Loss - 0.8078029184538541, Learning Rate - 0.00078125, magnitude of gradient - 2.037546884817962\n",
      "Step - 6937, Loss - 0.6805081459811108, Learning Rate - 0.00078125, magnitude of gradient - 0.9245175973016736\n",
      "Step - 6938, Loss - 0.8338938843792191, Learning Rate - 0.00078125, magnitude of gradient - 0.9000447917489712\n",
      "Step - 6939, Loss - 0.6232564244997782, Learning Rate - 0.00078125, magnitude of gradient - 1.246513812414792\n",
      "Step - 6940, Loss - 0.5068348909963634, Learning Rate - 0.00078125, magnitude of gradient - 2.3372139138653147\n",
      "Step - 6941, Loss - 0.8620833195352343, Learning Rate - 0.00078125, magnitude of gradient - 1.0343650807250335\n",
      "Step - 6942, Loss - 0.6512955933086723, Learning Rate - 0.00078125, magnitude of gradient - 1.0531919056549333\n",
      "Step - 6943, Loss - 0.9239444520206773, Learning Rate - 0.00078125, magnitude of gradient - 0.8360946156840123\n",
      "Step - 6944, Loss - 0.8973152128161133, Learning Rate - 0.00078125, magnitude of gradient - 1.5559539199582988\n",
      "Step - 6945, Loss - 0.4823762342735295, Learning Rate - 0.00078125, magnitude of gradient - 1.5019673010602772\n",
      "Step - 6946, Loss - 0.7974119526013104, Learning Rate - 0.00078125, magnitude of gradient - 2.5599033664197393\n",
      "Step - 6947, Loss - 0.6959375402243584, Learning Rate - 0.00078125, magnitude of gradient - 0.460025054553743\n",
      "Step - 6948, Loss - 0.5531932992381117, Learning Rate - 0.00078125, magnitude of gradient - 1.1441477375011329\n",
      "Step - 6949, Loss - 0.6280742949299357, Learning Rate - 0.00078125, magnitude of gradient - 1.0621036041573593\n",
      "Step - 6950, Loss - 0.6094436666244069, Learning Rate - 0.00078125, magnitude of gradient - 0.9420894173246023\n",
      "Step - 6951, Loss - 0.7982974065933762, Learning Rate - 0.00078125, magnitude of gradient - 2.444723535064802\n",
      "Step - 6952, Loss - 0.5935858294692385, Learning Rate - 0.00078125, magnitude of gradient - 0.8746560304382692\n",
      "Step - 6953, Loss - 0.8515842575584258, Learning Rate - 0.00078125, magnitude of gradient - 1.6392189931644516\n",
      "Step - 6954, Loss - 0.5741842522247822, Learning Rate - 0.00078125, magnitude of gradient - 1.3341926091322096\n",
      "Step - 6955, Loss - 0.5144714945495789, Learning Rate - 0.00078125, magnitude of gradient - 0.9471695221198568\n",
      "Step - 6956, Loss - 0.6828658953248297, Learning Rate - 0.00078125, magnitude of gradient - 0.6379546145395526\n",
      "Step - 6957, Loss - 0.47863295019846835, Learning Rate - 0.00078125, magnitude of gradient - 1.8592460943629143\n",
      "Step - 6958, Loss - 0.7852058650041134, Learning Rate - 0.00078125, magnitude of gradient - 0.8882358770722105\n",
      "Step - 6959, Loss - 0.48799229454167514, Learning Rate - 0.00078125, magnitude of gradient - 0.6494939737475337\n",
      "Step - 6960, Loss - 0.713760039781432, Learning Rate - 0.00078125, magnitude of gradient - 1.3203419819712638\n",
      "Step - 6961, Loss - 0.7595110479981311, Learning Rate - 0.00078125, magnitude of gradient - 0.9642838371878296\n",
      "Step - 6962, Loss - 0.6328953540870016, Learning Rate - 0.00078125, magnitude of gradient - 1.170845072367274\n",
      "Step - 6963, Loss - 0.7032917060051159, Learning Rate - 0.00078125, magnitude of gradient - 0.32091612587079643\n",
      "Step - 6964, Loss - 0.5602147188891574, Learning Rate - 0.00078125, magnitude of gradient - 1.0645142163953787\n",
      "Step - 6965, Loss - 0.8259329595047062, Learning Rate - 0.00078125, magnitude of gradient - 1.1340677730561146\n",
      "Step - 6966, Loss - 0.6902545795129558, Learning Rate - 0.00078125, magnitude of gradient - 1.07243586034165\n",
      "Step - 6967, Loss - 0.5821786064715146, Learning Rate - 0.00078125, magnitude of gradient - 1.1027509037031753\n",
      "Step - 6968, Loss - 0.7138405521377736, Learning Rate - 0.00078125, magnitude of gradient - 0.6094078761615931\n",
      "Step - 6969, Loss - 0.8100539090008163, Learning Rate - 0.00078125, magnitude of gradient - 1.0502146521310607\n",
      "Step - 6970, Loss - 0.6388080104532543, Learning Rate - 0.00078125, magnitude of gradient - 1.054563867381072\n",
      "Step - 6971, Loss - 0.6427937964555793, Learning Rate - 0.00078125, magnitude of gradient - 1.7445751934540523\n",
      "Step - 6972, Loss - 0.5812292363656802, Learning Rate - 0.00078125, magnitude of gradient - 0.26874107775154354\n",
      "Step - 6973, Loss - 0.6858581498106695, Learning Rate - 0.00078125, magnitude of gradient - 0.7169587336018229\n",
      "Step - 6974, Loss - 0.7124938588600457, Learning Rate - 0.00078125, magnitude of gradient - 0.9406177974294274\n",
      "Step - 6975, Loss - 0.8099831174716791, Learning Rate - 0.00078125, magnitude of gradient - 0.820427500641663\n",
      "Step - 6976, Loss - 0.6303456543597487, Learning Rate - 0.00078125, magnitude of gradient - 1.1726585421785503\n",
      "Step - 6977, Loss - 0.9000906846777568, Learning Rate - 0.00078125, magnitude of gradient - 2.182991469897146\n",
      "Step - 6978, Loss - 0.6723767692113144, Learning Rate - 0.00078125, magnitude of gradient - 1.1437622314623856\n",
      "Step - 6979, Loss - 0.7741139650536779, Learning Rate - 0.00078125, magnitude of gradient - 1.2345001851491604\n",
      "Step - 6980, Loss - 0.7018095845560081, Learning Rate - 0.00078125, magnitude of gradient - 2.5953560600044896\n",
      "Step - 6981, Loss - 0.6804684023463812, Learning Rate - 0.00078125, magnitude of gradient - 0.9637906764827239\n",
      "Step - 6982, Loss - 0.5246231341986054, Learning Rate - 0.00078125, magnitude of gradient - 0.3483800520475193\n",
      "Step - 6983, Loss - 0.7125118634927645, Learning Rate - 0.00078125, magnitude of gradient - 0.9691894443265772\n",
      "Step - 6984, Loss - 0.6477191058516544, Learning Rate - 0.00078125, magnitude of gradient - 0.8309935086756935\n",
      "Step - 6985, Loss - 0.8290173085698204, Learning Rate - 0.00078125, magnitude of gradient - 1.3017875893093098\n",
      "Step - 6986, Loss - 0.6912424282861065, Learning Rate - 0.00078125, magnitude of gradient - 1.4658091923245227\n",
      "Step - 6987, Loss - 0.7920448833282998, Learning Rate - 0.00078125, magnitude of gradient - 1.2324884441024113\n",
      "Step - 6988, Loss - 0.5923715256454885, Learning Rate - 0.00078125, magnitude of gradient - 2.236474097558914\n",
      "Step - 6989, Loss - 0.8042434802360195, Learning Rate - 0.00078125, magnitude of gradient - 1.2073507322601345\n",
      "Step - 6990, Loss - 0.5012971647493444, Learning Rate - 0.00078125, magnitude of gradient - 3.4443979816562176\n",
      "Step - 6991, Loss - 0.6522672702482608, Learning Rate - 0.00078125, magnitude of gradient - 0.902261353839665\n",
      "Step - 6992, Loss - 0.914107175196281, Learning Rate - 0.00078125, magnitude of gradient - 0.4313786414625186\n",
      "Step - 6993, Loss - 0.6413338607039614, Learning Rate - 0.00078125, magnitude of gradient - 0.6205468054915584\n",
      "Step - 6994, Loss - 0.8535805467036677, Learning Rate - 0.00078125, magnitude of gradient - 0.38393519337522247\n",
      "Step - 6995, Loss - 0.8288639656562615, Learning Rate - 0.00078125, magnitude of gradient - 2.7073176273227446\n",
      "Step - 6996, Loss - 0.7020419244441278, Learning Rate - 0.00078125, magnitude of gradient - 0.4048939635258153\n",
      "Step - 6997, Loss - 0.8404137995009294, Learning Rate - 0.00078125, magnitude of gradient - 2.5333530013530696\n",
      "Step - 6998, Loss - 0.8387947740954693, Learning Rate - 0.00078125, magnitude of gradient - 1.584364487269677\n",
      "Step - 6999, Loss - 0.647413751948644, Learning Rate - 0.00078125, magnitude of gradient - 2.0396090648855627\n",
      "Step - 7000, Loss - 0.7403469568940131, Learning Rate - 0.00078125, magnitude of gradient - 1.597029729031748\n",
      "Step - 7001, Loss - 0.7396707171657685, Learning Rate - 0.000390625, magnitude of gradient - 3.0640205361853785\n",
      "Step - 7002, Loss - 0.6104077035011668, Learning Rate - 0.000390625, magnitude of gradient - 0.41316247385420934\n",
      "Step - 7003, Loss - 0.6689831762655225, Learning Rate - 0.000390625, magnitude of gradient - 2.07568348922368\n",
      "Step - 7004, Loss - 0.7378744634081096, Learning Rate - 0.000390625, magnitude of gradient - 1.9244622249824024\n",
      "Step - 7005, Loss - 0.8115398245804157, Learning Rate - 0.000390625, magnitude of gradient - 1.2165722444205143\n",
      "Step - 7006, Loss - 0.5986099680330546, Learning Rate - 0.000390625, magnitude of gradient - 0.908992178459322\n",
      "Step - 7007, Loss - 0.6837309693927325, Learning Rate - 0.000390625, magnitude of gradient - 0.941017900334694\n",
      "Step - 7008, Loss - 1.0039997598507282, Learning Rate - 0.000390625, magnitude of gradient - 1.7996009301759806\n",
      "Step - 7009, Loss - 0.5562053089138693, Learning Rate - 0.000390625, magnitude of gradient - 1.5203080418524675\n",
      "Step - 7010, Loss - 0.5846195543361887, Learning Rate - 0.000390625, magnitude of gradient - 2.1780132033381663\n",
      "Step - 7011, Loss - 0.6036383903961724, Learning Rate - 0.000390625, magnitude of gradient - 0.5207685968101985\n",
      "Step - 7012, Loss - 0.7251132624581842, Learning Rate - 0.000390625, magnitude of gradient - 0.6008785360812933\n",
      "Step - 7013, Loss - 0.5314782423781022, Learning Rate - 0.000390625, magnitude of gradient - 2.7007066866341503\n",
      "Step - 7014, Loss - 0.6563158047993581, Learning Rate - 0.000390625, magnitude of gradient - 2.14405527124167\n",
      "Step - 7015, Loss - 0.7271085691578837, Learning Rate - 0.000390625, magnitude of gradient - 1.892314550001857\n",
      "Step - 7016, Loss - 0.7813915100865472, Learning Rate - 0.000390625, magnitude of gradient - 0.840162479671869\n",
      "Step - 7017, Loss - 0.7643234014690631, Learning Rate - 0.000390625, magnitude of gradient - 0.28996926756181546\n",
      "Step - 7018, Loss - 0.7678755018625001, Learning Rate - 0.000390625, magnitude of gradient - 1.1124473957422465\n",
      "Step - 7019, Loss - 0.673068628718843, Learning Rate - 0.000390625, magnitude of gradient - 1.1204883978569322\n",
      "Step - 7020, Loss - 0.7973502710590444, Learning Rate - 0.000390625, magnitude of gradient - 1.7761249888315183\n",
      "Step - 7021, Loss - 0.6925262649623857, Learning Rate - 0.000390625, magnitude of gradient - 1.3721531203414947\n",
      "Step - 7022, Loss - 0.7022764823408808, Learning Rate - 0.000390625, magnitude of gradient - 0.6489542615348576\n",
      "Step - 7023, Loss - 0.5683151521031489, Learning Rate - 0.000390625, magnitude of gradient - 0.6794012544800555\n",
      "Step - 7024, Loss - 0.6269074719002974, Learning Rate - 0.000390625, magnitude of gradient - 1.082381131239439\n",
      "Step - 7025, Loss - 0.6470243366345869, Learning Rate - 0.000390625, magnitude of gradient - 1.3560100417764636\n",
      "Step - 7026, Loss - 0.3939244160631841, Learning Rate - 0.000390625, magnitude of gradient - 1.904059133671052\n",
      "Step - 7027, Loss - 0.6883393934622206, Learning Rate - 0.000390625, magnitude of gradient - 0.9413613802162694\n",
      "Step - 7028, Loss - 0.613678387444704, Learning Rate - 0.000390625, magnitude of gradient - 1.307237869903406\n",
      "Step - 7029, Loss - 0.6560262075265, Learning Rate - 0.000390625, magnitude of gradient - 1.1556182312994223\n",
      "Step - 7030, Loss - 0.594282963913493, Learning Rate - 0.000390625, magnitude of gradient - 0.8027921488191417\n",
      "Step - 7031, Loss - 0.7822881761201208, Learning Rate - 0.000390625, magnitude of gradient - 2.523469525740511\n",
      "Step - 7032, Loss - 0.6214917755489522, Learning Rate - 0.000390625, magnitude of gradient - 0.516164282719672\n",
      "Step - 7033, Loss - 0.873582315472254, Learning Rate - 0.000390625, magnitude of gradient - 0.8934589649942537\n",
      "Step - 7034, Loss - 0.8248435978448928, Learning Rate - 0.000390625, magnitude of gradient - 1.3533630044905083\n",
      "Step - 7035, Loss - 0.6315284033071544, Learning Rate - 0.000390625, magnitude of gradient - 1.62101010087985\n",
      "Step - 7036, Loss - 0.5561559118602892, Learning Rate - 0.000390625, magnitude of gradient - 0.6789174098107756\n",
      "Step - 7037, Loss - 0.7801459775114874, Learning Rate - 0.000390625, magnitude of gradient - 1.348002743201855\n",
      "Step - 7038, Loss - 0.6017720588839888, Learning Rate - 0.000390625, magnitude of gradient - 0.9517136535958669\n",
      "Step - 7039, Loss - 0.6352064484627922, Learning Rate - 0.000390625, magnitude of gradient - 0.1627297287628174\n",
      "Step - 7040, Loss - 0.6648994002843334, Learning Rate - 0.000390625, magnitude of gradient - 1.2779812511776474\n",
      "Step - 7041, Loss - 0.7465985191272854, Learning Rate - 0.000390625, magnitude of gradient - 1.7476922118833917\n",
      "Step - 7042, Loss - 0.6893352492507477, Learning Rate - 0.000390625, magnitude of gradient - 2.63633474067796\n",
      "Step - 7043, Loss - 0.7655974094400178, Learning Rate - 0.000390625, magnitude of gradient - 1.489720098420957\n",
      "Step - 7044, Loss - 0.6674963547555087, Learning Rate - 0.000390625, magnitude of gradient - 1.0935393158979905\n",
      "Step - 7045, Loss - 0.6254587421457822, Learning Rate - 0.000390625, magnitude of gradient - 0.7538895392480119\n",
      "Step - 7046, Loss - 0.6533064943800391, Learning Rate - 0.000390625, magnitude of gradient - 0.9654361293414093\n",
      "Step - 7047, Loss - 0.8288263372876017, Learning Rate - 0.000390625, magnitude of gradient - 1.24549051076314\n",
      "Step - 7048, Loss - 0.6085496367151995, Learning Rate - 0.000390625, magnitude of gradient - 2.1230322943681874\n",
      "Step - 7049, Loss - 0.7110235918478092, Learning Rate - 0.000390625, magnitude of gradient - 0.6892308763315521\n",
      "Step - 7050, Loss - 0.6944614062931292, Learning Rate - 0.000390625, magnitude of gradient - 1.5811416237640281\n",
      "Step - 7051, Loss - 0.5901510615988571, Learning Rate - 0.000390625, magnitude of gradient - 1.4480065574486727\n",
      "Step - 7052, Loss - 0.686745230704015, Learning Rate - 0.000390625, magnitude of gradient - 1.895425337674566\n",
      "Step - 7053, Loss - 0.8512950961969825, Learning Rate - 0.000390625, magnitude of gradient - 1.8274720819488448\n",
      "Step - 7054, Loss - 0.6460055186346014, Learning Rate - 0.000390625, magnitude of gradient - 0.7690963808645341\n",
      "Step - 7055, Loss - 0.6630493265843759, Learning Rate - 0.000390625, magnitude of gradient - 1.7221848664479271\n",
      "Step - 7056, Loss - 0.5507258271723731, Learning Rate - 0.000390625, magnitude of gradient - 1.1538720721342026\n",
      "Step - 7057, Loss - 0.7051241669378692, Learning Rate - 0.000390625, magnitude of gradient - 2.3283396570821773\n",
      "Step - 7058, Loss - 0.6473316413820536, Learning Rate - 0.000390625, magnitude of gradient - 1.5498573963232951\n",
      "Step - 7059, Loss - 0.7953297150177833, Learning Rate - 0.000390625, magnitude of gradient - 0.9028901389245131\n",
      "Step - 7060, Loss - 0.6028587235304224, Learning Rate - 0.000390625, magnitude of gradient - 1.369206771125695\n",
      "Step - 7061, Loss - 0.7148345109156102, Learning Rate - 0.000390625, magnitude of gradient - 1.313008193519587\n",
      "Step - 7062, Loss - 0.6528166174474708, Learning Rate - 0.000390625, magnitude of gradient - 0.6333854192561084\n",
      "Step - 7063, Loss - 0.4290309522083989, Learning Rate - 0.000390625, magnitude of gradient - 0.41645324547800666\n",
      "Step - 7064, Loss - 0.5759573408760394, Learning Rate - 0.000390625, magnitude of gradient - 1.2372350601015636\n",
      "Step - 7065, Loss - 0.6627774332839473, Learning Rate - 0.000390625, magnitude of gradient - 0.5133777950005467\n",
      "Step - 7066, Loss - 0.8543094629111163, Learning Rate - 0.000390625, magnitude of gradient - 1.2738107814886073\n",
      "Step - 7067, Loss - 0.6884327798557242, Learning Rate - 0.000390625, magnitude of gradient - 0.415706927010983\n",
      "Step - 7068, Loss - 0.7108192099457635, Learning Rate - 0.000390625, magnitude of gradient - 0.5726846317812516\n",
      "Step - 7069, Loss - 0.7086930398744679, Learning Rate - 0.000390625, magnitude of gradient - 1.4177287673470755\n",
      "Step - 7070, Loss - 0.6454139218915897, Learning Rate - 0.000390625, magnitude of gradient - 1.6040010048342628\n",
      "Step - 7071, Loss - 0.7920524113138679, Learning Rate - 0.000390625, magnitude of gradient - 0.9219436889317512\n",
      "Step - 7072, Loss - 0.5718647291822497, Learning Rate - 0.000390625, magnitude of gradient - 0.609405993483048\n",
      "Step - 7073, Loss - 0.6203242810741203, Learning Rate - 0.000390625, magnitude of gradient - 0.4620539315971835\n",
      "Step - 7074, Loss - 0.8317418336412018, Learning Rate - 0.000390625, magnitude of gradient - 1.6881943821139207\n",
      "Step - 7075, Loss - 0.7433186969511817, Learning Rate - 0.000390625, magnitude of gradient - 1.5828094243290882\n",
      "Step - 7076, Loss - 0.898945143093577, Learning Rate - 0.000390625, magnitude of gradient - 1.957760772174855\n",
      "Step - 7077, Loss - 0.7360805758250214, Learning Rate - 0.000390625, magnitude of gradient - 1.0611534912238376\n",
      "Step - 7078, Loss - 0.6539226427841512, Learning Rate - 0.000390625, magnitude of gradient - 1.8976046152929718\n",
      "Step - 7079, Loss - 0.6411657881697572, Learning Rate - 0.000390625, magnitude of gradient - 0.7028435829718563\n",
      "Step - 7080, Loss - 0.6729797801688283, Learning Rate - 0.000390625, magnitude of gradient - 1.2992899674020943\n",
      "Step - 7081, Loss - 0.6522120592522948, Learning Rate - 0.000390625, magnitude of gradient - 1.853673419717435\n",
      "Step - 7082, Loss - 0.5915529643579528, Learning Rate - 0.000390625, magnitude of gradient - 1.5590875538111872\n",
      "Step - 7083, Loss - 0.628935448857437, Learning Rate - 0.000390625, magnitude of gradient - 0.7132996596798292\n",
      "Step - 7084, Loss - 0.7691827214919749, Learning Rate - 0.000390625, magnitude of gradient - 0.4908302431386301\n",
      "Step - 7085, Loss - 0.8114963616365889, Learning Rate - 0.000390625, magnitude of gradient - 1.3213192653289272\n",
      "Step - 7086, Loss - 0.601099272708473, Learning Rate - 0.000390625, magnitude of gradient - 0.6499620716151898\n",
      "Step - 7087, Loss - 0.7398140793117626, Learning Rate - 0.000390625, magnitude of gradient - 2.2665163954568195\n",
      "Step - 7088, Loss - 0.8322482701185883, Learning Rate - 0.000390625, magnitude of gradient - 1.3771301491550918\n",
      "Step - 7089, Loss - 0.7196829281792333, Learning Rate - 0.000390625, magnitude of gradient - 0.6036568004478564\n",
      "Step - 7090, Loss - 0.7232969340946661, Learning Rate - 0.000390625, magnitude of gradient - 1.0590162211947578\n",
      "Step - 7091, Loss - 0.6520606067231833, Learning Rate - 0.000390625, magnitude of gradient - 1.3766125575278552\n",
      "Step - 7092, Loss - 0.8025639943826449, Learning Rate - 0.000390625, magnitude of gradient - 2.1352164431431744\n",
      "Step - 7093, Loss - 0.721428974187402, Learning Rate - 0.000390625, magnitude of gradient - 0.5358533554538852\n",
      "Step - 7094, Loss - 0.5319786762998777, Learning Rate - 0.000390625, magnitude of gradient - 2.240811348205995\n",
      "Step - 7095, Loss - 0.6353330514736333, Learning Rate - 0.000390625, magnitude of gradient - 0.5758331960815762\n",
      "Step - 7096, Loss - 0.537860253771262, Learning Rate - 0.000390625, magnitude of gradient - 1.6017699898787832\n",
      "Step - 7097, Loss - 0.8082047079217776, Learning Rate - 0.000390625, magnitude of gradient - 0.29561275505183215\n",
      "Step - 7098, Loss - 0.6717046425641968, Learning Rate - 0.000390625, magnitude of gradient - 0.6873647712721159\n",
      "Step - 7099, Loss - 0.5272913292525961, Learning Rate - 0.000390625, magnitude of gradient - 0.3848481988542133\n",
      "Step - 7100, Loss - 0.6940886872485068, Learning Rate - 0.000390625, magnitude of gradient - 1.4379587972886654\n",
      "Step - 7101, Loss - 0.6538282357024082, Learning Rate - 0.000390625, magnitude of gradient - 1.7090053929025544\n",
      "Step - 7102, Loss - 0.7976948304988206, Learning Rate - 0.000390625, magnitude of gradient - 1.689616423152344\n",
      "Step - 7103, Loss - 0.6870498757265736, Learning Rate - 0.000390625, magnitude of gradient - 0.9669281326308694\n",
      "Step - 7104, Loss - 0.5725839502720218, Learning Rate - 0.000390625, magnitude of gradient - 2.0233102984015074\n",
      "Step - 7105, Loss - 0.7284936730356576, Learning Rate - 0.000390625, magnitude of gradient - 1.558527110009036\n",
      "Step - 7106, Loss - 0.5713541668093759, Learning Rate - 0.000390625, magnitude of gradient - 1.2696699499892135\n",
      "Step - 7107, Loss - 0.7327133974900565, Learning Rate - 0.000390625, magnitude of gradient - 1.0743415898464537\n",
      "Step - 7108, Loss - 0.9891723214990679, Learning Rate - 0.000390625, magnitude of gradient - 2.691371973196645\n",
      "Step - 7109, Loss - 0.6003351604041758, Learning Rate - 0.000390625, magnitude of gradient - 2.138538390720502\n",
      "Step - 7110, Loss - 0.6997317691456953, Learning Rate - 0.000390625, magnitude of gradient - 0.9547597851026691\n",
      "Step - 7111, Loss - 0.6588245719410748, Learning Rate - 0.000390625, magnitude of gradient - 1.0554726006957107\n",
      "Step - 7112, Loss - 0.8656483859059809, Learning Rate - 0.000390625, magnitude of gradient - 1.5699143803814037\n",
      "Step - 7113, Loss - 0.7949023341719119, Learning Rate - 0.000390625, magnitude of gradient - 1.3809802994469846\n",
      "Step - 7114, Loss - 0.6634624628337781, Learning Rate - 0.000390625, magnitude of gradient - 1.060251354064142\n",
      "Step - 7115, Loss - 0.7390121378395771, Learning Rate - 0.000390625, magnitude of gradient - 1.4924092543540008\n",
      "Step - 7116, Loss - 0.980552546031392, Learning Rate - 0.000390625, magnitude of gradient - 0.7781795919362655\n",
      "Step - 7117, Loss - 0.919544422455373, Learning Rate - 0.000390625, magnitude of gradient - 1.2094545168042763\n",
      "Step - 7118, Loss - 0.8614348313750896, Learning Rate - 0.000390625, magnitude of gradient - 1.0021251702801623\n",
      "Step - 7119, Loss - 0.6967915214605197, Learning Rate - 0.000390625, magnitude of gradient - 2.2121618971855037\n",
      "Step - 7120, Loss - 0.8136489915134327, Learning Rate - 0.000390625, magnitude of gradient - 1.6777803371009021\n",
      "Step - 7121, Loss - 0.4916798707361056, Learning Rate - 0.000390625, magnitude of gradient - 0.4386018005361118\n",
      "Step - 7122, Loss - 0.8010576141693955, Learning Rate - 0.000390625, magnitude of gradient - 1.6481290773332764\n",
      "Step - 7123, Loss - 0.6116538320646416, Learning Rate - 0.000390625, magnitude of gradient - 1.6259823563181173\n",
      "Step - 7124, Loss - 0.6509333130807935, Learning Rate - 0.000390625, magnitude of gradient - 2.1003372205496733\n",
      "Step - 7125, Loss - 0.5125371536770774, Learning Rate - 0.000390625, magnitude of gradient - 0.8703875103537417\n",
      "Step - 7126, Loss - 0.6812288050239592, Learning Rate - 0.000390625, magnitude of gradient - 0.980978928694946\n",
      "Step - 7127, Loss - 0.7286871389795418, Learning Rate - 0.000390625, magnitude of gradient - 1.1374505180570227\n",
      "Step - 7128, Loss - 0.5079810508581353, Learning Rate - 0.000390625, magnitude of gradient - 0.7989670660177557\n",
      "Step - 7129, Loss - 0.8055944350436132, Learning Rate - 0.000390625, magnitude of gradient - 0.5956357806980486\n",
      "Step - 7130, Loss - 0.6169240967123546, Learning Rate - 0.000390625, magnitude of gradient - 1.0177354137990722\n",
      "Step - 7131, Loss - 0.7288584441825793, Learning Rate - 0.000390625, magnitude of gradient - 0.6663476196945847\n",
      "Step - 7132, Loss - 0.4879252297949258, Learning Rate - 0.000390625, magnitude of gradient - 1.6478839854006273\n",
      "Step - 7133, Loss - 0.7053911697785463, Learning Rate - 0.000390625, magnitude of gradient - 0.6750335800587841\n",
      "Step - 7134, Loss - 0.8122804691570653, Learning Rate - 0.000390625, magnitude of gradient - 1.1342156338771836\n",
      "Step - 7135, Loss - 0.8093221761836863, Learning Rate - 0.000390625, magnitude of gradient - 1.0629543530472256\n",
      "Step - 7136, Loss - 0.7800620216148353, Learning Rate - 0.000390625, magnitude of gradient - 0.9225842902145663\n",
      "Step - 7137, Loss - 0.7123758275875978, Learning Rate - 0.000390625, magnitude of gradient - 0.910532315387342\n",
      "Step - 7138, Loss - 0.8746629512594544, Learning Rate - 0.000390625, magnitude of gradient - 1.599809237834447\n",
      "Step - 7139, Loss - 0.6029712601734658, Learning Rate - 0.000390625, magnitude of gradient - 1.580714440885613\n",
      "Step - 7140, Loss - 0.6685522812198132, Learning Rate - 0.000390625, magnitude of gradient - 1.2057480323090535\n",
      "Step - 7141, Loss - 0.7404272704162292, Learning Rate - 0.000390625, magnitude of gradient - 2.015081384312403\n",
      "Step - 7142, Loss - 0.581625981508477, Learning Rate - 0.000390625, magnitude of gradient - 1.1257325307863297\n",
      "Step - 7143, Loss - 0.7137711530835358, Learning Rate - 0.000390625, magnitude of gradient - 1.0690192591471765\n",
      "Step - 7144, Loss - 0.7779856417572361, Learning Rate - 0.000390625, magnitude of gradient - 1.1689174116570547\n",
      "Step - 7145, Loss - 0.9347375219911342, Learning Rate - 0.000390625, magnitude of gradient - 1.5148647902309205\n",
      "Step - 7146, Loss - 0.7834836677992822, Learning Rate - 0.000390625, magnitude of gradient - 1.8064117878456714\n",
      "Step - 7147, Loss - 0.740224123192175, Learning Rate - 0.000390625, magnitude of gradient - 0.9503982168164936\n",
      "Step - 7148, Loss - 0.619521323053925, Learning Rate - 0.000390625, magnitude of gradient - 0.648231512255585\n",
      "Step - 7149, Loss - 0.7087751466709107, Learning Rate - 0.000390625, magnitude of gradient - 1.186905751121009\n",
      "Step - 7150, Loss - 0.8638544128399277, Learning Rate - 0.000390625, magnitude of gradient - 1.4420109298129948\n",
      "Step - 7151, Loss - 0.4399366065342942, Learning Rate - 0.000390625, magnitude of gradient - 0.8899770178242957\n",
      "Step - 7152, Loss - 0.6251710500736327, Learning Rate - 0.000390625, magnitude of gradient - 1.3167971688880646\n",
      "Step - 7153, Loss - 0.9708774090254944, Learning Rate - 0.000390625, magnitude of gradient - 0.34383076943490165\n",
      "Step - 7154, Loss - 0.693519527072574, Learning Rate - 0.000390625, magnitude of gradient - 1.4636186876408872\n",
      "Step - 7155, Loss - 0.7525834863590539, Learning Rate - 0.000390625, magnitude of gradient - 1.1604166869191774\n",
      "Step - 7156, Loss - 0.8063477055297084, Learning Rate - 0.000390625, magnitude of gradient - 0.8345764917015209\n",
      "Step - 7157, Loss - 0.7980536527998126, Learning Rate - 0.000390625, magnitude of gradient - 1.3540537577320602\n",
      "Step - 7158, Loss - 1.0167983871461808, Learning Rate - 0.000390625, magnitude of gradient - 1.5495640551780683\n",
      "Step - 7159, Loss - 0.5414776573842092, Learning Rate - 0.000390625, magnitude of gradient - 0.7311170311567483\n",
      "Step - 7160, Loss - 0.5462070620673037, Learning Rate - 0.000390625, magnitude of gradient - 1.045503170199771\n",
      "Step - 7161, Loss - 0.824222736406415, Learning Rate - 0.000390625, magnitude of gradient - 1.8482933810337794\n",
      "Step - 7162, Loss - 0.4968731802981519, Learning Rate - 0.000390625, magnitude of gradient - 1.054648179991647\n",
      "Step - 7163, Loss - 0.6437971913776128, Learning Rate - 0.000390625, magnitude of gradient - 0.7027149604603438\n",
      "Step - 7164, Loss - 0.7366870816632647, Learning Rate - 0.000390625, magnitude of gradient - 0.6739814899215831\n",
      "Step - 7165, Loss - 0.725248127728741, Learning Rate - 0.000390625, magnitude of gradient - 1.4883811071042876\n",
      "Step - 7166, Loss - 0.5523484258066793, Learning Rate - 0.000390625, magnitude of gradient - 0.9721552908434465\n",
      "Step - 7167, Loss - 0.8466071324708383, Learning Rate - 0.000390625, magnitude of gradient - 2.1746371101643316\n",
      "Step - 7168, Loss - 0.5995833987170804, Learning Rate - 0.000390625, magnitude of gradient - 0.481627627572703\n",
      "Step - 7169, Loss - 0.579752156844537, Learning Rate - 0.000390625, magnitude of gradient - 1.457332942565989\n",
      "Step - 7170, Loss - 0.6593544571725616, Learning Rate - 0.000390625, magnitude of gradient - 0.26295978279298876\n",
      "Step - 7171, Loss - 0.9714581549366684, Learning Rate - 0.000390625, magnitude of gradient - 2.5005861800470273\n",
      "Step - 7172, Loss - 0.5373952554847768, Learning Rate - 0.000390625, magnitude of gradient - 2.0849736131475436\n",
      "Step - 7173, Loss - 0.5140247821549354, Learning Rate - 0.000390625, magnitude of gradient - 0.58657331150294\n",
      "Step - 7174, Loss - 0.8890921967394568, Learning Rate - 0.000390625, magnitude of gradient - 2.0030391917924324\n",
      "Step - 7175, Loss - 0.8087214721944859, Learning Rate - 0.000390625, magnitude of gradient - 1.2302867267674582\n",
      "Step - 7176, Loss - 0.7251511212815926, Learning Rate - 0.000390625, magnitude of gradient - 1.6472712573325168\n",
      "Step - 7177, Loss - 0.5731621962716873, Learning Rate - 0.000390625, magnitude of gradient - 1.0625296851645758\n",
      "Step - 7178, Loss - 0.7451185603646948, Learning Rate - 0.000390625, magnitude of gradient - 1.1196134752051314\n",
      "Step - 7179, Loss - 0.8659254316362317, Learning Rate - 0.000390625, magnitude of gradient - 0.7424975495180891\n",
      "Step - 7180, Loss - 0.7130010157330752, Learning Rate - 0.000390625, magnitude of gradient - 0.32525928594968556\n",
      "Step - 7181, Loss - 0.9063820669792745, Learning Rate - 0.000390625, magnitude of gradient - 1.7745240147376842\n",
      "Step - 7182, Loss - 0.5672422818789689, Learning Rate - 0.000390625, magnitude of gradient - 0.4889476736249084\n",
      "Step - 7183, Loss - 0.8236732918355154, Learning Rate - 0.000390625, magnitude of gradient - 1.6145272696793516\n",
      "Step - 7184, Loss - 0.9037979717982798, Learning Rate - 0.000390625, magnitude of gradient - 1.1514129931191994\n",
      "Step - 7185, Loss - 0.6808502687044276, Learning Rate - 0.000390625, magnitude of gradient - 1.4079810836037172\n",
      "Step - 7186, Loss - 0.6297588508389378, Learning Rate - 0.000390625, magnitude of gradient - 0.7669544270796338\n",
      "Step - 7187, Loss - 0.8073998489928614, Learning Rate - 0.000390625, magnitude of gradient - 1.6762544649998135\n",
      "Step - 7188, Loss - 0.8559887119952203, Learning Rate - 0.000390625, magnitude of gradient - 1.3820272987983744\n",
      "Step - 7189, Loss - 0.6499038003262576, Learning Rate - 0.000390625, magnitude of gradient - 0.6767329246860098\n",
      "Step - 7190, Loss - 0.9055044329694142, Learning Rate - 0.000390625, magnitude of gradient - 0.7471750984146903\n",
      "Step - 7191, Loss - 0.6619541046250949, Learning Rate - 0.000390625, magnitude of gradient - 1.0855707982099845\n",
      "Step - 7192, Loss - 0.5724068414652967, Learning Rate - 0.000390625, magnitude of gradient - 0.9821971733753935\n",
      "Step - 7193, Loss - 0.697218273082166, Learning Rate - 0.000390625, magnitude of gradient - 0.906124784028183\n",
      "Step - 7194, Loss - 0.6773802952309129, Learning Rate - 0.000390625, magnitude of gradient - 1.2945535187029096\n",
      "Step - 7195, Loss - 0.9057601265999365, Learning Rate - 0.000390625, magnitude of gradient - 1.3672217249439336\n",
      "Step - 7196, Loss - 0.6273887618004045, Learning Rate - 0.000390625, magnitude of gradient - 1.159744529679012\n",
      "Step - 7197, Loss - 0.6840097449317833, Learning Rate - 0.000390625, magnitude of gradient - 1.5062677075752446\n",
      "Step - 7198, Loss - 0.48415572814272356, Learning Rate - 0.000390625, magnitude of gradient - 2.420301727239511\n",
      "Step - 7199, Loss - 0.584332996873558, Learning Rate - 0.000390625, magnitude of gradient - 0.8843905372906097\n",
      "Step - 7200, Loss - 0.8983649235695362, Learning Rate - 0.000390625, magnitude of gradient - 1.3184474008577105\n",
      "Step - 7201, Loss - 0.6616267480628786, Learning Rate - 0.000390625, magnitude of gradient - 1.2763285950608159\n",
      "Step - 7202, Loss - 0.7411843808534638, Learning Rate - 0.000390625, magnitude of gradient - 0.684999361397476\n",
      "Step - 7203, Loss - 0.6628348970008078, Learning Rate - 0.000390625, magnitude of gradient - 1.0768591231168319\n",
      "Step - 7204, Loss - 0.6600328888584316, Learning Rate - 0.000390625, magnitude of gradient - 0.9850979137654496\n",
      "Step - 7205, Loss - 0.6239037015657275, Learning Rate - 0.000390625, magnitude of gradient - 1.0264747521426234\n",
      "Step - 7206, Loss - 0.7444605068063908, Learning Rate - 0.000390625, magnitude of gradient - 1.6143676934980729\n",
      "Step - 7207, Loss - 0.4302641481262741, Learning Rate - 0.000390625, magnitude of gradient - 1.5411923067435436\n",
      "Step - 7208, Loss - 0.6574187189000421, Learning Rate - 0.000390625, magnitude of gradient - 1.253336119865361\n",
      "Step - 7209, Loss - 0.5407418397333577, Learning Rate - 0.000390625, magnitude of gradient - 0.39800895950375187\n",
      "Step - 7210, Loss - 0.6220590916836465, Learning Rate - 0.000390625, magnitude of gradient - 1.6625755742991397\n",
      "Step - 7211, Loss - 0.6698874227493066, Learning Rate - 0.000390625, magnitude of gradient - 1.078968589716185\n",
      "Step - 7212, Loss - 0.8770123575858889, Learning Rate - 0.000390625, magnitude of gradient - 1.8778282314834733\n",
      "Step - 7213, Loss - 0.6446150665299826, Learning Rate - 0.000390625, magnitude of gradient - 1.2271677188622374\n",
      "Step - 7214, Loss - 0.6092549882697995, Learning Rate - 0.000390625, magnitude of gradient - 1.4632687883062152\n",
      "Step - 7215, Loss - 0.5934307931316981, Learning Rate - 0.000390625, magnitude of gradient - 1.2925716497199635\n",
      "Step - 7216, Loss - 0.6475148802501877, Learning Rate - 0.000390625, magnitude of gradient - 1.3471266124043237\n",
      "Step - 7217, Loss - 0.5641484824907563, Learning Rate - 0.000390625, magnitude of gradient - 0.4825930996910598\n",
      "Step - 7218, Loss - 0.6720501089241566, Learning Rate - 0.000390625, magnitude of gradient - 0.6558431945926323\n",
      "Step - 7219, Loss - 0.5922905394568689, Learning Rate - 0.000390625, magnitude of gradient - 0.8045154053475871\n",
      "Step - 7220, Loss - 0.500930098745433, Learning Rate - 0.000390625, magnitude of gradient - 2.285240363660865\n",
      "Step - 7221, Loss - 0.5801080623989032, Learning Rate - 0.000390625, magnitude of gradient - 1.425123893978161\n",
      "Step - 7222, Loss - 0.5165886248857373, Learning Rate - 0.000390625, magnitude of gradient - 1.1889673042731352\n",
      "Step - 7223, Loss - 0.6953104767868447, Learning Rate - 0.000390625, magnitude of gradient - 1.9262670784152998\n",
      "Step - 7224, Loss - 0.7056680916712874, Learning Rate - 0.000390625, magnitude of gradient - 1.3334062998179528\n",
      "Step - 7225, Loss - 0.9531374015776708, Learning Rate - 0.000390625, magnitude of gradient - 1.8510887338222017\n",
      "Step - 7226, Loss - 0.6623992853505611, Learning Rate - 0.000390625, magnitude of gradient - 1.1993852991382183\n",
      "Step - 7227, Loss - 0.675933843321308, Learning Rate - 0.000390625, magnitude of gradient - 0.8019122494300419\n",
      "Step - 7228, Loss - 0.5966624550681375, Learning Rate - 0.000390625, magnitude of gradient - 2.897807858287827\n",
      "Step - 7229, Loss - 0.7292618228484746, Learning Rate - 0.000390625, magnitude of gradient - 1.3043804399155532\n",
      "Step - 7230, Loss - 0.6004202114876307, Learning Rate - 0.000390625, magnitude of gradient - 2.024551807436861\n",
      "Step - 7231, Loss - 0.7105140911240062, Learning Rate - 0.000390625, magnitude of gradient - 1.8074293296998625\n",
      "Step - 7232, Loss - 0.7763741405291643, Learning Rate - 0.000390625, magnitude of gradient - 0.7541813169695997\n",
      "Step - 7233, Loss - 0.7485871700639191, Learning Rate - 0.000390625, magnitude of gradient - 1.2236824098032897\n",
      "Step - 7234, Loss - 0.7752228826047354, Learning Rate - 0.000390625, magnitude of gradient - 1.1350699522822465\n",
      "Step - 7235, Loss - 0.6005856504387165, Learning Rate - 0.000390625, magnitude of gradient - 1.0239068789375285\n",
      "Step - 7236, Loss - 0.8016805215112525, Learning Rate - 0.000390625, magnitude of gradient - 1.224287055736945\n",
      "Step - 7237, Loss - 0.8052113342479814, Learning Rate - 0.000390625, magnitude of gradient - 1.443156534448492\n",
      "Step - 7238, Loss - 0.7227950957877546, Learning Rate - 0.000390625, magnitude of gradient - 1.1635340759635608\n",
      "Step - 7239, Loss - 0.7622203962248633, Learning Rate - 0.000390625, magnitude of gradient - 0.3978257937449217\n",
      "Step - 7240, Loss - 0.9213063588559387, Learning Rate - 0.000390625, magnitude of gradient - 1.58694539834709\n",
      "Step - 7241, Loss - 0.6512780839269556, Learning Rate - 0.000390625, magnitude of gradient - 0.8726292840797023\n",
      "Step - 7242, Loss - 0.49733821828866154, Learning Rate - 0.000390625, magnitude of gradient - 1.2184553960623945\n",
      "Step - 7243, Loss - 0.5564355058659056, Learning Rate - 0.000390625, magnitude of gradient - 1.299664480276899\n",
      "Step - 7244, Loss - 0.6858901912402414, Learning Rate - 0.000390625, magnitude of gradient - 0.29426015098887887\n",
      "Step - 7245, Loss - 0.6633384289227287, Learning Rate - 0.000390625, magnitude of gradient - 0.8910429825838737\n",
      "Step - 7246, Loss - 0.7770049819720626, Learning Rate - 0.000390625, magnitude of gradient - 1.6132444955680698\n",
      "Step - 7247, Loss - 0.6943276263220202, Learning Rate - 0.000390625, magnitude of gradient - 1.8730562527005883\n",
      "Step - 7248, Loss - 0.7884592665210545, Learning Rate - 0.000390625, magnitude of gradient - 2.8553285821497894\n",
      "Step - 7249, Loss - 0.7959110857599956, Learning Rate - 0.000390625, magnitude of gradient - 1.430377411116835\n",
      "Step - 7250, Loss - 0.48779750056981325, Learning Rate - 0.000390625, magnitude of gradient - 1.150692045404565\n",
      "Step - 7251, Loss - 0.6600369596288561, Learning Rate - 0.000390625, magnitude of gradient - 0.544655354268771\n",
      "Step - 7252, Loss - 0.7117156117622127, Learning Rate - 0.000390625, magnitude of gradient - 0.5925903964514212\n",
      "Step - 7253, Loss - 0.5744472872668442, Learning Rate - 0.000390625, magnitude of gradient - 1.9771541769742575\n",
      "Step - 7254, Loss - 0.7630848294148926, Learning Rate - 0.000390625, magnitude of gradient - 1.5998589306801787\n",
      "Step - 7255, Loss - 0.6904597559351962, Learning Rate - 0.000390625, magnitude of gradient - 1.0833124678449844\n",
      "Step - 7256, Loss - 0.7473662277296935, Learning Rate - 0.000390625, magnitude of gradient - 3.0592137778615296\n",
      "Step - 7257, Loss - 0.6943131495795584, Learning Rate - 0.000390625, magnitude of gradient - 2.030338319084937\n",
      "Step - 7258, Loss - 0.774169116982313, Learning Rate - 0.000390625, magnitude of gradient - 0.48919969585500306\n",
      "Step - 7259, Loss - 0.6532971749735622, Learning Rate - 0.000390625, magnitude of gradient - 1.20387098587619\n",
      "Step - 7260, Loss - 0.6475461911115195, Learning Rate - 0.000390625, magnitude of gradient - 0.5739216788141964\n",
      "Step - 7261, Loss - 0.6703494923003389, Learning Rate - 0.000390625, magnitude of gradient - 0.9802370801309147\n",
      "Step - 7262, Loss - 0.7381557035579847, Learning Rate - 0.000390625, magnitude of gradient - 2.0039248069402946\n",
      "Step - 7263, Loss - 0.7666157486349241, Learning Rate - 0.000390625, magnitude of gradient - 1.7998331399882885\n",
      "Step - 7264, Loss - 0.7982525493151251, Learning Rate - 0.000390625, magnitude of gradient - 1.2033010407978784\n",
      "Step - 7265, Loss - 0.6511822346863607, Learning Rate - 0.000390625, magnitude of gradient - 1.6312506049633237\n",
      "Step - 7266, Loss - 0.8645488124803421, Learning Rate - 0.000390625, magnitude of gradient - 0.8346406389108063\n",
      "Step - 7267, Loss - 0.6516063870364974, Learning Rate - 0.000390625, magnitude of gradient - 0.6519944389348817\n",
      "Step - 7268, Loss - 0.8786079634813956, Learning Rate - 0.000390625, magnitude of gradient - 1.1293971233642777\n",
      "Step - 7269, Loss - 0.6582142197496349, Learning Rate - 0.000390625, magnitude of gradient - 1.2630333174791948\n",
      "Step - 7270, Loss - 0.6927750426437349, Learning Rate - 0.000390625, magnitude of gradient - 1.1876220742353902\n",
      "Step - 7271, Loss - 0.5680238701937367, Learning Rate - 0.000390625, magnitude of gradient - 1.0859014939005978\n",
      "Step - 7272, Loss - 0.8272843487809745, Learning Rate - 0.000390625, magnitude of gradient - 1.255019730287061\n",
      "Step - 7273, Loss - 0.7073112173057465, Learning Rate - 0.000390625, magnitude of gradient - 0.9866068144512636\n",
      "Step - 7274, Loss - 0.7261699088919972, Learning Rate - 0.000390625, magnitude of gradient - 1.4189338908873714\n",
      "Step - 7275, Loss - 0.5809311053890046, Learning Rate - 0.000390625, magnitude of gradient - 1.0893907824765443\n",
      "Step - 7276, Loss - 0.7426755102632523, Learning Rate - 0.000390625, magnitude of gradient - 1.7205661415506024\n",
      "Step - 7277, Loss - 0.7925827315608449, Learning Rate - 0.000390625, magnitude of gradient - 1.6202138195048166\n",
      "Step - 7278, Loss - 0.7755858172562529, Learning Rate - 0.000390625, magnitude of gradient - 1.3034360766038908\n",
      "Step - 7279, Loss - 0.7443305153444272, Learning Rate - 0.000390625, magnitude of gradient - 0.9582335531765849\n",
      "Step - 7280, Loss - 0.7578511981812257, Learning Rate - 0.000390625, magnitude of gradient - 1.2493927920754204\n",
      "Step - 7281, Loss - 0.7557524572646827, Learning Rate - 0.000390625, magnitude of gradient - 0.5449479470339224\n",
      "Step - 7282, Loss - 0.7501804172128264, Learning Rate - 0.000390625, magnitude of gradient - 1.7352765946580473\n",
      "Step - 7283, Loss - 0.7618614803289689, Learning Rate - 0.000390625, magnitude of gradient - 1.3564651796178573\n",
      "Step - 7284, Loss - 0.7729700223002994, Learning Rate - 0.000390625, magnitude of gradient - 0.46630584873996295\n",
      "Step - 7285, Loss - 0.7118177549598123, Learning Rate - 0.000390625, magnitude of gradient - 0.675437758657008\n",
      "Step - 7286, Loss - 0.778910129611381, Learning Rate - 0.000390625, magnitude of gradient - 0.5893121359156633\n",
      "Step - 7287, Loss - 0.5747888104685941, Learning Rate - 0.000390625, magnitude of gradient - 1.4323333779166068\n",
      "Step - 7288, Loss - 0.774554985332611, Learning Rate - 0.000390625, magnitude of gradient - 0.9150067896838685\n",
      "Step - 7289, Loss - 0.6102029195725891, Learning Rate - 0.000390625, magnitude of gradient - 3.226888597287882\n",
      "Step - 7290, Loss - 0.7788546929700971, Learning Rate - 0.000390625, magnitude of gradient - 0.877197328002406\n",
      "Step - 7291, Loss - 0.8110419752474044, Learning Rate - 0.000390625, magnitude of gradient - 1.0279046568994603\n",
      "Step - 7292, Loss - 0.695057506141434, Learning Rate - 0.000390625, magnitude of gradient - 2.1357089925034636\n",
      "Step - 7293, Loss - 0.7275498380276292, Learning Rate - 0.000390625, magnitude of gradient - 0.9893229870027013\n",
      "Step - 7294, Loss - 0.649056460606856, Learning Rate - 0.000390625, magnitude of gradient - 0.9771843045401356\n",
      "Step - 7295, Loss - 0.8528150569946855, Learning Rate - 0.000390625, magnitude of gradient - 1.1225067193356453\n",
      "Step - 7296, Loss - 0.7301142834571652, Learning Rate - 0.000390625, magnitude of gradient - 0.6247491477323828\n",
      "Step - 7297, Loss - 0.5822432976551362, Learning Rate - 0.000390625, magnitude of gradient - 1.158766842088992\n",
      "Step - 7298, Loss - 0.6886535032457813, Learning Rate - 0.000390625, magnitude of gradient - 1.3480203575580252\n",
      "Step - 7299, Loss - 0.6466862214372522, Learning Rate - 0.000390625, magnitude of gradient - 0.4156407055958612\n",
      "Step - 7300, Loss - 0.750103776393142, Learning Rate - 0.000390625, magnitude of gradient - 1.2694617318694772\n",
      "Step - 7301, Loss - 0.7120213906131684, Learning Rate - 0.000390625, magnitude of gradient - 1.556174610222897\n",
      "Step - 7302, Loss - 0.6976693575090459, Learning Rate - 0.000390625, magnitude of gradient - 0.9064024612980287\n",
      "Step - 7303, Loss - 0.6608540239836848, Learning Rate - 0.000390625, magnitude of gradient - 0.8365890855696713\n",
      "Step - 7304, Loss - 0.8486734006563775, Learning Rate - 0.000390625, magnitude of gradient - 1.0072239375913024\n",
      "Step - 7305, Loss - 0.6154997872564935, Learning Rate - 0.000390625, magnitude of gradient - 1.0916770432788356\n",
      "Step - 7306, Loss - 0.5336129756094848, Learning Rate - 0.000390625, magnitude of gradient - 2.221764615665917\n",
      "Step - 7307, Loss - 0.7415236172562637, Learning Rate - 0.000390625, magnitude of gradient - 1.6148730414964783\n",
      "Step - 7308, Loss - 0.6372130001988839, Learning Rate - 0.000390625, magnitude of gradient - 1.3306782491165579\n",
      "Step - 7309, Loss - 0.6878539524449712, Learning Rate - 0.000390625, magnitude of gradient - 1.5631851681237408\n",
      "Step - 7310, Loss - 0.8387263954605249, Learning Rate - 0.000390625, magnitude of gradient - 0.9433473445690755\n",
      "Step - 7311, Loss - 0.5324216746161015, Learning Rate - 0.000390625, magnitude of gradient - 0.4684061027926628\n",
      "Step - 7312, Loss - 0.6242425751209142, Learning Rate - 0.000390625, magnitude of gradient - 1.3824732160690183\n",
      "Step - 7313, Loss - 0.8006208681800708, Learning Rate - 0.000390625, magnitude of gradient - 1.2122242361186708\n",
      "Step - 7314, Loss - 0.7162526362157746, Learning Rate - 0.000390625, magnitude of gradient - 1.0492519964593128\n",
      "Step - 7315, Loss - 0.6022402978673115, Learning Rate - 0.000390625, magnitude of gradient - 1.0761614185138997\n",
      "Step - 7316, Loss - 0.6118043024114321, Learning Rate - 0.000390625, magnitude of gradient - 1.1602276952683825\n",
      "Step - 7317, Loss - 0.5668013639289001, Learning Rate - 0.000390625, magnitude of gradient - 0.8883245455373191\n",
      "Step - 7318, Loss - 0.6761871264748196, Learning Rate - 0.000390625, magnitude of gradient - 1.6799468717160513\n",
      "Step - 7319, Loss - 0.6590630942705115, Learning Rate - 0.000390625, magnitude of gradient - 0.7271052234905886\n",
      "Step - 7320, Loss - 0.6548230001163018, Learning Rate - 0.000390625, magnitude of gradient - 1.600220420639564\n",
      "Step - 7321, Loss - 0.8350859912439695, Learning Rate - 0.000390625, magnitude of gradient - 1.9745318766286308\n",
      "Step - 7322, Loss - 0.6858040191632934, Learning Rate - 0.000390625, magnitude of gradient - 1.2311946820065856\n",
      "Step - 7323, Loss - 0.7114814711140508, Learning Rate - 0.000390625, magnitude of gradient - 0.9760066871193199\n",
      "Step - 7324, Loss - 0.5941007757338588, Learning Rate - 0.000390625, magnitude of gradient - 1.4627808301203\n",
      "Step - 7325, Loss - 0.9270705744661979, Learning Rate - 0.000390625, magnitude of gradient - 1.7727727210879605\n",
      "Step - 7326, Loss - 0.8208884959108204, Learning Rate - 0.000390625, magnitude of gradient - 1.576312699637027\n",
      "Step - 7327, Loss - 0.953827792545484, Learning Rate - 0.000390625, magnitude of gradient - 2.033665801644346\n",
      "Step - 7328, Loss - 0.7226640189723453, Learning Rate - 0.000390625, magnitude of gradient - 1.667466386688286\n",
      "Step - 7329, Loss - 0.7249402113844889, Learning Rate - 0.000390625, magnitude of gradient - 0.8030922100786624\n",
      "Step - 7330, Loss - 0.7869363174023757, Learning Rate - 0.000390625, magnitude of gradient - 1.4190802470662156\n",
      "Step - 7331, Loss - 0.8053145843172034, Learning Rate - 0.000390625, magnitude of gradient - 1.9114231350213757\n",
      "Step - 7332, Loss - 0.6669811275567861, Learning Rate - 0.000390625, magnitude of gradient - 0.9686536645937578\n",
      "Step - 7333, Loss - 0.838744908302322, Learning Rate - 0.000390625, magnitude of gradient - 1.1925190691615075\n",
      "Step - 7334, Loss - 0.6919269860658315, Learning Rate - 0.000390625, magnitude of gradient - 2.1840663650417613\n",
      "Step - 7335, Loss - 0.6298012555962278, Learning Rate - 0.000390625, magnitude of gradient - 0.8549528176537027\n",
      "Step - 7336, Loss - 0.8608417744498118, Learning Rate - 0.000390625, magnitude of gradient - 1.3612348065385835\n",
      "Step - 7337, Loss - 0.6244219077107943, Learning Rate - 0.000390625, magnitude of gradient - 1.155102078538367\n",
      "Step - 7338, Loss - 0.6186462431135216, Learning Rate - 0.000390625, magnitude of gradient - 1.1880369630353143\n",
      "Step - 7339, Loss - 0.5257176856033041, Learning Rate - 0.000390625, magnitude of gradient - 0.9048871723201024\n",
      "Step - 7340, Loss - 0.6733401104845566, Learning Rate - 0.000390625, magnitude of gradient - 1.339096831518346\n",
      "Step - 7341, Loss - 0.6389804922102319, Learning Rate - 0.000390625, magnitude of gradient - 1.4792122465276312\n",
      "Step - 7342, Loss - 0.7031458430452726, Learning Rate - 0.000390625, magnitude of gradient - 0.7118182613153782\n",
      "Step - 7343, Loss - 0.6952776216132532, Learning Rate - 0.000390625, magnitude of gradient - 1.650849706837306\n",
      "Step - 7344, Loss - 0.5843870315031603, Learning Rate - 0.000390625, magnitude of gradient - 1.5011453359288562\n",
      "Step - 7345, Loss - 0.8912159899185612, Learning Rate - 0.000390625, magnitude of gradient - 1.7495289335880593\n",
      "Step - 7346, Loss - 0.6683711618595387, Learning Rate - 0.000390625, magnitude of gradient - 0.5997814092129315\n",
      "Step - 7347, Loss - 0.8770509053542597, Learning Rate - 0.000390625, magnitude of gradient - 0.8638200750995846\n",
      "Step - 7348, Loss - 0.7421139830276157, Learning Rate - 0.000390625, magnitude of gradient - 0.6347878510606313\n",
      "Step - 7349, Loss - 0.5990415488758496, Learning Rate - 0.000390625, magnitude of gradient - 1.302253093880151\n",
      "Step - 7350, Loss - 0.7680995141318352, Learning Rate - 0.000390625, magnitude of gradient - 1.246743737053803\n",
      "Step - 7351, Loss - 0.5073630700352141, Learning Rate - 0.000390625, magnitude of gradient - 0.94578912864248\n",
      "Step - 7352, Loss - 0.630962351793393, Learning Rate - 0.000390625, magnitude of gradient - 0.8994527924008743\n",
      "Step - 7353, Loss - 0.6444842587589767, Learning Rate - 0.000390625, magnitude of gradient - 1.5593904329777803\n",
      "Step - 7354, Loss - 1.102453792536129, Learning Rate - 0.000390625, magnitude of gradient - 3.324440747892202\n",
      "Step - 7355, Loss - 0.6553949477558749, Learning Rate - 0.000390625, magnitude of gradient - 1.2769965738842157\n",
      "Step - 7356, Loss - 0.6236997265542309, Learning Rate - 0.000390625, magnitude of gradient - 0.6461926716454025\n",
      "Step - 7357, Loss - 0.6174680814437358, Learning Rate - 0.000390625, magnitude of gradient - 0.8939607395041798\n",
      "Step - 7358, Loss - 0.5401680973635612, Learning Rate - 0.000390625, magnitude of gradient - 0.6537206939660043\n",
      "Step - 7359, Loss - 0.7076771065369153, Learning Rate - 0.000390625, magnitude of gradient - 2.858737297019711\n",
      "Step - 7360, Loss - 0.42411137614218375, Learning Rate - 0.000390625, magnitude of gradient - 0.9119874310229347\n",
      "Step - 7361, Loss - 0.6766897712855753, Learning Rate - 0.000390625, magnitude of gradient - 0.6102537290222205\n",
      "Step - 7362, Loss - 0.6978642932631621, Learning Rate - 0.000390625, magnitude of gradient - 0.5946542289463764\n",
      "Step - 7363, Loss - 0.7523469167398736, Learning Rate - 0.000390625, magnitude of gradient - 0.5835208313152719\n",
      "Step - 7364, Loss - 0.568427147826561, Learning Rate - 0.000390625, magnitude of gradient - 1.7073041197335928\n",
      "Step - 7365, Loss - 0.774785181565614, Learning Rate - 0.000390625, magnitude of gradient - 1.1648290132963488\n",
      "Step - 7366, Loss - 0.8228923560192714, Learning Rate - 0.000390625, magnitude of gradient - 1.0779969654858674\n",
      "Step - 7367, Loss - 0.6786704242216598, Learning Rate - 0.000390625, magnitude of gradient - 0.7766456688149599\n",
      "Step - 7368, Loss - 0.738967835361875, Learning Rate - 0.000390625, magnitude of gradient - 1.9793672389929262\n",
      "Step - 7369, Loss - 0.8043631303767214, Learning Rate - 0.000390625, magnitude of gradient - 1.6623519589723934\n",
      "Step - 7370, Loss - 0.6358462403091266, Learning Rate - 0.000390625, magnitude of gradient - 1.4785032453946727\n",
      "Step - 7371, Loss - 0.7990257284989116, Learning Rate - 0.000390625, magnitude of gradient - 1.0961099123725249\n",
      "Step - 7372, Loss - 0.8260470180630071, Learning Rate - 0.000390625, magnitude of gradient - 1.275099676633314\n",
      "Step - 7373, Loss - 0.5111650024734138, Learning Rate - 0.000390625, magnitude of gradient - 2.985673874615083\n",
      "Step - 7374, Loss - 0.8863184844441976, Learning Rate - 0.000390625, magnitude of gradient - 0.5914901200636556\n",
      "Step - 7375, Loss - 0.6508190986607573, Learning Rate - 0.000390625, magnitude of gradient - 0.7799650118943385\n",
      "Step - 7376, Loss - 0.7153558091837376, Learning Rate - 0.000390625, magnitude of gradient - 1.2448286570071168\n",
      "Step - 7377, Loss - 0.573948144331172, Learning Rate - 0.000390625, magnitude of gradient - 1.6077210911950497\n",
      "Step - 7378, Loss - 0.6374680858355941, Learning Rate - 0.000390625, magnitude of gradient - 1.76910817493401\n",
      "Step - 7379, Loss - 0.8706005716697908, Learning Rate - 0.000390625, magnitude of gradient - 1.9196355176571804\n",
      "Step - 7380, Loss - 0.6839381255502713, Learning Rate - 0.000390625, magnitude of gradient - 0.8732799461473804\n",
      "Step - 7381, Loss - 0.6956838282581487, Learning Rate - 0.000390625, magnitude of gradient - 1.2892983955511874\n",
      "Step - 7382, Loss - 0.6592571270147829, Learning Rate - 0.000390625, magnitude of gradient - 1.1958386134854726\n",
      "Step - 7383, Loss - 0.6756410767289079, Learning Rate - 0.000390625, magnitude of gradient - 0.8006996609877204\n",
      "Step - 7384, Loss - 0.6862093152844018, Learning Rate - 0.000390625, magnitude of gradient - 2.031968941787689\n",
      "Step - 7385, Loss - 0.6659449473510124, Learning Rate - 0.000390625, magnitude of gradient - 0.9150896836853821\n",
      "Step - 7386, Loss - 0.7278471408459659, Learning Rate - 0.000390625, magnitude of gradient - 0.659125330807603\n",
      "Step - 7387, Loss - 0.6822963871412089, Learning Rate - 0.000390625, magnitude of gradient - 1.4479501539440542\n",
      "Step - 7388, Loss - 0.6082849938229562, Learning Rate - 0.000390625, magnitude of gradient - 1.6818853891428702\n",
      "Step - 7389, Loss - 0.8838670575860621, Learning Rate - 0.000390625, magnitude of gradient - 2.1269132706718286\n",
      "Step - 7390, Loss - 0.6986174403734415, Learning Rate - 0.000390625, magnitude of gradient - 0.5685068968833871\n",
      "Step - 7391, Loss - 0.5081094599893419, Learning Rate - 0.000390625, magnitude of gradient - 0.49127791311660934\n",
      "Step - 7392, Loss - 0.6256907484228068, Learning Rate - 0.000390625, magnitude of gradient - 1.1306638343971147\n",
      "Step - 7393, Loss - 0.6460271091442683, Learning Rate - 0.000390625, magnitude of gradient - 1.134774600055997\n",
      "Step - 7394, Loss - 0.618268991751888, Learning Rate - 0.000390625, magnitude of gradient - 1.704651018465135\n",
      "Step - 7395, Loss - 0.6973079122894936, Learning Rate - 0.000390625, magnitude of gradient - 1.6720234295096674\n",
      "Step - 7396, Loss - 0.7772022892707914, Learning Rate - 0.000390625, magnitude of gradient - 1.4612318967116864\n",
      "Step - 7397, Loss - 0.8113814987409823, Learning Rate - 0.000390625, magnitude of gradient - 1.3471321592907648\n",
      "Step - 7398, Loss - 0.6690540544265219, Learning Rate - 0.000390625, magnitude of gradient - 1.0843345790965715\n",
      "Step - 7399, Loss - 0.5428994047633724, Learning Rate - 0.000390625, magnitude of gradient - 1.1552922649948196\n",
      "Step - 7400, Loss - 0.6451758031268813, Learning Rate - 0.000390625, magnitude of gradient - 1.0389060252723772\n",
      "Step - 7401, Loss - 0.7171388612497066, Learning Rate - 0.000390625, magnitude of gradient - 0.6649597849792805\n",
      "Step - 7402, Loss - 0.5453240373657473, Learning Rate - 0.000390625, magnitude of gradient - 1.2866543585192733\n",
      "Step - 7403, Loss - 0.6827952749278875, Learning Rate - 0.000390625, magnitude of gradient - 2.7412506332195767\n",
      "Step - 7404, Loss - 0.6976329983993473, Learning Rate - 0.000390625, magnitude of gradient - 1.4076978721802536\n",
      "Step - 7405, Loss - 0.5959740212537765, Learning Rate - 0.000390625, magnitude of gradient - 1.9315288448636745\n",
      "Step - 7406, Loss - 0.7837581369717821, Learning Rate - 0.000390625, magnitude of gradient - 1.213751884727444\n",
      "Step - 7407, Loss - 0.6966341881680108, Learning Rate - 0.000390625, magnitude of gradient - 2.2911199412917798\n",
      "Step - 7408, Loss - 0.683988121443132, Learning Rate - 0.000390625, magnitude of gradient - 0.7909205843315442\n",
      "Step - 7409, Loss - 0.7208716158199018, Learning Rate - 0.000390625, magnitude of gradient - 0.8113396846469285\n",
      "Step - 7410, Loss - 0.7102022369860255, Learning Rate - 0.000390625, magnitude of gradient - 0.4314302087075064\n",
      "Step - 7411, Loss - 0.6885601373129878, Learning Rate - 0.000390625, magnitude of gradient - 1.4820763728851816\n",
      "Step - 7412, Loss - 0.5423663267076011, Learning Rate - 0.000390625, magnitude of gradient - 0.3809067475933436\n",
      "Step - 7413, Loss - 0.5218763451973252, Learning Rate - 0.000390625, magnitude of gradient - 1.0770535558904428\n",
      "Step - 7414, Loss - 0.7227448086060891, Learning Rate - 0.000390625, magnitude of gradient - 2.7903175301460554\n",
      "Step - 7415, Loss - 0.6824263117753503, Learning Rate - 0.000390625, magnitude of gradient - 1.6428625493334736\n",
      "Step - 7416, Loss - 0.8995520040531118, Learning Rate - 0.000390625, magnitude of gradient - 1.6546085636472563\n",
      "Step - 7417, Loss - 0.6544184551321177, Learning Rate - 0.000390625, magnitude of gradient - 1.493558158016796\n",
      "Step - 7418, Loss - 0.7158485852629122, Learning Rate - 0.000390625, magnitude of gradient - 0.8514731271734516\n",
      "Step - 7419, Loss - 0.6800903464277355, Learning Rate - 0.000390625, magnitude of gradient - 1.1234575729242091\n",
      "Step - 7420, Loss - 0.6905178864368112, Learning Rate - 0.000390625, magnitude of gradient - 0.8944054224588607\n",
      "Step - 7421, Loss - 0.5427452845995651, Learning Rate - 0.000390625, magnitude of gradient - 1.3029129141138347\n",
      "Step - 7422, Loss - 0.7091560721134768, Learning Rate - 0.000390625, magnitude of gradient - 0.8939303775325866\n",
      "Step - 7423, Loss - 0.6713185082303188, Learning Rate - 0.000390625, magnitude of gradient - 1.3552491662091317\n",
      "Step - 7424, Loss - 0.6039768565300423, Learning Rate - 0.000390625, magnitude of gradient - 1.1308370066032467\n",
      "Step - 7425, Loss - 0.7974513851584119, Learning Rate - 0.000390625, magnitude of gradient - 0.7513567043558422\n",
      "Step - 7426, Loss - 0.5919053079644886, Learning Rate - 0.000390625, magnitude of gradient - 0.21090574824784236\n",
      "Step - 7427, Loss - 0.6773715318371658, Learning Rate - 0.000390625, magnitude of gradient - 2.121144982735048\n",
      "Step - 7428, Loss - 0.4895204240819609, Learning Rate - 0.000390625, magnitude of gradient - 2.324102023512271\n",
      "Step - 7429, Loss - 0.9669063738251107, Learning Rate - 0.000390625, magnitude of gradient - 2.5582328298899992\n",
      "Step - 7430, Loss - 0.6050639621234537, Learning Rate - 0.000390625, magnitude of gradient - 1.1329935367571746\n",
      "Step - 7431, Loss - 0.684678234083027, Learning Rate - 0.000390625, magnitude of gradient - 0.3946524165441096\n",
      "Step - 7432, Loss - 0.8230106455451781, Learning Rate - 0.000390625, magnitude of gradient - 1.4380550483184362\n",
      "Step - 7433, Loss - 0.8101984281072043, Learning Rate - 0.000390625, magnitude of gradient - 1.0825703025799571\n",
      "Step - 7434, Loss - 0.7945206752389817, Learning Rate - 0.000390625, magnitude of gradient - 1.0978006980214545\n",
      "Step - 7435, Loss - 0.7530125470919111, Learning Rate - 0.000390625, magnitude of gradient - 1.7962799080167955\n",
      "Step - 7436, Loss - 0.7789363722649919, Learning Rate - 0.000390625, magnitude of gradient - 1.4001580357805588\n",
      "Step - 7437, Loss - 0.7008854821093109, Learning Rate - 0.000390625, magnitude of gradient - 0.6884252430402291\n",
      "Step - 7438, Loss - 0.7758112847435404, Learning Rate - 0.000390625, magnitude of gradient - 0.851349727254131\n",
      "Step - 7439, Loss - 0.7246733225928861, Learning Rate - 0.000390625, magnitude of gradient - 1.115081886619084\n",
      "Step - 7440, Loss - 0.7117637485892491, Learning Rate - 0.000390625, magnitude of gradient - 1.1343040492545944\n",
      "Step - 7441, Loss - 0.7389657307348563, Learning Rate - 0.000390625, magnitude of gradient - 0.9174978468593968\n",
      "Step - 7442, Loss - 0.6743704081751931, Learning Rate - 0.000390625, magnitude of gradient - 0.772421297830772\n",
      "Step - 7443, Loss - 0.7332226020543486, Learning Rate - 0.000390625, magnitude of gradient - 0.8801319516683602\n",
      "Step - 7444, Loss - 0.635114853661752, Learning Rate - 0.000390625, magnitude of gradient - 1.7013474072318608\n",
      "Step - 7445, Loss - 0.8386612412304076, Learning Rate - 0.000390625, magnitude of gradient - 0.8141711531174415\n",
      "Step - 7446, Loss - 0.8132518290778129, Learning Rate - 0.000390625, magnitude of gradient - 2.1981338799864925\n",
      "Step - 7447, Loss - 0.6130770727348706, Learning Rate - 0.000390625, magnitude of gradient - 1.7589701725671714\n",
      "Step - 7448, Loss - 0.705960362927732, Learning Rate - 0.000390625, magnitude of gradient - 2.0333283901621964\n",
      "Step - 7449, Loss - 0.8450325511941275, Learning Rate - 0.000390625, magnitude of gradient - 2.482636339751219\n",
      "Step - 7450, Loss - 0.7249284873431379, Learning Rate - 0.000390625, magnitude of gradient - 1.2854102463708097\n",
      "Step - 7451, Loss - 0.8277599164285832, Learning Rate - 0.000390625, magnitude of gradient - 1.2496649168754523\n",
      "Step - 7452, Loss - 0.5192190451604437, Learning Rate - 0.000390625, magnitude of gradient - 1.7965109256741116\n",
      "Step - 7453, Loss - 1.0221000765887858, Learning Rate - 0.000390625, magnitude of gradient - 1.481667927564615\n",
      "Step - 7454, Loss - 0.5845217696865279, Learning Rate - 0.000390625, magnitude of gradient - 0.593969985144905\n",
      "Step - 7455, Loss - 0.9923062997967003, Learning Rate - 0.000390625, magnitude of gradient - 1.2184694114898484\n",
      "Step - 7456, Loss - 0.6957261746752206, Learning Rate - 0.000390625, magnitude of gradient - 1.0565734513553717\n",
      "Step - 7457, Loss - 0.6166053814735122, Learning Rate - 0.000390625, magnitude of gradient - 1.3353672807451842\n",
      "Step - 7458, Loss - 0.7039969314396892, Learning Rate - 0.000390625, magnitude of gradient - 1.3018760588888805\n",
      "Step - 7459, Loss - 0.7251141226270452, Learning Rate - 0.000390625, magnitude of gradient - 1.3173489986529576\n",
      "Step - 7460, Loss - 0.5517904664733961, Learning Rate - 0.000390625, magnitude of gradient - 0.7070978729194436\n",
      "Step - 7461, Loss - 0.6706409509122779, Learning Rate - 0.000390625, magnitude of gradient - 1.6091156586088342\n",
      "Step - 7462, Loss - 0.7195422801374833, Learning Rate - 0.000390625, magnitude of gradient - 1.0126415748568294\n",
      "Step - 7463, Loss - 0.6341821111736582, Learning Rate - 0.000390625, magnitude of gradient - 0.8274878506917452\n",
      "Step - 7464, Loss - 0.6713929525125955, Learning Rate - 0.000390625, magnitude of gradient - 0.49156350539609184\n",
      "Step - 7465, Loss - 0.6052137211099848, Learning Rate - 0.000390625, magnitude of gradient - 1.5640831559194373\n",
      "Step - 7466, Loss - 0.9025123066136304, Learning Rate - 0.000390625, magnitude of gradient - 1.2743680248282514\n",
      "Step - 7467, Loss - 0.7225579783331538, Learning Rate - 0.000390625, magnitude of gradient - 2.1215731104863003\n",
      "Step - 7468, Loss - 0.6471360994545945, Learning Rate - 0.000390625, magnitude of gradient - 3.4968186306912115\n",
      "Step - 7469, Loss - 0.763129222775642, Learning Rate - 0.000390625, magnitude of gradient - 0.6873605433108703\n",
      "Step - 7470, Loss - 0.7028262443424889, Learning Rate - 0.000390625, magnitude of gradient - 1.3238369987832992\n",
      "Step - 7471, Loss - 0.6779533485572627, Learning Rate - 0.000390625, magnitude of gradient - 0.5406105923634265\n",
      "Step - 7472, Loss - 0.5748468861325263, Learning Rate - 0.000390625, magnitude of gradient - 0.7101366960780694\n",
      "Step - 7473, Loss - 0.5805102766422257, Learning Rate - 0.000390625, magnitude of gradient - 1.6678727939185138\n",
      "Step - 7474, Loss - 0.813755705175565, Learning Rate - 0.000390625, magnitude of gradient - 1.9713158426911541\n",
      "Step - 7475, Loss - 0.7790729263205042, Learning Rate - 0.000390625, magnitude of gradient - 1.981908854086201\n",
      "Step - 7476, Loss - 0.8025859796028291, Learning Rate - 0.000390625, magnitude of gradient - 0.9202845048796942\n",
      "Step - 7477, Loss - 0.6179964723265338, Learning Rate - 0.000390625, magnitude of gradient - 0.7547911524260092\n",
      "Step - 7478, Loss - 0.6602557151198237, Learning Rate - 0.000390625, magnitude of gradient - 0.8283128285468316\n",
      "Step - 7479, Loss - 0.6792210616081995, Learning Rate - 0.000390625, magnitude of gradient - 1.5486812976209479\n",
      "Step - 7480, Loss - 0.8150242811084536, Learning Rate - 0.000390625, magnitude of gradient - 1.178660989259453\n",
      "Step - 7481, Loss - 0.5677175115644352, Learning Rate - 0.000390625, magnitude of gradient - 2.079533831710122\n",
      "Step - 7482, Loss - 0.6874478825997246, Learning Rate - 0.000390625, magnitude of gradient - 0.5825018688874493\n",
      "Step - 7483, Loss - 0.6534987724055429, Learning Rate - 0.000390625, magnitude of gradient - 1.1633951889026157\n",
      "Step - 7484, Loss - 1.0083404863345877, Learning Rate - 0.000390625, magnitude of gradient - 0.48003687535274403\n",
      "Step - 7485, Loss - 0.6430903261645755, Learning Rate - 0.000390625, magnitude of gradient - 1.6578571357390393\n",
      "Step - 7486, Loss - 0.6965471377462238, Learning Rate - 0.000390625, magnitude of gradient - 0.4685360160116029\n",
      "Step - 7487, Loss - 0.7208856761983262, Learning Rate - 0.000390625, magnitude of gradient - 0.6783993246783907\n",
      "Step - 7488, Loss - 0.7581755706448655, Learning Rate - 0.000390625, magnitude of gradient - 0.49041868176078063\n",
      "Step - 7489, Loss - 0.8356788034345946, Learning Rate - 0.000390625, magnitude of gradient - 0.24110780588291278\n",
      "Step - 7490, Loss - 0.6802212192767287, Learning Rate - 0.000390625, magnitude of gradient - 1.5080188259648297\n",
      "Step - 7491, Loss - 0.863755799452233, Learning Rate - 0.000390625, magnitude of gradient - 2.3329938609709413\n",
      "Step - 7492, Loss - 0.6178186448062412, Learning Rate - 0.000390625, magnitude of gradient - 2.4077714935708197\n",
      "Step - 7493, Loss - 0.7074896966379043, Learning Rate - 0.000390625, magnitude of gradient - 1.4522381900793735\n",
      "Step - 7494, Loss - 0.5993520522705134, Learning Rate - 0.000390625, magnitude of gradient - 0.8640228478763315\n",
      "Step - 7495, Loss - 0.7630761027231265, Learning Rate - 0.000390625, magnitude of gradient - 1.1069625156653218\n",
      "Step - 7496, Loss - 0.7781843643412152, Learning Rate - 0.000390625, magnitude of gradient - 0.6735616141205198\n",
      "Step - 7497, Loss - 0.7217255988687336, Learning Rate - 0.000390625, magnitude of gradient - 1.0815476081893367\n",
      "Step - 7498, Loss - 0.6106076879182722, Learning Rate - 0.000390625, magnitude of gradient - 1.4499739690999156\n",
      "Step - 7499, Loss - 0.6849058526573658, Learning Rate - 0.000390625, magnitude of gradient - 0.886182781040073\n",
      "Step - 7500, Loss - 0.4907869338286013, Learning Rate - 0.000390625, magnitude of gradient - 1.3146991558671817\n",
      "Step - 7501, Loss - 0.7740595959313378, Learning Rate - 0.000390625, magnitude of gradient - 1.651619022972918\n",
      "Step - 7502, Loss - 0.6375805834936562, Learning Rate - 0.000390625, magnitude of gradient - 1.2379528073558317\n",
      "Step - 7503, Loss - 0.5331691632101385, Learning Rate - 0.000390625, magnitude of gradient - 1.5139424465179518\n",
      "Step - 7504, Loss - 0.8997589225838001, Learning Rate - 0.000390625, magnitude of gradient - 1.3460213172692623\n",
      "Step - 7505, Loss - 0.7755323941194696, Learning Rate - 0.000390625, magnitude of gradient - 0.3682547006352333\n",
      "Step - 7506, Loss - 0.7761525729636009, Learning Rate - 0.000390625, magnitude of gradient - 0.6147920630699002\n",
      "Step - 7507, Loss - 1.0118117868634877, Learning Rate - 0.000390625, magnitude of gradient - 0.8933566196541904\n",
      "Step - 7508, Loss - 0.6981271613283191, Learning Rate - 0.000390625, magnitude of gradient - 1.523078721098063\n",
      "Step - 7509, Loss - 0.5652088175029829, Learning Rate - 0.000390625, magnitude of gradient - 1.456799972602107\n",
      "Step - 7510, Loss - 0.6646049116322359, Learning Rate - 0.000390625, magnitude of gradient - 0.48802862231232885\n",
      "Step - 7511, Loss - 0.7834514892246982, Learning Rate - 0.000390625, magnitude of gradient - 1.1342988739750641\n",
      "Step - 7512, Loss - 0.6176344369202197, Learning Rate - 0.000390625, magnitude of gradient - 0.9255910514262442\n",
      "Step - 7513, Loss - 0.7905311451817829, Learning Rate - 0.000390625, magnitude of gradient - 0.3599109040297836\n",
      "Step - 7514, Loss - 0.683771601332823, Learning Rate - 0.000390625, magnitude of gradient - 1.3951178549603278\n",
      "Step - 7515, Loss - 0.726518541413968, Learning Rate - 0.000390625, magnitude of gradient - 1.621598794573954\n",
      "Step - 7516, Loss - 0.7009628240601081, Learning Rate - 0.000390625, magnitude of gradient - 1.4889880156203517\n",
      "Step - 7517, Loss - 0.6937452741503053, Learning Rate - 0.000390625, magnitude of gradient - 2.4664011191350506\n",
      "Step - 7518, Loss - 0.7927899357529935, Learning Rate - 0.000390625, magnitude of gradient - 1.9928948278876422\n",
      "Step - 7519, Loss - 0.6375248994590537, Learning Rate - 0.000390625, magnitude of gradient - 2.078395029247939\n",
      "Step - 7520, Loss - 0.4846174240338784, Learning Rate - 0.000390625, magnitude of gradient - 2.17782267461917\n",
      "Step - 7521, Loss - 0.7579086431417186, Learning Rate - 0.000390625, magnitude of gradient - 1.3467939995574911\n",
      "Step - 7522, Loss - 0.5241470693953418, Learning Rate - 0.000390625, magnitude of gradient - 1.1541837280326581\n",
      "Step - 7523, Loss - 0.698728154854495, Learning Rate - 0.000390625, magnitude of gradient - 0.9499699344635985\n",
      "Step - 7524, Loss - 0.6760852466661214, Learning Rate - 0.000390625, magnitude of gradient - 1.020736043436037\n",
      "Step - 7525, Loss - 0.8404434879440605, Learning Rate - 0.000390625, magnitude of gradient - 1.2080533956128572\n",
      "Step - 7526, Loss - 0.7140851420927833, Learning Rate - 0.000390625, magnitude of gradient - 1.4316559780672753\n",
      "Step - 7527, Loss - 0.6427514453213805, Learning Rate - 0.000390625, magnitude of gradient - 0.773326318289387\n",
      "Step - 7528, Loss - 0.7323072173181493, Learning Rate - 0.000390625, magnitude of gradient - 1.322544466365019\n",
      "Step - 7529, Loss - 0.6970615445833734, Learning Rate - 0.000390625, magnitude of gradient - 0.6255376491152635\n",
      "Step - 7530, Loss - 0.6981076419348806, Learning Rate - 0.000390625, magnitude of gradient - 0.729885015933203\n",
      "Step - 7531, Loss - 0.7173128575869034, Learning Rate - 0.000390625, magnitude of gradient - 1.462418701449824\n",
      "Step - 7532, Loss - 0.6351710877770972, Learning Rate - 0.000390625, magnitude of gradient - 0.6282954416042826\n",
      "Step - 7533, Loss - 0.6693789372154881, Learning Rate - 0.000390625, magnitude of gradient - 0.7545963759104103\n",
      "Step - 7534, Loss - 0.5014446937374437, Learning Rate - 0.000390625, magnitude of gradient - 1.100281186118526\n",
      "Step - 7535, Loss - 0.4737730356971791, Learning Rate - 0.000390625, magnitude of gradient - 1.6201539425338183\n",
      "Step - 7536, Loss - 0.649216388707252, Learning Rate - 0.000390625, magnitude of gradient - 1.5010898079817356\n",
      "Step - 7537, Loss - 0.5927630162895297, Learning Rate - 0.000390625, magnitude of gradient - 2.1248505975470082\n",
      "Step - 7538, Loss - 0.8083605072030782, Learning Rate - 0.000390625, magnitude of gradient - 0.6640216440093976\n",
      "Step - 7539, Loss - 0.5960800792741324, Learning Rate - 0.000390625, magnitude of gradient - 0.9689372907175474\n",
      "Step - 7540, Loss - 0.6914669439441636, Learning Rate - 0.000390625, magnitude of gradient - 1.7980248656804225\n",
      "Step - 7541, Loss - 0.7195047537768736, Learning Rate - 0.000390625, magnitude of gradient - 0.4521982664452593\n",
      "Step - 7542, Loss - 0.8427918909291131, Learning Rate - 0.000390625, magnitude of gradient - 1.113155040315849\n",
      "Step - 7543, Loss - 0.688732124219197, Learning Rate - 0.000390625, magnitude of gradient - 0.8147916447442042\n",
      "Step - 7544, Loss - 0.6088207526058397, Learning Rate - 0.000390625, magnitude of gradient - 0.38252841024257667\n",
      "Step - 7545, Loss - 0.66469729876131, Learning Rate - 0.000390625, magnitude of gradient - 2.264555549746357\n",
      "Step - 7546, Loss - 0.7850980280668967, Learning Rate - 0.000390625, magnitude of gradient - 0.7046672038023989\n",
      "Step - 7547, Loss - 0.5272537845955545, Learning Rate - 0.000390625, magnitude of gradient - 0.9688628424567985\n",
      "Step - 7548, Loss - 0.6663292033574031, Learning Rate - 0.000390625, magnitude of gradient - 0.7167624183146677\n",
      "Step - 7549, Loss - 0.582633765414446, Learning Rate - 0.000390625, magnitude of gradient - 1.7963354849763304\n",
      "Step - 7550, Loss - 0.7864527361293571, Learning Rate - 0.000390625, magnitude of gradient - 1.6986036887616005\n",
      "Step - 7551, Loss - 0.6313767724280706, Learning Rate - 0.000390625, magnitude of gradient - 0.9022113552843698\n",
      "Step - 7552, Loss - 0.672828465069985, Learning Rate - 0.000390625, magnitude of gradient - 0.7448069354292433\n",
      "Step - 7553, Loss - 0.6442929146559477, Learning Rate - 0.000390625, magnitude of gradient - 1.1345284966133813\n",
      "Step - 7554, Loss - 0.6932204334198899, Learning Rate - 0.000390625, magnitude of gradient - 1.24203665129155\n",
      "Step - 7555, Loss - 0.6689262387597803, Learning Rate - 0.000390625, magnitude of gradient - 2.0484954693878055\n",
      "Step - 7556, Loss - 0.702566243735784, Learning Rate - 0.000390625, magnitude of gradient - 1.4181898888210136\n",
      "Step - 7557, Loss - 0.7745856154438331, Learning Rate - 0.000390625, magnitude of gradient - 2.1808824345958207\n",
      "Step - 7558, Loss - 0.6786799778921273, Learning Rate - 0.000390625, magnitude of gradient - 1.1848407199737456\n",
      "Step - 7559, Loss - 0.6037988153585836, Learning Rate - 0.000390625, magnitude of gradient - 1.9374461515294232\n",
      "Step - 7560, Loss - 0.6797600937203402, Learning Rate - 0.000390625, magnitude of gradient - 0.6958147212865263\n",
      "Step - 7561, Loss - 0.9846278521723211, Learning Rate - 0.000390625, magnitude of gradient - 1.2004960154325168\n",
      "Step - 7562, Loss - 0.7221429908642094, Learning Rate - 0.000390625, magnitude of gradient - 0.6919787446251107\n",
      "Step - 7563, Loss - 0.6109425173482558, Learning Rate - 0.000390625, magnitude of gradient - 0.7952955593649297\n",
      "Step - 7564, Loss - 0.65734764073287, Learning Rate - 0.000390625, magnitude of gradient - 0.9989821086358159\n",
      "Step - 7565, Loss - 0.8215062157857568, Learning Rate - 0.000390625, magnitude of gradient - 0.9310405109647614\n",
      "Step - 7566, Loss - 0.7765928977417078, Learning Rate - 0.000390625, magnitude of gradient - 0.7137737050264397\n",
      "Step - 7567, Loss - 0.5805770906173282, Learning Rate - 0.000390625, magnitude of gradient - 0.4574174540118519\n",
      "Step - 7568, Loss - 0.7670402602446059, Learning Rate - 0.000390625, magnitude of gradient - 0.6847764531329545\n",
      "Step - 7569, Loss - 0.6973312284929908, Learning Rate - 0.000390625, magnitude of gradient - 2.8528713209262615\n",
      "Step - 7570, Loss - 0.8068719455627955, Learning Rate - 0.000390625, magnitude of gradient - 0.6100079780374261\n",
      "Step - 7571, Loss - 0.8099497555298196, Learning Rate - 0.000390625, magnitude of gradient - 1.5897040836085201\n",
      "Step - 7572, Loss - 0.7537565180838532, Learning Rate - 0.000390625, magnitude of gradient - 1.1246344905883539\n",
      "Step - 7573, Loss - 0.7755120163412214, Learning Rate - 0.000390625, magnitude of gradient - 0.8449898209589334\n",
      "Step - 7574, Loss - 0.709166135524971, Learning Rate - 0.000390625, magnitude of gradient - 1.201950334989249\n",
      "Step - 7575, Loss - 0.7618238158395595, Learning Rate - 0.000390625, magnitude of gradient - 1.9466777616643802\n",
      "Step - 7576, Loss - 0.5800455473085321, Learning Rate - 0.000390625, magnitude of gradient - 2.2426712402512385\n",
      "Step - 7577, Loss - 0.6020450831116365, Learning Rate - 0.000390625, magnitude of gradient - 0.8622053929104575\n",
      "Step - 7578, Loss - 0.7192052197943397, Learning Rate - 0.000390625, magnitude of gradient - 1.1586250433258045\n",
      "Step - 7579, Loss - 0.7123956401768791, Learning Rate - 0.000390625, magnitude of gradient - 0.8492690805227687\n",
      "Step - 7580, Loss - 0.7008752271984706, Learning Rate - 0.000390625, magnitude of gradient - 1.8355398504524698\n",
      "Step - 7581, Loss - 0.6019496019354289, Learning Rate - 0.000390625, magnitude of gradient - 0.9517699922891057\n",
      "Step - 7582, Loss - 0.6961384097825175, Learning Rate - 0.000390625, magnitude of gradient - 1.3890769638170517\n",
      "Step - 7583, Loss - 0.7325137616492973, Learning Rate - 0.000390625, magnitude of gradient - 0.41480108154812134\n",
      "Step - 7584, Loss - 0.5505036100066862, Learning Rate - 0.000390625, magnitude of gradient - 1.2021756659475007\n",
      "Step - 7585, Loss - 0.6620430681050112, Learning Rate - 0.000390625, magnitude of gradient - 0.9669858801049309\n",
      "Step - 7586, Loss - 0.7489253702713692, Learning Rate - 0.000390625, magnitude of gradient - 0.83857937428433\n",
      "Step - 7587, Loss - 0.6602927007551924, Learning Rate - 0.000390625, magnitude of gradient - 1.0681696646511851\n",
      "Step - 7588, Loss - 0.6731401367040999, Learning Rate - 0.000390625, magnitude of gradient - 0.5889687753195355\n",
      "Step - 7589, Loss - 0.7091502052141168, Learning Rate - 0.000390625, magnitude of gradient - 0.9447937272691402\n",
      "Step - 7590, Loss - 0.7706773056767964, Learning Rate - 0.000390625, magnitude of gradient - 1.585887746306174\n",
      "Step - 7591, Loss - 0.5366308435753574, Learning Rate - 0.000390625, magnitude of gradient - 1.4214261239380557\n",
      "Step - 7592, Loss - 0.8610996859800991, Learning Rate - 0.000390625, magnitude of gradient - 0.9121942096586789\n",
      "Step - 7593, Loss - 0.6618789817175406, Learning Rate - 0.000390625, magnitude of gradient - 1.1040949022519808\n",
      "Step - 7594, Loss - 0.906157831912299, Learning Rate - 0.000390625, magnitude of gradient - 1.7630007085259611\n",
      "Step - 7595, Loss - 0.7749217132494792, Learning Rate - 0.000390625, magnitude of gradient - 0.3944906900020037\n",
      "Step - 7596, Loss - 0.6022294749612468, Learning Rate - 0.000390625, magnitude of gradient - 1.3042061981719975\n",
      "Step - 7597, Loss - 0.5843776010275858, Learning Rate - 0.000390625, magnitude of gradient - 1.311279934341966\n",
      "Step - 7598, Loss - 0.5157166933327217, Learning Rate - 0.000390625, magnitude of gradient - 1.8957484405207887\n",
      "Step - 7599, Loss - 0.7155574069084596, Learning Rate - 0.000390625, magnitude of gradient - 0.6299624995649825\n",
      "Step - 7600, Loss - 0.6039783484499401, Learning Rate - 0.000390625, magnitude of gradient - 1.043854873054721\n",
      "Step - 7601, Loss - 0.848143442154455, Learning Rate - 0.000390625, magnitude of gradient - 1.9802332079704361\n",
      "Step - 7602, Loss - 0.45811114883519166, Learning Rate - 0.000390625, magnitude of gradient - 1.1439797146207298\n",
      "Step - 7603, Loss - 0.6762113103346512, Learning Rate - 0.000390625, magnitude of gradient - 1.371143034523626\n",
      "Step - 7604, Loss - 0.8222189927842302, Learning Rate - 0.000390625, magnitude of gradient - 0.3363585154509255\n",
      "Step - 7605, Loss - 0.7615984966932016, Learning Rate - 0.000390625, magnitude of gradient - 1.1380448940797891\n",
      "Step - 7606, Loss - 0.6807914186864689, Learning Rate - 0.000390625, magnitude of gradient - 1.8106817211964883\n",
      "Step - 7607, Loss - 0.6495419352685001, Learning Rate - 0.000390625, magnitude of gradient - 1.6760623671530757\n",
      "Step - 7608, Loss - 0.6826645811606973, Learning Rate - 0.000390625, magnitude of gradient - 1.1066786534996755\n",
      "Step - 7609, Loss - 0.7483299008321525, Learning Rate - 0.000390625, magnitude of gradient - 0.8764378086740818\n",
      "Step - 7610, Loss - 0.5507357008294449, Learning Rate - 0.000390625, magnitude of gradient - 2.2756980187271867\n",
      "Step - 7611, Loss - 0.8218106016024388, Learning Rate - 0.000390625, magnitude of gradient - 2.161976762174132\n",
      "Step - 7612, Loss - 0.7158741155017567, Learning Rate - 0.000390625, magnitude of gradient - 0.9013629484573316\n",
      "Step - 7613, Loss - 0.6933182521087028, Learning Rate - 0.000390625, magnitude of gradient - 1.510759242776809\n",
      "Step - 7614, Loss - 0.6373892868675805, Learning Rate - 0.000390625, magnitude of gradient - 0.8228989240052125\n",
      "Step - 7615, Loss - 0.7546204679309748, Learning Rate - 0.000390625, magnitude of gradient - 1.090451600479666\n",
      "Step - 7616, Loss - 0.7587237585479512, Learning Rate - 0.000390625, magnitude of gradient - 1.3435593324291886\n",
      "Step - 7617, Loss - 0.7156317906512141, Learning Rate - 0.000390625, magnitude of gradient - 0.7415184532257594\n",
      "Step - 7618, Loss - 0.7730784394710518, Learning Rate - 0.000390625, magnitude of gradient - 0.9935072728778952\n",
      "Step - 7619, Loss - 0.5919855047914199, Learning Rate - 0.000390625, magnitude of gradient - 1.2224787888695736\n",
      "Step - 7620, Loss - 0.9464351203379696, Learning Rate - 0.000390625, magnitude of gradient - 2.6905472666457313\n",
      "Step - 7621, Loss - 0.8827435157253045, Learning Rate - 0.000390625, magnitude of gradient - 1.2002701506593945\n",
      "Step - 7622, Loss - 0.545180801263655, Learning Rate - 0.000390625, magnitude of gradient - 0.7117854976840706\n",
      "Step - 7623, Loss - 0.5800264052571138, Learning Rate - 0.000390625, magnitude of gradient - 1.1563266718305305\n",
      "Step - 7624, Loss - 0.6484722538077416, Learning Rate - 0.000390625, magnitude of gradient - 1.2955804608241108\n",
      "Step - 7625, Loss - 0.7257705918453785, Learning Rate - 0.000390625, magnitude of gradient - 0.522846803877402\n",
      "Step - 7626, Loss - 0.8381710908609521, Learning Rate - 0.000390625, magnitude of gradient - 1.094939313816275\n",
      "Step - 7627, Loss - 0.46034130163069564, Learning Rate - 0.000390625, magnitude of gradient - 1.8961850341322894\n",
      "Step - 7628, Loss - 0.6611461282711029, Learning Rate - 0.000390625, magnitude of gradient - 0.7232730720768636\n",
      "Step - 7629, Loss - 0.704181794595933, Learning Rate - 0.000390625, magnitude of gradient - 0.6642256037495541\n",
      "Step - 7630, Loss - 0.7342655487519554, Learning Rate - 0.000390625, magnitude of gradient - 1.619684421518605\n",
      "Step - 7631, Loss - 0.8665315208504362, Learning Rate - 0.000390625, magnitude of gradient - 1.5283459026519137\n",
      "Step - 7632, Loss - 0.7260449505861605, Learning Rate - 0.000390625, magnitude of gradient - 1.75766404994985\n",
      "Step - 7633, Loss - 0.7392257378721983, Learning Rate - 0.000390625, magnitude of gradient - 0.8623393228774006\n",
      "Step - 7634, Loss - 0.7258570149530585, Learning Rate - 0.000390625, magnitude of gradient - 0.7086838584744658\n",
      "Step - 7635, Loss - 0.8121914271068504, Learning Rate - 0.000390625, magnitude of gradient - 0.7940357401404927\n",
      "Step - 7636, Loss - 0.7209447291656108, Learning Rate - 0.000390625, magnitude of gradient - 1.4239036283610915\n",
      "Step - 7637, Loss - 0.5289192662968171, Learning Rate - 0.000390625, magnitude of gradient - 1.678446910169378\n",
      "Step - 7638, Loss - 0.7208782500572616, Learning Rate - 0.000390625, magnitude of gradient - 1.0986577156546906\n",
      "Step - 7639, Loss - 0.7787797864281173, Learning Rate - 0.000390625, magnitude of gradient - 0.8808286534196239\n",
      "Step - 7640, Loss - 0.7642753023402152, Learning Rate - 0.000390625, magnitude of gradient - 1.4701827092333555\n",
      "Step - 7641, Loss - 0.994318328106457, Learning Rate - 0.000390625, magnitude of gradient - 1.829935119765375\n",
      "Step - 7642, Loss - 0.6508764404028455, Learning Rate - 0.000390625, magnitude of gradient - 1.3785820149152028\n",
      "Step - 7643, Loss - 0.7340444243854716, Learning Rate - 0.000390625, magnitude of gradient - 1.2004971519971068\n",
      "Step - 7644, Loss - 0.7963118925488097, Learning Rate - 0.000390625, magnitude of gradient - 1.872648066878114\n",
      "Step - 7645, Loss - 0.7899794356872689, Learning Rate - 0.000390625, magnitude of gradient - 2.0102349014599628\n",
      "Step - 7646, Loss - 0.8345705966799639, Learning Rate - 0.000390625, magnitude of gradient - 1.360109757539468\n",
      "Step - 7647, Loss - 0.5965055321907123, Learning Rate - 0.000390625, magnitude of gradient - 3.26301519136531\n",
      "Step - 7648, Loss - 0.8493426991589715, Learning Rate - 0.000390625, magnitude of gradient - 0.7027019538355114\n",
      "Step - 7649, Loss - 0.6570575512804481, Learning Rate - 0.000390625, magnitude of gradient - 2.029792564585245\n",
      "Step - 7650, Loss - 0.6990433784054093, Learning Rate - 0.000390625, magnitude of gradient - 0.1489181348900114\n",
      "Step - 7651, Loss - 0.6770634056752538, Learning Rate - 0.000390625, magnitude of gradient - 0.5254714716357786\n",
      "Step - 7652, Loss - 0.6100781294385841, Learning Rate - 0.000390625, magnitude of gradient - 0.7071345836457505\n",
      "Step - 7653, Loss - 0.6775465386003326, Learning Rate - 0.000390625, magnitude of gradient - 0.6612913710185192\n",
      "Step - 7654, Loss - 0.8062466901340682, Learning Rate - 0.000390625, magnitude of gradient - 1.60497097565119\n",
      "Step - 7655, Loss - 0.8286288877138748, Learning Rate - 0.000390625, magnitude of gradient - 2.242818499801269\n",
      "Step - 7656, Loss - 0.6923022830404817, Learning Rate - 0.000390625, magnitude of gradient - 0.5735984231843859\n",
      "Step - 7657, Loss - 0.597662184098992, Learning Rate - 0.000390625, magnitude of gradient - 1.173304109528979\n",
      "Step - 7658, Loss - 0.7442405169098051, Learning Rate - 0.000390625, magnitude of gradient - 1.455425418943959\n",
      "Step - 7659, Loss - 0.6558433529274428, Learning Rate - 0.000390625, magnitude of gradient - 1.0314896705265182\n",
      "Step - 7660, Loss - 0.669801389062077, Learning Rate - 0.000390625, magnitude of gradient - 2.088177425291383\n",
      "Step - 7661, Loss - 0.65923763320829, Learning Rate - 0.000390625, magnitude of gradient - 1.199726788588804\n",
      "Step - 7662, Loss - 0.6249091922914616, Learning Rate - 0.000390625, magnitude of gradient - 0.9986877909369785\n",
      "Step - 7663, Loss - 0.7214716316444132, Learning Rate - 0.000390625, magnitude of gradient - 2.6838754310662796\n",
      "Step - 7664, Loss - 0.80514743219764, Learning Rate - 0.000390625, magnitude of gradient - 1.0875616929784642\n",
      "Step - 7665, Loss - 0.7340870961383309, Learning Rate - 0.000390625, magnitude of gradient - 1.0161007756179687\n",
      "Step - 7666, Loss - 0.6340532547777452, Learning Rate - 0.000390625, magnitude of gradient - 0.5915932783286082\n",
      "Step - 7667, Loss - 0.7019942110582368, Learning Rate - 0.000390625, magnitude of gradient - 1.233697410754793\n",
      "Step - 7668, Loss - 0.7793529963449936, Learning Rate - 0.000390625, magnitude of gradient - 1.1573936503682738\n",
      "Step - 7669, Loss - 0.7676919263439963, Learning Rate - 0.000390625, magnitude of gradient - 2.022066223943699\n",
      "Step - 7670, Loss - 0.5903340925791569, Learning Rate - 0.000390625, magnitude of gradient - 1.2561224746319728\n",
      "Step - 7671, Loss - 0.8542052326302387, Learning Rate - 0.000390625, magnitude of gradient - 0.9095964291834628\n",
      "Step - 7672, Loss - 0.5814690864084765, Learning Rate - 0.000390625, magnitude of gradient - 0.5860385665683899\n",
      "Step - 7673, Loss - 0.6209151940261547, Learning Rate - 0.000390625, magnitude of gradient - 0.8735310436803402\n",
      "Step - 7674, Loss - 0.6107987100924936, Learning Rate - 0.000390625, magnitude of gradient - 0.36567168844026676\n",
      "Step - 7675, Loss - 0.5558577761851107, Learning Rate - 0.000390625, magnitude of gradient - 1.9745053772209604\n",
      "Step - 7676, Loss - 0.7344237081635312, Learning Rate - 0.000390625, magnitude of gradient - 1.322180482547593\n",
      "Step - 7677, Loss - 0.6701821400862805, Learning Rate - 0.000390625, magnitude of gradient - 1.3701133250004178\n",
      "Step - 7678, Loss - 0.7808375767891405, Learning Rate - 0.000390625, magnitude of gradient - 1.2920087852077489\n",
      "Step - 7679, Loss - 0.6496672780236364, Learning Rate - 0.000390625, magnitude of gradient - 1.1762057777979065\n",
      "Step - 7680, Loss - 0.46409330890630685, Learning Rate - 0.000390625, magnitude of gradient - 1.1699980349096224\n",
      "Step - 7681, Loss - 0.7365867704815288, Learning Rate - 0.000390625, magnitude of gradient - 0.9929762038640398\n",
      "Step - 7682, Loss - 1.0520867425808036, Learning Rate - 0.000390625, magnitude of gradient - 3.2874900934106615\n",
      "Step - 7683, Loss - 0.7134854026214845, Learning Rate - 0.000390625, magnitude of gradient - 0.45489313004702014\n",
      "Step - 7684, Loss - 0.7031021569869672, Learning Rate - 0.000390625, magnitude of gradient - 1.2147510486555066\n",
      "Step - 7685, Loss - 0.4766223461470295, Learning Rate - 0.000390625, magnitude of gradient - 1.1571010114998854\n",
      "Step - 7686, Loss - 0.6202537079625515, Learning Rate - 0.000390625, magnitude of gradient - 1.4911338035754642\n",
      "Step - 7687, Loss - 0.5899472842275238, Learning Rate - 0.000390625, magnitude of gradient - 0.6163896484790491\n",
      "Step - 7688, Loss - 0.6964194054206176, Learning Rate - 0.000390625, magnitude of gradient - 1.3384164687306506\n",
      "Step - 7689, Loss - 0.6951420151629125, Learning Rate - 0.000390625, magnitude of gradient - 0.4259042448096137\n",
      "Step - 7690, Loss - 0.6589564187734326, Learning Rate - 0.000390625, magnitude of gradient - 1.0454883376479638\n",
      "Step - 7691, Loss - 0.6939708708843484, Learning Rate - 0.000390625, magnitude of gradient - 1.0681045363114743\n",
      "Step - 7692, Loss - 0.5902419934294934, Learning Rate - 0.000390625, magnitude of gradient - 1.824516129654197\n",
      "Step - 7693, Loss - 0.6878437894214073, Learning Rate - 0.000390625, magnitude of gradient - 0.32095388325189883\n",
      "Step - 7694, Loss - 0.7242432810857526, Learning Rate - 0.000390625, magnitude of gradient - 1.017385775576682\n",
      "Step - 7695, Loss - 0.6843522534354959, Learning Rate - 0.000390625, magnitude of gradient - 1.2247693041260235\n",
      "Step - 7696, Loss - 0.7647917473913404, Learning Rate - 0.000390625, magnitude of gradient - 1.123170455465838\n",
      "Step - 7697, Loss - 0.5890816068111796, Learning Rate - 0.000390625, magnitude of gradient - 1.243271657707262\n",
      "Step - 7698, Loss - 0.5887664485076094, Learning Rate - 0.000390625, magnitude of gradient - 2.0372502812870485\n",
      "Step - 7699, Loss - 0.5968603776856498, Learning Rate - 0.000390625, magnitude of gradient - 1.0082762857126255\n",
      "Step - 7700, Loss - 0.8042353331224641, Learning Rate - 0.000390625, magnitude of gradient - 1.7881319837053216\n",
      "Step - 7701, Loss - 0.737811998475183, Learning Rate - 0.000390625, magnitude of gradient - 0.8487883058765082\n",
      "Step - 7702, Loss - 0.8640117667251823, Learning Rate - 0.000390625, magnitude of gradient - 1.3328352915280228\n",
      "Step - 7703, Loss - 0.7172777063582629, Learning Rate - 0.000390625, magnitude of gradient - 0.9141376685140044\n",
      "Step - 7704, Loss - 0.7272295908329656, Learning Rate - 0.000390625, magnitude of gradient - 1.8095551854886673\n",
      "Step - 7705, Loss - 0.6511977928895982, Learning Rate - 0.000390625, magnitude of gradient - 0.4637812911489875\n",
      "Step - 7706, Loss - 0.5600317458814204, Learning Rate - 0.000390625, magnitude of gradient - 0.4846751613241155\n",
      "Step - 7707, Loss - 0.6314926450225102, Learning Rate - 0.000390625, magnitude of gradient - 1.0805145394610962\n",
      "Step - 7708, Loss - 0.7026995683995727, Learning Rate - 0.000390625, magnitude of gradient - 1.0959008459081938\n",
      "Step - 7709, Loss - 0.5484354467093677, Learning Rate - 0.000390625, magnitude of gradient - 1.552644550351467\n",
      "Step - 7710, Loss - 0.6647897123292696, Learning Rate - 0.000390625, magnitude of gradient - 1.7017069555524893\n",
      "Step - 7711, Loss - 0.9057870165432486, Learning Rate - 0.000390625, magnitude of gradient - 1.8566727137951229\n",
      "Step - 7712, Loss - 0.6640005275605717, Learning Rate - 0.000390625, magnitude of gradient - 0.7006996989050939\n",
      "Step - 7713, Loss - 0.6566029129547455, Learning Rate - 0.000390625, magnitude of gradient - 1.3228300970264337\n",
      "Step - 7714, Loss - 0.7023416015058249, Learning Rate - 0.000390625, magnitude of gradient - 1.2063841049656971\n",
      "Step - 7715, Loss - 0.7004541588983033, Learning Rate - 0.000390625, magnitude of gradient - 0.9990188401966487\n",
      "Step - 7716, Loss - 0.7121304554665358, Learning Rate - 0.000390625, magnitude of gradient - 0.8262889266489086\n",
      "Step - 7717, Loss - 0.8397562699366967, Learning Rate - 0.000390625, magnitude of gradient - 2.5170943160008297\n",
      "Step - 7718, Loss - 0.6089164952311097, Learning Rate - 0.000390625, magnitude of gradient - 0.47344888841914934\n",
      "Step - 7719, Loss - 0.7004815589553817, Learning Rate - 0.000390625, magnitude of gradient - 1.222621686182669\n",
      "Step - 7720, Loss - 0.5592584561995422, Learning Rate - 0.000390625, magnitude of gradient - 2.282805418797293\n",
      "Step - 7721, Loss - 0.552912467882772, Learning Rate - 0.000390625, magnitude of gradient - 0.8876952921858164\n",
      "Step - 7722, Loss - 0.8129125130492006, Learning Rate - 0.000390625, magnitude of gradient - 1.2952533113725286\n",
      "Step - 7723, Loss - 0.7673721446260985, Learning Rate - 0.000390625, magnitude of gradient - 0.34627008426959693\n",
      "Step - 7724, Loss - 0.6901799355981646, Learning Rate - 0.000390625, magnitude of gradient - 1.2788285672232713\n",
      "Step - 7725, Loss - 0.7179280125087165, Learning Rate - 0.000390625, magnitude of gradient - 0.9732498904572898\n",
      "Step - 7726, Loss - 0.579178371366555, Learning Rate - 0.000390625, magnitude of gradient - 1.8972068479775877\n",
      "Step - 7727, Loss - 0.6559513535481941, Learning Rate - 0.000390625, magnitude of gradient - 0.32892645488893657\n",
      "Step - 7728, Loss - 0.576198061767428, Learning Rate - 0.000390625, magnitude of gradient - 0.6515765747318842\n",
      "Step - 7729, Loss - 0.731221947441281, Learning Rate - 0.000390625, magnitude of gradient - 0.8660219027265519\n",
      "Step - 7730, Loss - 0.6228112204073082, Learning Rate - 0.000390625, magnitude of gradient - 1.5832865826936762\n",
      "Step - 7731, Loss - 0.6725462595375156, Learning Rate - 0.000390625, magnitude of gradient - 0.6889497074821785\n",
      "Step - 7732, Loss - 0.695102653581976, Learning Rate - 0.000390625, magnitude of gradient - 2.8001900070018175\n",
      "Step - 7733, Loss - 0.7179398763799206, Learning Rate - 0.000390625, magnitude of gradient - 0.7337587595959099\n",
      "Step - 7734, Loss - 0.8070221056218909, Learning Rate - 0.000390625, magnitude of gradient - 0.4093150773115759\n",
      "Step - 7735, Loss - 0.7857481040758312, Learning Rate - 0.000390625, magnitude of gradient - 1.3609686637868188\n",
      "Step - 7736, Loss - 0.8166387833246647, Learning Rate - 0.000390625, magnitude of gradient - 0.8670867649019762\n",
      "Step - 7737, Loss - 0.7296559946834345, Learning Rate - 0.000390625, magnitude of gradient - 1.3045573299526663\n",
      "Step - 7738, Loss - 0.736501693600428, Learning Rate - 0.000390625, magnitude of gradient - 0.804569331111026\n",
      "Step - 7739, Loss - 0.6129814933671036, Learning Rate - 0.000390625, magnitude of gradient - 2.3278339672266664\n",
      "Step - 7740, Loss - 0.6212419665917073, Learning Rate - 0.000390625, magnitude of gradient - 0.9936875910989003\n",
      "Step - 7741, Loss - 0.5931829202875043, Learning Rate - 0.000390625, magnitude of gradient - 0.8068453727209324\n",
      "Step - 7742, Loss - 0.8024964463916242, Learning Rate - 0.000390625, magnitude of gradient - 1.8147120018623164\n",
      "Step - 7743, Loss - 0.7052911156861412, Learning Rate - 0.000390625, magnitude of gradient - 1.138955170991594\n",
      "Step - 7744, Loss - 0.6644095101639563, Learning Rate - 0.000390625, magnitude of gradient - 1.4791796941912176\n",
      "Step - 7745, Loss - 0.7571666575219962, Learning Rate - 0.000390625, magnitude of gradient - 1.4332460222732282\n",
      "Step - 7746, Loss - 0.8548961367954139, Learning Rate - 0.000390625, magnitude of gradient - 1.1207266740040218\n",
      "Step - 7747, Loss - 0.9558126520166716, Learning Rate - 0.000390625, magnitude of gradient - 2.0550186148202556\n",
      "Step - 7748, Loss - 0.693374220676262, Learning Rate - 0.000390625, magnitude of gradient - 0.9605337545710038\n",
      "Step - 7749, Loss - 0.672772216160078, Learning Rate - 0.000390625, magnitude of gradient - 1.3933868238459295\n",
      "Step - 7750, Loss - 0.7137756847389238, Learning Rate - 0.000390625, magnitude of gradient - 1.8588414262167354\n",
      "Step - 7751, Loss - 0.8296866936172446, Learning Rate - 0.000390625, magnitude of gradient - 0.8257133200498704\n",
      "Step - 7752, Loss - 0.7169041106333265, Learning Rate - 0.000390625, magnitude of gradient - 0.6794913435960622\n",
      "Step - 7753, Loss - 0.7558616364923939, Learning Rate - 0.000390625, magnitude of gradient - 1.4070781390576554\n",
      "Step - 7754, Loss - 0.5200207629191146, Learning Rate - 0.000390625, magnitude of gradient - 0.9574520742802952\n",
      "Step - 7755, Loss - 0.7017430430358222, Learning Rate - 0.000390625, magnitude of gradient - 0.5754269890132302\n",
      "Step - 7756, Loss - 0.6030217828351276, Learning Rate - 0.000390625, magnitude of gradient - 1.0063667292822267\n",
      "Step - 7757, Loss - 0.8891199373086128, Learning Rate - 0.000390625, magnitude of gradient - 1.6295655158447828\n",
      "Step - 7758, Loss - 0.8227731111802563, Learning Rate - 0.000390625, magnitude of gradient - 2.189277166326069\n",
      "Step - 7759, Loss - 0.6256273557666371, Learning Rate - 0.000390625, magnitude of gradient - 0.40613827916069145\n",
      "Step - 7760, Loss - 0.7753484633080611, Learning Rate - 0.000390625, magnitude of gradient - 1.7777866979208994\n",
      "Step - 7761, Loss - 0.8622703465769117, Learning Rate - 0.000390625, magnitude of gradient - 1.2441733498389054\n",
      "Step - 7762, Loss - 0.7423592662631762, Learning Rate - 0.000390625, magnitude of gradient - 1.0903907693081514\n",
      "Step - 7763, Loss - 0.6352284712566741, Learning Rate - 0.000390625, magnitude of gradient - 0.824640383441029\n",
      "Step - 7764, Loss - 0.6405308700673595, Learning Rate - 0.000390625, magnitude of gradient - 1.08915913980812\n",
      "Step - 7765, Loss - 0.742098552600699, Learning Rate - 0.000390625, magnitude of gradient - 0.9620538368492498\n",
      "Step - 7766, Loss - 0.6346539461366595, Learning Rate - 0.000390625, magnitude of gradient - 2.352972871196535\n",
      "Step - 7767, Loss - 0.668533282815442, Learning Rate - 0.000390625, magnitude of gradient - 1.9291915401237894\n",
      "Step - 7768, Loss - 0.6658643238074595, Learning Rate - 0.000390625, magnitude of gradient - 0.9689642372983188\n",
      "Step - 7769, Loss - 0.7897627044318857, Learning Rate - 0.000390625, magnitude of gradient - 0.9829714822323568\n",
      "Step - 7770, Loss - 0.732713179771515, Learning Rate - 0.000390625, magnitude of gradient - 0.7620014174556947\n",
      "Step - 7771, Loss - 0.9620456116242028, Learning Rate - 0.000390625, magnitude of gradient - 0.9992316077242964\n",
      "Step - 7772, Loss - 0.7738141865915689, Learning Rate - 0.000390625, magnitude of gradient - 0.9848897004575152\n",
      "Step - 7773, Loss - 0.7846902460464737, Learning Rate - 0.000390625, magnitude of gradient - 0.28624628339346053\n",
      "Step - 7774, Loss - 0.8186002425520424, Learning Rate - 0.000390625, magnitude of gradient - 2.338569305651287\n",
      "Step - 7775, Loss - 0.6067649243753187, Learning Rate - 0.000390625, magnitude of gradient - 1.2569537840267047\n",
      "Step - 7776, Loss - 0.7288689874593405, Learning Rate - 0.000390625, magnitude of gradient - 0.7091501445816505\n",
      "Step - 7777, Loss - 0.8071318469057736, Learning Rate - 0.000390625, magnitude of gradient - 1.5109639258144363\n",
      "Step - 7778, Loss - 0.9265404404724971, Learning Rate - 0.000390625, magnitude of gradient - 0.3778828868431555\n",
      "Step - 7779, Loss - 0.5573507612707895, Learning Rate - 0.000390625, magnitude of gradient - 0.9850276649100506\n",
      "Step - 7780, Loss - 0.7163687982310349, Learning Rate - 0.000390625, magnitude of gradient - 1.300084633581269\n",
      "Step - 7781, Loss - 0.5603155733671814, Learning Rate - 0.000390625, magnitude of gradient - 0.9612908670776785\n",
      "Step - 7782, Loss - 0.7807183088074154, Learning Rate - 0.000390625, magnitude of gradient - 1.2878362041623244\n",
      "Step - 7783, Loss - 0.6098948235308422, Learning Rate - 0.000390625, magnitude of gradient - 0.9285697846090016\n",
      "Step - 7784, Loss - 0.6387241521091699, Learning Rate - 0.000390625, magnitude of gradient - 0.7393799631303354\n",
      "Step - 7785, Loss - 0.6103363654492727, Learning Rate - 0.000390625, magnitude of gradient - 1.1857555571786451\n",
      "Step - 7786, Loss - 0.7647071745562363, Learning Rate - 0.000390625, magnitude of gradient - 0.7216344188017098\n",
      "Step - 7787, Loss - 0.7775723742959941, Learning Rate - 0.000390625, magnitude of gradient - 0.8886705287843916\n",
      "Step - 7788, Loss - 0.6573423872690407, Learning Rate - 0.000390625, magnitude of gradient - 1.2491011496534759\n",
      "Step - 7789, Loss - 0.5618092410696213, Learning Rate - 0.000390625, magnitude of gradient - 1.6183913573754591\n",
      "Step - 7790, Loss - 0.6162465988292392, Learning Rate - 0.000390625, magnitude of gradient - 1.305403654564451\n",
      "Step - 7791, Loss - 0.7547852594402648, Learning Rate - 0.000390625, magnitude of gradient - 1.0574553448742732\n",
      "Step - 7792, Loss - 0.7271316180055628, Learning Rate - 0.000390625, magnitude of gradient - 0.9363733064173108\n",
      "Step - 7793, Loss - 0.7799995227570115, Learning Rate - 0.000390625, magnitude of gradient - 0.36964198935495035\n",
      "Step - 7794, Loss - 0.6867430474535852, Learning Rate - 0.000390625, magnitude of gradient - 1.2212020843904363\n",
      "Step - 7795, Loss - 1.017114828270018, Learning Rate - 0.000390625, magnitude of gradient - 2.059080856552785\n",
      "Step - 7796, Loss - 0.7272846807703464, Learning Rate - 0.000390625, magnitude of gradient - 1.68064347020409\n",
      "Step - 7797, Loss - 0.5637830438860532, Learning Rate - 0.000390625, magnitude of gradient - 0.9784154492581033\n",
      "Step - 7798, Loss - 0.7588119253498105, Learning Rate - 0.000390625, magnitude of gradient - 0.8349620838274204\n",
      "Step - 7799, Loss - 0.802655127746362, Learning Rate - 0.000390625, magnitude of gradient - 0.785344919515933\n",
      "Step - 7800, Loss - 0.7048756518151308, Learning Rate - 0.000390625, magnitude of gradient - 0.9878048517130446\n",
      "Step - 7801, Loss - 0.610287380601326, Learning Rate - 0.000390625, magnitude of gradient - 0.8905147607199128\n",
      "Step - 7802, Loss - 0.7330233839855662, Learning Rate - 0.000390625, magnitude of gradient - 1.6525785190586846\n",
      "Step - 7803, Loss - 0.6840156708770475, Learning Rate - 0.000390625, magnitude of gradient - 0.8627611641399018\n",
      "Step - 7804, Loss - 0.7784231969418937, Learning Rate - 0.000390625, magnitude of gradient - 0.8544837805452863\n",
      "Step - 7805, Loss - 0.5850193165099317, Learning Rate - 0.000390625, magnitude of gradient - 1.2438655263714822\n",
      "Step - 7806, Loss - 0.6781575078240167, Learning Rate - 0.000390625, magnitude of gradient - 1.3112521471365928\n",
      "Step - 7807, Loss - 0.960731107901649, Learning Rate - 0.000390625, magnitude of gradient - 1.6357948031686451\n",
      "Step - 7808, Loss - 0.7798732064379301, Learning Rate - 0.000390625, magnitude of gradient - 0.7631877014807902\n",
      "Step - 7809, Loss - 0.6991059618816466, Learning Rate - 0.000390625, magnitude of gradient - 0.9138751220729432\n",
      "Step - 7810, Loss - 0.8304782878243379, Learning Rate - 0.000390625, magnitude of gradient - 1.2016819466497113\n",
      "Step - 7811, Loss - 0.6708799207464883, Learning Rate - 0.000390625, magnitude of gradient - 0.707841188604953\n",
      "Step - 7812, Loss - 0.7849068834362992, Learning Rate - 0.000390625, magnitude of gradient - 1.0297389574849654\n",
      "Step - 7813, Loss - 0.8634465555077118, Learning Rate - 0.000390625, magnitude of gradient - 1.0506532541208888\n",
      "Step - 7814, Loss - 0.8424221395933273, Learning Rate - 0.000390625, magnitude of gradient - 2.806095401773718\n",
      "Step - 7815, Loss - 0.8980181903848169, Learning Rate - 0.000390625, magnitude of gradient - 1.120668439869408\n",
      "Step - 7816, Loss - 0.6521594305690748, Learning Rate - 0.000390625, magnitude of gradient - 0.5154807554573303\n",
      "Step - 7817, Loss - 0.7965194295704037, Learning Rate - 0.000390625, magnitude of gradient - 1.4064668184441158\n",
      "Step - 7818, Loss - 0.5388202979225808, Learning Rate - 0.000390625, magnitude of gradient - 0.42409843108034484\n",
      "Step - 7819, Loss - 0.6332032194909433, Learning Rate - 0.000390625, magnitude of gradient - 2.0541067396278625\n",
      "Step - 7820, Loss - 0.6579113517681482, Learning Rate - 0.000390625, magnitude of gradient - 2.510615631806057\n",
      "Step - 7821, Loss - 0.6002050347488469, Learning Rate - 0.000390625, magnitude of gradient - 1.2215505869166552\n",
      "Step - 7822, Loss - 0.8270520531208756, Learning Rate - 0.000390625, magnitude of gradient - 0.8210797091958216\n",
      "Step - 7823, Loss - 0.5242664805181332, Learning Rate - 0.000390625, magnitude of gradient - 1.9273327470034205\n",
      "Step - 7824, Loss - 0.8424611602642654, Learning Rate - 0.000390625, magnitude of gradient - 1.491081160947352\n",
      "Step - 7825, Loss - 0.6887170267323217, Learning Rate - 0.000390625, magnitude of gradient - 0.8277855368179576\n",
      "Step - 7826, Loss - 0.847537248012313, Learning Rate - 0.000390625, magnitude of gradient - 1.2452228445326083\n",
      "Step - 7827, Loss - 0.8642342980333085, Learning Rate - 0.000390625, magnitude of gradient - 1.7703733616249897\n",
      "Step - 7828, Loss - 0.6476602268165883, Learning Rate - 0.000390625, magnitude of gradient - 0.5749490853384571\n",
      "Step - 7829, Loss - 0.7308134230506534, Learning Rate - 0.000390625, magnitude of gradient - 0.9944305057956222\n",
      "Step - 7830, Loss - 0.5466545095736286, Learning Rate - 0.000390625, magnitude of gradient - 0.6904382882396488\n",
      "Step - 7831, Loss - 0.6817907963317804, Learning Rate - 0.000390625, magnitude of gradient - 2.4854774451540775\n",
      "Step - 7832, Loss - 0.7466122882225139, Learning Rate - 0.000390625, magnitude of gradient - 1.269259444058577\n",
      "Step - 7833, Loss - 0.5422822283787877, Learning Rate - 0.000390625, magnitude of gradient - 1.5032365870391928\n",
      "Step - 7834, Loss - 0.6265828936564372, Learning Rate - 0.000390625, magnitude of gradient - 1.118421982630905\n",
      "Step - 7835, Loss - 0.808150148699372, Learning Rate - 0.000390625, magnitude of gradient - 0.7346356867191339\n",
      "Step - 7836, Loss - 0.5145601964950031, Learning Rate - 0.000390625, magnitude of gradient - 0.8682141742643166\n",
      "Step - 7837, Loss - 0.6935852506218231, Learning Rate - 0.000390625, magnitude of gradient - 1.370305174898212\n",
      "Step - 7838, Loss - 0.529348442596611, Learning Rate - 0.000390625, magnitude of gradient - 1.4275940132744456\n",
      "Step - 7839, Loss - 0.5183036086256171, Learning Rate - 0.000390625, magnitude of gradient - 0.8979017570069481\n",
      "Step - 7840, Loss - 0.6811074250864586, Learning Rate - 0.000390625, magnitude of gradient - 0.5928208190861194\n",
      "Step - 7841, Loss - 0.5624504500041899, Learning Rate - 0.000390625, magnitude of gradient - 2.390544746637703\n",
      "Step - 7842, Loss - 0.670816219307668, Learning Rate - 0.000390625, magnitude of gradient - 1.855245435377144\n",
      "Step - 7843, Loss - 0.8447404536749648, Learning Rate - 0.000390625, magnitude of gradient - 1.756694207591652\n",
      "Step - 7844, Loss - 0.5955871463571502, Learning Rate - 0.000390625, magnitude of gradient - 0.8482498707080683\n",
      "Step - 7845, Loss - 0.7700134564177287, Learning Rate - 0.000390625, magnitude of gradient - 1.2960902453313452\n",
      "Step - 7846, Loss - 0.7937670891604233, Learning Rate - 0.000390625, magnitude of gradient - 0.6449064345431834\n",
      "Step - 7847, Loss - 0.6817262941451252, Learning Rate - 0.000390625, magnitude of gradient - 1.5773238739898205\n",
      "Step - 7848, Loss - 0.6520818693157673, Learning Rate - 0.000390625, magnitude of gradient - 0.9824091368266487\n",
      "Step - 7849, Loss - 0.7927380630065225, Learning Rate - 0.000390625, magnitude of gradient - 1.09327183397666\n",
      "Step - 7850, Loss - 0.5987159672548625, Learning Rate - 0.000390625, magnitude of gradient - 1.2249127740527117\n",
      "Step - 7851, Loss - 0.7141305226024293, Learning Rate - 0.000390625, magnitude of gradient - 0.9285164710839702\n",
      "Step - 7852, Loss - 0.807155141068257, Learning Rate - 0.000390625, magnitude of gradient - 1.4471338265422957\n",
      "Step - 7853, Loss - 0.6553045878331426, Learning Rate - 0.000390625, magnitude of gradient - 0.7308475832420692\n",
      "Step - 7854, Loss - 0.5982001128742361, Learning Rate - 0.000390625, magnitude of gradient - 0.7780414611485437\n",
      "Step - 7855, Loss - 0.8453483071389071, Learning Rate - 0.000390625, magnitude of gradient - 1.213848988107854\n",
      "Step - 7856, Loss - 0.6641399167441782, Learning Rate - 0.000390625, magnitude of gradient - 0.7651197229835067\n",
      "Step - 7857, Loss - 0.7953915444351842, Learning Rate - 0.000390625, magnitude of gradient - 1.133618959732265\n",
      "Step - 7858, Loss - 0.5337473064073511, Learning Rate - 0.000390625, magnitude of gradient - 1.0029512964188996\n",
      "Step - 7859, Loss - 0.7648956584669401, Learning Rate - 0.000390625, magnitude of gradient - 0.4941509945119008\n",
      "Step - 7860, Loss - 0.6258377867397451, Learning Rate - 0.000390625, magnitude of gradient - 1.7094996057766594\n",
      "Step - 7861, Loss - 0.491403002361153, Learning Rate - 0.000390625, magnitude of gradient - 0.6215591739345685\n",
      "Step - 7862, Loss - 0.7458628452946937, Learning Rate - 0.000390625, magnitude of gradient - 0.8491269812935194\n",
      "Step - 7863, Loss - 0.8419828034682345, Learning Rate - 0.000390625, magnitude of gradient - 2.1111549703074526\n",
      "Step - 7864, Loss - 0.539474707343664, Learning Rate - 0.000390625, magnitude of gradient - 1.0671594161663929\n",
      "Step - 7865, Loss - 0.6527839515058649, Learning Rate - 0.000390625, magnitude of gradient - 0.491768422399068\n",
      "Step - 7866, Loss - 0.7317747855128126, Learning Rate - 0.000390625, magnitude of gradient - 0.9073877735948033\n",
      "Step - 7867, Loss - 0.6597896809135546, Learning Rate - 0.000390625, magnitude of gradient - 2.167426675392711\n",
      "Step - 7868, Loss - 0.5060662981478115, Learning Rate - 0.000390625, magnitude of gradient - 1.6060669449878275\n",
      "Step - 7869, Loss - 0.653995590725795, Learning Rate - 0.000390625, magnitude of gradient - 1.156319835484555\n",
      "Step - 7870, Loss - 0.6666703629464982, Learning Rate - 0.000390625, magnitude of gradient - 0.2103328133631977\n",
      "Step - 7871, Loss - 0.781018366034689, Learning Rate - 0.000390625, magnitude of gradient - 1.1286506143362294\n",
      "Step - 7872, Loss - 0.8831050744426305, Learning Rate - 0.000390625, magnitude of gradient - 1.3627366956614422\n",
      "Step - 7873, Loss - 0.7082845217363037, Learning Rate - 0.000390625, magnitude of gradient - 1.63921184649542\n",
      "Step - 7874, Loss - 0.6003468396691891, Learning Rate - 0.000390625, magnitude of gradient - 0.9985089305263549\n",
      "Step - 7875, Loss - 0.8057097525335294, Learning Rate - 0.000390625, magnitude of gradient - 1.2050081849871297\n",
      "Step - 7876, Loss - 0.6973935462659268, Learning Rate - 0.000390625, magnitude of gradient - 1.0954460819823992\n",
      "Step - 7877, Loss - 0.7084245025023681, Learning Rate - 0.000390625, magnitude of gradient - 2.033927331277909\n",
      "Step - 7878, Loss - 0.770375067169911, Learning Rate - 0.000390625, magnitude of gradient - 0.8407834529451962\n",
      "Step - 7879, Loss - 0.7308953702848694, Learning Rate - 0.000390625, magnitude of gradient - 0.6739955859173433\n",
      "Step - 7880, Loss - 0.6249697725012628, Learning Rate - 0.000390625, magnitude of gradient - 1.6141011940448173\n",
      "Step - 7881, Loss - 0.6289234088911531, Learning Rate - 0.000390625, magnitude of gradient - 0.8732319512946355\n",
      "Step - 7882, Loss - 0.5423983662467157, Learning Rate - 0.000390625, magnitude of gradient - 0.6606939452052764\n",
      "Step - 7883, Loss - 0.8825558523305832, Learning Rate - 0.000390625, magnitude of gradient - 1.318945404381915\n",
      "Step - 7884, Loss - 0.7351627047454516, Learning Rate - 0.000390625, magnitude of gradient - 1.2495387696101818\n",
      "Step - 7885, Loss - 0.546706001305773, Learning Rate - 0.000390625, magnitude of gradient - 0.9409349272286301\n",
      "Step - 7886, Loss - 0.8494069549482113, Learning Rate - 0.000390625, magnitude of gradient - 1.0724601329442949\n",
      "Step - 7887, Loss - 0.8411309023270159, Learning Rate - 0.000390625, magnitude of gradient - 1.3791078996113728\n",
      "Step - 7888, Loss - 0.47298689719934817, Learning Rate - 0.000390625, magnitude of gradient - 1.200228908823183\n",
      "Step - 7889, Loss - 0.7896396629730613, Learning Rate - 0.000390625, magnitude of gradient - 0.7054442061412781\n",
      "Step - 7890, Loss - 0.6872964652883004, Learning Rate - 0.000390625, magnitude of gradient - 0.4046312130318079\n",
      "Step - 7891, Loss - 0.7573094043359871, Learning Rate - 0.000390625, magnitude of gradient - 1.3971606074395382\n",
      "Step - 7892, Loss - 0.5923495278027322, Learning Rate - 0.000390625, magnitude of gradient - 0.8968482095046986\n",
      "Step - 7893, Loss - 0.7322523880298806, Learning Rate - 0.000390625, magnitude of gradient - 0.7870378358200639\n",
      "Step - 7894, Loss - 0.7895417553486606, Learning Rate - 0.000390625, magnitude of gradient - 0.701945544562206\n",
      "Step - 7895, Loss - 0.6778653958583206, Learning Rate - 0.000390625, magnitude of gradient - 0.8203913329278812\n",
      "Step - 7896, Loss - 0.6362290015526562, Learning Rate - 0.000390625, magnitude of gradient - 1.9211640595437436\n",
      "Step - 7897, Loss - 0.8531130644735128, Learning Rate - 0.000390625, magnitude of gradient - 3.0014321966326127\n",
      "Step - 7898, Loss - 0.6675096454043161, Learning Rate - 0.000390625, magnitude of gradient - 0.6026363970454729\n",
      "Step - 7899, Loss - 0.6403365216342799, Learning Rate - 0.000390625, magnitude of gradient - 3.6596269174214644\n",
      "Step - 7900, Loss - 0.5877641905327742, Learning Rate - 0.000390625, magnitude of gradient - 1.0121741393176618\n",
      "Step - 7901, Loss - 0.7776317963174644, Learning Rate - 0.000390625, magnitude of gradient - 1.3212561363330213\n",
      "Step - 7902, Loss - 0.7870050282692443, Learning Rate - 0.000390625, magnitude of gradient - 1.2948556897567296\n",
      "Step - 7903, Loss - 0.7899711706499566, Learning Rate - 0.000390625, magnitude of gradient - 1.1243910401542256\n",
      "Step - 7904, Loss - 0.7911385375033401, Learning Rate - 0.000390625, magnitude of gradient - 0.7957710308705871\n",
      "Step - 7905, Loss - 0.7647268672699115, Learning Rate - 0.000390625, magnitude of gradient - 1.0542735011942277\n",
      "Step - 7906, Loss - 0.5115510553983191, Learning Rate - 0.000390625, magnitude of gradient - 1.9020421201127238\n",
      "Step - 7907, Loss - 0.7173097053805539, Learning Rate - 0.000390625, magnitude of gradient - 0.6855214008592903\n",
      "Step - 7908, Loss - 0.6774250793099926, Learning Rate - 0.000390625, magnitude of gradient - 1.3763346097561242\n",
      "Step - 7909, Loss - 0.7987685438186457, Learning Rate - 0.000390625, magnitude of gradient - 0.9776607758400497\n",
      "Step - 7910, Loss - 0.7587897696888313, Learning Rate - 0.000390625, magnitude of gradient - 1.7869829445987437\n",
      "Step - 7911, Loss - 0.5228519287267909, Learning Rate - 0.000390625, magnitude of gradient - 0.7659470641394328\n",
      "Step - 7912, Loss - 0.6295945246448956, Learning Rate - 0.000390625, magnitude of gradient - 0.3489721607070652\n",
      "Step - 7913, Loss - 0.8253643309352482, Learning Rate - 0.000390625, magnitude of gradient - 1.7185423261036903\n",
      "Step - 7914, Loss - 0.7793119650352948, Learning Rate - 0.000390625, magnitude of gradient - 1.7733149187415949\n",
      "Step - 7915, Loss - 0.7183707221005764, Learning Rate - 0.000390625, magnitude of gradient - 0.6864907723916179\n",
      "Step - 7916, Loss - 0.7258329240807219, Learning Rate - 0.000390625, magnitude of gradient - 0.7546768302270206\n",
      "Step - 7917, Loss - 0.579067708764134, Learning Rate - 0.000390625, magnitude of gradient - 1.415628758461814\n",
      "Step - 7918, Loss - 0.7462778747875756, Learning Rate - 0.000390625, magnitude of gradient - 0.9989922560686738\n",
      "Step - 7919, Loss - 0.7069604816189963, Learning Rate - 0.000390625, magnitude of gradient - 1.389013506621868\n",
      "Step - 7920, Loss - 0.7338209275547126, Learning Rate - 0.000390625, magnitude of gradient - 1.1812500049807855\n",
      "Step - 7921, Loss - 0.7743554575335716, Learning Rate - 0.000390625, magnitude of gradient - 0.9689465520076043\n",
      "Step - 7922, Loss - 0.6480793964361595, Learning Rate - 0.000390625, magnitude of gradient - 0.7254438929331002\n",
      "Step - 7923, Loss - 0.7699147175886326, Learning Rate - 0.000390625, magnitude of gradient - 1.8713471568846243\n",
      "Step - 7924, Loss - 0.9476528327952469, Learning Rate - 0.000390625, magnitude of gradient - 1.148806950373182\n",
      "Step - 7925, Loss - 0.7117659499959452, Learning Rate - 0.000390625, magnitude of gradient - 0.9521862836255748\n",
      "Step - 7926, Loss - 0.7344380671026952, Learning Rate - 0.000390625, magnitude of gradient - 0.525672000928017\n",
      "Step - 7927, Loss - 0.7415249415297217, Learning Rate - 0.000390625, magnitude of gradient - 0.9618879521661108\n",
      "Step - 7928, Loss - 0.7734425062620108, Learning Rate - 0.000390625, magnitude of gradient - 1.353387613235357\n",
      "Step - 7929, Loss - 0.8417082226065848, Learning Rate - 0.000390625, magnitude of gradient - 1.258030070211573\n",
      "Step - 7930, Loss - 0.5481303691146967, Learning Rate - 0.000390625, magnitude of gradient - 0.7316077311350679\n",
      "Step - 7931, Loss - 0.6373070906213737, Learning Rate - 0.000390625, magnitude of gradient - 1.1820446177367774\n",
      "Step - 7932, Loss - 0.6075260894855645, Learning Rate - 0.000390625, magnitude of gradient - 1.2597692099992126\n",
      "Step - 7933, Loss - 0.7535460010707082, Learning Rate - 0.000390625, magnitude of gradient - 1.3404299555884822\n",
      "Step - 7934, Loss - 0.660255190355993, Learning Rate - 0.000390625, magnitude of gradient - 0.2772760759751774\n",
      "Step - 7935, Loss - 0.6475187702290298, Learning Rate - 0.000390625, magnitude of gradient - 1.545773488304735\n",
      "Step - 7936, Loss - 0.6565475184885449, Learning Rate - 0.000390625, magnitude of gradient - 1.0434199117898129\n",
      "Step - 7937, Loss - 0.6172082656917757, Learning Rate - 0.000390625, magnitude of gradient - 2.766800361927173\n",
      "Step - 7938, Loss - 0.5637263513208178, Learning Rate - 0.000390625, magnitude of gradient - 0.9398997561469443\n",
      "Step - 7939, Loss - 0.6823619647942424, Learning Rate - 0.000390625, magnitude of gradient - 1.7082434247423184\n",
      "Step - 7940, Loss - 0.6412610477024429, Learning Rate - 0.000390625, magnitude of gradient - 0.9232688821554811\n",
      "Step - 7941, Loss - 0.7641425973604923, Learning Rate - 0.000390625, magnitude of gradient - 1.8564594975540205\n",
      "Step - 7942, Loss - 0.5694947179808802, Learning Rate - 0.000390625, magnitude of gradient - 1.5175748040100263\n",
      "Step - 7943, Loss - 0.8872591493006272, Learning Rate - 0.000390625, magnitude of gradient - 1.9642969344214336\n",
      "Step - 7944, Loss - 0.6078297413475471, Learning Rate - 0.000390625, magnitude of gradient - 0.6074090521119765\n",
      "Step - 7945, Loss - 0.827689998137026, Learning Rate - 0.000390625, magnitude of gradient - 1.1132287460041959\n",
      "Step - 7946, Loss - 0.6871860175169773, Learning Rate - 0.000390625, magnitude of gradient - 2.2324221095135126\n",
      "Step - 7947, Loss - 0.7144574790235677, Learning Rate - 0.000390625, magnitude of gradient - 0.847361957844511\n",
      "Step - 7948, Loss - 0.8035677108587045, Learning Rate - 0.000390625, magnitude of gradient - 1.5833049502077305\n",
      "Step - 7949, Loss - 1.0005197573387596, Learning Rate - 0.000390625, magnitude of gradient - 2.1954734568513836\n",
      "Step - 7950, Loss - 0.523458044140687, Learning Rate - 0.000390625, magnitude of gradient - 1.346911481234524\n",
      "Step - 7951, Loss - 0.768111342579781, Learning Rate - 0.000390625, magnitude of gradient - 0.6388236132871562\n",
      "Step - 7952, Loss - 0.6101516749995236, Learning Rate - 0.000390625, magnitude of gradient - 1.1244662756060926\n",
      "Step - 7953, Loss - 0.6910056859038849, Learning Rate - 0.000390625, magnitude of gradient - 0.9182570082211071\n",
      "Step - 7954, Loss - 0.6597602618333759, Learning Rate - 0.000390625, magnitude of gradient - 0.6273501106696615\n",
      "Step - 7955, Loss - 0.7662919070289049, Learning Rate - 0.000390625, magnitude of gradient - 0.979162767408017\n",
      "Step - 7956, Loss - 0.5183338831330737, Learning Rate - 0.000390625, magnitude of gradient - 0.933018386778537\n",
      "Step - 7957, Loss - 0.6156729168405488, Learning Rate - 0.000390625, magnitude of gradient - 1.945556684186758\n",
      "Step - 7958, Loss - 0.7021413052779475, Learning Rate - 0.000390625, magnitude of gradient - 2.245056883479929\n",
      "Step - 7959, Loss - 0.6632795760904788, Learning Rate - 0.000390625, magnitude of gradient - 1.0238134472746987\n",
      "Step - 7960, Loss - 0.7734238561421349, Learning Rate - 0.000390625, magnitude of gradient - 1.6363560861094204\n",
      "Step - 7961, Loss - 0.8510360980100542, Learning Rate - 0.000390625, magnitude of gradient - 0.9753457816835314\n",
      "Step - 7962, Loss - 0.6862425117688824, Learning Rate - 0.000390625, magnitude of gradient - 0.9402272355949294\n",
      "Step - 7963, Loss - 0.6977326917660397, Learning Rate - 0.000390625, magnitude of gradient - 1.409803625292947\n",
      "Step - 7964, Loss - 0.7113612008982848, Learning Rate - 0.000390625, magnitude of gradient - 1.8110970019239037\n",
      "Step - 7965, Loss - 0.6565444529822332, Learning Rate - 0.000390625, magnitude of gradient - 1.2781737084913223\n",
      "Step - 7966, Loss - 0.46687774143843125, Learning Rate - 0.000390625, magnitude of gradient - 1.2241864614308546\n",
      "Step - 7967, Loss - 0.7553217847217952, Learning Rate - 0.000390625, magnitude of gradient - 0.9987214464862034\n",
      "Step - 7968, Loss - 0.6459367873173184, Learning Rate - 0.000390625, magnitude of gradient - 0.6926122702467559\n",
      "Step - 7969, Loss - 0.6633249268254209, Learning Rate - 0.000390625, magnitude of gradient - 1.8156733075392646\n",
      "Step - 7970, Loss - 0.6110075413023939, Learning Rate - 0.000390625, magnitude of gradient - 2.4873386050426958\n",
      "Step - 7971, Loss - 0.8430739787825481, Learning Rate - 0.000390625, magnitude of gradient - 2.0227293207455137\n",
      "Step - 7972, Loss - 0.7134303038038975, Learning Rate - 0.000390625, magnitude of gradient - 1.5126734953735201\n",
      "Step - 7973, Loss - 0.7051164214090829, Learning Rate - 0.000390625, magnitude of gradient - 0.5708085504821938\n",
      "Step - 7974, Loss - 0.751576100265096, Learning Rate - 0.000390625, magnitude of gradient - 2.155671817194815\n",
      "Step - 7975, Loss - 0.6419347668403368, Learning Rate - 0.000390625, magnitude of gradient - 1.1760138953388832\n",
      "Step - 7976, Loss - 0.570703303342277, Learning Rate - 0.000390625, magnitude of gradient - 0.5601989561872078\n",
      "Step - 7977, Loss - 0.7232393675728962, Learning Rate - 0.000390625, magnitude of gradient - 0.9727028179505822\n",
      "Step - 7978, Loss - 0.6425918778121719, Learning Rate - 0.000390625, magnitude of gradient - 1.7711113508599774\n",
      "Step - 7979, Loss - 0.7727723471062704, Learning Rate - 0.000390625, magnitude of gradient - 2.8278145049351293\n",
      "Step - 7980, Loss - 0.5464182824433702, Learning Rate - 0.000390625, magnitude of gradient - 1.774353447728731\n",
      "Step - 7981, Loss - 0.8751732705430242, Learning Rate - 0.000390625, magnitude of gradient - 1.1387922584728376\n",
      "Step - 7982, Loss - 0.671576511292142, Learning Rate - 0.000390625, magnitude of gradient - 1.885613800423012\n",
      "Step - 7983, Loss - 0.6605356977800216, Learning Rate - 0.000390625, magnitude of gradient - 2.8730253806365624\n",
      "Step - 7984, Loss - 0.7843425317750903, Learning Rate - 0.000390625, magnitude of gradient - 0.67764559307122\n",
      "Step - 7985, Loss - 0.649280859841213, Learning Rate - 0.000390625, magnitude of gradient - 0.6360736803560924\n",
      "Step - 7986, Loss - 0.7260062920453311, Learning Rate - 0.000390625, magnitude of gradient - 1.3471749403382118\n",
      "Step - 7987, Loss - 0.6918133677442657, Learning Rate - 0.000390625, magnitude of gradient - 1.1537857600606887\n",
      "Step - 7988, Loss - 0.5359357595174178, Learning Rate - 0.000390625, magnitude of gradient - 0.6886264037919052\n",
      "Step - 7989, Loss - 0.8229650422198056, Learning Rate - 0.000390625, magnitude of gradient - 0.9723668251746533\n",
      "Step - 7990, Loss - 0.5441843562868016, Learning Rate - 0.000390625, magnitude of gradient - 1.5530509743110505\n",
      "Step - 7991, Loss - 0.753224751486748, Learning Rate - 0.000390625, magnitude of gradient - 0.5018032368064006\n",
      "Step - 7992, Loss - 0.8179187555936012, Learning Rate - 0.000390625, magnitude of gradient - 1.564854532849063\n",
      "Step - 7993, Loss - 0.7524657764731953, Learning Rate - 0.000390625, magnitude of gradient - 0.22301183964516327\n",
      "Step - 7994, Loss - 0.9166048030176511, Learning Rate - 0.000390625, magnitude of gradient - 0.5075779481985596\n",
      "Step - 7995, Loss - 0.781598258435173, Learning Rate - 0.000390625, magnitude of gradient - 1.1476927916613358\n",
      "Step - 7996, Loss - 0.5212698789998906, Learning Rate - 0.000390625, magnitude of gradient - 0.7558761372345304\n",
      "Step - 7997, Loss - 0.7268707990512577, Learning Rate - 0.000390625, magnitude of gradient - 2.1119169252787344\n",
      "Step - 7998, Loss - 0.5140433248391015, Learning Rate - 0.000390625, magnitude of gradient - 3.1032339543779317\n",
      "Step - 7999, Loss - 0.805662545742951, Learning Rate - 0.000390625, magnitude of gradient - 1.1151539275601883\n",
      "Step - 8000, Loss - 0.6580471083485735, Learning Rate - 0.000390625, magnitude of gradient - 1.8195716381287332\n",
      "Step - 8001, Loss - 0.6602188469996224, Learning Rate - 0.0001953125, magnitude of gradient - 1.180153693595026\n",
      "Step - 8002, Loss - 0.5765532181185326, Learning Rate - 0.0001953125, magnitude of gradient - 2.3315806443050713\n",
      "Step - 8003, Loss - 0.7173985567472072, Learning Rate - 0.0001953125, magnitude of gradient - 0.8172504723264996\n",
      "Step - 8004, Loss - 0.5700450735118816, Learning Rate - 0.0001953125, magnitude of gradient - 2.2156723939016896\n",
      "Step - 8005, Loss - 0.6645910225956045, Learning Rate - 0.0001953125, magnitude of gradient - 1.349583261952256\n",
      "Step - 8006, Loss - 0.7083451774362715, Learning Rate - 0.0001953125, magnitude of gradient - 0.8075130635829509\n",
      "Step - 8007, Loss - 0.864818894836716, Learning Rate - 0.0001953125, magnitude of gradient - 0.8300717091846453\n",
      "Step - 8008, Loss - 0.6527355748270951, Learning Rate - 0.0001953125, magnitude of gradient - 0.7220538317081368\n",
      "Step - 8009, Loss - 0.859533671350837, Learning Rate - 0.0001953125, magnitude of gradient - 1.110229429173574\n",
      "Step - 8010, Loss - 0.6866877062407297, Learning Rate - 0.0001953125, magnitude of gradient - 0.8982464804276488\n",
      "Step - 8011, Loss - 0.7356881144603713, Learning Rate - 0.0001953125, magnitude of gradient - 1.319761841778645\n",
      "Step - 8012, Loss - 0.720122254706461, Learning Rate - 0.0001953125, magnitude of gradient - 1.1586975518216711\n",
      "Step - 8013, Loss - 0.676694361866397, Learning Rate - 0.0001953125, magnitude of gradient - 1.1187625221347433\n",
      "Step - 8014, Loss - 0.7303716203981337, Learning Rate - 0.0001953125, magnitude of gradient - 1.0028428290335212\n",
      "Step - 8015, Loss - 0.8147402504202772, Learning Rate - 0.0001953125, magnitude of gradient - 1.3226488811719803\n",
      "Step - 8016, Loss - 0.9686797677221743, Learning Rate - 0.0001953125, magnitude of gradient - 1.381268826974561\n",
      "Step - 8017, Loss - 0.851305656750932, Learning Rate - 0.0001953125, magnitude of gradient - 3.0639717910725413\n",
      "Step - 8018, Loss - 0.7976977203917942, Learning Rate - 0.0001953125, magnitude of gradient - 1.0263507748834086\n",
      "Step - 8019, Loss - 0.6333897779760896, Learning Rate - 0.0001953125, magnitude of gradient - 0.9836925811822524\n",
      "Step - 8020, Loss - 0.5602035799482185, Learning Rate - 0.0001953125, magnitude of gradient - 1.469981242785213\n",
      "Step - 8021, Loss - 0.693690132449849, Learning Rate - 0.0001953125, magnitude of gradient - 0.9856392255687685\n",
      "Step - 8022, Loss - 0.5683587062418719, Learning Rate - 0.0001953125, magnitude of gradient - 1.1629529647889794\n",
      "Step - 8023, Loss - 0.8256732626885986, Learning Rate - 0.0001953125, magnitude of gradient - 2.84415632780333\n",
      "Step - 8024, Loss - 0.6369737291222026, Learning Rate - 0.0001953125, magnitude of gradient - 1.164382467781812\n",
      "Step - 8025, Loss - 0.5189039861046592, Learning Rate - 0.0001953125, magnitude of gradient - 1.5467139407468216\n",
      "Step - 8026, Loss - 0.8193059279472222, Learning Rate - 0.0001953125, magnitude of gradient - 1.6335625236826699\n",
      "Step - 8027, Loss - 0.7655841375033658, Learning Rate - 0.0001953125, magnitude of gradient - 1.1288603636526084\n",
      "Step - 8028, Loss - 0.9025821048810666, Learning Rate - 0.0001953125, magnitude of gradient - 2.0624462062093123\n",
      "Step - 8029, Loss - 0.6180445367131285, Learning Rate - 0.0001953125, magnitude of gradient - 0.8498379371126186\n",
      "Step - 8030, Loss - 0.6007630580505519, Learning Rate - 0.0001953125, magnitude of gradient - 0.986761276413261\n",
      "Step - 8031, Loss - 0.7936886095447359, Learning Rate - 0.0001953125, magnitude of gradient - 1.0145848929188777\n",
      "Step - 8032, Loss - 0.5928574638662654, Learning Rate - 0.0001953125, magnitude of gradient - 1.2075269642176099\n",
      "Step - 8033, Loss - 0.7900827724047927, Learning Rate - 0.0001953125, magnitude of gradient - 1.8898247354749107\n",
      "Step - 8034, Loss - 0.7488930184022591, Learning Rate - 0.0001953125, magnitude of gradient - 0.9833206799688996\n",
      "Step - 8035, Loss - 0.41218218338793794, Learning Rate - 0.0001953125, magnitude of gradient - 1.6149821032525022\n",
      "Step - 8036, Loss - 0.7742477640887658, Learning Rate - 0.0001953125, magnitude of gradient - 1.1112774292456042\n",
      "Step - 8037, Loss - 0.5873656389476464, Learning Rate - 0.0001953125, magnitude of gradient - 1.1790443968580646\n",
      "Step - 8038, Loss - 0.5925181851578095, Learning Rate - 0.0001953125, magnitude of gradient - 1.7822551965402225\n",
      "Step - 8039, Loss - 0.580612104037154, Learning Rate - 0.0001953125, magnitude of gradient - 1.0610740977034785\n",
      "Step - 8040, Loss - 0.8709408859647445, Learning Rate - 0.0001953125, magnitude of gradient - 1.0790021491512858\n",
      "Step - 8041, Loss - 0.625771279782547, Learning Rate - 0.0001953125, magnitude of gradient - 0.5061492599632299\n",
      "Step - 8042, Loss - 0.7542906852000807, Learning Rate - 0.0001953125, magnitude of gradient - 2.278264105969389\n",
      "Step - 8043, Loss - 0.8084666839696696, Learning Rate - 0.0001953125, magnitude of gradient - 2.117336580814734\n",
      "Step - 8044, Loss - 0.7661665245318985, Learning Rate - 0.0001953125, magnitude of gradient - 0.9776299779614529\n",
      "Step - 8045, Loss - 0.6552965727471167, Learning Rate - 0.0001953125, magnitude of gradient - 2.2989304422969465\n",
      "Step - 8046, Loss - 0.6750663674022495, Learning Rate - 0.0001953125, magnitude of gradient - 0.8909354614544334\n",
      "Step - 8047, Loss - 0.5463646884392791, Learning Rate - 0.0001953125, magnitude of gradient - 1.0948636493040518\n",
      "Step - 8048, Loss - 0.6980316718861078, Learning Rate - 0.0001953125, magnitude of gradient - 1.4859899927904474\n",
      "Step - 8049, Loss - 0.7921761353801591, Learning Rate - 0.0001953125, magnitude of gradient - 1.2632696650946\n",
      "Step - 8050, Loss - 0.6372394571954048, Learning Rate - 0.0001953125, magnitude of gradient - 0.6758643206185329\n",
      "Step - 8051, Loss - 0.8985572950092886, Learning Rate - 0.0001953125, magnitude of gradient - 1.0142659706714832\n",
      "Step - 8052, Loss - 0.8829322433935552, Learning Rate - 0.0001953125, magnitude of gradient - 0.8936515430757329\n",
      "Step - 8053, Loss - 0.6404515173096531, Learning Rate - 0.0001953125, magnitude of gradient - 0.6980458376799831\n",
      "Step - 8054, Loss - 0.6668656451264018, Learning Rate - 0.0001953125, magnitude of gradient - 1.078069169198314\n",
      "Step - 8055, Loss - 0.4908385398461433, Learning Rate - 0.0001953125, magnitude of gradient - 1.5278853963351269\n",
      "Step - 8056, Loss - 0.6188499276543621, Learning Rate - 0.0001953125, magnitude of gradient - 0.995125123908107\n",
      "Step - 8057, Loss - 0.862734223634987, Learning Rate - 0.0001953125, magnitude of gradient - 0.3864053576994345\n",
      "Step - 8058, Loss - 0.9378011173106229, Learning Rate - 0.0001953125, magnitude of gradient - 1.7358374360445248\n",
      "Step - 8059, Loss - 0.7655581535323104, Learning Rate - 0.0001953125, magnitude of gradient - 0.5387268127014496\n",
      "Step - 8060, Loss - 0.7815624942460933, Learning Rate - 0.0001953125, magnitude of gradient - 1.5259117752604414\n",
      "Step - 8061, Loss - 0.6911266797011364, Learning Rate - 0.0001953125, magnitude of gradient - 1.416496712833623\n",
      "Step - 8062, Loss - 0.6575020489687969, Learning Rate - 0.0001953125, magnitude of gradient - 1.686677550327014\n",
      "Step - 8063, Loss - 0.7093821440421261, Learning Rate - 0.0001953125, magnitude of gradient - 1.1377523315542792\n",
      "Step - 8064, Loss - 0.6799094168540316, Learning Rate - 0.0001953125, magnitude of gradient - 1.0486491478517537\n",
      "Step - 8065, Loss - 0.5807462779300008, Learning Rate - 0.0001953125, magnitude of gradient - 0.6206020324994537\n",
      "Step - 8066, Loss - 0.7128672653583638, Learning Rate - 0.0001953125, magnitude of gradient - 1.3426134924375535\n",
      "Step - 8067, Loss - 0.6725459831903312, Learning Rate - 0.0001953125, magnitude of gradient - 0.6310167228932599\n",
      "Step - 8068, Loss - 0.6146316182567466, Learning Rate - 0.0001953125, magnitude of gradient - 1.8991908646559923\n",
      "Step - 8069, Loss - 0.5818828432262921, Learning Rate - 0.0001953125, magnitude of gradient - 0.8363260137289734\n",
      "Step - 8070, Loss - 0.5886826694396101, Learning Rate - 0.0001953125, magnitude of gradient - 0.46615326548392166\n",
      "Step - 8071, Loss - 0.7546165323049954, Learning Rate - 0.0001953125, magnitude of gradient - 0.7158249792803826\n",
      "Step - 8072, Loss - 0.8963154593473557, Learning Rate - 0.0001953125, magnitude of gradient - 0.8198603865999589\n",
      "Step - 8073, Loss - 0.5219858445170388, Learning Rate - 0.0001953125, magnitude of gradient - 0.523220779752003\n",
      "Step - 8074, Loss - 0.6934546805549177, Learning Rate - 0.0001953125, magnitude of gradient - 1.7503416341248332\n",
      "Step - 8075, Loss - 0.8610442657545357, Learning Rate - 0.0001953125, magnitude of gradient - 2.481640336134307\n",
      "Step - 8076, Loss - 0.8118077152071043, Learning Rate - 0.0001953125, magnitude of gradient - 1.2639508243853885\n",
      "Step - 8077, Loss - 0.5799890131022802, Learning Rate - 0.0001953125, magnitude of gradient - 1.8304760683816126\n",
      "Step - 8078, Loss - 0.6876329921272952, Learning Rate - 0.0001953125, magnitude of gradient - 1.649350951158185\n",
      "Step - 8079, Loss - 0.7545420538592551, Learning Rate - 0.0001953125, magnitude of gradient - 2.1704275954884253\n",
      "Step - 8080, Loss - 0.5569565371687644, Learning Rate - 0.0001953125, magnitude of gradient - 1.6138798646500123\n",
      "Step - 8081, Loss - 0.8010702895448691, Learning Rate - 0.0001953125, magnitude of gradient - 2.2604962100839323\n",
      "Step - 8082, Loss - 0.6724100417796354, Learning Rate - 0.0001953125, magnitude of gradient - 0.5686065587881908\n",
      "Step - 8083, Loss - 0.6018721717079488, Learning Rate - 0.0001953125, magnitude of gradient - 1.689403946087089\n",
      "Step - 8084, Loss - 0.6890805384578845, Learning Rate - 0.0001953125, magnitude of gradient - 1.1741325043435071\n",
      "Step - 8085, Loss - 0.7093808159107272, Learning Rate - 0.0001953125, magnitude of gradient - 1.162092525937788\n",
      "Step - 8086, Loss - 0.7664172844552564, Learning Rate - 0.0001953125, magnitude of gradient - 1.2546494880143702\n",
      "Step - 8087, Loss - 0.7307314809565062, Learning Rate - 0.0001953125, magnitude of gradient - 1.8615252471992916\n",
      "Step - 8088, Loss - 0.7481304900191864, Learning Rate - 0.0001953125, magnitude of gradient - 1.3077927937372236\n",
      "Step - 8089, Loss - 0.5109688096358207, Learning Rate - 0.0001953125, magnitude of gradient - 0.3163289928831653\n",
      "Step - 8090, Loss - 0.6059895434786785, Learning Rate - 0.0001953125, magnitude of gradient - 1.1682691853216363\n",
      "Step - 8091, Loss - 0.6057735593418063, Learning Rate - 0.0001953125, magnitude of gradient - 0.9049645829882006\n",
      "Step - 8092, Loss - 0.5607263118200969, Learning Rate - 0.0001953125, magnitude of gradient - 1.650892157704622\n",
      "Step - 8093, Loss - 0.7848838346096089, Learning Rate - 0.0001953125, magnitude of gradient - 2.574979928307686\n",
      "Step - 8094, Loss - 0.687782851252912, Learning Rate - 0.0001953125, magnitude of gradient - 1.5192064558926108\n",
      "Step - 8095, Loss - 0.47291424825065864, Learning Rate - 0.0001953125, magnitude of gradient - 2.925720613001173\n",
      "Step - 8096, Loss - 0.9060055347067407, Learning Rate - 0.0001953125, magnitude of gradient - 1.3242794016784587\n",
      "Step - 8097, Loss - 0.7826340376318642, Learning Rate - 0.0001953125, magnitude of gradient - 1.2151912039830965\n",
      "Step - 8098, Loss - 0.6539907793280975, Learning Rate - 0.0001953125, magnitude of gradient - 1.7413521952375253\n",
      "Step - 8099, Loss - 0.5808027527614199, Learning Rate - 0.0001953125, magnitude of gradient - 1.0438263682267759\n",
      "Step - 8100, Loss - 0.7498047653484627, Learning Rate - 0.0001953125, magnitude of gradient - 1.5108841559722084\n",
      "Step - 8101, Loss - 0.7394918341187772, Learning Rate - 0.0001953125, magnitude of gradient - 1.3069889714884977\n",
      "Step - 8102, Loss - 0.7835096905069651, Learning Rate - 0.0001953125, magnitude of gradient - 1.421512022275666\n",
      "Step - 8103, Loss - 0.6914547395448799, Learning Rate - 0.0001953125, magnitude of gradient - 1.4605273550822062\n",
      "Step - 8104, Loss - 0.6854367081905599, Learning Rate - 0.0001953125, magnitude of gradient - 0.9937970734425964\n",
      "Step - 8105, Loss - 0.6167276376964882, Learning Rate - 0.0001953125, magnitude of gradient - 1.2929118343499872\n",
      "Step - 8106, Loss - 0.553637609399908, Learning Rate - 0.0001953125, magnitude of gradient - 2.3329054210352997\n",
      "Step - 8107, Loss - 0.5887761816599802, Learning Rate - 0.0001953125, magnitude of gradient - 1.4563621297930345\n",
      "Step - 8108, Loss - 0.6364044198743375, Learning Rate - 0.0001953125, magnitude of gradient - 0.8215530944385545\n",
      "Step - 8109, Loss - 0.7848505479700194, Learning Rate - 0.0001953125, magnitude of gradient - 1.0496066857297248\n",
      "Step - 8110, Loss - 0.5407458457561832, Learning Rate - 0.0001953125, magnitude of gradient - 0.46042668223088423\n",
      "Step - 8111, Loss - 0.5922913314978434, Learning Rate - 0.0001953125, magnitude of gradient - 0.8273640758966683\n",
      "Step - 8112, Loss - 0.5777860493464446, Learning Rate - 0.0001953125, magnitude of gradient - 2.094691317317401\n",
      "Step - 8113, Loss - 0.7379300353492494, Learning Rate - 0.0001953125, magnitude of gradient - 1.3165537863421326\n",
      "Step - 8114, Loss - 0.6702329140172301, Learning Rate - 0.0001953125, magnitude of gradient - 1.19339664619447\n",
      "Step - 8115, Loss - 0.7378201425267961, Learning Rate - 0.0001953125, magnitude of gradient - 0.7444764230277776\n",
      "Step - 8116, Loss - 0.556435804361231, Learning Rate - 0.0001953125, magnitude of gradient - 2.321475211681348\n",
      "Step - 8117, Loss - 0.7617996010287238, Learning Rate - 0.0001953125, magnitude of gradient - 1.423328992909961\n",
      "Step - 8118, Loss - 0.7157115802780962, Learning Rate - 0.0001953125, magnitude of gradient - 0.5683308744496838\n",
      "Step - 8119, Loss - 0.6387742179651742, Learning Rate - 0.0001953125, magnitude of gradient - 1.4295979244874333\n",
      "Step - 8120, Loss - 0.6894669771773468, Learning Rate - 0.0001953125, magnitude of gradient - 1.7385561828257634\n",
      "Step - 8121, Loss - 0.6121747480349427, Learning Rate - 0.0001953125, magnitude of gradient - 0.6160509353358374\n",
      "Step - 8122, Loss - 0.7576298723590462, Learning Rate - 0.0001953125, magnitude of gradient - 0.6348710330683227\n",
      "Step - 8123, Loss - 0.7696289084900667, Learning Rate - 0.0001953125, magnitude of gradient - 1.498672821705076\n",
      "Step - 8124, Loss - 0.7755770730042063, Learning Rate - 0.0001953125, magnitude of gradient - 0.7746890531935219\n",
      "Step - 8125, Loss - 0.7673736227864412, Learning Rate - 0.0001953125, magnitude of gradient - 0.9217007070778397\n",
      "Step - 8126, Loss - 0.7269610546079575, Learning Rate - 0.0001953125, magnitude of gradient - 0.8302966399359486\n",
      "Step - 8127, Loss - 0.5094111113784842, Learning Rate - 0.0001953125, magnitude of gradient - 0.7482207319250538\n",
      "Step - 8128, Loss - 0.48333910812340725, Learning Rate - 0.0001953125, magnitude of gradient - 0.5923417690008156\n",
      "Step - 8129, Loss - 0.6122877253521687, Learning Rate - 0.0001953125, magnitude of gradient - 0.9236928946771389\n",
      "Step - 8130, Loss - 0.7584766572860935, Learning Rate - 0.0001953125, magnitude of gradient - 1.0169347436868366\n",
      "Step - 8131, Loss - 0.5112601924506692, Learning Rate - 0.0001953125, magnitude of gradient - 2.068639580954208\n",
      "Step - 8132, Loss - 0.6167785119518636, Learning Rate - 0.0001953125, magnitude of gradient - 1.9723815968200167\n",
      "Step - 8133, Loss - 0.7759835051062243, Learning Rate - 0.0001953125, magnitude of gradient - 1.1880106445354246\n",
      "Step - 8134, Loss - 0.698060514646008, Learning Rate - 0.0001953125, magnitude of gradient - 1.546082807730646\n",
      "Step - 8135, Loss - 0.6707910485821782, Learning Rate - 0.0001953125, magnitude of gradient - 2.894285216536419\n",
      "Step - 8136, Loss - 0.6881261477572617, Learning Rate - 0.0001953125, magnitude of gradient - 1.4100583138292568\n",
      "Step - 8137, Loss - 0.863873565720213, Learning Rate - 0.0001953125, magnitude of gradient - 1.6875814245058876\n",
      "Step - 8138, Loss - 0.6520200463094459, Learning Rate - 0.0001953125, magnitude of gradient - 1.417121973628497\n",
      "Step - 8139, Loss - 0.5283911175926029, Learning Rate - 0.0001953125, magnitude of gradient - 1.092555549893725\n",
      "Step - 8140, Loss - 0.7409767246555221, Learning Rate - 0.0001953125, magnitude of gradient - 0.8565439118853636\n",
      "Step - 8141, Loss - 0.6084289284958979, Learning Rate - 0.0001953125, magnitude of gradient - 0.8490817347733245\n",
      "Step - 8142, Loss - 0.5624469430235155, Learning Rate - 0.0001953125, magnitude of gradient - 1.0241419181709825\n",
      "Step - 8143, Loss - 0.644354088309605, Learning Rate - 0.0001953125, magnitude of gradient - 0.541670908154355\n",
      "Step - 8144, Loss - 0.5810898826720658, Learning Rate - 0.0001953125, magnitude of gradient - 1.1801520005029817\n",
      "Step - 8145, Loss - 0.7656411793295692, Learning Rate - 0.0001953125, magnitude of gradient - 0.6443523583623578\n",
      "Step - 8146, Loss - 0.9105545837132383, Learning Rate - 0.0001953125, magnitude of gradient - 1.8989443821603609\n",
      "Step - 8147, Loss - 0.6721792877997452, Learning Rate - 0.0001953125, magnitude of gradient - 1.5837495639848715\n",
      "Step - 8148, Loss - 0.7337450790696014, Learning Rate - 0.0001953125, magnitude of gradient - 1.4237332357503445\n",
      "Step - 8149, Loss - 0.7966757588144716, Learning Rate - 0.0001953125, magnitude of gradient - 2.014755642568273\n",
      "Step - 8150, Loss - 0.716183587379324, Learning Rate - 0.0001953125, magnitude of gradient - 1.1591097598955609\n",
      "Step - 8151, Loss - 0.6518683733672783, Learning Rate - 0.0001953125, magnitude of gradient - 1.3534536945325297\n",
      "Step - 8152, Loss - 0.6707091677158638, Learning Rate - 0.0001953125, magnitude of gradient - 0.5401387599513734\n",
      "Step - 8153, Loss - 0.672855955067595, Learning Rate - 0.0001953125, magnitude of gradient - 2.107093095065214\n",
      "Step - 8154, Loss - 0.6118897526673682, Learning Rate - 0.0001953125, magnitude of gradient - 1.9293046047508486\n",
      "Step - 8155, Loss - 0.6300313238271136, Learning Rate - 0.0001953125, magnitude of gradient - 0.993528782886202\n",
      "Step - 8156, Loss - 0.7135097729010573, Learning Rate - 0.0001953125, magnitude of gradient - 2.016874882425407\n",
      "Step - 8157, Loss - 0.77833503335814, Learning Rate - 0.0001953125, magnitude of gradient - 1.3690289913090739\n",
      "Step - 8158, Loss - 0.6740359584499473, Learning Rate - 0.0001953125, magnitude of gradient - 0.43965655142886795\n",
      "Step - 8159, Loss - 0.6682134422327363, Learning Rate - 0.0001953125, magnitude of gradient - 0.8170493361906068\n",
      "Step - 8160, Loss - 0.7034584315654843, Learning Rate - 0.0001953125, magnitude of gradient - 0.9719214510025501\n",
      "Step - 8161, Loss - 0.6482358715522702, Learning Rate - 0.0001953125, magnitude of gradient - 0.30846328020494135\n",
      "Step - 8162, Loss - 0.6161417432658838, Learning Rate - 0.0001953125, magnitude of gradient - 0.38337672536828565\n",
      "Step - 8163, Loss - 0.5987245893126738, Learning Rate - 0.0001953125, magnitude of gradient - 1.503065647877776\n",
      "Step - 8164, Loss - 0.731768310203341, Learning Rate - 0.0001953125, magnitude of gradient - 0.30190875331912126\n",
      "Step - 8165, Loss - 0.6506036715130578, Learning Rate - 0.0001953125, magnitude of gradient - 0.3006804033061756\n",
      "Step - 8166, Loss - 0.9141701833545594, Learning Rate - 0.0001953125, magnitude of gradient - 2.2913271004437554\n",
      "Step - 8167, Loss - 0.7491629337982346, Learning Rate - 0.0001953125, magnitude of gradient - 1.6856054156738656\n",
      "Step - 8168, Loss - 0.6772277787690351, Learning Rate - 0.0001953125, magnitude of gradient - 1.227573900540328\n",
      "Step - 8169, Loss - 0.6383480311116826, Learning Rate - 0.0001953125, magnitude of gradient - 1.8527785478676055\n",
      "Step - 8170, Loss - 0.7096580034266631, Learning Rate - 0.0001953125, magnitude of gradient - 2.610816684404747\n",
      "Step - 8171, Loss - 0.6941853387306269, Learning Rate - 0.0001953125, magnitude of gradient - 1.3942382948697443\n",
      "Step - 8172, Loss - 0.9908075443120011, Learning Rate - 0.0001953125, magnitude of gradient - 2.5163548844195858\n",
      "Step - 8173, Loss - 0.7889216009286151, Learning Rate - 0.0001953125, magnitude of gradient - 1.0241740491535412\n",
      "Step - 8174, Loss - 0.7018839957349322, Learning Rate - 0.0001953125, magnitude of gradient - 1.2583278032124514\n",
      "Step - 8175, Loss - 0.8160818253298845, Learning Rate - 0.0001953125, magnitude of gradient - 1.9542770848692077\n",
      "Step - 8176, Loss - 0.7820610405858962, Learning Rate - 0.0001953125, magnitude of gradient - 1.1736518718796936\n",
      "Step - 8177, Loss - 0.9210820457200695, Learning Rate - 0.0001953125, magnitude of gradient - 1.075825246427914\n",
      "Step - 8178, Loss - 0.5105699346072706, Learning Rate - 0.0001953125, magnitude of gradient - 1.8873053523369276\n",
      "Step - 8179, Loss - 0.9200064895786231, Learning Rate - 0.0001953125, magnitude of gradient - 1.9344549191901206\n",
      "Step - 8180, Loss - 0.7068893491997815, Learning Rate - 0.0001953125, magnitude of gradient - 0.9235543972388577\n",
      "Step - 8181, Loss - 0.8712522192557254, Learning Rate - 0.0001953125, magnitude of gradient - 0.9396883436780642\n",
      "Step - 8182, Loss - 0.78302527916681, Learning Rate - 0.0001953125, magnitude of gradient - 1.9091210417412907\n",
      "Step - 8183, Loss - 0.616776592028479, Learning Rate - 0.0001953125, magnitude of gradient - 1.3185121868836058\n",
      "Step - 8184, Loss - 0.7686802244023068, Learning Rate - 0.0001953125, magnitude of gradient - 0.922743888620767\n",
      "Step - 8185, Loss - 0.7283938957921922, Learning Rate - 0.0001953125, magnitude of gradient - 1.2705238513598613\n",
      "Step - 8186, Loss - 0.6224648356233985, Learning Rate - 0.0001953125, magnitude of gradient - 4.099437765029538\n",
      "Step - 8187, Loss - 0.563123374515607, Learning Rate - 0.0001953125, magnitude of gradient - 0.3729634674991761\n",
      "Step - 8188, Loss - 0.8746569526393043, Learning Rate - 0.0001953125, magnitude of gradient - 1.065569916684498\n",
      "Step - 8189, Loss - 0.7722143673070717, Learning Rate - 0.0001953125, magnitude of gradient - 1.9226577001779463\n",
      "Step - 8190, Loss - 0.5543317900811817, Learning Rate - 0.0001953125, magnitude of gradient - 1.3746792482314993\n",
      "Step - 8191, Loss - 0.8051364209620522, Learning Rate - 0.0001953125, magnitude of gradient - 2.2756711912846592\n",
      "Step - 8192, Loss - 0.7056983249047306, Learning Rate - 0.0001953125, magnitude of gradient - 1.0740631645610124\n",
      "Step - 8193, Loss - 0.7360792356021351, Learning Rate - 0.0001953125, magnitude of gradient - 0.8725939979467305\n",
      "Step - 8194, Loss - 0.7451104758405162, Learning Rate - 0.0001953125, magnitude of gradient - 0.4409458096143383\n",
      "Step - 8195, Loss - 0.726740527531736, Learning Rate - 0.0001953125, magnitude of gradient - 1.716295790156372\n",
      "Step - 8196, Loss - 0.8734664768855142, Learning Rate - 0.0001953125, magnitude of gradient - 1.4731584315735513\n",
      "Step - 8197, Loss - 0.7484665470917301, Learning Rate - 0.0001953125, magnitude of gradient - 0.6873335437529248\n",
      "Step - 8198, Loss - 0.5795179646429641, Learning Rate - 0.0001953125, magnitude of gradient - 1.5909243689991046\n",
      "Step - 8199, Loss - 0.8055149424307522, Learning Rate - 0.0001953125, magnitude of gradient - 1.2564789499443787\n",
      "Step - 8200, Loss - 0.7306707518974711, Learning Rate - 0.0001953125, magnitude of gradient - 0.44918495769245587\n",
      "Step - 8201, Loss - 1.0041294341339948, Learning Rate - 0.0001953125, magnitude of gradient - 1.13669907166451\n",
      "Step - 8202, Loss - 0.6683897588670806, Learning Rate - 0.0001953125, magnitude of gradient - 0.38569852190939846\n",
      "Step - 8203, Loss - 0.7171529770818496, Learning Rate - 0.0001953125, magnitude of gradient - 1.3379566756356505\n",
      "Step - 8204, Loss - 0.7072027140336796, Learning Rate - 0.0001953125, magnitude of gradient - 0.5761438250886518\n",
      "Step - 8205, Loss - 0.612211304387746, Learning Rate - 0.0001953125, magnitude of gradient - 0.5377308310073032\n",
      "Step - 8206, Loss - 0.8619401475446147, Learning Rate - 0.0001953125, magnitude of gradient - 0.8121267203899661\n",
      "Step - 8207, Loss - 0.5949865290972368, Learning Rate - 0.0001953125, magnitude of gradient - 0.8722542079842164\n",
      "Step - 8208, Loss - 0.5659830329612058, Learning Rate - 0.0001953125, magnitude of gradient - 1.1138334381835784\n",
      "Step - 8209, Loss - 0.7093958700900467, Learning Rate - 0.0001953125, magnitude of gradient - 1.6233984490038278\n",
      "Step - 8210, Loss - 0.6414761231110421, Learning Rate - 0.0001953125, magnitude of gradient - 1.0172121872082238\n",
      "Step - 8211, Loss - 0.6538833167094632, Learning Rate - 0.0001953125, magnitude of gradient - 0.9879093439527187\n",
      "Step - 8212, Loss - 0.7370938229618275, Learning Rate - 0.0001953125, magnitude of gradient - 1.4395092030755894\n",
      "Step - 8213, Loss - 0.619413347964056, Learning Rate - 0.0001953125, magnitude of gradient - 1.3411367860122447\n",
      "Step - 8214, Loss - 0.6630470722020154, Learning Rate - 0.0001953125, magnitude of gradient - 1.3142956966427255\n",
      "Step - 8215, Loss - 0.5657988459842477, Learning Rate - 0.0001953125, magnitude of gradient - 1.1916666925583221\n",
      "Step - 8216, Loss - 0.5975214108802883, Learning Rate - 0.0001953125, magnitude of gradient - 0.3348347294013659\n",
      "Step - 8217, Loss - 0.5387459819790213, Learning Rate - 0.0001953125, magnitude of gradient - 1.5816437534202976\n",
      "Step - 8218, Loss - 0.7978830196620733, Learning Rate - 0.0001953125, magnitude of gradient - 1.1179077405511957\n",
      "Step - 8219, Loss - 0.48654271468800275, Learning Rate - 0.0001953125, magnitude of gradient - 1.647281903853887\n",
      "Step - 8220, Loss - 0.7011840416861806, Learning Rate - 0.0001953125, magnitude of gradient - 1.0611122812681473\n",
      "Step - 8221, Loss - 0.6443810980002895, Learning Rate - 0.0001953125, magnitude of gradient - 0.7367992358379752\n",
      "Step - 8222, Loss - 0.6002683031103649, Learning Rate - 0.0001953125, magnitude of gradient - 1.2141426165417641\n",
      "Step - 8223, Loss - 0.6816938116017125, Learning Rate - 0.0001953125, magnitude of gradient - 0.9154640560138567\n",
      "Step - 8224, Loss - 0.6139726023330122, Learning Rate - 0.0001953125, magnitude of gradient - 1.3566971851986172\n",
      "Step - 8225, Loss - 0.7593628658453312, Learning Rate - 0.0001953125, magnitude of gradient - 1.2080148817016152\n",
      "Step - 8226, Loss - 0.8015460410067452, Learning Rate - 0.0001953125, magnitude of gradient - 0.8029645745248996\n",
      "Step - 8227, Loss - 0.7085303045806788, Learning Rate - 0.0001953125, magnitude of gradient - 1.404859581301683\n",
      "Step - 8228, Loss - 0.5858982663848361, Learning Rate - 0.0001953125, magnitude of gradient - 0.8430560672444166\n",
      "Step - 8229, Loss - 0.5475067832300029, Learning Rate - 0.0001953125, magnitude of gradient - 0.49541848943596944\n",
      "Step - 8230, Loss - 0.6543597037430781, Learning Rate - 0.0001953125, magnitude of gradient - 1.0747232943792817\n",
      "Step - 8231, Loss - 0.6042332177655901, Learning Rate - 0.0001953125, magnitude of gradient - 1.2305646702413087\n",
      "Step - 8232, Loss - 0.6332003147968469, Learning Rate - 0.0001953125, magnitude of gradient - 1.2205888868444816\n",
      "Step - 8233, Loss - 0.8967222906210299, Learning Rate - 0.0001953125, magnitude of gradient - 1.370580958928286\n",
      "Step - 8234, Loss - 0.6754467644379082, Learning Rate - 0.0001953125, magnitude of gradient - 0.5684517411896285\n",
      "Step - 8235, Loss - 0.7195550211380519, Learning Rate - 0.0001953125, magnitude of gradient - 1.623027765647476\n",
      "Step - 8236, Loss - 0.7948299447203429, Learning Rate - 0.0001953125, magnitude of gradient - 1.3771060402755266\n",
      "Step - 8237, Loss - 0.9259010295234069, Learning Rate - 0.0001953125, magnitude of gradient - 1.0153761553759262\n",
      "Step - 8238, Loss - 0.7650499786549625, Learning Rate - 0.0001953125, magnitude of gradient - 0.9518537002326198\n",
      "Step - 8239, Loss - 0.7954527024148624, Learning Rate - 0.0001953125, magnitude of gradient - 0.5189929585266045\n",
      "Step - 8240, Loss - 0.7217095442874265, Learning Rate - 0.0001953125, magnitude of gradient - 1.3853700361026395\n",
      "Step - 8241, Loss - 0.764624523631836, Learning Rate - 0.0001953125, magnitude of gradient - 0.42440180194558763\n",
      "Step - 8242, Loss - 0.6958394757933242, Learning Rate - 0.0001953125, magnitude of gradient - 1.637908283093509\n",
      "Step - 8243, Loss - 0.6924140341528149, Learning Rate - 0.0001953125, magnitude of gradient - 1.5727973065658511\n",
      "Step - 8244, Loss - 0.8195347415811024, Learning Rate - 0.0001953125, magnitude of gradient - 0.763413975818411\n",
      "Step - 8245, Loss - 0.5313454713699232, Learning Rate - 0.0001953125, magnitude of gradient - 0.8494627805459108\n",
      "Step - 8246, Loss - 0.698213020644392, Learning Rate - 0.0001953125, magnitude of gradient - 1.3342270240601817\n",
      "Step - 8247, Loss - 0.699656418744829, Learning Rate - 0.0001953125, magnitude of gradient - 1.9561761685190593\n",
      "Step - 8248, Loss - 0.6130408747276159, Learning Rate - 0.0001953125, magnitude of gradient - 0.9059046778914378\n",
      "Step - 8249, Loss - 0.6039155912028591, Learning Rate - 0.0001953125, magnitude of gradient - 1.2719554131803519\n",
      "Step - 8250, Loss - 0.6694964600754141, Learning Rate - 0.0001953125, magnitude of gradient - 1.9224276970384888\n",
      "Step - 8251, Loss - 0.7772484127511214, Learning Rate - 0.0001953125, magnitude of gradient - 3.472607476891185\n",
      "Step - 8252, Loss - 0.573637673366686, Learning Rate - 0.0001953125, magnitude of gradient - 1.03893465300974\n",
      "Step - 8253, Loss - 0.7198762400493879, Learning Rate - 0.0001953125, magnitude of gradient - 1.207472464293583\n",
      "Step - 8254, Loss - 0.764495219889096, Learning Rate - 0.0001953125, magnitude of gradient - 0.4457564179967071\n",
      "Step - 8255, Loss - 0.6481888287331921, Learning Rate - 0.0001953125, magnitude of gradient - 1.4242165459094471\n",
      "Step - 8256, Loss - 0.7050398654612631, Learning Rate - 0.0001953125, magnitude of gradient - 1.2383084503490944\n",
      "Step - 8257, Loss - 0.6584488906203637, Learning Rate - 0.0001953125, magnitude of gradient - 1.2170723781316355\n",
      "Step - 8258, Loss - 0.5491389203518546, Learning Rate - 0.0001953125, magnitude of gradient - 2.9028310726125697\n",
      "Step - 8259, Loss - 0.7219770046175965, Learning Rate - 0.0001953125, magnitude of gradient - 0.7983719400838735\n",
      "Step - 8260, Loss - 0.7386468340425411, Learning Rate - 0.0001953125, magnitude of gradient - 0.7695699366002289\n",
      "Step - 8261, Loss - 0.6965429766945876, Learning Rate - 0.0001953125, magnitude of gradient - 1.1609281716951025\n",
      "Step - 8262, Loss - 0.7344075549086649, Learning Rate - 0.0001953125, magnitude of gradient - 0.22649079831433347\n",
      "Step - 8263, Loss - 0.6454334095322856, Learning Rate - 0.0001953125, magnitude of gradient - 1.870528005939816\n",
      "Step - 8264, Loss - 0.8079925585753163, Learning Rate - 0.0001953125, magnitude of gradient - 1.5267432257245859\n",
      "Step - 8265, Loss - 0.6291825694809317, Learning Rate - 0.0001953125, magnitude of gradient - 1.0310641229044257\n",
      "Step - 8266, Loss - 0.6435263338214708, Learning Rate - 0.0001953125, magnitude of gradient - 0.47345815392419566\n",
      "Step - 8267, Loss - 0.6299151608479577, Learning Rate - 0.0001953125, magnitude of gradient - 0.4251383190899478\n",
      "Step - 8268, Loss - 0.7113656369360649, Learning Rate - 0.0001953125, magnitude of gradient - 1.01806727010445\n",
      "Step - 8269, Loss - 0.6961904288241196, Learning Rate - 0.0001953125, magnitude of gradient - 2.336534207591491\n",
      "Step - 8270, Loss - 0.7274487846023907, Learning Rate - 0.0001953125, magnitude of gradient - 1.2460591799717509\n",
      "Step - 8271, Loss - 0.7305952578715426, Learning Rate - 0.0001953125, magnitude of gradient - 1.8206998109295958\n",
      "Step - 8272, Loss - 0.6485471124633796, Learning Rate - 0.0001953125, magnitude of gradient - 1.8123582964539515\n",
      "Step - 8273, Loss - 0.7230446200410243, Learning Rate - 0.0001953125, magnitude of gradient - 1.3304749136790472\n",
      "Step - 8274, Loss - 0.863137953085657, Learning Rate - 0.0001953125, magnitude of gradient - 0.9050970640322691\n",
      "Step - 8275, Loss - 0.7183425615834056, Learning Rate - 0.0001953125, magnitude of gradient - 3.05364366665271\n",
      "Step - 8276, Loss - 0.7125823079674141, Learning Rate - 0.0001953125, magnitude of gradient - 0.95273256332371\n",
      "Step - 8277, Loss - 0.7871319987371613, Learning Rate - 0.0001953125, magnitude of gradient - 2.3148053688933086\n",
      "Step - 8278, Loss - 0.7035315215120592, Learning Rate - 0.0001953125, magnitude of gradient - 0.9885182613369746\n",
      "Step - 8279, Loss - 0.7622696943617242, Learning Rate - 0.0001953125, magnitude of gradient - 1.789137269036281\n",
      "Step - 8280, Loss - 0.618571126815922, Learning Rate - 0.0001953125, magnitude of gradient - 0.6876414970689128\n",
      "Step - 8281, Loss - 0.7214890487989533, Learning Rate - 0.0001953125, magnitude of gradient - 1.3233355421828406\n",
      "Step - 8282, Loss - 0.5566545903867463, Learning Rate - 0.0001953125, magnitude of gradient - 2.2119334844953813\n",
      "Step - 8283, Loss - 0.9798093543886105, Learning Rate - 0.0001953125, magnitude of gradient - 1.186739212189283\n",
      "Step - 8284, Loss - 0.5079397428031465, Learning Rate - 0.0001953125, magnitude of gradient - 1.958820821158667\n",
      "Step - 8285, Loss - 0.7885370569189527, Learning Rate - 0.0001953125, magnitude of gradient - 1.2625012497919863\n",
      "Step - 8286, Loss - 0.7406404318738495, Learning Rate - 0.0001953125, magnitude of gradient - 0.6616682281714985\n",
      "Step - 8287, Loss - 0.6096124743755102, Learning Rate - 0.0001953125, magnitude of gradient - 1.3269483350112523\n",
      "Step - 8288, Loss - 0.7381621892073786, Learning Rate - 0.0001953125, magnitude of gradient - 1.6806418735022746\n",
      "Step - 8289, Loss - 0.6415734330247044, Learning Rate - 0.0001953125, magnitude of gradient - 0.9268522909626418\n",
      "Step - 8290, Loss - 0.7078228118312462, Learning Rate - 0.0001953125, magnitude of gradient - 0.6521844469020541\n",
      "Step - 8291, Loss - 0.6479940180309955, Learning Rate - 0.0001953125, magnitude of gradient - 1.110564743540062\n",
      "Step - 8292, Loss - 0.4988947158959749, Learning Rate - 0.0001953125, magnitude of gradient - 0.6979472466032955\n",
      "Step - 8293, Loss - 0.641754138415838, Learning Rate - 0.0001953125, magnitude of gradient - 1.7385099122951087\n",
      "Step - 8294, Loss - 0.7205546994673785, Learning Rate - 0.0001953125, magnitude of gradient - 1.17915320243722\n",
      "Step - 8295, Loss - 0.7818069093057802, Learning Rate - 0.0001953125, magnitude of gradient - 1.9696387316675832\n",
      "Step - 8296, Loss - 0.7520524512547103, Learning Rate - 0.0001953125, magnitude of gradient - 1.3866031943178914\n",
      "Step - 8297, Loss - 0.6736879023049661, Learning Rate - 0.0001953125, magnitude of gradient - 1.17743845993366\n",
      "Step - 8298, Loss - 0.7482808456946695, Learning Rate - 0.0001953125, magnitude of gradient - 0.9905276199313233\n",
      "Step - 8299, Loss - 0.7257198815809413, Learning Rate - 0.0001953125, magnitude of gradient - 1.3353373189746705\n",
      "Step - 8300, Loss - 0.6259041449895476, Learning Rate - 0.0001953125, magnitude of gradient - 1.429997009910552\n",
      "Step - 8301, Loss - 0.6313898933123459, Learning Rate - 0.0001953125, magnitude of gradient - 1.5918048050727522\n",
      "Step - 8302, Loss - 0.756805586181718, Learning Rate - 0.0001953125, magnitude of gradient - 1.2572165386636005\n",
      "Step - 8303, Loss - 0.5380540982874824, Learning Rate - 0.0001953125, magnitude of gradient - 0.49898615755897474\n",
      "Step - 8304, Loss - 0.6504761355982812, Learning Rate - 0.0001953125, magnitude of gradient - 0.7443946314159656\n",
      "Step - 8305, Loss - 0.8366114941742623, Learning Rate - 0.0001953125, magnitude of gradient - 1.7481064641748278\n",
      "Step - 8306, Loss - 0.6994299813895514, Learning Rate - 0.0001953125, magnitude of gradient - 1.3703750520749811\n",
      "Step - 8307, Loss - 0.9051388855876041, Learning Rate - 0.0001953125, magnitude of gradient - 0.6527310305757609\n",
      "Step - 8308, Loss - 0.6215989768119898, Learning Rate - 0.0001953125, magnitude of gradient - 1.1105470287400998\n",
      "Step - 8309, Loss - 0.8802132353502565, Learning Rate - 0.0001953125, magnitude of gradient - 0.4133460296163517\n",
      "Step - 8310, Loss - 0.7403203828166475, Learning Rate - 0.0001953125, magnitude of gradient - 0.4947939835627112\n",
      "Step - 8311, Loss - 0.7074319035343837, Learning Rate - 0.0001953125, magnitude of gradient - 0.6921790063318636\n",
      "Step - 8312, Loss - 0.756499233881802, Learning Rate - 0.0001953125, magnitude of gradient - 0.6364815794967869\n",
      "Step - 8313, Loss - 1.0000282849058344, Learning Rate - 0.0001953125, magnitude of gradient - 1.2305930991856848\n",
      "Step - 8314, Loss - 0.7963160149673572, Learning Rate - 0.0001953125, magnitude of gradient - 1.3162559253861101\n",
      "Step - 8315, Loss - 0.8086458581386868, Learning Rate - 0.0001953125, magnitude of gradient - 2.6418767913959518\n",
      "Step - 8316, Loss - 1.2181388114763374, Learning Rate - 0.0001953125, magnitude of gradient - 1.9831117154668847\n",
      "Step - 8317, Loss - 0.5929304831192312, Learning Rate - 0.0001953125, magnitude of gradient - 1.3074054260687367\n",
      "Step - 8318, Loss - 0.680673333659689, Learning Rate - 0.0001953125, magnitude of gradient - 1.510060831628636\n",
      "Step - 8319, Loss - 0.7134328773794268, Learning Rate - 0.0001953125, magnitude of gradient - 1.3485609466094406\n",
      "Step - 8320, Loss - 0.7718891818149332, Learning Rate - 0.0001953125, magnitude of gradient - 2.095026070980051\n",
      "Step - 8321, Loss - 0.5320926460666602, Learning Rate - 0.0001953125, magnitude of gradient - 1.7091358086941402\n",
      "Step - 8322, Loss - 0.7068471868575861, Learning Rate - 0.0001953125, magnitude of gradient - 0.34635654273303706\n",
      "Step - 8323, Loss - 0.9513027965450158, Learning Rate - 0.0001953125, magnitude of gradient - 1.9055500591978967\n",
      "Step - 8324, Loss - 0.6834364087197486, Learning Rate - 0.0001953125, magnitude of gradient - 1.3968377753216785\n",
      "Step - 8325, Loss - 0.5086052787596662, Learning Rate - 0.0001953125, magnitude of gradient - 1.5938398623530174\n",
      "Step - 8326, Loss - 0.590376448000615, Learning Rate - 0.0001953125, magnitude of gradient - 0.7571301515582501\n",
      "Step - 8327, Loss - 0.5715077098411979, Learning Rate - 0.0001953125, magnitude of gradient - 1.3737452876321676\n",
      "Step - 8328, Loss - 0.6586799821089169, Learning Rate - 0.0001953125, magnitude of gradient - 1.1622488538220954\n",
      "Step - 8329, Loss - 0.7799742531414309, Learning Rate - 0.0001953125, magnitude of gradient - 1.2148293317384662\n",
      "Step - 8330, Loss - 0.6426560728337696, Learning Rate - 0.0001953125, magnitude of gradient - 1.7512479019820146\n",
      "Step - 8331, Loss - 0.7374381880260741, Learning Rate - 0.0001953125, magnitude of gradient - 1.1007135356390378\n",
      "Step - 8332, Loss - 0.6015555425303275, Learning Rate - 0.0001953125, magnitude of gradient - 1.3761949356607024\n",
      "Step - 8333, Loss - 0.6738954192724402, Learning Rate - 0.0001953125, magnitude of gradient - 2.824649570934033\n",
      "Step - 8334, Loss - 0.558745107791111, Learning Rate - 0.0001953125, magnitude of gradient - 0.923097111430723\n",
      "Step - 8335, Loss - 0.5533530578356248, Learning Rate - 0.0001953125, magnitude of gradient - 2.5330546977700217\n",
      "Step - 8336, Loss - 0.6375101572600116, Learning Rate - 0.0001953125, magnitude of gradient - 2.1447081114579953\n",
      "Step - 8337, Loss - 0.7338507901190241, Learning Rate - 0.0001953125, magnitude of gradient - 1.1329273710052234\n",
      "Step - 8338, Loss - 0.8120021999528348, Learning Rate - 0.0001953125, magnitude of gradient - 1.3473601857979087\n",
      "Step - 8339, Loss - 0.808254000177746, Learning Rate - 0.0001953125, magnitude of gradient - 1.4596885564216775\n",
      "Step - 8340, Loss - 0.835286866019203, Learning Rate - 0.0001953125, magnitude of gradient - 0.7758384698313848\n",
      "Step - 8341, Loss - 0.5402777452646402, Learning Rate - 0.0001953125, magnitude of gradient - 1.407285227479045\n",
      "Step - 8342, Loss - 0.7187587971763874, Learning Rate - 0.0001953125, magnitude of gradient - 1.9556277491538048\n",
      "Step - 8343, Loss - 0.5631223694432299, Learning Rate - 0.0001953125, magnitude of gradient - 1.8984272243640152\n",
      "Step - 8344, Loss - 0.7188947198745461, Learning Rate - 0.0001953125, magnitude of gradient - 1.3715877677359622\n",
      "Step - 8345, Loss - 0.6219216543331799, Learning Rate - 0.0001953125, magnitude of gradient - 0.7407588414906191\n",
      "Step - 8346, Loss - 0.675151522199511, Learning Rate - 0.0001953125, magnitude of gradient - 2.138883494278772\n",
      "Step - 8347, Loss - 0.6439540989634545, Learning Rate - 0.0001953125, magnitude of gradient - 0.9905072566496818\n",
      "Step - 8348, Loss - 0.7848865239390604, Learning Rate - 0.0001953125, magnitude of gradient - 2.1827970091511952\n",
      "Step - 8349, Loss - 0.7043316700580741, Learning Rate - 0.0001953125, magnitude of gradient - 1.248661461905225\n",
      "Step - 8350, Loss - 0.5906597179601683, Learning Rate - 0.0001953125, magnitude of gradient - 1.3379918136607802\n",
      "Step - 8351, Loss - 0.7094042375641516, Learning Rate - 0.0001953125, magnitude of gradient - 2.3904158321836606\n",
      "Step - 8352, Loss - 0.6460686236332638, Learning Rate - 0.0001953125, magnitude of gradient - 0.8615018337926794\n",
      "Step - 8353, Loss - 0.7145253402450688, Learning Rate - 0.0001953125, magnitude of gradient - 0.9022522264834422\n",
      "Step - 8354, Loss - 0.6069782906129594, Learning Rate - 0.0001953125, magnitude of gradient - 1.1277241053179907\n",
      "Step - 8355, Loss - 0.619494600676028, Learning Rate - 0.0001953125, magnitude of gradient - 1.6032909304876068\n",
      "Step - 8356, Loss - 0.5283970699813952, Learning Rate - 0.0001953125, magnitude of gradient - 0.8036892119900145\n",
      "Step - 8357, Loss - 0.8520651026007715, Learning Rate - 0.0001953125, magnitude of gradient - 1.7766874167717202\n",
      "Step - 8358, Loss - 0.6315790831238806, Learning Rate - 0.0001953125, magnitude of gradient - 2.059434579702784\n",
      "Step - 8359, Loss - 0.6393776732857654, Learning Rate - 0.0001953125, magnitude of gradient - 1.0893973103688983\n",
      "Step - 8360, Loss - 0.7689988467490787, Learning Rate - 0.0001953125, magnitude of gradient - 1.5676786274421655\n",
      "Step - 8361, Loss - 0.6260458832804164, Learning Rate - 0.0001953125, magnitude of gradient - 0.8547216367123791\n",
      "Step - 8362, Loss - 0.7127871516041379, Learning Rate - 0.0001953125, magnitude of gradient - 1.5552484728853126\n",
      "Step - 8363, Loss - 0.9254924711988847, Learning Rate - 0.0001953125, magnitude of gradient - 1.8137879114955755\n",
      "Step - 8364, Loss - 0.6033427656655693, Learning Rate - 0.0001953125, magnitude of gradient - 0.45039550208082535\n",
      "Step - 8365, Loss - 0.9636330181985736, Learning Rate - 0.0001953125, magnitude of gradient - 1.108841210590645\n",
      "Step - 8366, Loss - 0.4669320362435736, Learning Rate - 0.0001953125, magnitude of gradient - 0.692758269097193\n",
      "Step - 8367, Loss - 0.8315761780392886, Learning Rate - 0.0001953125, magnitude of gradient - 1.8576920276110405\n",
      "Step - 8368, Loss - 0.6729692557167994, Learning Rate - 0.0001953125, magnitude of gradient - 0.7523058137192243\n",
      "Step - 8369, Loss - 0.6720138481025729, Learning Rate - 0.0001953125, magnitude of gradient - 1.576846618534191\n",
      "Step - 8370, Loss - 0.7631493612150146, Learning Rate - 0.0001953125, magnitude of gradient - 1.1716046481655735\n",
      "Step - 8371, Loss - 0.8251273056119415, Learning Rate - 0.0001953125, magnitude of gradient - 0.8448520385045831\n",
      "Step - 8372, Loss - 0.6969370176579567, Learning Rate - 0.0001953125, magnitude of gradient - 1.8851133189889704\n",
      "Step - 8373, Loss - 0.6671445420715884, Learning Rate - 0.0001953125, magnitude of gradient - 1.3953266309468655\n",
      "Step - 8374, Loss - 0.6723897713900963, Learning Rate - 0.0001953125, magnitude of gradient - 2.101643514151958\n",
      "Step - 8375, Loss - 0.675785962749676, Learning Rate - 0.0001953125, magnitude of gradient - 1.5486944828083156\n",
      "Step - 8376, Loss - 0.7199322316229817, Learning Rate - 0.0001953125, magnitude of gradient - 0.9316514190806305\n",
      "Step - 8377, Loss - 0.3678335199847589, Learning Rate - 0.0001953125, magnitude of gradient - 0.8520908356874033\n",
      "Step - 8378, Loss - 0.6262199533189048, Learning Rate - 0.0001953125, magnitude of gradient - 0.5848055631399096\n",
      "Step - 8379, Loss - 0.6339561753194973, Learning Rate - 0.0001953125, magnitude of gradient - 1.0613329676985328\n",
      "Step - 8380, Loss - 0.6945598842612626, Learning Rate - 0.0001953125, magnitude of gradient - 1.2308955518925777\n",
      "Step - 8381, Loss - 0.648896320870296, Learning Rate - 0.0001953125, magnitude of gradient - 1.7109800495953411\n",
      "Step - 8382, Loss - 0.7679953375975114, Learning Rate - 0.0001953125, magnitude of gradient - 1.837235560627517\n",
      "Step - 8383, Loss - 0.6787340849359667, Learning Rate - 0.0001953125, magnitude of gradient - 2.345697764973525\n",
      "Step - 8384, Loss - 0.6518962615479573, Learning Rate - 0.0001953125, magnitude of gradient - 0.8718641341874853\n",
      "Step - 8385, Loss - 0.6395504893471902, Learning Rate - 0.0001953125, magnitude of gradient - 0.26161584574601915\n",
      "Step - 8386, Loss - 0.7479998097453537, Learning Rate - 0.0001953125, magnitude of gradient - 1.2770650307232787\n",
      "Step - 8387, Loss - 0.7020527803394259, Learning Rate - 0.0001953125, magnitude of gradient - 1.943808956393929\n",
      "Step - 8388, Loss - 0.8020752825540998, Learning Rate - 0.0001953125, magnitude of gradient - 1.3659994619609825\n",
      "Step - 8389, Loss - 0.7327162174006407, Learning Rate - 0.0001953125, magnitude of gradient - 0.5980085174346949\n",
      "Step - 8390, Loss - 0.812149334168762, Learning Rate - 0.0001953125, magnitude of gradient - 2.0041739789511164\n",
      "Step - 8391, Loss - 0.6922891815588713, Learning Rate - 0.0001953125, magnitude of gradient - 0.7999795680195368\n",
      "Step - 8392, Loss - 0.5445082257122628, Learning Rate - 0.0001953125, magnitude of gradient - 0.504339504139025\n",
      "Step - 8393, Loss - 0.7303509234900334, Learning Rate - 0.0001953125, magnitude of gradient - 1.5062759079725805\n",
      "Step - 8394, Loss - 0.683330822889988, Learning Rate - 0.0001953125, magnitude of gradient - 1.547985010773149\n",
      "Step - 8395, Loss - 0.5564314335860188, Learning Rate - 0.0001953125, magnitude of gradient - 0.8671112012656694\n",
      "Step - 8396, Loss - 0.8130587689042753, Learning Rate - 0.0001953125, magnitude of gradient - 0.8052865052259164\n",
      "Step - 8397, Loss - 0.47940429044622207, Learning Rate - 0.0001953125, magnitude of gradient - 1.3922451004640062\n",
      "Step - 8398, Loss - 0.7518426746050481, Learning Rate - 0.0001953125, magnitude of gradient - 1.639068331182141\n",
      "Step - 8399, Loss - 0.6773843052138033, Learning Rate - 0.0001953125, magnitude of gradient - 0.9368237172773763\n",
      "Step - 8400, Loss - 0.6075853536113588, Learning Rate - 0.0001953125, magnitude of gradient - 2.386918774466102\n",
      "Step - 8401, Loss - 0.7362784934133298, Learning Rate - 0.0001953125, magnitude of gradient - 0.9973372696358994\n",
      "Step - 8402, Loss - 0.7569373211743311, Learning Rate - 0.0001953125, magnitude of gradient - 0.4714708381224924\n",
      "Step - 8403, Loss - 0.5805859909752651, Learning Rate - 0.0001953125, magnitude of gradient - 1.6796873442951672\n",
      "Step - 8404, Loss - 0.44660617034278693, Learning Rate - 0.0001953125, magnitude of gradient - 1.639622999638839\n",
      "Step - 8405, Loss - 0.5882936739874471, Learning Rate - 0.0001953125, magnitude of gradient - 0.3505339550993751\n",
      "Step - 8406, Loss - 0.8093673677693953, Learning Rate - 0.0001953125, magnitude of gradient - 0.6297384693134527\n",
      "Step - 8407, Loss - 0.5135925099065864, Learning Rate - 0.0001953125, magnitude of gradient - 1.1942943485301918\n",
      "Step - 8408, Loss - 0.7661586825888098, Learning Rate - 0.0001953125, magnitude of gradient - 1.487109387869542\n",
      "Step - 8409, Loss - 0.7360926770395089, Learning Rate - 0.0001953125, magnitude of gradient - 2.270876503529473\n",
      "Step - 8410, Loss - 0.7434675549498155, Learning Rate - 0.0001953125, magnitude of gradient - 1.9818575509495666\n",
      "Step - 8411, Loss - 0.8960843445950866, Learning Rate - 0.0001953125, magnitude of gradient - 1.8031025775369602\n",
      "Step - 8412, Loss - 0.7459936309584168, Learning Rate - 0.0001953125, magnitude of gradient - 1.1712646400534852\n",
      "Step - 8413, Loss - 0.8047677041125763, Learning Rate - 0.0001953125, magnitude of gradient - 0.5410808562771203\n",
      "Step - 8414, Loss - 0.6965936843849689, Learning Rate - 0.0001953125, magnitude of gradient - 0.6817452343577257\n",
      "Step - 8415, Loss - 0.7993661108129302, Learning Rate - 0.0001953125, magnitude of gradient - 0.4626826096070289\n",
      "Step - 8416, Loss - 0.6421598102429262, Learning Rate - 0.0001953125, magnitude of gradient - 1.2377960778983939\n",
      "Step - 8417, Loss - 0.6221289731057539, Learning Rate - 0.0001953125, magnitude of gradient - 0.9943412677265366\n",
      "Step - 8418, Loss - 0.7475742004974435, Learning Rate - 0.0001953125, magnitude of gradient - 2.1072035913239873\n",
      "Step - 8419, Loss - 0.6205256350293056, Learning Rate - 0.0001953125, magnitude of gradient - 0.777113064411268\n",
      "Step - 8420, Loss - 0.7510495397429479, Learning Rate - 0.0001953125, magnitude of gradient - 2.620093100370271\n",
      "Step - 8421, Loss - 0.5738486902424257, Learning Rate - 0.0001953125, magnitude of gradient - 0.9866612925736604\n",
      "Step - 8422, Loss - 0.49057274417801056, Learning Rate - 0.0001953125, magnitude of gradient - 1.4938344202473666\n",
      "Step - 8423, Loss - 0.4250385834248911, Learning Rate - 0.0001953125, magnitude of gradient - 1.602812817721962\n",
      "Step - 8424, Loss - 0.6787394570970968, Learning Rate - 0.0001953125, magnitude of gradient - 1.0335842356331957\n",
      "Step - 8425, Loss - 0.6920060374490536, Learning Rate - 0.0001953125, magnitude of gradient - 1.185123431920136\n",
      "Step - 8426, Loss - 0.7849238560428261, Learning Rate - 0.0001953125, magnitude of gradient - 0.6387742328842465\n",
      "Step - 8427, Loss - 0.7221485557523263, Learning Rate - 0.0001953125, magnitude of gradient - 1.275238172344789\n",
      "Step - 8428, Loss - 0.7562980359219693, Learning Rate - 0.0001953125, magnitude of gradient - 0.9478566120664275\n",
      "Step - 8429, Loss - 0.6555368910115598, Learning Rate - 0.0001953125, magnitude of gradient - 0.6499739700980742\n",
      "Step - 8430, Loss - 0.7105131531860955, Learning Rate - 0.0001953125, magnitude of gradient - 1.4687757580156644\n",
      "Step - 8431, Loss - 0.7283555289622965, Learning Rate - 0.0001953125, magnitude of gradient - 1.369864069226159\n",
      "Step - 8432, Loss - 0.5353823743316466, Learning Rate - 0.0001953125, magnitude of gradient - 0.3593491652477831\n",
      "Step - 8433, Loss - 0.7406268728161398, Learning Rate - 0.0001953125, magnitude of gradient - 1.132532752262667\n",
      "Step - 8434, Loss - 0.685793950632679, Learning Rate - 0.0001953125, magnitude of gradient - 2.5242019881875053\n",
      "Step - 8435, Loss - 0.8253823324929968, Learning Rate - 0.0001953125, magnitude of gradient - 0.6811561267618883\n",
      "Step - 8436, Loss - 0.6434720734624599, Learning Rate - 0.0001953125, magnitude of gradient - 0.5291245099319528\n",
      "Step - 8437, Loss - 0.6907746862757979, Learning Rate - 0.0001953125, magnitude of gradient - 1.3954496756573207\n",
      "Step - 8438, Loss - 0.6356661180553553, Learning Rate - 0.0001953125, magnitude of gradient - 2.468741859315703\n",
      "Step - 8439, Loss - 0.5970578602361282, Learning Rate - 0.0001953125, magnitude of gradient - 1.901400040847252\n",
      "Step - 8440, Loss - 0.7583790246250869, Learning Rate - 0.0001953125, magnitude of gradient - 1.655020240098221\n",
      "Step - 8441, Loss - 0.6723527094802206, Learning Rate - 0.0001953125, magnitude of gradient - 0.23500059160341125\n",
      "Step - 8442, Loss - 0.6578369463853541, Learning Rate - 0.0001953125, magnitude of gradient - 1.7887625351739527\n",
      "Step - 8443, Loss - 0.7254475809466066, Learning Rate - 0.0001953125, magnitude of gradient - 0.752222269757685\n",
      "Step - 8444, Loss - 0.6895019879922217, Learning Rate - 0.0001953125, magnitude of gradient - 1.3980082213727854\n",
      "Step - 8445, Loss - 0.7008020520757718, Learning Rate - 0.0001953125, magnitude of gradient - 1.0136024457749004\n",
      "Step - 8446, Loss - 0.7100345986491527, Learning Rate - 0.0001953125, magnitude of gradient - 1.2917550708440282\n",
      "Step - 8447, Loss - 0.5041048062856115, Learning Rate - 0.0001953125, magnitude of gradient - 1.051398126141742\n",
      "Step - 8448, Loss - 0.6961713543031741, Learning Rate - 0.0001953125, magnitude of gradient - 2.063483012832828\n",
      "Step - 8449, Loss - 0.6699888086409854, Learning Rate - 0.0001953125, magnitude of gradient - 0.9704049029728558\n",
      "Step - 8450, Loss - 0.8011894943887766, Learning Rate - 0.0001953125, magnitude of gradient - 1.0087683539396664\n",
      "Step - 8451, Loss - 0.7114552150210487, Learning Rate - 0.0001953125, magnitude of gradient - 1.1871185969529041\n",
      "Step - 8452, Loss - 0.6531962052474366, Learning Rate - 0.0001953125, magnitude of gradient - 1.0689934813858635\n",
      "Step - 8453, Loss - 0.7489923999599831, Learning Rate - 0.0001953125, magnitude of gradient - 1.1554510417692492\n",
      "Step - 8454, Loss - 0.5770120254357044, Learning Rate - 0.0001953125, magnitude of gradient - 1.2937163573270802\n",
      "Step - 8455, Loss - 0.7556712957715517, Learning Rate - 0.0001953125, magnitude of gradient - 1.2248228219281\n",
      "Step - 8456, Loss - 0.48160895401562265, Learning Rate - 0.0001953125, magnitude of gradient - 0.6850955677066143\n",
      "Step - 8457, Loss - 0.6859236157741138, Learning Rate - 0.0001953125, magnitude of gradient - 1.241623082753238\n",
      "Step - 8458, Loss - 0.7439301126271458, Learning Rate - 0.0001953125, magnitude of gradient - 1.6594308632478245\n",
      "Step - 8459, Loss - 0.5372248841134889, Learning Rate - 0.0001953125, magnitude of gradient - 0.8725105406400683\n",
      "Step - 8460, Loss - 0.5835428638242525, Learning Rate - 0.0001953125, magnitude of gradient - 1.5570403066719773\n",
      "Step - 8461, Loss - 0.767409595943391, Learning Rate - 0.0001953125, magnitude of gradient - 0.7913728315429149\n",
      "Step - 8462, Loss - 0.6466105347253733, Learning Rate - 0.0001953125, magnitude of gradient - 0.8407430948351943\n",
      "Step - 8463, Loss - 0.6432466158641172, Learning Rate - 0.0001953125, magnitude of gradient - 0.5954696793461343\n",
      "Step - 8464, Loss - 0.6705916773218128, Learning Rate - 0.0001953125, magnitude of gradient - 2.0594358294976765\n",
      "Step - 8465, Loss - 0.6745491026760824, Learning Rate - 0.0001953125, magnitude of gradient - 0.7076573233475715\n",
      "Step - 8466, Loss - 0.6478157871660745, Learning Rate - 0.0001953125, magnitude of gradient - 1.2541477288445007\n",
      "Step - 8467, Loss - 0.9436926617975852, Learning Rate - 0.0001953125, magnitude of gradient - 1.4696521963457334\n",
      "Step - 8468, Loss - 0.5772535728245609, Learning Rate - 0.0001953125, magnitude of gradient - 1.1362227271489507\n",
      "Step - 8469, Loss - 0.5983813957138197, Learning Rate - 0.0001953125, magnitude of gradient - 1.032415549287904\n",
      "Step - 8470, Loss - 0.8997384596667451, Learning Rate - 0.0001953125, magnitude of gradient - 1.511397855085238\n",
      "Step - 8471, Loss - 0.581394581471024, Learning Rate - 0.0001953125, magnitude of gradient - 0.8105287481704522\n",
      "Step - 8472, Loss - 0.7327770125561791, Learning Rate - 0.0001953125, magnitude of gradient - 0.8955401314642407\n",
      "Step - 8473, Loss - 0.7813649322654102, Learning Rate - 0.0001953125, magnitude of gradient - 0.788014496083706\n",
      "Step - 8474, Loss - 0.8530167608744431, Learning Rate - 0.0001953125, magnitude of gradient - 1.0914550038194029\n",
      "Step - 8475, Loss - 0.6262923131251064, Learning Rate - 0.0001953125, magnitude of gradient - 0.8432564340777982\n",
      "Step - 8476, Loss - 0.5811280921967192, Learning Rate - 0.0001953125, magnitude of gradient - 2.715107405933117\n",
      "Step - 8477, Loss - 0.5319222277277934, Learning Rate - 0.0001953125, magnitude of gradient - 0.7384772098715751\n",
      "Step - 8478, Loss - 0.7418290711414209, Learning Rate - 0.0001953125, magnitude of gradient - 0.6614240986036464\n",
      "Step - 8479, Loss - 0.5666106550235226, Learning Rate - 0.0001953125, magnitude of gradient - 0.937059203862532\n",
      "Step - 8480, Loss - 0.7688194914338572, Learning Rate - 0.0001953125, magnitude of gradient - 2.190244602150476\n",
      "Step - 8481, Loss - 0.8111000329418293, Learning Rate - 0.0001953125, magnitude of gradient - 1.4875154577724667\n",
      "Step - 8482, Loss - 0.8344874359644702, Learning Rate - 0.0001953125, magnitude of gradient - 0.937219282694475\n",
      "Step - 8483, Loss - 0.6095589206298869, Learning Rate - 0.0001953125, magnitude of gradient - 0.5664473230114552\n",
      "Step - 8484, Loss - 0.5444733975529154, Learning Rate - 0.0001953125, magnitude of gradient - 2.6807362852251253\n",
      "Step - 8485, Loss - 0.729466031694672, Learning Rate - 0.0001953125, magnitude of gradient - 0.9598924706439101\n",
      "Step - 8486, Loss - 0.8531438904909512, Learning Rate - 0.0001953125, magnitude of gradient - 0.9447950710179834\n",
      "Step - 8487, Loss - 0.8766047873806342, Learning Rate - 0.0001953125, magnitude of gradient - 1.8746825993459821\n",
      "Step - 8488, Loss - 0.6263591318441903, Learning Rate - 0.0001953125, magnitude of gradient - 0.7643324529559186\n",
      "Step - 8489, Loss - 0.6396680689424593, Learning Rate - 0.0001953125, magnitude of gradient - 0.4089075525724877\n",
      "Step - 8490, Loss - 0.6944252996317711, Learning Rate - 0.0001953125, magnitude of gradient - 1.1826987279250145\n",
      "Step - 8491, Loss - 0.7147413120767852, Learning Rate - 0.0001953125, magnitude of gradient - 1.5699898892564648\n",
      "Step - 8492, Loss - 0.7835133324735891, Learning Rate - 0.0001953125, magnitude of gradient - 1.2611025970711227\n",
      "Step - 8493, Loss - 0.7102618063558643, Learning Rate - 0.0001953125, magnitude of gradient - 0.9222911115757535\n",
      "Step - 8494, Loss - 0.7129323399903573, Learning Rate - 0.0001953125, magnitude of gradient - 1.4944112316664515\n",
      "Step - 8495, Loss - 0.6955636632436185, Learning Rate - 0.0001953125, magnitude of gradient - 1.2331619728436223\n",
      "Step - 8496, Loss - 0.567992680123199, Learning Rate - 0.0001953125, magnitude of gradient - 0.44344787265958885\n",
      "Step - 8497, Loss - 0.5479560924629786, Learning Rate - 0.0001953125, magnitude of gradient - 1.3681540520666953\n",
      "Step - 8498, Loss - 0.7843813855664913, Learning Rate - 0.0001953125, magnitude of gradient - 0.5022676403144286\n",
      "Step - 8499, Loss - 0.7921246630153467, Learning Rate - 0.0001953125, magnitude of gradient - 0.46168815757901416\n",
      "Step - 8500, Loss - 0.5676846364920456, Learning Rate - 0.0001953125, magnitude of gradient - 1.5034250661266455\n",
      "Step - 8501, Loss - 0.6028765396204852, Learning Rate - 0.0001953125, magnitude of gradient - 0.8362120141313347\n",
      "Step - 8502, Loss - 0.5670949426598562, Learning Rate - 0.0001953125, magnitude of gradient - 1.2725453976436498\n",
      "Step - 8503, Loss - 0.7228566690265931, Learning Rate - 0.0001953125, magnitude of gradient - 0.45524942692798115\n",
      "Step - 8504, Loss - 0.6766596332396797, Learning Rate - 0.0001953125, magnitude of gradient - 1.8191091393124836\n",
      "Step - 8505, Loss - 0.7775092839766977, Learning Rate - 0.0001953125, magnitude of gradient - 0.9166481638419888\n",
      "Step - 8506, Loss - 0.7446907851984286, Learning Rate - 0.0001953125, magnitude of gradient - 0.7942204466108139\n",
      "Step - 8507, Loss - 0.742021571891943, Learning Rate - 0.0001953125, magnitude of gradient - 0.7524371120441175\n",
      "Step - 8508, Loss - 0.6347073352009878, Learning Rate - 0.0001953125, magnitude of gradient - 1.4599831195284023\n",
      "Step - 8509, Loss - 0.6636614638250158, Learning Rate - 0.0001953125, magnitude of gradient - 1.8520898824378165\n",
      "Step - 8510, Loss - 0.6293106569616493, Learning Rate - 0.0001953125, magnitude of gradient - 1.722473901581125\n",
      "Step - 8511, Loss - 0.8171513064394641, Learning Rate - 0.0001953125, magnitude of gradient - 1.521994741403875\n",
      "Step - 8512, Loss - 0.9728832789262627, Learning Rate - 0.0001953125, magnitude of gradient - 2.012789714281923\n",
      "Step - 8513, Loss - 0.6324909168556996, Learning Rate - 0.0001953125, magnitude of gradient - 1.380289458496468\n",
      "Step - 8514, Loss - 0.912502966994565, Learning Rate - 0.0001953125, magnitude of gradient - 2.010312550068928\n",
      "Step - 8515, Loss - 0.7818245539386338, Learning Rate - 0.0001953125, magnitude of gradient - 0.5281116156843588\n",
      "Step - 8516, Loss - 0.663734187893911, Learning Rate - 0.0001953125, magnitude of gradient - 0.8718769054589717\n",
      "Step - 8517, Loss - 1.0109440702223615, Learning Rate - 0.0001953125, magnitude of gradient - 2.159075269811184\n",
      "Step - 8518, Loss - 0.6174097981391153, Learning Rate - 0.0001953125, magnitude of gradient - 0.7543798298859566\n",
      "Step - 8519, Loss - 0.7918189791548084, Learning Rate - 0.0001953125, magnitude of gradient - 0.7863859451586256\n",
      "Step - 8520, Loss - 0.7381711422305519, Learning Rate - 0.0001953125, magnitude of gradient - 1.9075839884597159\n",
      "Step - 8521, Loss - 0.6049145272090958, Learning Rate - 0.0001953125, magnitude of gradient - 0.47188535048407587\n",
      "Step - 8522, Loss - 0.8241369156168504, Learning Rate - 0.0001953125, magnitude of gradient - 0.782618628539307\n",
      "Step - 8523, Loss - 0.668955623926862, Learning Rate - 0.0001953125, magnitude of gradient - 0.5853711120334733\n",
      "Step - 8524, Loss - 0.6104488475114818, Learning Rate - 0.0001953125, magnitude of gradient - 1.2562035170193007\n",
      "Step - 8525, Loss - 0.6145714584551761, Learning Rate - 0.0001953125, magnitude of gradient - 0.9735224789442802\n",
      "Step - 8526, Loss - 0.801075001000244, Learning Rate - 0.0001953125, magnitude of gradient - 0.9176800651524634\n",
      "Step - 8527, Loss - 0.5740591215179836, Learning Rate - 0.0001953125, magnitude of gradient - 2.197753038416175\n",
      "Step - 8528, Loss - 0.6727429768303674, Learning Rate - 0.0001953125, magnitude of gradient - 1.1832768762484658\n",
      "Step - 8529, Loss - 0.5289870041629854, Learning Rate - 0.0001953125, magnitude of gradient - 0.5023370753886927\n",
      "Step - 8530, Loss - 0.558349186413351, Learning Rate - 0.0001953125, magnitude of gradient - 0.5792589602839395\n",
      "Step - 8531, Loss - 0.6734869271327497, Learning Rate - 0.0001953125, magnitude of gradient - 1.131814782278042\n",
      "Step - 8532, Loss - 0.8648077234478615, Learning Rate - 0.0001953125, magnitude of gradient - 1.681831701439011\n",
      "Step - 8533, Loss - 0.6902338410700037, Learning Rate - 0.0001953125, magnitude of gradient - 0.6932358068260172\n",
      "Step - 8534, Loss - 0.7745121297051777, Learning Rate - 0.0001953125, magnitude of gradient - 1.5317399541298489\n",
      "Step - 8535, Loss - 0.7143240628001505, Learning Rate - 0.0001953125, magnitude of gradient - 0.5779517519713424\n",
      "Step - 8536, Loss - 0.7514750441076955, Learning Rate - 0.0001953125, magnitude of gradient - 1.001623120511216\n",
      "Step - 8537, Loss - 0.7564562840244837, Learning Rate - 0.0001953125, magnitude of gradient - 1.066253206487352\n",
      "Step - 8538, Loss - 0.8692409822670266, Learning Rate - 0.0001953125, magnitude of gradient - 0.8986758047003969\n",
      "Step - 8539, Loss - 0.5404449638312175, Learning Rate - 0.0001953125, magnitude of gradient - 0.8759031257472468\n",
      "Step - 8540, Loss - 0.7390748982154954, Learning Rate - 0.0001953125, magnitude of gradient - 0.5660183224949565\n",
      "Step - 8541, Loss - 0.8172437531906448, Learning Rate - 0.0001953125, magnitude of gradient - 0.7878620686206054\n",
      "Step - 8542, Loss - 0.6367168210307832, Learning Rate - 0.0001953125, magnitude of gradient - 0.8745274970435897\n",
      "Step - 8543, Loss - 0.7347999137266001, Learning Rate - 0.0001953125, magnitude of gradient - 0.4444752294492583\n",
      "Step - 8544, Loss - 0.7265649015748012, Learning Rate - 0.0001953125, magnitude of gradient - 1.6447507474397516\n",
      "Step - 8545, Loss - 0.7275388679064411, Learning Rate - 0.0001953125, magnitude of gradient - 1.7529595639916091\n",
      "Step - 8546, Loss - 0.8537891002765161, Learning Rate - 0.0001953125, magnitude of gradient - 1.2610047526486443\n",
      "Step - 8547, Loss - 0.6365787323729627, Learning Rate - 0.0001953125, magnitude of gradient - 1.1430653121016765\n",
      "Step - 8548, Loss - 0.6128730909165976, Learning Rate - 0.0001953125, magnitude of gradient - 2.22468594080842\n",
      "Step - 8549, Loss - 0.8029920301330691, Learning Rate - 0.0001953125, magnitude of gradient - 2.1815395961165427\n",
      "Step - 8550, Loss - 0.6884182316287486, Learning Rate - 0.0001953125, magnitude of gradient - 1.1723751611883848\n",
      "Step - 8551, Loss - 0.7166372234099494, Learning Rate - 0.0001953125, magnitude of gradient - 1.6025846248476474\n",
      "Step - 8552, Loss - 0.6816308191935133, Learning Rate - 0.0001953125, magnitude of gradient - 0.40138311641737745\n",
      "Step - 8553, Loss - 0.8511735291334289, Learning Rate - 0.0001953125, magnitude of gradient - 1.9231147727816582\n",
      "Step - 8554, Loss - 0.6918969424836138, Learning Rate - 0.0001953125, magnitude of gradient - 0.41770127509557187\n",
      "Step - 8555, Loss - 0.7280488620718002, Learning Rate - 0.0001953125, magnitude of gradient - 0.39681404346514415\n",
      "Step - 8556, Loss - 0.4305231491623259, Learning Rate - 0.0001953125, magnitude of gradient - 1.2887979139773598\n",
      "Step - 8557, Loss - 0.7546519305510437, Learning Rate - 0.0001953125, magnitude of gradient - 0.7684687974731668\n",
      "Step - 8558, Loss - 0.7245295291927298, Learning Rate - 0.0001953125, magnitude of gradient - 1.7851028905774007\n",
      "Step - 8559, Loss - 0.7092219737025424, Learning Rate - 0.0001953125, magnitude of gradient - 0.58275261843279\n",
      "Step - 8560, Loss - 0.8961116866532344, Learning Rate - 0.0001953125, magnitude of gradient - 0.8770081638197477\n",
      "Step - 8561, Loss - 0.8018748863655896, Learning Rate - 0.0001953125, magnitude of gradient - 1.5413268780560867\n",
      "Step - 8562, Loss - 0.6703571315265567, Learning Rate - 0.0001953125, magnitude of gradient - 1.0948491757011074\n",
      "Step - 8563, Loss - 0.7106812616096903, Learning Rate - 0.0001953125, magnitude of gradient - 0.8615463348426418\n",
      "Step - 8564, Loss - 0.6482412674499095, Learning Rate - 0.0001953125, magnitude of gradient - 1.1791030153336546\n",
      "Step - 8565, Loss - 0.9677422811903786, Learning Rate - 0.0001953125, magnitude of gradient - 1.2208724837777898\n",
      "Step - 8566, Loss - 0.685043979535433, Learning Rate - 0.0001953125, magnitude of gradient - 1.797686405817469\n",
      "Step - 8567, Loss - 0.6350817446538458, Learning Rate - 0.0001953125, magnitude of gradient - 1.6034417104636587\n",
      "Step - 8568, Loss - 0.6738643679706805, Learning Rate - 0.0001953125, magnitude of gradient - 2.02896674564706\n",
      "Step - 8569, Loss - 0.7155177741594719, Learning Rate - 0.0001953125, magnitude of gradient - 0.7585126244731396\n",
      "Step - 8570, Loss - 0.6676738208329904, Learning Rate - 0.0001953125, magnitude of gradient - 1.8196332975434524\n",
      "Step - 8571, Loss - 0.6589715528322219, Learning Rate - 0.0001953125, magnitude of gradient - 1.2171237598879445\n",
      "Step - 8572, Loss - 0.6548816106490757, Learning Rate - 0.0001953125, magnitude of gradient - 1.0742239782094933\n",
      "Step - 8573, Loss - 0.6210847754353235, Learning Rate - 0.0001953125, magnitude of gradient - 0.478677405223358\n",
      "Step - 8574, Loss - 0.5603816194451354, Learning Rate - 0.0001953125, magnitude of gradient - 1.5530615548371407\n",
      "Step - 8575, Loss - 0.8506057399762456, Learning Rate - 0.0001953125, magnitude of gradient - 1.649274333807891\n",
      "Step - 8576, Loss - 0.6427342570610237, Learning Rate - 0.0001953125, magnitude of gradient - 0.6045654369918629\n",
      "Step - 8577, Loss - 0.7937121928153437, Learning Rate - 0.0001953125, magnitude of gradient - 1.4160932993085353\n",
      "Step - 8578, Loss - 0.7213064884823226, Learning Rate - 0.0001953125, magnitude of gradient - 0.4370379438927001\n",
      "Step - 8579, Loss - 0.8583280227822045, Learning Rate - 0.0001953125, magnitude of gradient - 1.710626994093015\n",
      "Step - 8580, Loss - 0.7336590879332662, Learning Rate - 0.0001953125, magnitude of gradient - 0.6489101202586841\n",
      "Step - 8581, Loss - 0.709988642738911, Learning Rate - 0.0001953125, magnitude of gradient - 0.9294790249323135\n",
      "Step - 8582, Loss - 0.7907102486965409, Learning Rate - 0.0001953125, magnitude of gradient - 1.1502167275016983\n",
      "Step - 8583, Loss - 0.7606702140577006, Learning Rate - 0.0001953125, magnitude of gradient - 1.6631967432549633\n",
      "Step - 8584, Loss - 0.6727761364802917, Learning Rate - 0.0001953125, magnitude of gradient - 1.968775949520319\n",
      "Step - 8585, Loss - 0.9892511139219212, Learning Rate - 0.0001953125, magnitude of gradient - 2.129479117054461\n",
      "Step - 8586, Loss - 0.755234063559231, Learning Rate - 0.0001953125, magnitude of gradient - 0.7702896655626024\n",
      "Step - 8587, Loss - 0.713649467036962, Learning Rate - 0.0001953125, magnitude of gradient - 0.7354791424667673\n",
      "Step - 8588, Loss - 0.6253076579944473, Learning Rate - 0.0001953125, magnitude of gradient - 1.306193584318905\n",
      "Step - 8589, Loss - 0.694958194916336, Learning Rate - 0.0001953125, magnitude of gradient - 1.2096003603759269\n",
      "Step - 8590, Loss - 0.7419535015422664, Learning Rate - 0.0001953125, magnitude of gradient - 1.3639110933132899\n",
      "Step - 8591, Loss - 0.9553312296183241, Learning Rate - 0.0001953125, magnitude of gradient - 0.8246336925338251\n",
      "Step - 8592, Loss - 0.6113967650187785, Learning Rate - 0.0001953125, magnitude of gradient - 1.7891635815804707\n",
      "Step - 8593, Loss - 0.5035682846322964, Learning Rate - 0.0001953125, magnitude of gradient - 2.730444717597393\n",
      "Step - 8594, Loss - 0.7137128139343623, Learning Rate - 0.0001953125, magnitude of gradient - 1.5545568321804533\n",
      "Step - 8595, Loss - 0.812736157021865, Learning Rate - 0.0001953125, magnitude of gradient - 2.7821828468611822\n",
      "Step - 8596, Loss - 0.5142807921090182, Learning Rate - 0.0001953125, magnitude of gradient - 0.7522333374960736\n",
      "Step - 8597, Loss - 0.6520171660153011, Learning Rate - 0.0001953125, magnitude of gradient - 0.9817222899314215\n",
      "Step - 8598, Loss - 0.830098909105274, Learning Rate - 0.0001953125, magnitude of gradient - 1.3389859067165055\n",
      "Step - 8599, Loss - 0.6511417175928447, Learning Rate - 0.0001953125, magnitude of gradient - 0.7828057512057865\n",
      "Step - 8600, Loss - 0.5770453472733895, Learning Rate - 0.0001953125, magnitude of gradient - 0.9494985841694226\n",
      "Step - 8601, Loss - 0.7262731458925771, Learning Rate - 0.0001953125, magnitude of gradient - 1.7021005866141967\n",
      "Step - 8602, Loss - 0.7109432759703536, Learning Rate - 0.0001953125, magnitude of gradient - 1.850998537768446\n",
      "Step - 8603, Loss - 0.7136696209432511, Learning Rate - 0.0001953125, magnitude of gradient - 0.5590311869018728\n",
      "Step - 8604, Loss - 0.8958412536887543, Learning Rate - 0.0001953125, magnitude of gradient - 2.9890707483029777\n",
      "Step - 8605, Loss - 0.6088920843312114, Learning Rate - 0.0001953125, magnitude of gradient - 0.9282176771628693\n",
      "Step - 8606, Loss - 0.5933909747436434, Learning Rate - 0.0001953125, magnitude of gradient - 1.559985975789687\n",
      "Step - 8607, Loss - 0.6659802607406754, Learning Rate - 0.0001953125, magnitude of gradient - 1.415230218495212\n",
      "Step - 8608, Loss - 0.629296150493427, Learning Rate - 0.0001953125, magnitude of gradient - 0.7071849059897273\n",
      "Step - 8609, Loss - 0.856671892429875, Learning Rate - 0.0001953125, magnitude of gradient - 1.4179403817844192\n",
      "Step - 8610, Loss - 0.7539842388751963, Learning Rate - 0.0001953125, magnitude of gradient - 1.2583334108311075\n",
      "Step - 8611, Loss - 0.7398674281583815, Learning Rate - 0.0001953125, magnitude of gradient - 1.2855402379527912\n",
      "Step - 8612, Loss - 0.6396652108054155, Learning Rate - 0.0001953125, magnitude of gradient - 0.5553153294174566\n",
      "Step - 8613, Loss - 0.7304140714595856, Learning Rate - 0.0001953125, magnitude of gradient - 1.5525187783111334\n",
      "Step - 8614, Loss - 0.7114500869656224, Learning Rate - 0.0001953125, magnitude of gradient - 1.069074032222223\n",
      "Step - 8615, Loss - 0.6806434100255083, Learning Rate - 0.0001953125, magnitude of gradient - 0.7720889774043534\n",
      "Step - 8616, Loss - 0.6718937966809256, Learning Rate - 0.0001953125, magnitude of gradient - 1.5511755624075754\n",
      "Step - 8617, Loss - 0.5914600096889925, Learning Rate - 0.0001953125, magnitude of gradient - 0.6905628224951388\n",
      "Step - 8618, Loss - 0.5411935243118629, Learning Rate - 0.0001953125, magnitude of gradient - 1.0602429567198965\n",
      "Step - 8619, Loss - 0.6391541167841501, Learning Rate - 0.0001953125, magnitude of gradient - 0.20242665272941593\n",
      "Step - 8620, Loss - 0.6501730955812635, Learning Rate - 0.0001953125, magnitude of gradient - 0.594559417786346\n",
      "Step - 8621, Loss - 0.8447096158770714, Learning Rate - 0.0001953125, magnitude of gradient - 0.7202373847578256\n",
      "Step - 8622, Loss - 0.5656285233961558, Learning Rate - 0.0001953125, magnitude of gradient - 1.0540462541972455\n",
      "Step - 8623, Loss - 0.6327039261796836, Learning Rate - 0.0001953125, magnitude of gradient - 1.3396677557986596\n",
      "Step - 8624, Loss - 0.8160474451224662, Learning Rate - 0.0001953125, magnitude of gradient - 1.3945658661146392\n",
      "Step - 8625, Loss - 0.6025515203272911, Learning Rate - 0.0001953125, magnitude of gradient - 1.0410562571536988\n",
      "Step - 8626, Loss - 0.7762116522576565, Learning Rate - 0.0001953125, magnitude of gradient - 0.6476580521706459\n",
      "Step - 8627, Loss - 0.6311440559697145, Learning Rate - 0.0001953125, magnitude of gradient - 1.0043255908623858\n",
      "Step - 8628, Loss - 0.6219011470144428, Learning Rate - 0.0001953125, magnitude of gradient - 0.5642661150075811\n",
      "Step - 8629, Loss - 0.594719144657855, Learning Rate - 0.0001953125, magnitude of gradient - 1.9789067237613265\n",
      "Step - 8630, Loss - 0.7975437989544885, Learning Rate - 0.0001953125, magnitude of gradient - 1.147688616985111\n",
      "Step - 8631, Loss - 0.7806515571749094, Learning Rate - 0.0001953125, magnitude of gradient - 1.3664345466548191\n",
      "Step - 8632, Loss - 0.8649073233781767, Learning Rate - 0.0001953125, magnitude of gradient - 1.230280864978535\n",
      "Step - 8633, Loss - 0.843215348647926, Learning Rate - 0.0001953125, magnitude of gradient - 1.7710622878246214\n",
      "Step - 8634, Loss - 0.7185921429913886, Learning Rate - 0.0001953125, magnitude of gradient - 1.4904931225054465\n",
      "Step - 8635, Loss - 0.4902377666200058, Learning Rate - 0.0001953125, magnitude of gradient - 1.8825381497637226\n",
      "Step - 8636, Loss - 0.6072654902612351, Learning Rate - 0.0001953125, magnitude of gradient - 0.41912901445087625\n",
      "Step - 8637, Loss - 0.6596079028956969, Learning Rate - 0.0001953125, magnitude of gradient - 0.8474653253962448\n",
      "Step - 8638, Loss - 0.6422487240697741, Learning Rate - 0.0001953125, magnitude of gradient - 1.7924945210316139\n",
      "Step - 8639, Loss - 0.8187373286232534, Learning Rate - 0.0001953125, magnitude of gradient - 1.9100351891845548\n",
      "Step - 8640, Loss - 0.6775592480060334, Learning Rate - 0.0001953125, magnitude of gradient - 0.9238211407031626\n",
      "Step - 8641, Loss - 0.490113513491353, Learning Rate - 0.0001953125, magnitude of gradient - 0.45688435818621537\n",
      "Step - 8642, Loss - 0.6885504977605004, Learning Rate - 0.0001953125, magnitude of gradient - 1.2051430068999751\n",
      "Step - 8643, Loss - 0.7212867940099532, Learning Rate - 0.0001953125, magnitude of gradient - 1.150686748567327\n",
      "Step - 8644, Loss - 0.645819608029411, Learning Rate - 0.0001953125, magnitude of gradient - 1.2449177428032054\n",
      "Step - 8645, Loss - 0.7000767405445142, Learning Rate - 0.0001953125, magnitude of gradient - 1.0229318150243158\n",
      "Step - 8646, Loss - 0.49445705289984054, Learning Rate - 0.0001953125, magnitude of gradient - 1.4933302854682098\n",
      "Step - 8647, Loss - 0.624008789519704, Learning Rate - 0.0001953125, magnitude of gradient - 0.8072708511236698\n",
      "Step - 8648, Loss - 0.7764751190541908, Learning Rate - 0.0001953125, magnitude of gradient - 0.9230950176081746\n",
      "Step - 8649, Loss - 0.6328313326262444, Learning Rate - 0.0001953125, magnitude of gradient - 1.4442058453137576\n",
      "Step - 8650, Loss - 0.648597522988311, Learning Rate - 0.0001953125, magnitude of gradient - 1.5270162982187683\n",
      "Step - 8651, Loss - 0.8527061652575736, Learning Rate - 0.0001953125, magnitude of gradient - 2.9516410603093566\n",
      "Step - 8652, Loss - 0.6368933754614545, Learning Rate - 0.0001953125, magnitude of gradient - 2.263132130309504\n",
      "Step - 8653, Loss - 0.8259336956473025, Learning Rate - 0.0001953125, magnitude of gradient - 1.8313620630436973\n",
      "Step - 8654, Loss - 0.599323519933705, Learning Rate - 0.0001953125, magnitude of gradient - 2.280872989944892\n",
      "Step - 8655, Loss - 0.8523315878216895, Learning Rate - 0.0001953125, magnitude of gradient - 1.7760556888445267\n",
      "Step - 8656, Loss - 0.7567510830026967, Learning Rate - 0.0001953125, magnitude of gradient - 0.28762413226778577\n",
      "Step - 8657, Loss - 0.6835368461257447, Learning Rate - 0.0001953125, magnitude of gradient - 0.5342573795708823\n",
      "Step - 8658, Loss - 0.5459933053560416, Learning Rate - 0.0001953125, magnitude of gradient - 0.7984691735313044\n",
      "Step - 8659, Loss - 0.7094610605984664, Learning Rate - 0.0001953125, magnitude of gradient - 1.8306468569192964\n",
      "Step - 8660, Loss - 0.8508701778433357, Learning Rate - 0.0001953125, magnitude of gradient - 2.211917190128023\n",
      "Step - 8661, Loss - 0.7148900289773163, Learning Rate - 0.0001953125, magnitude of gradient - 2.161266054491211\n",
      "Step - 8662, Loss - 0.5662878793880234, Learning Rate - 0.0001953125, magnitude of gradient - 1.5828878938139275\n",
      "Step - 8663, Loss - 0.6118985274821938, Learning Rate - 0.0001953125, magnitude of gradient - 2.123069411888807\n",
      "Step - 8664, Loss - 0.7476940061818518, Learning Rate - 0.0001953125, magnitude of gradient - 1.0265815973670889\n",
      "Step - 8665, Loss - 0.7878758203577019, Learning Rate - 0.0001953125, magnitude of gradient - 1.1753690225964106\n",
      "Step - 8666, Loss - 0.6914507893896977, Learning Rate - 0.0001953125, magnitude of gradient - 0.3047954571633671\n",
      "Step - 8667, Loss - 0.639105806434129, Learning Rate - 0.0001953125, magnitude of gradient - 0.7125537700002373\n",
      "Step - 8668, Loss - 0.7878985324579069, Learning Rate - 0.0001953125, magnitude of gradient - 2.0693179800745813\n",
      "Step - 8669, Loss - 0.6716063785494469, Learning Rate - 0.0001953125, magnitude of gradient - 1.09928093329796\n",
      "Step - 8670, Loss - 0.7283189869654229, Learning Rate - 0.0001953125, magnitude of gradient - 1.0968137211586144\n",
      "Step - 8671, Loss - 0.6715220376239133, Learning Rate - 0.0001953125, magnitude of gradient - 3.0016275807728867\n",
      "Step - 8672, Loss - 0.769388223460234, Learning Rate - 0.0001953125, magnitude of gradient - 1.419410909051761\n",
      "Step - 8673, Loss - 0.5704896616288928, Learning Rate - 0.0001953125, magnitude of gradient - 0.34369014604726245\n",
      "Step - 8674, Loss - 0.5780154915127578, Learning Rate - 0.0001953125, magnitude of gradient - 1.4984726696292068\n",
      "Step - 8675, Loss - 0.8300030023202633, Learning Rate - 0.0001953125, magnitude of gradient - 1.248740798720668\n",
      "Step - 8676, Loss - 0.7998412277086765, Learning Rate - 0.0001953125, magnitude of gradient - 1.3914732482481649\n",
      "Step - 8677, Loss - 0.6843211595429274, Learning Rate - 0.0001953125, magnitude of gradient - 2.1080081209076273\n",
      "Step - 8678, Loss - 0.48901161508621227, Learning Rate - 0.0001953125, magnitude of gradient - 0.4455279552834124\n",
      "Step - 8679, Loss - 0.6751135580453461, Learning Rate - 0.0001953125, magnitude of gradient - 1.5949753422195727\n",
      "Step - 8680, Loss - 0.6149589712877672, Learning Rate - 0.0001953125, magnitude of gradient - 0.7124109207884214\n",
      "Step - 8681, Loss - 0.6454967117778503, Learning Rate - 0.0001953125, magnitude of gradient - 1.3082837582240827\n",
      "Step - 8682, Loss - 0.8673570497717837, Learning Rate - 0.0001953125, magnitude of gradient - 1.5509211533246687\n",
      "Step - 8683, Loss - 0.6204960673820346, Learning Rate - 0.0001953125, magnitude of gradient - 0.7984434188290543\n",
      "Step - 8684, Loss - 0.8290499488207979, Learning Rate - 0.0001953125, magnitude of gradient - 1.0650770877235234\n",
      "Step - 8685, Loss - 0.43251983421546025, Learning Rate - 0.0001953125, magnitude of gradient - 1.2072021443695884\n",
      "Step - 8686, Loss - 0.9574984323245295, Learning Rate - 0.0001953125, magnitude of gradient - 1.7444068109299242\n",
      "Step - 8687, Loss - 0.4726138396540794, Learning Rate - 0.0001953125, magnitude of gradient - 1.1824616348072616\n",
      "Step - 8688, Loss - 0.6044165742005077, Learning Rate - 0.0001953125, magnitude of gradient - 1.1708085887699018\n",
      "Step - 8689, Loss - 0.7648482974818752, Learning Rate - 0.0001953125, magnitude of gradient - 1.1988479207084093\n",
      "Step - 8690, Loss - 0.7906747001406916, Learning Rate - 0.0001953125, magnitude of gradient - 1.5478247967115277\n",
      "Step - 8691, Loss - 0.643798817224843, Learning Rate - 0.0001953125, magnitude of gradient - 0.4458746547280043\n",
      "Step - 8692, Loss - 0.8498958077671541, Learning Rate - 0.0001953125, magnitude of gradient - 0.9310804315475244\n",
      "Step - 8693, Loss - 0.6088012968491393, Learning Rate - 0.0001953125, magnitude of gradient - 0.367362916851722\n",
      "Step - 8694, Loss - 0.798525495034252, Learning Rate - 0.0001953125, magnitude of gradient - 1.899020579307405\n",
      "Step - 8695, Loss - 0.7205639389382468, Learning Rate - 0.0001953125, magnitude of gradient - 0.5177007444724231\n",
      "Step - 8696, Loss - 0.5816685161915595, Learning Rate - 0.0001953125, magnitude of gradient - 1.9781816836063473\n",
      "Step - 8697, Loss - 0.7680539641552611, Learning Rate - 0.0001953125, magnitude of gradient - 1.0201934940758133\n",
      "Step - 8698, Loss - 0.7310538885273836, Learning Rate - 0.0001953125, magnitude of gradient - 0.8669286275793283\n",
      "Step - 8699, Loss - 0.525287594561061, Learning Rate - 0.0001953125, magnitude of gradient - 1.1368244481546723\n",
      "Step - 8700, Loss - 0.693834479640926, Learning Rate - 0.0001953125, magnitude of gradient - 0.8069956220719763\n",
      "Step - 8701, Loss - 0.6725056568546797, Learning Rate - 0.0001953125, magnitude of gradient - 2.258309308557493\n",
      "Step - 8702, Loss - 0.678482102076958, Learning Rate - 0.0001953125, magnitude of gradient - 1.28558715022711\n",
      "Step - 8703, Loss - 0.9982585655544096, Learning Rate - 0.0001953125, magnitude of gradient - 2.8104944048518616\n",
      "Step - 8704, Loss - 0.7647228420963008, Learning Rate - 0.0001953125, magnitude of gradient - 0.9687543505906505\n",
      "Step - 8705, Loss - 0.6064790695122086, Learning Rate - 0.0001953125, magnitude of gradient - 1.7341516972773021\n",
      "Step - 8706, Loss - 0.983924437500656, Learning Rate - 0.0001953125, magnitude of gradient - 1.6448346924352808\n",
      "Step - 8707, Loss - 0.850059985546661, Learning Rate - 0.0001953125, magnitude of gradient - 1.3099758470688425\n",
      "Step - 8708, Loss - 0.7142953889614168, Learning Rate - 0.0001953125, magnitude of gradient - 1.120661512376239\n",
      "Step - 8709, Loss - 0.7660670531660148, Learning Rate - 0.0001953125, magnitude of gradient - 0.7570983713467874\n",
      "Step - 8710, Loss - 0.6401890154024358, Learning Rate - 0.0001953125, magnitude of gradient - 0.7466073968606218\n",
      "Step - 8711, Loss - 0.8242042458418399, Learning Rate - 0.0001953125, magnitude of gradient - 0.34211348086656995\n",
      "Step - 8712, Loss - 0.7881617905500462, Learning Rate - 0.0001953125, magnitude of gradient - 0.6354355064409298\n",
      "Step - 8713, Loss - 0.6668079538245728, Learning Rate - 0.0001953125, magnitude of gradient - 0.9762629641585093\n",
      "Step - 8714, Loss - 0.5726171989301018, Learning Rate - 0.0001953125, magnitude of gradient - 1.1108617978569093\n",
      "Step - 8715, Loss - 0.6309126697759107, Learning Rate - 0.0001953125, magnitude of gradient - 0.21863490386273687\n",
      "Step - 8716, Loss - 0.6803901889172623, Learning Rate - 0.0001953125, magnitude of gradient - 1.5272216733616963\n",
      "Step - 8717, Loss - 0.7576919016821261, Learning Rate - 0.0001953125, magnitude of gradient - 0.6576038466505242\n",
      "Step - 8718, Loss - 0.8880795334487713, Learning Rate - 0.0001953125, magnitude of gradient - 1.9837270182880655\n",
      "Step - 8719, Loss - 0.5951273663030991, Learning Rate - 0.0001953125, magnitude of gradient - 0.25008005379913684\n",
      "Step - 8720, Loss - 0.7118778800498456, Learning Rate - 0.0001953125, magnitude of gradient - 0.7928472027273668\n",
      "Step - 8721, Loss - 0.5598617888719463, Learning Rate - 0.0001953125, magnitude of gradient - 2.0047370464612566\n",
      "Step - 8722, Loss - 0.7032636526126067, Learning Rate - 0.0001953125, magnitude of gradient - 2.160934143007234\n",
      "Step - 8723, Loss - 0.7553852354197201, Learning Rate - 0.0001953125, magnitude of gradient - 0.8548455978760909\n",
      "Step - 8724, Loss - 0.691142105838746, Learning Rate - 0.0001953125, magnitude of gradient - 1.177830289646576\n",
      "Step - 8725, Loss - 0.7574363301373779, Learning Rate - 0.0001953125, magnitude of gradient - 1.353900199840059\n",
      "Step - 8726, Loss - 0.5171574588180649, Learning Rate - 0.0001953125, magnitude of gradient - 0.42629507045577525\n",
      "Step - 8727, Loss - 0.749809478253673, Learning Rate - 0.0001953125, magnitude of gradient - 1.2700909721073497\n",
      "Step - 8728, Loss - 0.8129425276740909, Learning Rate - 0.0001953125, magnitude of gradient - 1.0838534309330219\n",
      "Step - 8729, Loss - 0.4691590390266076, Learning Rate - 0.0001953125, magnitude of gradient - 0.7192531471103268\n",
      "Step - 8730, Loss - 0.8481780752526136, Learning Rate - 0.0001953125, magnitude of gradient - 1.561848942150943\n",
      "Step - 8731, Loss - 0.5681744868019354, Learning Rate - 0.0001953125, magnitude of gradient - 1.375720816672567\n",
      "Step - 8732, Loss - 0.6618721145776209, Learning Rate - 0.0001953125, magnitude of gradient - 0.40860340311442384\n",
      "Step - 8733, Loss - 0.8335618025704268, Learning Rate - 0.0001953125, magnitude of gradient - 1.9422005002370069\n",
      "Step - 8734, Loss - 0.7827108150629356, Learning Rate - 0.0001953125, magnitude of gradient - 0.9844409507893008\n",
      "Step - 8735, Loss - 0.777627454075478, Learning Rate - 0.0001953125, magnitude of gradient - 2.0164825955098453\n",
      "Step - 8736, Loss - 0.7229042510232373, Learning Rate - 0.0001953125, magnitude of gradient - 1.443642856737351\n",
      "Step - 8737, Loss - 0.7273921559317694, Learning Rate - 0.0001953125, magnitude of gradient - 2.507095300998517\n",
      "Step - 8738, Loss - 0.4696678757748709, Learning Rate - 0.0001953125, magnitude of gradient - 0.7310824381469655\n",
      "Step - 8739, Loss - 0.8837056393919518, Learning Rate - 0.0001953125, magnitude of gradient - 1.4931136419640951\n",
      "Step - 8740, Loss - 0.6929960139813269, Learning Rate - 0.0001953125, magnitude of gradient - 1.1618953405454155\n",
      "Step - 8741, Loss - 0.844285101326664, Learning Rate - 0.0001953125, magnitude of gradient - 1.0140381277469148\n",
      "Step - 8742, Loss - 0.6409694804437014, Learning Rate - 0.0001953125, magnitude of gradient - 0.9638522494901965\n",
      "Step - 8743, Loss - 0.6503843407115426, Learning Rate - 0.0001953125, magnitude of gradient - 1.745083057128104\n",
      "Step - 8744, Loss - 0.6179045476817043, Learning Rate - 0.0001953125, magnitude of gradient - 0.729179499726119\n",
      "Step - 8745, Loss - 0.6382364866679668, Learning Rate - 0.0001953125, magnitude of gradient - 0.5288601759866616\n",
      "Step - 8746, Loss - 0.6339744998444361, Learning Rate - 0.0001953125, magnitude of gradient - 1.6885468370505505\n",
      "Step - 8747, Loss - 0.8445456400627145, Learning Rate - 0.0001953125, magnitude of gradient - 0.8552903034199265\n",
      "Step - 8748, Loss - 0.6031895076577932, Learning Rate - 0.0001953125, magnitude of gradient - 1.9721108670346805\n",
      "Step - 8749, Loss - 0.8292439950532511, Learning Rate - 0.0001953125, magnitude of gradient - 1.7600476639717988\n",
      "Step - 8750, Loss - 0.7147013692061294, Learning Rate - 0.0001953125, magnitude of gradient - 1.1752469000194994\n",
      "Step - 8751, Loss - 0.7921616530244532, Learning Rate - 0.0001953125, magnitude of gradient - 1.2205399026624022\n",
      "Step - 8752, Loss - 0.7665660009803251, Learning Rate - 0.0001953125, magnitude of gradient - 0.6442709551034167\n",
      "Step - 8753, Loss - 0.5640029699681091, Learning Rate - 0.0001953125, magnitude of gradient - 1.0042197888107738\n",
      "Step - 8754, Loss - 0.8714201468052382, Learning Rate - 0.0001953125, magnitude of gradient - 1.7084731671234905\n",
      "Step - 8755, Loss - 0.7176666252879196, Learning Rate - 0.0001953125, magnitude of gradient - 1.6590868984385898\n",
      "Step - 8756, Loss - 0.6591269747183321, Learning Rate - 0.0001953125, magnitude of gradient - 0.9708637695272854\n",
      "Step - 8757, Loss - 0.7053790994060788, Learning Rate - 0.0001953125, magnitude of gradient - 2.389445339160464\n",
      "Step - 8758, Loss - 0.8879860766009895, Learning Rate - 0.0001953125, magnitude of gradient - 1.2214308322875287\n",
      "Step - 8759, Loss - 0.535225873479551, Learning Rate - 0.0001953125, magnitude of gradient - 0.4341660138714821\n",
      "Step - 8760, Loss - 0.8602019973481512, Learning Rate - 0.0001953125, magnitude of gradient - 1.425512882201631\n",
      "Step - 8761, Loss - 0.5732690267607824, Learning Rate - 0.0001953125, magnitude of gradient - 2.5728325915250454\n",
      "Step - 8762, Loss - 0.8595999719797636, Learning Rate - 0.0001953125, magnitude of gradient - 0.8928619962817161\n",
      "Step - 8763, Loss - 0.7117651186213785, Learning Rate - 0.0001953125, magnitude of gradient - 1.1329197426511448\n",
      "Step - 8764, Loss - 0.6877500242666474, Learning Rate - 0.0001953125, magnitude of gradient - 1.4889154639055695\n",
      "Step - 8765, Loss - 0.7888693814816918, Learning Rate - 0.0001953125, magnitude of gradient - 2.0031856756326287\n",
      "Step - 8766, Loss - 0.8863600977915316, Learning Rate - 0.0001953125, magnitude of gradient - 1.5008089169218184\n",
      "Step - 8767, Loss - 0.6948432261623921, Learning Rate - 0.0001953125, magnitude of gradient - 0.621482374920181\n",
      "Step - 8768, Loss - 0.7624584919936968, Learning Rate - 0.0001953125, magnitude of gradient - 2.0373610807298155\n",
      "Step - 8769, Loss - 0.5571048966352697, Learning Rate - 0.0001953125, magnitude of gradient - 1.610865730419472\n",
      "Step - 8770, Loss - 0.7258903162509117, Learning Rate - 0.0001953125, magnitude of gradient - 1.6046100639432561\n",
      "Step - 8771, Loss - 0.7246141639863206, Learning Rate - 0.0001953125, magnitude of gradient - 0.9807041857040825\n",
      "Step - 8772, Loss - 0.5622288753693307, Learning Rate - 0.0001953125, magnitude of gradient - 1.3406372750959898\n",
      "Step - 8773, Loss - 0.7099881247069938, Learning Rate - 0.0001953125, magnitude of gradient - 0.9053087623564242\n",
      "Step - 8774, Loss - 0.6831758673855766, Learning Rate - 0.0001953125, magnitude of gradient - 0.7428410638423321\n",
      "Step - 8775, Loss - 0.6562918785316051, Learning Rate - 0.0001953125, magnitude of gradient - 1.0157421929223291\n",
      "Step - 8776, Loss - 0.622695535563715, Learning Rate - 0.0001953125, magnitude of gradient - 0.8106961709810154\n",
      "Step - 8777, Loss - 0.559547987752279, Learning Rate - 0.0001953125, magnitude of gradient - 0.6265269987689897\n",
      "Step - 8778, Loss - 0.48099090298480474, Learning Rate - 0.0001953125, magnitude of gradient - 1.3555999305798287\n",
      "Step - 8779, Loss - 0.7656100843885995, Learning Rate - 0.0001953125, magnitude of gradient - 1.5947851484778415\n",
      "Step - 8780, Loss - 0.539921430166799, Learning Rate - 0.0001953125, magnitude of gradient - 2.2782375502966974\n",
      "Step - 8781, Loss - 0.7438121126452858, Learning Rate - 0.0001953125, magnitude of gradient - 1.73272247174596\n",
      "Step - 8782, Loss - 0.6992432641206859, Learning Rate - 0.0001953125, magnitude of gradient - 0.9736450439358494\n",
      "Step - 8783, Loss - 0.7069279502641302, Learning Rate - 0.0001953125, magnitude of gradient - 1.2765998707840658\n",
      "Step - 8784, Loss - 0.9121792227198124, Learning Rate - 0.0001953125, magnitude of gradient - 0.8947135043588967\n",
      "Step - 8785, Loss - 0.7538359095614576, Learning Rate - 0.0001953125, magnitude of gradient - 1.030357051098259\n",
      "Step - 8786, Loss - 0.8787597371620657, Learning Rate - 0.0001953125, magnitude of gradient - 1.5669139559499348\n",
      "Step - 8787, Loss - 0.6528310546556516, Learning Rate - 0.0001953125, magnitude of gradient - 1.5275842474464787\n",
      "Step - 8788, Loss - 0.6382720458216908, Learning Rate - 0.0001953125, magnitude of gradient - 0.5055832723995377\n",
      "Step - 8789, Loss - 0.8480817570917061, Learning Rate - 0.0001953125, magnitude of gradient - 0.8642691398524546\n",
      "Step - 8790, Loss - 0.9055183375864201, Learning Rate - 0.0001953125, magnitude of gradient - 1.2235310538829214\n",
      "Step - 8791, Loss - 0.7911550045403286, Learning Rate - 0.0001953125, magnitude of gradient - 0.4772274386923293\n",
      "Step - 8792, Loss - 0.8590367638183962, Learning Rate - 0.0001953125, magnitude of gradient - 0.9596404138659919\n",
      "Step - 8793, Loss - 0.5199561072629082, Learning Rate - 0.0001953125, magnitude of gradient - 2.111812381290808\n",
      "Step - 8794, Loss - 0.7108985044166474, Learning Rate - 0.0001953125, magnitude of gradient - 0.6964242634602426\n",
      "Step - 8795, Loss - 0.8341011218902074, Learning Rate - 0.0001953125, magnitude of gradient - 2.5544757027733818\n",
      "Step - 8796, Loss - 0.7087489703122828, Learning Rate - 0.0001953125, magnitude of gradient - 2.6037637972431464\n",
      "Step - 8797, Loss - 0.6169712689867486, Learning Rate - 0.0001953125, magnitude of gradient - 0.4370030186336437\n",
      "Step - 8798, Loss - 0.76059682317365, Learning Rate - 0.0001953125, magnitude of gradient - 1.091169514985229\n",
      "Step - 8799, Loss - 0.6629631137257883, Learning Rate - 0.0001953125, magnitude of gradient - 0.42035217127864766\n",
      "Step - 8800, Loss - 0.7119049306512296, Learning Rate - 0.0001953125, magnitude of gradient - 1.7674356444279693\n",
      "Step - 8801, Loss - 0.7895644893478944, Learning Rate - 0.0001953125, magnitude of gradient - 1.793300760127158\n",
      "Step - 8802, Loss - 0.7179381078449356, Learning Rate - 0.0001953125, magnitude of gradient - 0.9490160814531298\n",
      "Step - 8803, Loss - 0.7480668176553483, Learning Rate - 0.0001953125, magnitude of gradient - 0.8611377078527213\n",
      "Step - 8804, Loss - 0.7268397238371027, Learning Rate - 0.0001953125, magnitude of gradient - 1.0635596890982648\n",
      "Step - 8805, Loss - 0.8654415377166544, Learning Rate - 0.0001953125, magnitude of gradient - 0.8402914973440736\n",
      "Step - 8806, Loss - 0.72337694308612, Learning Rate - 0.0001953125, magnitude of gradient - 1.4092735177187337\n",
      "Step - 8807, Loss - 0.7522076774449666, Learning Rate - 0.0001953125, magnitude of gradient - 1.51331749965109\n",
      "Step - 8808, Loss - 0.6369959363988227, Learning Rate - 0.0001953125, magnitude of gradient - 0.9885554203055139\n",
      "Step - 8809, Loss - 0.6261437874350751, Learning Rate - 0.0001953125, magnitude of gradient - 0.8523535257229718\n",
      "Step - 8810, Loss - 0.95679873411014, Learning Rate - 0.0001953125, magnitude of gradient - 2.494587267479462\n",
      "Step - 8811, Loss - 0.6406454606589288, Learning Rate - 0.0001953125, magnitude of gradient - 1.5103835237763044\n",
      "Step - 8812, Loss - 0.5487218763237273, Learning Rate - 0.0001953125, magnitude of gradient - 2.999600879035701\n",
      "Step - 8813, Loss - 0.7855901338318958, Learning Rate - 0.0001953125, magnitude of gradient - 0.7282512420457531\n",
      "Step - 8814, Loss - 0.6905737083263961, Learning Rate - 0.0001953125, magnitude of gradient - 1.0755210907332267\n",
      "Step - 8815, Loss - 0.9435416569219615, Learning Rate - 0.0001953125, magnitude of gradient - 3.348497420155748\n",
      "Step - 8816, Loss - 0.48247040308001715, Learning Rate - 0.0001953125, magnitude of gradient - 2.1018707854512777\n",
      "Step - 8817, Loss - 0.6781238952332801, Learning Rate - 0.0001953125, magnitude of gradient - 1.198963384132172\n",
      "Step - 8818, Loss - 0.5850438704785655, Learning Rate - 0.0001953125, magnitude of gradient - 1.0711231366989127\n",
      "Step - 8819, Loss - 0.8265648496905973, Learning Rate - 0.0001953125, magnitude of gradient - 0.9126476877264237\n",
      "Step - 8820, Loss - 0.8390318633931435, Learning Rate - 0.0001953125, magnitude of gradient - 0.6208434634221918\n",
      "Step - 8821, Loss - 0.6292180074315933, Learning Rate - 0.0001953125, magnitude of gradient - 1.509374435412241\n",
      "Step - 8822, Loss - 0.7402228649882752, Learning Rate - 0.0001953125, magnitude of gradient - 0.6130096355664261\n",
      "Step - 8823, Loss - 0.7233820195797418, Learning Rate - 0.0001953125, magnitude of gradient - 0.5963381056815976\n",
      "Step - 8824, Loss - 0.6051262613885959, Learning Rate - 0.0001953125, magnitude of gradient - 1.9300280226691062\n",
      "Step - 8825, Loss - 0.7149448488720059, Learning Rate - 0.0001953125, magnitude of gradient - 0.23631144424610642\n",
      "Step - 8826, Loss - 0.6654723939806912, Learning Rate - 0.0001953125, magnitude of gradient - 2.445738449517439\n",
      "Step - 8827, Loss - 0.8191959514470601, Learning Rate - 0.0001953125, magnitude of gradient - 1.5793851856621064\n",
      "Step - 8828, Loss - 0.6604399329085904, Learning Rate - 0.0001953125, magnitude of gradient - 0.36590603826059015\n",
      "Step - 8829, Loss - 0.5278793083019389, Learning Rate - 0.0001953125, magnitude of gradient - 1.3148189310646685\n",
      "Step - 8830, Loss - 0.8120482776655016, Learning Rate - 0.0001953125, magnitude of gradient - 1.0462679458082018\n",
      "Step - 8831, Loss - 0.6296328559400101, Learning Rate - 0.0001953125, magnitude of gradient - 1.2800769348910883\n",
      "Step - 8832, Loss - 0.6302331796897411, Learning Rate - 0.0001953125, magnitude of gradient - 1.0645545690755909\n",
      "Step - 8833, Loss - 0.6563370177670912, Learning Rate - 0.0001953125, magnitude of gradient - 0.571596519024681\n",
      "Step - 8834, Loss - 0.5047441598268166, Learning Rate - 0.0001953125, magnitude of gradient - 1.137768578640866\n",
      "Step - 8835, Loss - 0.6776894231724817, Learning Rate - 0.0001953125, magnitude of gradient - 1.3930897160541782\n",
      "Step - 8836, Loss - 0.7779666082036457, Learning Rate - 0.0001953125, magnitude of gradient - 0.6693841765410438\n",
      "Step - 8837, Loss - 0.583308679932143, Learning Rate - 0.0001953125, magnitude of gradient - 0.6867170892964317\n",
      "Step - 8838, Loss - 0.7212028346959772, Learning Rate - 0.0001953125, magnitude of gradient - 2.1952555389327473\n",
      "Step - 8839, Loss - 0.631016960727429, Learning Rate - 0.0001953125, magnitude of gradient - 0.586791238756698\n",
      "Step - 8840, Loss - 0.9943420485271756, Learning Rate - 0.0001953125, magnitude of gradient - 1.4854057970662995\n",
      "Step - 8841, Loss - 0.635367709896923, Learning Rate - 0.0001953125, magnitude of gradient - 0.8616901445002558\n",
      "Step - 8842, Loss - 0.8685149417535958, Learning Rate - 0.0001953125, magnitude of gradient - 1.3157518100343344\n",
      "Step - 8843, Loss - 0.694911337161804, Learning Rate - 0.0001953125, magnitude of gradient - 1.4940936184957339\n",
      "Step - 8844, Loss - 0.6506786372516572, Learning Rate - 0.0001953125, magnitude of gradient - 1.03637784692797\n",
      "Step - 8845, Loss - 0.8368105098997889, Learning Rate - 0.0001953125, magnitude of gradient - 2.099315657472712\n",
      "Step - 8846, Loss - 0.5641008335569384, Learning Rate - 0.0001953125, magnitude of gradient - 2.334066613377559\n",
      "Step - 8847, Loss - 0.6193081624154593, Learning Rate - 0.0001953125, magnitude of gradient - 1.328552910803812\n",
      "Step - 8848, Loss - 0.6741770393785647, Learning Rate - 0.0001953125, magnitude of gradient - 0.7580319091331861\n",
      "Step - 8849, Loss - 1.0492455714463746, Learning Rate - 0.0001953125, magnitude of gradient - 2.3933266733598937\n",
      "Step - 8850, Loss - 0.4782958882349962, Learning Rate - 0.0001953125, magnitude of gradient - 3.1345297283942335\n",
      "Step - 8851, Loss - 0.7011516325456675, Learning Rate - 0.0001953125, magnitude of gradient - 0.5763103734087305\n",
      "Step - 8852, Loss - 0.5621766753476404, Learning Rate - 0.0001953125, magnitude of gradient - 1.017616172637154\n",
      "Step - 8853, Loss - 0.5747652409697549, Learning Rate - 0.0001953125, magnitude of gradient - 1.291288376607558\n",
      "Step - 8854, Loss - 0.7927261981898392, Learning Rate - 0.0001953125, magnitude of gradient - 1.7411363391771846\n",
      "Step - 8855, Loss - 0.7251537438822105, Learning Rate - 0.0001953125, magnitude of gradient - 0.9594345702689439\n",
      "Step - 8856, Loss - 0.8893332331310978, Learning Rate - 0.0001953125, magnitude of gradient - 1.3914649016099234\n",
      "Step - 8857, Loss - 0.6897632639676821, Learning Rate - 0.0001953125, magnitude of gradient - 0.9049176744768475\n",
      "Step - 8858, Loss - 0.7088203053262796, Learning Rate - 0.0001953125, magnitude of gradient - 1.328403598037951\n",
      "Step - 8859, Loss - 0.8509320070468425, Learning Rate - 0.0001953125, magnitude of gradient - 1.076136836207097\n",
      "Step - 8860, Loss - 0.877370563402579, Learning Rate - 0.0001953125, magnitude of gradient - 0.6936903224830605\n",
      "Step - 8861, Loss - 0.6212445073464334, Learning Rate - 0.0001953125, magnitude of gradient - 0.7078380239267228\n",
      "Step - 8862, Loss - 0.7237264217383264, Learning Rate - 0.0001953125, magnitude of gradient - 1.6069305715405242\n",
      "Step - 8863, Loss - 0.7885545639058875, Learning Rate - 0.0001953125, magnitude of gradient - 1.6907868036759346\n",
      "Step - 8864, Loss - 0.733260585385746, Learning Rate - 0.0001953125, magnitude of gradient - 0.9405870740343688\n",
      "Step - 8865, Loss - 0.8439537200163545, Learning Rate - 0.0001953125, magnitude of gradient - 1.04373708339654\n",
      "Step - 8866, Loss - 0.6269641250978945, Learning Rate - 0.0001953125, magnitude of gradient - 1.5772280498477684\n",
      "Step - 8867, Loss - 0.9389598430848922, Learning Rate - 0.0001953125, magnitude of gradient - 0.9586764063670674\n",
      "Step - 8868, Loss - 0.7828038220919165, Learning Rate - 0.0001953125, magnitude of gradient - 0.7532111113047218\n",
      "Step - 8869, Loss - 0.6690267542320665, Learning Rate - 0.0001953125, magnitude of gradient - 1.5737299571511318\n",
      "Step - 8870, Loss - 0.7830573629081615, Learning Rate - 0.0001953125, magnitude of gradient - 2.8064010839471405\n",
      "Step - 8871, Loss - 0.7042948951404738, Learning Rate - 0.0001953125, magnitude of gradient - 0.6666566095457613\n",
      "Step - 8872, Loss - 0.6825909186375987, Learning Rate - 0.0001953125, magnitude of gradient - 1.4291759891824423\n",
      "Step - 8873, Loss - 0.5640313710752705, Learning Rate - 0.0001953125, magnitude of gradient - 2.3927290057493247\n",
      "Step - 8874, Loss - 0.7385585333146922, Learning Rate - 0.0001953125, magnitude of gradient - 1.183838886768701\n",
      "Step - 8875, Loss - 0.6433203869198958, Learning Rate - 0.0001953125, magnitude of gradient - 2.022231466767016\n",
      "Step - 8876, Loss - 0.7613283260651627, Learning Rate - 0.0001953125, magnitude of gradient - 0.40775946648940653\n",
      "Step - 8877, Loss - 0.5882911463928067, Learning Rate - 0.0001953125, magnitude of gradient - 1.7323869003588597\n",
      "Step - 8878, Loss - 0.6821151169053833, Learning Rate - 0.0001953125, magnitude of gradient - 0.9422579717761034\n",
      "Step - 8879, Loss - 0.8181820749693804, Learning Rate - 0.0001953125, magnitude of gradient - 0.757508976305572\n",
      "Step - 8880, Loss - 0.7214101352165816, Learning Rate - 0.0001953125, magnitude of gradient - 1.2522946358985199\n",
      "Step - 8881, Loss - 0.6485374525875641, Learning Rate - 0.0001953125, magnitude of gradient - 1.4547278729731843\n",
      "Step - 8882, Loss - 0.5907489738858425, Learning Rate - 0.0001953125, magnitude of gradient - 1.85899892719951\n",
      "Step - 8883, Loss - 0.6355869213925814, Learning Rate - 0.0001953125, magnitude of gradient - 0.7933116099623114\n",
      "Step - 8884, Loss - 0.8188726732165058, Learning Rate - 0.0001953125, magnitude of gradient - 1.9067921208422347\n",
      "Step - 8885, Loss - 0.46965253084717157, Learning Rate - 0.0001953125, magnitude of gradient - 0.6874496568556206\n",
      "Step - 8886, Loss - 0.7230926287835064, Learning Rate - 0.0001953125, magnitude of gradient - 2.1992661749507607\n",
      "Step - 8887, Loss - 0.6106379347628725, Learning Rate - 0.0001953125, magnitude of gradient - 1.3134189854879064\n",
      "Step - 8888, Loss - 0.6519107241163465, Learning Rate - 0.0001953125, magnitude of gradient - 1.648622433255481\n",
      "Step - 8889, Loss - 0.7360782207959085, Learning Rate - 0.0001953125, magnitude of gradient - 0.7082634817148662\n",
      "Step - 8890, Loss - 0.7946268463643591, Learning Rate - 0.0001953125, magnitude of gradient - 0.7112063547281551\n",
      "Step - 8891, Loss - 0.8612046818178616, Learning Rate - 0.0001953125, magnitude of gradient - 0.5752044628103208\n",
      "Step - 8892, Loss - 0.6227939378294337, Learning Rate - 0.0001953125, magnitude of gradient - 3.859583847338013\n",
      "Step - 8893, Loss - 0.7050125234195157, Learning Rate - 0.0001953125, magnitude of gradient - 1.484059221734618\n",
      "Step - 8894, Loss - 0.5800199066490729, Learning Rate - 0.0001953125, magnitude of gradient - 0.7172102743221057\n",
      "Step - 8895, Loss - 0.46626418559817334, Learning Rate - 0.0001953125, magnitude of gradient - 1.7067249445424715\n",
      "Step - 8896, Loss - 0.7176461388331542, Learning Rate - 0.0001953125, magnitude of gradient - 1.1347838346807482\n",
      "Step - 8897, Loss - 0.6646511612892089, Learning Rate - 0.0001953125, magnitude of gradient - 1.4122411460516069\n",
      "Step - 8898, Loss - 0.5569105048668781, Learning Rate - 0.0001953125, magnitude of gradient - 0.8673303232711765\n",
      "Step - 8899, Loss - 0.7804884803890823, Learning Rate - 0.0001953125, magnitude of gradient - 0.8574744619495324\n",
      "Step - 8900, Loss - 0.7977241361765053, Learning Rate - 0.0001953125, magnitude of gradient - 1.4047053888582797\n",
      "Step - 8901, Loss - 0.6501992199249997, Learning Rate - 0.0001953125, magnitude of gradient - 1.1868103376286512\n",
      "Step - 8902, Loss - 0.7361450404109602, Learning Rate - 0.0001953125, magnitude of gradient - 0.9320532849059667\n",
      "Step - 8903, Loss - 0.7954423599444884, Learning Rate - 0.0001953125, magnitude of gradient - 0.8196962491866928\n",
      "Step - 8904, Loss - 0.6417347074094741, Learning Rate - 0.0001953125, magnitude of gradient - 0.8753514771172228\n",
      "Step - 8905, Loss - 0.5131156101274146, Learning Rate - 0.0001953125, magnitude of gradient - 1.1385463501084232\n",
      "Step - 8906, Loss - 0.6456569223688844, Learning Rate - 0.0001953125, magnitude of gradient - 0.5515479172238121\n",
      "Step - 8907, Loss - 0.5966098375699455, Learning Rate - 0.0001953125, magnitude of gradient - 1.0617084884363421\n",
      "Step - 8908, Loss - 0.556080257986821, Learning Rate - 0.0001953125, magnitude of gradient - 1.7260271325195438\n",
      "Step - 8909, Loss - 0.4928821919751498, Learning Rate - 0.0001953125, magnitude of gradient - 0.9147261035971824\n",
      "Step - 8910, Loss - 0.7475192757894605, Learning Rate - 0.0001953125, magnitude of gradient - 0.6221187029676983\n",
      "Step - 8911, Loss - 0.690422520306128, Learning Rate - 0.0001953125, magnitude of gradient - 1.526234011209527\n",
      "Step - 8912, Loss - 0.7216349131233053, Learning Rate - 0.0001953125, magnitude of gradient - 1.9981629785360173\n",
      "Step - 8913, Loss - 0.6327391526329876, Learning Rate - 0.0001953125, magnitude of gradient - 1.4855979798901495\n",
      "Step - 8914, Loss - 0.7759325918443146, Learning Rate - 0.0001953125, magnitude of gradient - 0.5985923053211482\n",
      "Step - 8915, Loss - 0.701411047351239, Learning Rate - 0.0001953125, magnitude of gradient - 0.724856383443879\n",
      "Step - 8916, Loss - 0.6580054997700487, Learning Rate - 0.0001953125, magnitude of gradient - 1.1865587526711854\n",
      "Step - 8917, Loss - 0.6905074782876706, Learning Rate - 0.0001953125, magnitude of gradient - 0.4941528648313435\n",
      "Step - 8918, Loss - 0.9020832818162773, Learning Rate - 0.0001953125, magnitude of gradient - 1.3929550096318977\n",
      "Step - 8919, Loss - 0.6333195114727799, Learning Rate - 0.0001953125, magnitude of gradient - 0.9862119810250112\n",
      "Step - 8920, Loss - 0.6327451401511395, Learning Rate - 0.0001953125, magnitude of gradient - 1.218169999662769\n",
      "Step - 8921, Loss - 0.729298718934793, Learning Rate - 0.0001953125, magnitude of gradient - 1.02347409816965\n",
      "Step - 8922, Loss - 0.7925349642649409, Learning Rate - 0.0001953125, magnitude of gradient - 0.711460876505062\n",
      "Step - 8923, Loss - 0.6351174056080706, Learning Rate - 0.0001953125, magnitude of gradient - 1.9875014342571113\n",
      "Step - 8924, Loss - 0.6616800666282812, Learning Rate - 0.0001953125, magnitude of gradient - 0.3185237586434879\n",
      "Step - 8925, Loss - 0.6597581416505695, Learning Rate - 0.0001953125, magnitude of gradient - 0.7168892668171848\n",
      "Step - 8926, Loss - 0.7821929338787119, Learning Rate - 0.0001953125, magnitude of gradient - 2.2335570663563242\n",
      "Step - 8927, Loss - 0.6880196181406852, Learning Rate - 0.0001953125, magnitude of gradient - 1.931058070227249\n",
      "Step - 8928, Loss - 0.78419707839116, Learning Rate - 0.0001953125, magnitude of gradient - 2.451297878541628\n",
      "Step - 8929, Loss - 0.7319291754573669, Learning Rate - 0.0001953125, magnitude of gradient - 1.1602209513320219\n",
      "Step - 8930, Loss - 0.5486114510188511, Learning Rate - 0.0001953125, magnitude of gradient - 0.9151015524003\n",
      "Step - 8931, Loss - 0.6944405612520484, Learning Rate - 0.0001953125, magnitude of gradient - 1.429733452963567\n",
      "Step - 8932, Loss - 0.5883981563123156, Learning Rate - 0.0001953125, magnitude of gradient - 1.4449427770786116\n",
      "Step - 8933, Loss - 0.661364587356909, Learning Rate - 0.0001953125, magnitude of gradient - 1.625990910189399\n",
      "Step - 8934, Loss - 0.8731683646609325, Learning Rate - 0.0001953125, magnitude of gradient - 0.9181440141664394\n",
      "Step - 8935, Loss - 0.6336157638826193, Learning Rate - 0.0001953125, magnitude of gradient - 1.4788225472793854\n",
      "Step - 8936, Loss - 0.5500400756092112, Learning Rate - 0.0001953125, magnitude of gradient - 1.9106295529131134\n",
      "Step - 8937, Loss - 0.7336667777393403, Learning Rate - 0.0001953125, magnitude of gradient - 1.3132216619582326\n",
      "Step - 8938, Loss - 0.6719572454275663, Learning Rate - 0.0001953125, magnitude of gradient - 1.2437437291567481\n",
      "Step - 8939, Loss - 0.6716144587761215, Learning Rate - 0.0001953125, magnitude of gradient - 1.7223719465977492\n",
      "Step - 8940, Loss - 0.6449580045373402, Learning Rate - 0.0001953125, magnitude of gradient - 3.5637177063817647\n",
      "Step - 8941, Loss - 0.7966394083201277, Learning Rate - 0.0001953125, magnitude of gradient - 1.7818731042137037\n",
      "Step - 8942, Loss - 0.9705994197215754, Learning Rate - 0.0001953125, magnitude of gradient - 0.7707892223844888\n",
      "Step - 8943, Loss - 0.7365178695694348, Learning Rate - 0.0001953125, magnitude of gradient - 0.6628562910342394\n",
      "Step - 8944, Loss - 0.7155400675985677, Learning Rate - 0.0001953125, magnitude of gradient - 0.9170712830249018\n",
      "Step - 8945, Loss - 0.5432105669009774, Learning Rate - 0.0001953125, magnitude of gradient - 1.1539429355735036\n",
      "Step - 8946, Loss - 0.7646118163603763, Learning Rate - 0.0001953125, magnitude of gradient - 1.8119409161903097\n",
      "Step - 8947, Loss - 0.730686591390988, Learning Rate - 0.0001953125, magnitude of gradient - 1.9526326636852263\n",
      "Step - 8948, Loss - 0.5508323615530216, Learning Rate - 0.0001953125, magnitude of gradient - 1.2023853630323877\n",
      "Step - 8949, Loss - 0.7951160671608449, Learning Rate - 0.0001953125, magnitude of gradient - 1.793320705150309\n",
      "Step - 8950, Loss - 0.7110976936080977, Learning Rate - 0.0001953125, magnitude of gradient - 1.270663001159969\n",
      "Step - 8951, Loss - 0.7559545455921147, Learning Rate - 0.0001953125, magnitude of gradient - 1.3225204891470859\n",
      "Step - 8952, Loss - 0.7922555378607133, Learning Rate - 0.0001953125, magnitude of gradient - 1.950291716908425\n",
      "Step - 8953, Loss - 0.6315118819185503, Learning Rate - 0.0001953125, magnitude of gradient - 1.682986897096153\n",
      "Step - 8954, Loss - 0.5956767922629811, Learning Rate - 0.0001953125, magnitude of gradient - 0.7003622089616132\n",
      "Step - 8955, Loss - 0.7361345200066565, Learning Rate - 0.0001953125, magnitude of gradient - 1.27400618430855\n",
      "Step - 8956, Loss - 0.7128232186935677, Learning Rate - 0.0001953125, magnitude of gradient - 1.2615968995190974\n",
      "Step - 8957, Loss - 0.7834304729246488, Learning Rate - 0.0001953125, magnitude of gradient - 1.2011265115958634\n",
      "Step - 8958, Loss - 0.7431693159715943, Learning Rate - 0.0001953125, magnitude of gradient - 0.7994529031999966\n",
      "Step - 8959, Loss - 0.5709039513293553, Learning Rate - 0.0001953125, magnitude of gradient - 2.1671168547258333\n",
      "Step - 8960, Loss - 0.6181333283539584, Learning Rate - 0.0001953125, magnitude of gradient - 0.649215210734083\n",
      "Step - 8961, Loss - 0.6064842680850739, Learning Rate - 0.0001953125, magnitude of gradient - 0.6407216439270108\n",
      "Step - 8962, Loss - 0.5694115009388299, Learning Rate - 0.0001953125, magnitude of gradient - 2.006799364100145\n",
      "Step - 8963, Loss - 0.8374582889053248, Learning Rate - 0.0001953125, magnitude of gradient - 1.001635333137109\n",
      "Step - 8964, Loss - 1.0166075980592901, Learning Rate - 0.0001953125, magnitude of gradient - 3.1351885850079837\n",
      "Step - 8965, Loss - 0.7397016714708752, Learning Rate - 0.0001953125, magnitude of gradient - 1.8428659161789753\n",
      "Step - 8966, Loss - 0.9515705549853336, Learning Rate - 0.0001953125, magnitude of gradient - 1.6501476341745567\n",
      "Step - 8967, Loss - 0.6340764992056189, Learning Rate - 0.0001953125, magnitude of gradient - 1.1230696412006358\n",
      "Step - 8968, Loss - 0.7455583450627059, Learning Rate - 0.0001953125, magnitude of gradient - 0.6801300397236565\n",
      "Step - 8969, Loss - 0.7915218337065468, Learning Rate - 0.0001953125, magnitude of gradient - 2.610766027323712\n",
      "Step - 8970, Loss - 0.7783762924913605, Learning Rate - 0.0001953125, magnitude of gradient - 1.5590460397062538\n",
      "Step - 8971, Loss - 0.7826426012984024, Learning Rate - 0.0001953125, magnitude of gradient - 0.7986560622468893\n",
      "Step - 8972, Loss - 0.751206599047757, Learning Rate - 0.0001953125, magnitude of gradient - 1.4472538988880408\n",
      "Step - 8973, Loss - 0.881622066786772, Learning Rate - 0.0001953125, magnitude of gradient - 1.165944520196482\n",
      "Step - 8974, Loss - 0.7252480857318208, Learning Rate - 0.0001953125, magnitude of gradient - 0.5185688530883009\n",
      "Step - 8975, Loss - 0.6244615636921087, Learning Rate - 0.0001953125, magnitude of gradient - 0.6556889308699693\n",
      "Step - 8976, Loss - 0.5725624379623138, Learning Rate - 0.0001953125, magnitude of gradient - 0.47607125401966305\n",
      "Step - 8977, Loss - 0.5631019831702083, Learning Rate - 0.0001953125, magnitude of gradient - 1.8561488587293975\n",
      "Step - 8978, Loss - 0.6728821763543945, Learning Rate - 0.0001953125, magnitude of gradient - 2.760917699150454\n",
      "Step - 8979, Loss - 0.5731795682780438, Learning Rate - 0.0001953125, magnitude of gradient - 1.7816753367790035\n",
      "Step - 8980, Loss - 0.6723965294632006, Learning Rate - 0.0001953125, magnitude of gradient - 1.5657694191570182\n",
      "Step - 8981, Loss - 0.6943149816096624, Learning Rate - 0.0001953125, magnitude of gradient - 1.5421326968201199\n",
      "Step - 8982, Loss - 0.7810403462371887, Learning Rate - 0.0001953125, magnitude of gradient - 1.0863073309304292\n",
      "Step - 8983, Loss - 0.5595151115461428, Learning Rate - 0.0001953125, magnitude of gradient - 0.8150831546880904\n",
      "Step - 8984, Loss - 0.7485415450202196, Learning Rate - 0.0001953125, magnitude of gradient - 1.8450583846237516\n",
      "Step - 8985, Loss - 0.8037584438675451, Learning Rate - 0.0001953125, magnitude of gradient - 0.4511772095202608\n",
      "Step - 8986, Loss - 0.8307621920485939, Learning Rate - 0.0001953125, magnitude of gradient - 1.2758705025435517\n",
      "Step - 8987, Loss - 0.8758806669001283, Learning Rate - 0.0001953125, magnitude of gradient - 1.9517659682171997\n",
      "Step - 8988, Loss - 0.6825262562890302, Learning Rate - 0.0001953125, magnitude of gradient - 0.9363824454374464\n",
      "Step - 8989, Loss - 0.6033985878601471, Learning Rate - 0.0001953125, magnitude of gradient - 0.7184208379891561\n",
      "Step - 8990, Loss - 0.5820780870896524, Learning Rate - 0.0001953125, magnitude of gradient - 0.5357782490998931\n",
      "Step - 8991, Loss - 0.6545646142816788, Learning Rate - 0.0001953125, magnitude of gradient - 1.4942596208123184\n",
      "Step - 8992, Loss - 0.6483233798703979, Learning Rate - 0.0001953125, magnitude of gradient - 1.204309085377985\n",
      "Step - 8993, Loss - 0.681448208659095, Learning Rate - 0.0001953125, magnitude of gradient - 0.9111843036596499\n",
      "Step - 8994, Loss - 0.6657732791739903, Learning Rate - 0.0001953125, magnitude of gradient - 2.0027510006351075\n",
      "Step - 8995, Loss - 0.6860548452504626, Learning Rate - 0.0001953125, magnitude of gradient - 0.6485004719674758\n",
      "Step - 8996, Loss - 0.743601202565515, Learning Rate - 0.0001953125, magnitude of gradient - 0.9691746135887841\n",
      "Step - 8997, Loss - 0.799489387926229, Learning Rate - 0.0001953125, magnitude of gradient - 1.2125041039978286\n",
      "Step - 8998, Loss - 0.6627552964529533, Learning Rate - 0.0001953125, magnitude of gradient - 1.3754507377711231\n",
      "Step - 8999, Loss - 0.6175929823843133, Learning Rate - 0.0001953125, magnitude of gradient - 0.7073222542534021\n",
      "Step - 9000, Loss - 0.7075664941104863, Learning Rate - 0.0001953125, magnitude of gradient - 0.8087718908627982\n",
      "Step - 9001, Loss - 0.8338460880758835, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3412740578809113\n",
      "Step - 9002, Loss - 0.6346656832844031, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2181206597931438\n",
      "Step - 9003, Loss - 0.687982267601341, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1977688802320994\n",
      "Step - 9004, Loss - 0.7143045887017296, Learning Rate - 9.765625e-05, magnitude of gradient - 2.0392196752701826\n",
      "Step - 9005, Loss - 0.5311924365127783, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9959364122901584\n",
      "Step - 9006, Loss - 0.8058133111389142, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1866967908516186\n",
      "Step - 9007, Loss - 0.5407965320515326, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1269804803276522\n",
      "Step - 9008, Loss - 0.8390489182102021, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7874364925914996\n",
      "Step - 9009, Loss - 0.7501034111707259, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4540452582720884\n",
      "Step - 9010, Loss - 0.832894658040259, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9102105985942415\n",
      "Step - 9011, Loss - 0.6900220833698643, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9559713177799385\n",
      "Step - 9012, Loss - 0.6764282196712345, Learning Rate - 9.765625e-05, magnitude of gradient - 2.345763349697669\n",
      "Step - 9013, Loss - 0.8618374694295134, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6609698773775483\n",
      "Step - 9014, Loss - 0.6144261648783321, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9456195249206825\n",
      "Step - 9015, Loss - 0.7208178185463814, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6917929163934209\n",
      "Step - 9016, Loss - 0.7076591443083807, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1087022876481767\n",
      "Step - 9017, Loss - 0.7155606190071189, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8507147131467648\n",
      "Step - 9018, Loss - 0.6885263246137929, Learning Rate - 9.765625e-05, magnitude of gradient - 1.089874254169938\n",
      "Step - 9019, Loss - 0.7244918159291758, Learning Rate - 9.765625e-05, magnitude of gradient - 1.54547599716231\n",
      "Step - 9020, Loss - 0.6054215959316627, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9688529855457984\n",
      "Step - 9021, Loss - 0.7829322364161357, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8334939527010067\n",
      "Step - 9022, Loss - 0.5099512652479857, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5625096960705034\n",
      "Step - 9023, Loss - 0.8065340464388668, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0728478929614151\n",
      "Step - 9024, Loss - 0.7520487394050557, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1776111923841976\n",
      "Step - 9025, Loss - 0.6844905026604371, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6997222850705874\n",
      "Step - 9026, Loss - 0.8286610730239342, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9423388990223298\n",
      "Step - 9027, Loss - 0.8049848180075201, Learning Rate - 9.765625e-05, magnitude of gradient - 1.479082982589105\n",
      "Step - 9028, Loss - 0.6263133235258408, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8326936466938253\n",
      "Step - 9029, Loss - 0.7911401403278174, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8954926661224358\n",
      "Step - 9030, Loss - 0.5501678721524972, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1179013902154038\n",
      "Step - 9031, Loss - 0.7235785799290277, Learning Rate - 9.765625e-05, magnitude of gradient - 2.8773781309075193\n",
      "Step - 9032, Loss - 0.7004806425657372, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1337520638988918\n",
      "Step - 9033, Loss - 0.7023546712750173, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5713326635410148\n",
      "Step - 9034, Loss - 0.8380892692099307, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4386705803119868\n",
      "Step - 9035, Loss - 0.7253274811460731, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2631233301082192\n",
      "Step - 9036, Loss - 0.8022657333431797, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0852484212311786\n",
      "Step - 9037, Loss - 0.5830439182444322, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7714571407872872\n",
      "Step - 9038, Loss - 0.7486203765073961, Learning Rate - 9.765625e-05, magnitude of gradient - 0.788173352677002\n",
      "Step - 9039, Loss - 0.662045507658748, Learning Rate - 9.765625e-05, magnitude of gradient - 1.129813172917648\n",
      "Step - 9040, Loss - 0.85625584513323, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3371352243702315\n",
      "Step - 9041, Loss - 0.6620915427781187, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8255592160061376\n",
      "Step - 9042, Loss - 0.6375654665453561, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0090552823244257\n",
      "Step - 9043, Loss - 0.8229548201517559, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5380263644354566\n",
      "Step - 9044, Loss - 0.5997165515499856, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4550512855095342\n",
      "Step - 9045, Loss - 0.7656600552680604, Learning Rate - 9.765625e-05, magnitude of gradient - 2.269174689333692\n",
      "Step - 9046, Loss - 0.6180337108178895, Learning Rate - 9.765625e-05, magnitude of gradient - 1.687511748443603\n",
      "Step - 9047, Loss - 0.6649246530548079, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0720731766780611\n",
      "Step - 9048, Loss - 0.6160109600076358, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7565935635060157\n",
      "Step - 9049, Loss - 0.6964046998881902, Learning Rate - 9.765625e-05, magnitude of gradient - 0.3721192987661456\n",
      "Step - 9050, Loss - 0.8954434637742827, Learning Rate - 9.765625e-05, magnitude of gradient - 0.40478496828301336\n",
      "Step - 9051, Loss - 0.49500765504747163, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9821012972509443\n",
      "Step - 9052, Loss - 0.6833523354819374, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8371461088203789\n",
      "Step - 9053, Loss - 0.7340371706925151, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5359445828117961\n",
      "Step - 9054, Loss - 0.6865161109980734, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6951509869954033\n",
      "Step - 9055, Loss - 0.559155546220531, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1247265415199523\n",
      "Step - 9056, Loss - 0.45736111749649944, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0705129355554097\n",
      "Step - 9057, Loss - 0.6518972883645077, Learning Rate - 9.765625e-05, magnitude of gradient - 0.827018399981617\n",
      "Step - 9058, Loss - 0.7083911231853002, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8385104904384673\n",
      "Step - 9059, Loss - 0.734001621661669, Learning Rate - 9.765625e-05, magnitude of gradient - 2.1023926435574896\n",
      "Step - 9060, Loss - 0.677089641939244, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4500470529306702\n",
      "Step - 9061, Loss - 0.7025094600516794, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7109606307544074\n",
      "Step - 9062, Loss - 0.5457257863094851, Learning Rate - 9.765625e-05, magnitude of gradient - 2.0372329344920725\n",
      "Step - 9063, Loss - 0.906329099971008, Learning Rate - 9.765625e-05, magnitude of gradient - 1.498794587848269\n",
      "Step - 9064, Loss - 0.9755535424374017, Learning Rate - 9.765625e-05, magnitude of gradient - 1.82526845224838\n",
      "Step - 9065, Loss - 0.5575668565583154, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3801663151036867\n",
      "Step - 9066, Loss - 0.7337219832493609, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6337551064855416\n",
      "Step - 9067, Loss - 0.48104667863312756, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5915350619421431\n",
      "Step - 9068, Loss - 0.5662264902486187, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2547180427300062\n",
      "Step - 9069, Loss - 0.5682081813631463, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1950443915352587\n",
      "Step - 9070, Loss - 0.4413576872880042, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7291971373454451\n",
      "Step - 9071, Loss - 0.7121219699223144, Learning Rate - 9.765625e-05, magnitude of gradient - 0.2876346467882897\n",
      "Step - 9072, Loss - 0.625298924715909, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8007251137087954\n",
      "Step - 9073, Loss - 0.8459791381284121, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2732715116079885\n",
      "Step - 9074, Loss - 0.6973859551551341, Learning Rate - 9.765625e-05, magnitude of gradient - 1.189006124652474\n",
      "Step - 9075, Loss - 0.65206842111402, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2630765007505649\n",
      "Step - 9076, Loss - 0.7636946942486549, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8664334396577483\n",
      "Step - 9077, Loss - 0.7282268184927023, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5705221723988405\n",
      "Step - 9078, Loss - 0.5854964686678684, Learning Rate - 9.765625e-05, magnitude of gradient - 0.3994702206473821\n",
      "Step - 9079, Loss - 0.6997326567611151, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2869815997327483\n",
      "Step - 9080, Loss - 0.5811063559657504, Learning Rate - 9.765625e-05, magnitude of gradient - 2.104695151952433\n",
      "Step - 9081, Loss - 0.7766813053744814, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5754541097406696\n",
      "Step - 9082, Loss - 0.6895201223895434, Learning Rate - 9.765625e-05, magnitude of gradient - 2.036388764519453\n",
      "Step - 9083, Loss - 0.7600028834175927, Learning Rate - 9.765625e-05, magnitude of gradient - 1.589194337341011\n",
      "Step - 9084, Loss - 0.5798297919118539, Learning Rate - 9.765625e-05, magnitude of gradient - 3.423697709513691\n",
      "Step - 9085, Loss - 0.6280019311330219, Learning Rate - 9.765625e-05, magnitude of gradient - 1.077684603704305\n",
      "Step - 9086, Loss - 0.6000382979860821, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1094215085220946\n",
      "Step - 9087, Loss - 0.709898821548539, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6648643981608128\n",
      "Step - 9088, Loss - 0.575945418918179, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5521872476691319\n",
      "Step - 9089, Loss - 0.5934312202156683, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5599879259841626\n",
      "Step - 9090, Loss - 0.8461170955354163, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1853324142509816\n",
      "Step - 9091, Loss - 0.8284018976393008, Learning Rate - 9.765625e-05, magnitude of gradient - 2.956401488749103\n",
      "Step - 9092, Loss - 0.8126490954899384, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9894011134177515\n",
      "Step - 9093, Loss - 0.741551768717064, Learning Rate - 9.765625e-05, magnitude of gradient - 2.0544472130649942\n",
      "Step - 9094, Loss - 0.589278707717253, Learning Rate - 9.765625e-05, magnitude of gradient - 2.2020235378212805\n",
      "Step - 9095, Loss - 0.6138610738045822, Learning Rate - 9.765625e-05, magnitude of gradient - 0.47161578444488644\n",
      "Step - 9096, Loss - 0.6238603425927653, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5739146345503128\n",
      "Step - 9097, Loss - 0.5844260097728227, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3000461642241015\n",
      "Step - 9098, Loss - 0.799716103701966, Learning Rate - 9.765625e-05, magnitude of gradient - 1.00679521250658\n",
      "Step - 9099, Loss - 0.6955124463283877, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8608704789250405\n",
      "Step - 9100, Loss - 0.6679818621077112, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6579412529993239\n",
      "Step - 9101, Loss - 0.5908524233250003, Learning Rate - 9.765625e-05, magnitude of gradient - 1.068133650413941\n",
      "Step - 9102, Loss - 0.7375886561236544, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3998135884065237\n",
      "Step - 9103, Loss - 0.5637768172287, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1852281033975482\n",
      "Step - 9104, Loss - 0.7546277311730085, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6382952797784686\n",
      "Step - 9105, Loss - 0.6496949387552734, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7044887619185513\n",
      "Step - 9106, Loss - 0.6903212727239466, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8039919772551276\n",
      "Step - 9107, Loss - 0.7428211485326942, Learning Rate - 9.765625e-05, magnitude of gradient - 0.521849836594488\n",
      "Step - 9108, Loss - 0.6943207245321088, Learning Rate - 9.765625e-05, magnitude of gradient - 0.22614826647236907\n",
      "Step - 9109, Loss - 0.8593658492059867, Learning Rate - 9.765625e-05, magnitude of gradient - 2.3310595240122387\n",
      "Step - 9110, Loss - 0.7118696743078561, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1812169331135072\n",
      "Step - 9111, Loss - 0.799945612376829, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7162519651160664\n",
      "Step - 9112, Loss - 1.0037677522025041, Learning Rate - 9.765625e-05, magnitude of gradient - 2.778300267302763\n",
      "Step - 9113, Loss - 0.5826961940158752, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4173070538479272\n",
      "Step - 9114, Loss - 0.7444425802281223, Learning Rate - 9.765625e-05, magnitude of gradient - 0.44201380912686117\n",
      "Step - 9115, Loss - 0.5424887352844909, Learning Rate - 9.765625e-05, magnitude of gradient - 1.763849140089884\n",
      "Step - 9116, Loss - 0.9386574310637645, Learning Rate - 9.765625e-05, magnitude of gradient - 1.028653266468949\n",
      "Step - 9117, Loss - 0.7665288677299359, Learning Rate - 9.765625e-05, magnitude of gradient - 0.3904045378551537\n",
      "Step - 9118, Loss - 0.6426544998672155, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5281101340888763\n",
      "Step - 9119, Loss - 0.6662450394028607, Learning Rate - 9.765625e-05, magnitude of gradient - 2.9306424226834955\n",
      "Step - 9120, Loss - 0.697187563981324, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7106446246294154\n",
      "Step - 9121, Loss - 0.49293203114222073, Learning Rate - 9.765625e-05, magnitude of gradient - 2.684798972931882\n",
      "Step - 9122, Loss - 0.7068705829253416, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6804910456642265\n",
      "Step - 9123, Loss - 0.7674892883663451, Learning Rate - 9.765625e-05, magnitude of gradient - 1.382258468530269\n",
      "Step - 9124, Loss - 0.865032474152821, Learning Rate - 9.765625e-05, magnitude of gradient - 1.59481763685545\n",
      "Step - 9125, Loss - 0.7080373044442427, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6987913886138651\n",
      "Step - 9126, Loss - 0.7168482586737572, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5499205682452784\n",
      "Step - 9127, Loss - 0.6609111254359223, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9950768991067724\n",
      "Step - 9128, Loss - 0.8627372762644813, Learning Rate - 9.765625e-05, magnitude of gradient - 1.513320273261262\n",
      "Step - 9129, Loss - 0.7166844632586492, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4035633308725191\n",
      "Step - 9130, Loss - 0.6014743252597028, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6019498827122667\n",
      "Step - 9131, Loss - 0.6530232214535081, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8382544891236156\n",
      "Step - 9132, Loss - 0.6061593625116128, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0354884581384034\n",
      "Step - 9133, Loss - 0.6839097082347442, Learning Rate - 9.765625e-05, magnitude of gradient - 1.034990102365853\n",
      "Step - 9134, Loss - 0.9472423769709003, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1322773556550025\n",
      "Step - 9135, Loss - 0.739813411483601, Learning Rate - 9.765625e-05, magnitude of gradient - 1.14523798456008\n",
      "Step - 9136, Loss - 0.8230176900300932, Learning Rate - 9.765625e-05, magnitude of gradient - 2.0763233919699866\n",
      "Step - 9137, Loss - 0.6097924782592355, Learning Rate - 9.765625e-05, magnitude of gradient - 0.34496071608303\n",
      "Step - 9138, Loss - 0.7251285198604162, Learning Rate - 9.765625e-05, magnitude of gradient - 2.0030384191395383\n",
      "Step - 9139, Loss - 0.6530985628210892, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0603055485561175\n",
      "Step - 9140, Loss - 0.6268159835196564, Learning Rate - 9.765625e-05, magnitude of gradient - 3.3613168874273747\n",
      "Step - 9141, Loss - 0.6077581765031645, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0495848986457805\n",
      "Step - 9142, Loss - 0.4839587023441706, Learning Rate - 9.765625e-05, magnitude of gradient - 1.304523876427761\n",
      "Step - 9143, Loss - 0.7109657863639811, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5205099713742032\n",
      "Step - 9144, Loss - 0.5627362748470284, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4841720194893189\n",
      "Step - 9145, Loss - 0.7800276455519684, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0836076652503628\n",
      "Step - 9146, Loss - 0.5028275124365633, Learning Rate - 9.765625e-05, magnitude of gradient - 1.067984247256637\n",
      "Step - 9147, Loss - 0.5892207145784446, Learning Rate - 9.765625e-05, magnitude of gradient - 0.809857012844878\n",
      "Step - 9148, Loss - 0.5890410466992271, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6338932288053174\n",
      "Step - 9149, Loss - 0.781492214382503, Learning Rate - 9.765625e-05, magnitude of gradient - 1.015222785616164\n",
      "Step - 9150, Loss - 0.712655303944357, Learning Rate - 9.765625e-05, magnitude of gradient - 1.34735493838414\n",
      "Step - 9151, Loss - 0.6467079221967301, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4619722342466756\n",
      "Step - 9152, Loss - 0.805740859677188, Learning Rate - 9.765625e-05, magnitude of gradient - 2.056843096883698\n",
      "Step - 9153, Loss - 0.5330320099611393, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7195753927389998\n",
      "Step - 9154, Loss - 0.7794628006284171, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9656492156687501\n",
      "Step - 9155, Loss - 0.47450307564520056, Learning Rate - 9.765625e-05, magnitude of gradient - 0.28749128968702253\n",
      "Step - 9156, Loss - 0.6722146264839807, Learning Rate - 9.765625e-05, magnitude of gradient - 1.640903363206579\n",
      "Step - 9157, Loss - 0.8006006881420035, Learning Rate - 9.765625e-05, magnitude of gradient - 0.47434474295418766\n",
      "Step - 9158, Loss - 0.6590561236860449, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0754900852578906\n",
      "Step - 9159, Loss - 0.7656732684073884, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7710036918090021\n",
      "Step - 9160, Loss - 0.7498498143249254, Learning Rate - 9.765625e-05, magnitude of gradient - 1.488222664381542\n",
      "Step - 9161, Loss - 0.865605950934908, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5004330844860563\n",
      "Step - 9162, Loss - 0.6918917703656895, Learning Rate - 9.765625e-05, magnitude of gradient - 0.878573953339562\n",
      "Step - 9163, Loss - 0.7747466967639153, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8314852502420231\n",
      "Step - 9164, Loss - 0.7277334312990703, Learning Rate - 9.765625e-05, magnitude of gradient - 1.327517690256215\n",
      "Step - 9165, Loss - 0.5806738415125365, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5572346253466358\n",
      "Step - 9166, Loss - 0.7654039198996532, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7876450503341625\n",
      "Step - 9167, Loss - 0.9364899868535584, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7166250132203705\n",
      "Step - 9168, Loss - 0.7379696561815681, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1703585691973974\n",
      "Step - 9169, Loss - 0.9016171831713292, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6975755356503405\n",
      "Step - 9170, Loss - 0.7609838413595086, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7226683008975008\n",
      "Step - 9171, Loss - 0.6645894002990306, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9140145446372336\n",
      "Step - 9172, Loss - 0.5823452717945884, Learning Rate - 9.765625e-05, magnitude of gradient - 1.58045023735434\n",
      "Step - 9173, Loss - 0.553413608529037, Learning Rate - 9.765625e-05, magnitude of gradient - 1.263474485625017\n",
      "Step - 9174, Loss - 0.9235962667135559, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5006231681182334\n",
      "Step - 9175, Loss - 0.8079034778260665, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0080274905711932\n",
      "Step - 9176, Loss - 0.9353411247278108, Learning Rate - 9.765625e-05, magnitude of gradient - 2.6407469638799164\n",
      "Step - 9177, Loss - 0.5759283069244946, Learning Rate - 9.765625e-05, magnitude of gradient - 2.1680594125347574\n",
      "Step - 9178, Loss - 0.8029080871099533, Learning Rate - 9.765625e-05, magnitude of gradient - 1.116660357306864\n",
      "Step - 9179, Loss - 0.685615954738471, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8764186970072693\n",
      "Step - 9180, Loss - 0.5696899353443643, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1675557202884845\n",
      "Step - 9181, Loss - 0.6020175850793874, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8184725402363373\n",
      "Step - 9182, Loss - 0.5038363261575882, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2386110216126094\n",
      "Step - 9183, Loss - 0.7079807409048938, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3470113256612437\n",
      "Step - 9184, Loss - 0.7253201455857627, Learning Rate - 9.765625e-05, magnitude of gradient - 0.49744654725106\n",
      "Step - 9185, Loss - 0.6070537759918581, Learning Rate - 9.765625e-05, magnitude of gradient - 0.663304955327979\n",
      "Step - 9186, Loss - 0.7213018495467647, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6644106269241336\n",
      "Step - 9187, Loss - 0.7130539614814084, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6817453843084371\n",
      "Step - 9188, Loss - 0.685686545734129, Learning Rate - 9.765625e-05, magnitude of gradient - 2.197728550545344\n",
      "Step - 9189, Loss - 0.5633406057694509, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1398798424990062\n",
      "Step - 9190, Loss - 0.6174807679277635, Learning Rate - 9.765625e-05, magnitude of gradient - 2.111911495356591\n",
      "Step - 9191, Loss - 0.7300633403368263, Learning Rate - 9.765625e-05, magnitude of gradient - 1.22546158018567\n",
      "Step - 9192, Loss - 0.7410797564687162, Learning Rate - 9.765625e-05, magnitude of gradient - 1.164394958998575\n",
      "Step - 9193, Loss - 0.6056557200648345, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7226556580671061\n",
      "Step - 9194, Loss - 0.8580382100897181, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6711782711197406\n",
      "Step - 9195, Loss - 0.7090151533340953, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0465645083960602\n",
      "Step - 9196, Loss - 0.6106797880399506, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7986563705960649\n",
      "Step - 9197, Loss - 0.8656067028108274, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6293117856448984\n",
      "Step - 9198, Loss - 0.590972503311274, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0726359723387782\n",
      "Step - 9199, Loss - 0.6527755723353876, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6632072896185643\n",
      "Step - 9200, Loss - 0.6549812082010965, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8211901204835077\n",
      "Step - 9201, Loss - 0.6255081953605302, Learning Rate - 9.765625e-05, magnitude of gradient - 3.1760353625019624\n",
      "Step - 9202, Loss - 0.6764058587741967, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7506941022537268\n",
      "Step - 9203, Loss - 0.5101612582137167, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1711265092743406\n",
      "Step - 9204, Loss - 0.6208374681991192, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8394820100251651\n",
      "Step - 9205, Loss - 0.8102861659774765, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8827655195234945\n",
      "Step - 9206, Loss - 0.6174669272936205, Learning Rate - 9.765625e-05, magnitude of gradient - 1.545601122807263\n",
      "Step - 9207, Loss - 0.7447825991678014, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3641870820256436\n",
      "Step - 9208, Loss - 0.7779333667056615, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4260336933507926\n",
      "Step - 9209, Loss - 0.6968886624141997, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1855289360395094\n",
      "Step - 9210, Loss - 0.9170076699720122, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7753032000402398\n",
      "Step - 9211, Loss - 0.6214095444701488, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8938712286409298\n",
      "Step - 9212, Loss - 0.6733053297512617, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0577124191191545\n",
      "Step - 9213, Loss - 0.8440963407485007, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5920034643545498\n",
      "Step - 9214, Loss - 0.5405501936396039, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6748767986405496\n",
      "Step - 9215, Loss - 0.7565007030035329, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7973165961820832\n",
      "Step - 9216, Loss - 0.6702729464771966, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9144714725319874\n",
      "Step - 9217, Loss - 0.6145083173658199, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3579328142339695\n",
      "Step - 9218, Loss - 0.977091340010839, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4645090617709169\n",
      "Step - 9219, Loss - 0.7886887268478149, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0514752325565553\n",
      "Step - 9220, Loss - 0.6919210644349979, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3245224060939003\n",
      "Step - 9221, Loss - 0.8212837306645028, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1010699355816023\n",
      "Step - 9222, Loss - 0.9132225668007494, Learning Rate - 9.765625e-05, magnitude of gradient - 1.59724257184838\n",
      "Step - 9223, Loss - 0.6150170665823348, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3276732258305608\n",
      "Step - 9224, Loss - 0.663592180860537, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7356114872888475\n",
      "Step - 9225, Loss - 0.6235888719645255, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1127180426084382\n",
      "Step - 9226, Loss - 0.68361208584071, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9131011953718409\n",
      "Step - 9227, Loss - 0.775535179896243, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8162393315285784\n",
      "Step - 9228, Loss - 0.4849927963431181, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5638311930003026\n",
      "Step - 9229, Loss - 0.7839200410223104, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7283831159974187\n",
      "Step - 9230, Loss - 0.7016825654963238, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2092463270085225\n",
      "Step - 9231, Loss - 0.5303655776061054, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9416533940373677\n",
      "Step - 9232, Loss - 0.5468947331275431, Learning Rate - 9.765625e-05, magnitude of gradient - 0.623879839408366\n",
      "Step - 9233, Loss - 0.76960971750696, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6822804745898301\n",
      "Step - 9234, Loss - 0.6708936075924532, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9693587095637488\n",
      "Step - 9235, Loss - 0.6461559164899682, Learning Rate - 9.765625e-05, magnitude of gradient - 1.384450987350409\n",
      "Step - 9236, Loss - 0.7349024831775167, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5832163998566171\n",
      "Step - 9237, Loss - 0.5938677748410189, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2537060749793563\n",
      "Step - 9238, Loss - 0.5593947224533073, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4418477718109364\n",
      "Step - 9239, Loss - 0.7838852741572688, Learning Rate - 9.765625e-05, magnitude of gradient - 2.639965834771089\n",
      "Step - 9240, Loss - 0.8419334895846066, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8742344329902945\n",
      "Step - 9241, Loss - 0.6729840958603998, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5525205278892076\n",
      "Step - 9242, Loss - 0.8936948679871662, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7996067697532887\n",
      "Step - 9243, Loss - 0.6657472034026397, Learning Rate - 9.765625e-05, magnitude of gradient - 0.39427622853622996\n",
      "Step - 9244, Loss - 0.7507817008098338, Learning Rate - 9.765625e-05, magnitude of gradient - 1.281312191412871\n",
      "Step - 9245, Loss - 0.6400210709496139, Learning Rate - 9.765625e-05, magnitude of gradient - 0.521806289821623\n",
      "Step - 9246, Loss - 0.7395611410184182, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4967087723647295\n",
      "Step - 9247, Loss - 0.7653642215092131, Learning Rate - 9.765625e-05, magnitude of gradient - 2.691006346673245\n",
      "Step - 9248, Loss - 0.660572055751212, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7230933080577235\n",
      "Step - 9249, Loss - 0.6739530325432221, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7404339129132969\n",
      "Step - 9250, Loss - 0.5153515220905456, Learning Rate - 9.765625e-05, magnitude of gradient - 0.37349605898876903\n",
      "Step - 9251, Loss - 0.8308505945386421, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8245457950547349\n",
      "Step - 9252, Loss - 0.8811607608521745, Learning Rate - 9.765625e-05, magnitude of gradient - 2.6777509370311265\n",
      "Step - 9253, Loss - 0.8904535900858318, Learning Rate - 9.765625e-05, magnitude of gradient - 2.0103963486852474\n",
      "Step - 9254, Loss - 0.9662043534314513, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6993942599826615\n",
      "Step - 9255, Loss - 0.8759819461578184, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2716005247153794\n",
      "Step - 9256, Loss - 0.5698020763366304, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5399557407553013\n",
      "Step - 9257, Loss - 0.842498878103918, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5609491058563656\n",
      "Step - 9258, Loss - 0.7400775106193105, Learning Rate - 9.765625e-05, magnitude of gradient - 1.54943457237041\n",
      "Step - 9259, Loss - 0.7368294523320837, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2221933921622352\n",
      "Step - 9260, Loss - 0.7319484925893216, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5474093237512284\n",
      "Step - 9261, Loss - 0.7020116960003929, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4137855435160622\n",
      "Step - 9262, Loss - 0.7935928962781927, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4025804563449449\n",
      "Step - 9263, Loss - 0.7465101336207992, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7384227250701332\n",
      "Step - 9264, Loss - 0.6774944750014433, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7745422765874913\n",
      "Step - 9265, Loss - 0.6210829500094162, Learning Rate - 9.765625e-05, magnitude of gradient - 0.47642233889664215\n",
      "Step - 9266, Loss - 0.6692142757221472, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0332560687109662\n",
      "Step - 9267, Loss - 0.6745605930995624, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1518530090821102\n",
      "Step - 9268, Loss - 0.5571193877662215, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0483375205015304\n",
      "Step - 9269, Loss - 0.8547729802728092, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3121310543770077\n",
      "Step - 9270, Loss - 0.7022864327605359, Learning Rate - 9.765625e-05, magnitude of gradient - 0.47885901438878953\n",
      "Step - 9271, Loss - 0.6137695737431389, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7219720367074731\n",
      "Step - 9272, Loss - 0.877969817706041, Learning Rate - 9.765625e-05, magnitude of gradient - 2.213502546451712\n",
      "Step - 9273, Loss - 0.6592114880049132, Learning Rate - 9.765625e-05, magnitude of gradient - 0.46328886825367727\n",
      "Step - 9274, Loss - 0.663908822361381, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9282852124653376\n",
      "Step - 9275, Loss - 0.6617617396083777, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1253597542633074\n",
      "Step - 9276, Loss - 0.8524097495480367, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2207654449858234\n",
      "Step - 9277, Loss - 0.5860413506424353, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0687159398537802\n",
      "Step - 9278, Loss - 0.7155168353750155, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8997143410373851\n",
      "Step - 9279, Loss - 0.8298091881014011, Learning Rate - 9.765625e-05, magnitude of gradient - 2.006536803465569\n",
      "Step - 9280, Loss - 0.6283028266017967, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7480652079443844\n",
      "Step - 9281, Loss - 0.7655948435967265, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5590921896253855\n",
      "Step - 9282, Loss - 0.709061920526935, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9371268986895345\n",
      "Step - 9283, Loss - 0.5020528953837069, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6528858618854259\n",
      "Step - 9284, Loss - 0.8073084260511774, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1135257606935267\n",
      "Step - 9285, Loss - 0.6544443373922916, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8742652740957217\n",
      "Step - 9286, Loss - 0.8475929216161325, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3313234830844296\n",
      "Step - 9287, Loss - 0.6688870484517118, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4140569356427033\n",
      "Step - 9288, Loss - 0.7408031418324152, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6465174498309882\n",
      "Step - 9289, Loss - 0.5778716840969207, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9613026778586582\n",
      "Step - 9290, Loss - 0.6798094036694858, Learning Rate - 9.765625e-05, magnitude of gradient - 0.768120194068276\n",
      "Step - 9291, Loss - 0.8904320606146245, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6189576601185448\n",
      "Step - 9292, Loss - 0.6012479281569763, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5112053735696269\n",
      "Step - 9293, Loss - 0.8250578701052673, Learning Rate - 9.765625e-05, magnitude of gradient - 1.905011116872249\n",
      "Step - 9294, Loss - 0.5490690357467469, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8832427680516206\n",
      "Step - 9295, Loss - 0.6521853556321601, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4182892352771395\n",
      "Step - 9296, Loss - 0.6199965127920021, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6675275752712243\n",
      "Step - 9297, Loss - 0.6079466701326636, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7484158765796614\n",
      "Step - 9298, Loss - 0.7613044879421865, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8033363249691116\n",
      "Step - 9299, Loss - 0.6743794268174741, Learning Rate - 9.765625e-05, magnitude of gradient - 1.900934781605441\n",
      "Step - 9300, Loss - 0.7426141804393502, Learning Rate - 9.765625e-05, magnitude of gradient - 0.32296219174941293\n",
      "Step - 9301, Loss - 0.8153176993586355, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9071234893451103\n",
      "Step - 9302, Loss - 0.7951515359396789, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7200285329422083\n",
      "Step - 9303, Loss - 0.7262716880319896, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7752660734774067\n",
      "Step - 9304, Loss - 0.5856180342414862, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4125124905448201\n",
      "Step - 9305, Loss - 0.6993454395149794, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5730853657537855\n",
      "Step - 9306, Loss - 0.7760803143563624, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5206879949157622\n",
      "Step - 9307, Loss - 0.6921196938642813, Learning Rate - 9.765625e-05, magnitude of gradient - 1.161437786711607\n",
      "Step - 9308, Loss - 0.5645682449187319, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4640527114050594\n",
      "Step - 9309, Loss - 0.8661875216851733, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8600578997394915\n",
      "Step - 9310, Loss - 0.8308573219316151, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1662733798690228\n",
      "Step - 9311, Loss - 0.7146854540530972, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8102582867681586\n",
      "Step - 9312, Loss - 0.6119291611932461, Learning Rate - 9.765625e-05, magnitude of gradient - 2.010086445603254\n",
      "Step - 9313, Loss - 0.8914467655661915, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8447995979957237\n",
      "Step - 9314, Loss - 0.8565482072260229, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5791861221800867\n",
      "Step - 9315, Loss - 0.796358607692379, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5839282603150637\n",
      "Step - 9316, Loss - 0.6250430392883253, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9489828246900822\n",
      "Step - 9317, Loss - 0.8455731319559611, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8141182771166189\n",
      "Step - 9318, Loss - 0.6652895463283521, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5029469636609771\n",
      "Step - 9319, Loss - 0.8215248909210535, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1482360044844935\n",
      "Step - 9320, Loss - 0.6398534086631391, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4138254401346029\n",
      "Step - 9321, Loss - 0.7926805887480193, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0114522712410046\n",
      "Step - 9322, Loss - 0.7607923991418815, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7176631639722706\n",
      "Step - 9323, Loss - 0.6949663412681453, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7059477116644162\n",
      "Step - 9324, Loss - 0.6781452488649544, Learning Rate - 9.765625e-05, magnitude of gradient - 1.194987317164694\n",
      "Step - 9325, Loss - 0.5567666011854555, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8239301271966049\n",
      "Step - 9326, Loss - 0.6645278373798961, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8499421725917653\n",
      "Step - 9327, Loss - 0.6576115285538948, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4707914982812376\n",
      "Step - 9328, Loss - 0.7561694758906599, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3669549209857312\n",
      "Step - 9329, Loss - 0.7504719536593186, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9438105172941643\n",
      "Step - 9330, Loss - 0.7735739373220472, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6551892289893104\n",
      "Step - 9331, Loss - 0.6157238492221163, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1268311239931295\n",
      "Step - 9332, Loss - 0.7393703920998325, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5397694297116906\n",
      "Step - 9333, Loss - 0.5606727882848805, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0182353969758144\n",
      "Step - 9334, Loss - 0.813122208932544, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4587544900757323\n",
      "Step - 9335, Loss - 0.4493023577284361, Learning Rate - 9.765625e-05, magnitude of gradient - 1.012521086739635\n",
      "Step - 9336, Loss - 0.9309660421675109, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8286670218506416\n",
      "Step - 9337, Loss - 0.7723615505815045, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1385596320073288\n",
      "Step - 9338, Loss - 0.6821104931856401, Learning Rate - 9.765625e-05, magnitude of gradient - 0.16689276892049293\n",
      "Step - 9339, Loss - 0.7890918125029773, Learning Rate - 9.765625e-05, magnitude of gradient - 1.807221384821994\n",
      "Step - 9340, Loss - 0.5670556726965508, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2059986034790657\n",
      "Step - 9341, Loss - 0.9031228874967484, Learning Rate - 9.765625e-05, magnitude of gradient - 1.301349568135656\n",
      "Step - 9342, Loss - 0.6142724558454146, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9374568696864234\n",
      "Step - 9343, Loss - 0.7865183533941484, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1702274531027543\n",
      "Step - 9344, Loss - 0.6208325704667259, Learning Rate - 9.765625e-05, magnitude of gradient - 0.14066630322490012\n",
      "Step - 9345, Loss - 0.8685388989036537, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8163407737410675\n",
      "Step - 9346, Loss - 0.769835835326548, Learning Rate - 9.765625e-05, magnitude of gradient - 0.3330885246469406\n",
      "Step - 9347, Loss - 0.8423412047829201, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5888587051683961\n",
      "Step - 9348, Loss - 0.506800315442339, Learning Rate - 9.765625e-05, magnitude of gradient - 2.1820410883776824\n",
      "Step - 9349, Loss - 0.7195689534244799, Learning Rate - 9.765625e-05, magnitude of gradient - 1.935085851851722\n",
      "Step - 9350, Loss - 0.7086673662528081, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8543525159161818\n",
      "Step - 9351, Loss - 0.622595777854412, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8004862626967648\n",
      "Step - 9352, Loss - 0.706081576979167, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0037882077937716\n",
      "Step - 9353, Loss - 0.5144713071882021, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8656930941826755\n",
      "Step - 9354, Loss - 0.7117515966388388, Learning Rate - 9.765625e-05, magnitude of gradient - 1.823580277215739\n",
      "Step - 9355, Loss - 0.6304096543019311, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9248911236937608\n",
      "Step - 9356, Loss - 0.6251243810954917, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6216202589756992\n",
      "Step - 9357, Loss - 0.7010870902959059, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5461889936010567\n",
      "Step - 9358, Loss - 0.6235406072391784, Learning Rate - 9.765625e-05, magnitude of gradient - 1.679962542616057\n",
      "Step - 9359, Loss - 0.5492012616417937, Learning Rate - 9.765625e-05, magnitude of gradient - 1.956322805398488\n",
      "Step - 9360, Loss - 0.8776974772623614, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8022618456877566\n",
      "Step - 9361, Loss - 0.5137090669956935, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1577523855916636\n",
      "Step - 9362, Loss - 0.6939878569900255, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5965026784692571\n",
      "Step - 9363, Loss - 0.8627085409861263, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9803317931309852\n",
      "Step - 9364, Loss - 0.7509967472454939, Learning Rate - 9.765625e-05, magnitude of gradient - 1.491619314408177\n",
      "Step - 9365, Loss - 0.5999968922906672, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6438741484785542\n",
      "Step - 9366, Loss - 0.6033607555136201, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8317175582301872\n",
      "Step - 9367, Loss - 0.6413969535621071, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6758587343085685\n",
      "Step - 9368, Loss - 0.6341045943142302, Learning Rate - 9.765625e-05, magnitude of gradient - 2.0159508519953584\n",
      "Step - 9369, Loss - 0.7496355621883334, Learning Rate - 9.765625e-05, magnitude of gradient - 2.108866246233243\n",
      "Step - 9370, Loss - 0.5446270573369325, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4536488455997577\n",
      "Step - 9371, Loss - 0.6812277304265808, Learning Rate - 9.765625e-05, magnitude of gradient - 1.501076627074322\n",
      "Step - 9372, Loss - 0.5667397559056322, Learning Rate - 9.765625e-05, magnitude of gradient - 3.2748118752897106\n",
      "Step - 9373, Loss - 0.8126589778056937, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8504448917847505\n",
      "Step - 9374, Loss - 0.8147196537358747, Learning Rate - 9.765625e-05, magnitude of gradient - 1.143056959109526\n",
      "Step - 9375, Loss - 0.9016129204779562, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3626507773513306\n",
      "Step - 9376, Loss - 0.746484384100029, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7491118332012046\n",
      "Step - 9377, Loss - 0.8651984124661982, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9654728990073008\n",
      "Step - 9378, Loss - 0.7782650838034919, Learning Rate - 9.765625e-05, magnitude of gradient - 1.506168345499726\n",
      "Step - 9379, Loss - 0.5717211130868962, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1307801653648801\n",
      "Step - 9380, Loss - 0.7387545550963854, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5896720517878488\n",
      "Step - 9381, Loss - 0.5740988797617262, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2057150032081951\n",
      "Step - 9382, Loss - 0.6531237568840857, Learning Rate - 9.765625e-05, magnitude of gradient - 2.520817059145722\n",
      "Step - 9383, Loss - 0.8392875928491581, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0237029951432721\n",
      "Step - 9384, Loss - 0.6488164870678306, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9963444025739314\n",
      "Step - 9385, Loss - 0.7300949602419066, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7713996862990982\n",
      "Step - 9386, Loss - 0.6306542054298837, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0481975651981352\n",
      "Step - 9387, Loss - 0.7213625485727535, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4603783136332005\n",
      "Step - 9388, Loss - 0.6756984799047074, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2720514307017265\n",
      "Step - 9389, Loss - 0.811313518549488, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2592791008377544\n",
      "Step - 9390, Loss - 0.7714208986726229, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9049607969757296\n",
      "Step - 9391, Loss - 0.5393200723711805, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9751570236906553\n",
      "Step - 9392, Loss - 0.7822989613244736, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6676058621037873\n",
      "Step - 9393, Loss - 0.5477922378135824, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9811693158684796\n",
      "Step - 9394, Loss - 0.6617918268738402, Learning Rate - 9.765625e-05, magnitude of gradient - 2.5587568297881917\n",
      "Step - 9395, Loss - 0.7403877261225925, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9891129069351063\n",
      "Step - 9396, Loss - 0.6782939907655905, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9524147206227724\n",
      "Step - 9397, Loss - 0.8140901333655931, Learning Rate - 9.765625e-05, magnitude of gradient - 1.20242360021618\n",
      "Step - 9398, Loss - 0.8219776867110814, Learning Rate - 9.765625e-05, magnitude of gradient - 2.3724460842693698\n",
      "Step - 9399, Loss - 0.6668833186897568, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4498127910378034\n",
      "Step - 9400, Loss - 0.6413847190172064, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7884505854626404\n",
      "Step - 9401, Loss - 0.6538543874150713, Learning Rate - 9.765625e-05, magnitude of gradient - 2.722181430797365\n",
      "Step - 9402, Loss - 0.6641171110342926, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8031363070060912\n",
      "Step - 9403, Loss - 0.5992064642820485, Learning Rate - 9.765625e-05, magnitude of gradient - 1.152511428057625\n",
      "Step - 9404, Loss - 0.7851573218000243, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0305393149124356\n",
      "Step - 9405, Loss - 0.8965981189776624, Learning Rate - 9.765625e-05, magnitude of gradient - 2.0496212608357793\n",
      "Step - 9406, Loss - 0.7447339593461344, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7807257369189367\n",
      "Step - 9407, Loss - 0.7675267006159349, Learning Rate - 9.765625e-05, magnitude of gradient - 1.438100393542323\n",
      "Step - 9408, Loss - 0.782077711516211, Learning Rate - 9.765625e-05, magnitude of gradient - 1.096103792090389\n",
      "Step - 9409, Loss - 0.6966019619467823, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8540903154413433\n",
      "Step - 9410, Loss - 0.6498520455615937, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7345177419535196\n",
      "Step - 9411, Loss - 0.6229971237162759, Learning Rate - 9.765625e-05, magnitude of gradient - 1.466914221979012\n",
      "Step - 9412, Loss - 0.620568695465241, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1811663254465206\n",
      "Step - 9413, Loss - 0.8883884019016195, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1909005793602752\n",
      "Step - 9414, Loss - 0.6746734836784102, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3217728922987502\n",
      "Step - 9415, Loss - 0.5344217445450996, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8960991276037187\n",
      "Step - 9416, Loss - 0.9607583433865926, Learning Rate - 9.765625e-05, magnitude of gradient - 3.064336982471176\n",
      "Step - 9417, Loss - 0.5829297714098516, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4630206773129999\n",
      "Step - 9418, Loss - 0.7578944875876028, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9409142399450917\n",
      "Step - 9419, Loss - 0.6811212916692538, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6261763594928516\n",
      "Step - 9420, Loss - 0.6937216691726793, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7189944429227642\n",
      "Step - 9421, Loss - 0.6771307612730142, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7741387534923532\n",
      "Step - 9422, Loss - 0.6880781256619697, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4096972005183135\n",
      "Step - 9423, Loss - 0.6843370987762134, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4374756130152555\n",
      "Step - 9424, Loss - 0.6950786037213628, Learning Rate - 9.765625e-05, magnitude of gradient - 2.377556881874514\n",
      "Step - 9425, Loss - 0.6465673870861801, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4943620371706963\n",
      "Step - 9426, Loss - 0.5999455987824673, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4414802874155369\n",
      "Step - 9427, Loss - 0.7618646165520497, Learning Rate - 9.765625e-05, magnitude of gradient - 2.5154271545461873\n",
      "Step - 9428, Loss - 0.7759093630725965, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6356716184697502\n",
      "Step - 9429, Loss - 0.7202885634295975, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8516521512358823\n",
      "Step - 9430, Loss - 0.6627459501841436, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2156678792193512\n",
      "Step - 9431, Loss - 0.7680341965643739, Learning Rate - 9.765625e-05, magnitude of gradient - 2.0671201620900144\n",
      "Step - 9432, Loss - 0.6742123824051134, Learning Rate - 9.765625e-05, magnitude of gradient - 1.799784448817283\n",
      "Step - 9433, Loss - 0.7667697405822874, Learning Rate - 9.765625e-05, magnitude of gradient - 1.467580321145054\n",
      "Step - 9434, Loss - 0.8419598950053804, Learning Rate - 9.765625e-05, magnitude of gradient - 2.5343373323466794\n",
      "Step - 9435, Loss - 0.8876625847953369, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5325317692874868\n",
      "Step - 9436, Loss - 0.5276693322995707, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6432832743127426\n",
      "Step - 9437, Loss - 0.7159804487699977, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1524127771224388\n",
      "Step - 9438, Loss - 0.8096729313262562, Learning Rate - 9.765625e-05, magnitude of gradient - 0.380908146806581\n",
      "Step - 9439, Loss - 0.7240036342941969, Learning Rate - 9.765625e-05, magnitude of gradient - 2.0323805803966923\n",
      "Step - 9440, Loss - 0.6286387508686058, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8873991612482423\n",
      "Step - 9441, Loss - 0.7593359557656058, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5362644485357032\n",
      "Step - 9442, Loss - 0.8331765365181465, Learning Rate - 9.765625e-05, magnitude of gradient - 0.804648622166899\n",
      "Step - 9443, Loss - 0.6219174347641088, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4016297643593618\n",
      "Step - 9444, Loss - 0.5871053328330144, Learning Rate - 9.765625e-05, magnitude of gradient - 2.3208993202490644\n",
      "Step - 9445, Loss - 0.5244885445236694, Learning Rate - 9.765625e-05, magnitude of gradient - 2.1095359127054465\n",
      "Step - 9446, Loss - 0.7889947533005566, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3381852984700024\n",
      "Step - 9447, Loss - 0.7384651824169229, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1326470000545297\n",
      "Step - 9448, Loss - 0.8729902799412177, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7226420610461786\n",
      "Step - 9449, Loss - 0.7375073535982003, Learning Rate - 9.765625e-05, magnitude of gradient - 1.215206877130257\n",
      "Step - 9450, Loss - 0.7419533331740857, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0059037801172048\n",
      "Step - 9451, Loss - 0.8014614575603063, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4434858339981853\n",
      "Step - 9452, Loss - 0.8125376262830137, Learning Rate - 9.765625e-05, magnitude of gradient - 2.347985443150467\n",
      "Step - 9453, Loss - 0.7669489672663866, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0229222862878575\n",
      "Step - 9454, Loss - 0.8996180548212953, Learning Rate - 9.765625e-05, magnitude of gradient - 0.923006055378181\n",
      "Step - 9455, Loss - 0.7891907560413096, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3710641513245247\n",
      "Step - 9456, Loss - 0.8089243670451525, Learning Rate - 9.765625e-05, magnitude of gradient - 2.1665725166823817\n",
      "Step - 9457, Loss - 0.8412353090338521, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2535478305565086\n",
      "Step - 9458, Loss - 0.536368901547556, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6983474807027129\n",
      "Step - 9459, Loss - 0.9066866330015066, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6424426971621398\n",
      "Step - 9460, Loss - 0.8378902549752458, Learning Rate - 9.765625e-05, magnitude of gradient - 0.893281601276823\n",
      "Step - 9461, Loss - 0.7353312505114333, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7981513054279289\n",
      "Step - 9462, Loss - 0.5480690060379447, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3046589881342237\n",
      "Step - 9463, Loss - 0.9559656554498707, Learning Rate - 9.765625e-05, magnitude of gradient - 0.684584751151534\n",
      "Step - 9464, Loss - 0.8095090701332174, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6049144930347965\n",
      "Step - 9465, Loss - 0.8174606790464902, Learning Rate - 9.765625e-05, magnitude of gradient - 2.240216968787227\n",
      "Step - 9466, Loss - 0.4977325995380415, Learning Rate - 9.765625e-05, magnitude of gradient - 1.13672037291186\n",
      "Step - 9467, Loss - 0.5970297379950456, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4880154032338928\n",
      "Step - 9468, Loss - 0.7454766858005503, Learning Rate - 9.765625e-05, magnitude of gradient - 1.142604129688791\n",
      "Step - 9469, Loss - 0.6315970669664867, Learning Rate - 9.765625e-05, magnitude of gradient - 0.26633779044511396\n",
      "Step - 9470, Loss - 0.6310475149563746, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1932258094335808\n",
      "Step - 9471, Loss - 0.6046442718468987, Learning Rate - 9.765625e-05, magnitude of gradient - 0.45075816390466666\n",
      "Step - 9472, Loss - 0.7745267176173739, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5078678494083209\n",
      "Step - 9473, Loss - 0.9153008372456317, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0877772588711432\n",
      "Step - 9474, Loss - 0.6796446352666992, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9105801930503753\n",
      "Step - 9475, Loss - 0.6252817171302322, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5765842719295581\n",
      "Step - 9476, Loss - 0.5268619548885356, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6677800114370331\n",
      "Step - 9477, Loss - 0.8329602897519777, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9712445848423141\n",
      "Step - 9478, Loss - 0.8137081210762243, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8269616194958578\n",
      "Step - 9479, Loss - 0.46422966043764263, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8876003035825906\n",
      "Step - 9480, Loss - 0.6292161497512114, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3265381890089158\n",
      "Step - 9481, Loss - 0.6327916202481775, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0626123460086774\n",
      "Step - 9482, Loss - 0.7534999649687231, Learning Rate - 9.765625e-05, magnitude of gradient - 2.1387280711782433\n",
      "Step - 9483, Loss - 0.6868050526041224, Learning Rate - 9.765625e-05, magnitude of gradient - 2.097407439811032\n",
      "Step - 9484, Loss - 0.7852465860011673, Learning Rate - 9.765625e-05, magnitude of gradient - 1.241077213201545\n",
      "Step - 9485, Loss - 0.6856184774526821, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0528385103863713\n",
      "Step - 9486, Loss - 0.7897248599381436, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7953472853683898\n",
      "Step - 9487, Loss - 0.7102247021620652, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9898614617403508\n",
      "Step - 9488, Loss - 0.871887310645101, Learning Rate - 9.765625e-05, magnitude of gradient - 2.1216206152906807\n",
      "Step - 9489, Loss - 0.6648188567269464, Learning Rate - 9.765625e-05, magnitude of gradient - 1.003961222620836\n",
      "Step - 9490, Loss - 0.7769251482467323, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3446363785021518\n",
      "Step - 9491, Loss - 0.8165483030951712, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4791696398396361\n",
      "Step - 9492, Loss - 0.681404128574312, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9147325888426832\n",
      "Step - 9493, Loss - 0.7758357188154972, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2017911113152322\n",
      "Step - 9494, Loss - 0.6068149980362486, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4115435317865883\n",
      "Step - 9495, Loss - 0.6906479398550023, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8476334446822688\n",
      "Step - 9496, Loss - 0.8500450871892775, Learning Rate - 9.765625e-05, magnitude of gradient - 2.138684585724337\n",
      "Step - 9497, Loss - 0.6112646625813087, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8374920365848747\n",
      "Step - 9498, Loss - 0.6363378800815873, Learning Rate - 9.765625e-05, magnitude of gradient - 0.91300077130096\n",
      "Step - 9499, Loss - 0.786198960852604, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9195679765507352\n",
      "Step - 9500, Loss - 0.46748555616393916, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9731574302339847\n",
      "Step - 9501, Loss - 0.6913087805801235, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9371361712850514\n",
      "Step - 9502, Loss - 0.58364677246802, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4687665746634988\n",
      "Step - 9503, Loss - 0.7094116749536232, Learning Rate - 9.765625e-05, magnitude of gradient - 2.061444093659201\n",
      "Step - 9504, Loss - 0.9526790509326166, Learning Rate - 9.765625e-05, magnitude of gradient - 2.685165772853939\n",
      "Step - 9505, Loss - 0.5926722628660827, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9768585642220222\n",
      "Step - 9506, Loss - 0.7404018529493481, Learning Rate - 9.765625e-05, magnitude of gradient - 2.2164200217493213\n",
      "Step - 9507, Loss - 0.5280438815853906, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9215991141947256\n",
      "Step - 9508, Loss - 0.8642874194022359, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5337131498450414\n",
      "Step - 9509, Loss - 0.9075209337399455, Learning Rate - 9.765625e-05, magnitude of gradient - 2.783338664105423\n",
      "Step - 9510, Loss - 0.6416339296285708, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6418698222391879\n",
      "Step - 9511, Loss - 0.9503321832925805, Learning Rate - 9.765625e-05, magnitude of gradient - 2.0546678477920657\n",
      "Step - 9512, Loss - 0.7395274067621103, Learning Rate - 9.765625e-05, magnitude of gradient - 2.024159859738005\n",
      "Step - 9513, Loss - 0.6202569617561308, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6289035850198419\n",
      "Step - 9514, Loss - 0.8175997488274931, Learning Rate - 9.765625e-05, magnitude of gradient - 2.1271960940968406\n",
      "Step - 9515, Loss - 0.8212272824246419, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5435032566493523\n",
      "Step - 9516, Loss - 0.7023772764620152, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2363858224260713\n",
      "Step - 9517, Loss - 0.638025876808012, Learning Rate - 9.765625e-05, magnitude of gradient - 0.3344606395489755\n",
      "Step - 9518, Loss - 0.8736743315729447, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4405751115817698\n",
      "Step - 9519, Loss - 0.6453550689199241, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9441031164692251\n",
      "Step - 9520, Loss - 0.8129189126069803, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3995821330931935\n",
      "Step - 9521, Loss - 0.6580860325580903, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6101819143386445\n",
      "Step - 9522, Loss - 0.6765083722746017, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7352925228822306\n",
      "Step - 9523, Loss - 0.7394568593891561, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5335184712361314\n",
      "Step - 9524, Loss - 0.668034708690407, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3366972962639205\n",
      "Step - 9525, Loss - 0.6458637854263987, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0023865743159692\n",
      "Step - 9526, Loss - 0.7679952018564132, Learning Rate - 9.765625e-05, magnitude of gradient - 0.516042249447618\n",
      "Step - 9527, Loss - 0.7657827830259744, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4519485064729438\n",
      "Step - 9528, Loss - 0.6844032807884788, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4766931605781322\n",
      "Step - 9529, Loss - 0.5594528613019323, Learning Rate - 9.765625e-05, magnitude of gradient - 2.853011591436883\n",
      "Step - 9530, Loss - 0.7910648811798289, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1050599963250771\n",
      "Step - 9531, Loss - 0.5946009886548218, Learning Rate - 9.765625e-05, magnitude of gradient - 2.027133189688181\n",
      "Step - 9532, Loss - 0.690922047553749, Learning Rate - 9.765625e-05, magnitude of gradient - 2.144582585871917\n",
      "Step - 9533, Loss - 0.6383087820917065, Learning Rate - 9.765625e-05, magnitude of gradient - 0.48683461820239177\n",
      "Step - 9534, Loss - 0.5658985421055918, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5435478022972572\n",
      "Step - 9535, Loss - 0.6673807329949005, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6840968042189334\n",
      "Step - 9536, Loss - 0.6539760411134427, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7164464511663902\n",
      "Step - 9537, Loss - 0.7341054648715263, Learning Rate - 9.765625e-05, magnitude of gradient - 2.5462319102507416\n",
      "Step - 9538, Loss - 0.5384477097682043, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7365856412564322\n",
      "Step - 9539, Loss - 0.757858144709519, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4869313721095967\n",
      "Step - 9540, Loss - 0.8050690004093245, Learning Rate - 9.765625e-05, magnitude of gradient - 0.37108813136107227\n",
      "Step - 9541, Loss - 0.7895625617244748, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7794574788298823\n",
      "Step - 9542, Loss - 0.9016394126527398, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0342280953606047\n",
      "Step - 9543, Loss - 0.6397098487034749, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8306241573409312\n",
      "Step - 9544, Loss - 0.7128197091279025, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2683116419881681\n",
      "Step - 9545, Loss - 0.6560881394730451, Learning Rate - 9.765625e-05, magnitude of gradient - 2.0691795434625266\n",
      "Step - 9546, Loss - 0.8436295563187359, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9037772129407622\n",
      "Step - 9547, Loss - 0.6374226459179075, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7813200752255707\n",
      "Step - 9548, Loss - 0.9319105959211847, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8903302973149422\n",
      "Step - 9549, Loss - 0.801265444998239, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9119675949421779\n",
      "Step - 9550, Loss - 0.6765336050628081, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1820497641841767\n",
      "Step - 9551, Loss - 0.5743076265250866, Learning Rate - 9.765625e-05, magnitude of gradient - 2.7426549171468104\n",
      "Step - 9552, Loss - 0.5143423421982904, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6635227257688576\n",
      "Step - 9553, Loss - 0.7749620981912029, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7124064942718068\n",
      "Step - 9554, Loss - 0.6154277319052579, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7395721753143192\n",
      "Step - 9555, Loss - 0.5712960703396096, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5886546431098536\n",
      "Step - 9556, Loss - 0.8401107520978972, Learning Rate - 9.765625e-05, magnitude of gradient - 1.040518032494625\n",
      "Step - 9557, Loss - 0.6857341691268344, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6036476977209827\n",
      "Step - 9558, Loss - 0.6774947998794001, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4979315510884712\n",
      "Step - 9559, Loss - 0.7147692252515874, Learning Rate - 9.765625e-05, magnitude of gradient - 1.567899443403948\n",
      "Step - 9560, Loss - 0.6680762445930044, Learning Rate - 9.765625e-05, magnitude of gradient - 1.525426556921162\n",
      "Step - 9561, Loss - 0.5642645490866129, Learning Rate - 9.765625e-05, magnitude of gradient - 2.214060613118312\n",
      "Step - 9562, Loss - 0.5956480659277403, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6254644500677743\n",
      "Step - 9563, Loss - 0.7720100164418299, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9027189281244155\n",
      "Step - 9564, Loss - 0.7053954960624703, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1637529010899583\n",
      "Step - 9565, Loss - 0.6480079238477466, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9700627747126233\n",
      "Step - 9566, Loss - 0.8016802970298001, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7449440491023225\n",
      "Step - 9567, Loss - 0.6577296069922175, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6367647294428758\n",
      "Step - 9568, Loss - 0.7959547364892696, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5114687272134981\n",
      "Step - 9569, Loss - 0.6197112334724646, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8755112927742016\n",
      "Step - 9570, Loss - 0.5774868380172192, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7752052550967101\n",
      "Step - 9571, Loss - 0.6347492634576747, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1056886342023267\n",
      "Step - 9572, Loss - 0.8754979723912264, Learning Rate - 9.765625e-05, magnitude of gradient - 0.3335970228185663\n",
      "Step - 9573, Loss - 0.6611396907990502, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9577338158216019\n",
      "Step - 9574, Loss - 0.6888483127864593, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7172644266582869\n",
      "Step - 9575, Loss - 0.63369801787309, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8189440781794028\n",
      "Step - 9576, Loss - 0.8489689371245158, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5106694044353903\n",
      "Step - 9577, Loss - 0.6194588023721325, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2798206347203835\n",
      "Step - 9578, Loss - 0.6195119097893991, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8042886459496111\n",
      "Step - 9579, Loss - 0.6321654763772813, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7677451093943487\n",
      "Step - 9580, Loss - 0.6456504179679925, Learning Rate - 9.765625e-05, magnitude of gradient - 0.17960126019023273\n",
      "Step - 9581, Loss - 0.5518360343869979, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4075739077732058\n",
      "Step - 9582, Loss - 0.6357081416166264, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8484598144446776\n",
      "Step - 9583, Loss - 0.7288441492233404, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9616613383535457\n",
      "Step - 9584, Loss - 0.7870154567988261, Learning Rate - 9.765625e-05, magnitude of gradient - 1.64958236387483\n",
      "Step - 9585, Loss - 0.7499655680857245, Learning Rate - 9.765625e-05, magnitude of gradient - 2.1705014248198458\n",
      "Step - 9586, Loss - 0.6806257384198517, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1946257609681248\n",
      "Step - 9587, Loss - 0.7860416445019528, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6767449803495991\n",
      "Step - 9588, Loss - 0.805126742741503, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8154223273438213\n",
      "Step - 9589, Loss - 0.5193426299868544, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6868722363241372\n",
      "Step - 9590, Loss - 0.6888919728841639, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1174278224093057\n",
      "Step - 9591, Loss - 0.6927889955824862, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7457087968467369\n",
      "Step - 9592, Loss - 0.4697032429728956, Learning Rate - 9.765625e-05, magnitude of gradient - 2.016163876011891\n",
      "Step - 9593, Loss - 0.6902260126590078, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7293311498652448\n",
      "Step - 9594, Loss - 0.8162827731835935, Learning Rate - 9.765625e-05, magnitude of gradient - 1.261831814608745\n",
      "Step - 9595, Loss - 0.593022408500592, Learning Rate - 9.765625e-05, magnitude of gradient - 2.029996128427895\n",
      "Step - 9596, Loss - 0.5332708404581024, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5168106546194908\n",
      "Step - 9597, Loss - 0.8128619544528353, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4550249935964993\n",
      "Step - 9598, Loss - 0.7048945760869919, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4728461900677918\n",
      "Step - 9599, Loss - 0.717577559948145, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6113520220739149\n",
      "Step - 9600, Loss - 0.5617473004419147, Learning Rate - 9.765625e-05, magnitude of gradient - 0.35306072196676686\n",
      "Step - 9601, Loss - 0.6274882404918318, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4812392040674908\n",
      "Step - 9602, Loss - 0.680462968747778, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8766905262511511\n",
      "Step - 9603, Loss - 0.7626825878387631, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5151891209398347\n",
      "Step - 9604, Loss - 0.6366274513447744, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7285349339362845\n",
      "Step - 9605, Loss - 0.966347959095479, Learning Rate - 9.765625e-05, magnitude of gradient - 1.895908719899449\n",
      "Step - 9606, Loss - 0.7120304393893551, Learning Rate - 9.765625e-05, magnitude of gradient - 0.25539299042078956\n",
      "Step - 9607, Loss - 0.9808895026286105, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3006643262804118\n",
      "Step - 9608, Loss - 0.8866323534160652, Learning Rate - 9.765625e-05, magnitude of gradient - 1.114241205381842\n",
      "Step - 9609, Loss - 0.5168052202298266, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6676540983895305\n",
      "Step - 9610, Loss - 0.7794230943433395, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6998674522305521\n",
      "Step - 9611, Loss - 0.9084870523471615, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8479026820511235\n",
      "Step - 9612, Loss - 0.4843267752364177, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4725639666952046\n",
      "Step - 9613, Loss - 0.5593518578567217, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4407722086143602\n",
      "Step - 9614, Loss - 0.7264988409162916, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0865747718375491\n",
      "Step - 9615, Loss - 0.6036708279288583, Learning Rate - 9.765625e-05, magnitude of gradient - 2.438083879442883\n",
      "Step - 9616, Loss - 0.7252135635523924, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7716408512612507\n",
      "Step - 9617, Loss - 0.8189213244222808, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8865734056271043\n",
      "Step - 9618, Loss - 0.5165092821302243, Learning Rate - 9.765625e-05, magnitude of gradient - 1.327951704441595\n",
      "Step - 9619, Loss - 0.4786269281409957, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4942193316997783\n",
      "Step - 9620, Loss - 0.7072582855261794, Learning Rate - 9.765625e-05, magnitude of gradient - 0.32698342115538853\n",
      "Step - 9621, Loss - 0.8584651601772597, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6017242735495563\n",
      "Step - 9622, Loss - 0.7427472907598491, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4627091934702636\n",
      "Step - 9623, Loss - 0.827879984873383, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9005984852549176\n",
      "Step - 9624, Loss - 0.6573059501876917, Learning Rate - 9.765625e-05, magnitude of gradient - 2.067682588792025\n",
      "Step - 9625, Loss - 0.7928715610304679, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5004001969953007\n",
      "Step - 9626, Loss - 0.737907195260157, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9359901614112697\n",
      "Step - 9627, Loss - 0.7930088415374471, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3847046788000255\n",
      "Step - 9628, Loss - 0.7409736415220731, Learning Rate - 9.765625e-05, magnitude of gradient - 0.19875744040031634\n",
      "Step - 9629, Loss - 0.7578086097478554, Learning Rate - 9.765625e-05, magnitude of gradient - 2.2594813427434017\n",
      "Step - 9630, Loss - 0.6844697403315503, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2087065894644795\n",
      "Step - 9631, Loss - 0.8812799225300278, Learning Rate - 9.765625e-05, magnitude of gradient - 1.246447933279377\n",
      "Step - 9632, Loss - 0.7032183345419349, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4438932598028007\n",
      "Step - 9633, Loss - 0.7362778175472238, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8443637074968365\n",
      "Step - 9634, Loss - 0.7372042623473141, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5671111275859475\n",
      "Step - 9635, Loss - 0.6850077418483976, Learning Rate - 9.765625e-05, magnitude of gradient - 2.6909168961737713\n",
      "Step - 9636, Loss - 0.8455207859136192, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6483527676337342\n",
      "Step - 9637, Loss - 0.7343158117885478, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7588264825285115\n",
      "Step - 9638, Loss - 0.555956680031983, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9598709147506902\n",
      "Step - 9639, Loss - 0.7136043812080635, Learning Rate - 9.765625e-05, magnitude of gradient - 1.144904350827538\n",
      "Step - 9640, Loss - 0.8454645504501891, Learning Rate - 9.765625e-05, magnitude of gradient - 2.5433357859513475\n",
      "Step - 9641, Loss - 0.6672134999935013, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9978741817373166\n",
      "Step - 9642, Loss - 0.7298832389343773, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6179444143568897\n",
      "Step - 9643, Loss - 0.9411708785173594, Learning Rate - 9.765625e-05, magnitude of gradient - 2.4743209181793198\n",
      "Step - 9644, Loss - 0.8692095202848187, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5793508924027977\n",
      "Step - 9645, Loss - 0.5366521271419199, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8045637865729256\n",
      "Step - 9646, Loss - 0.8553629664218175, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4955963899098788\n",
      "Step - 9647, Loss - 0.7168935225178816, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1156935547924944\n",
      "Step - 9648, Loss - 0.6833695655690882, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3106761363847916\n",
      "Step - 9649, Loss - 0.7240363675898706, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8042414243086893\n",
      "Step - 9650, Loss - 0.6710852368983509, Learning Rate - 9.765625e-05, magnitude of gradient - 1.170670114140775\n",
      "Step - 9651, Loss - 0.6791782650124115, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7092652990861171\n",
      "Step - 9652, Loss - 0.802906586474309, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4264203002033096\n",
      "Step - 9653, Loss - 0.7824402135579629, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4166227550433383\n",
      "Step - 9654, Loss - 0.59780186425531, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2322342936944248\n",
      "Step - 9655, Loss - 0.6868705047678291, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5309084320611404\n",
      "Step - 9656, Loss - 0.7111657672782252, Learning Rate - 9.765625e-05, magnitude of gradient - 2.0526933014039153\n",
      "Step - 9657, Loss - 0.6579157480811182, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6093045840996318\n",
      "Step - 9658, Loss - 0.6959985637604093, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5330881559690824\n",
      "Step - 9659, Loss - 0.6409124895701184, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9364458103216083\n",
      "Step - 9660, Loss - 0.8118160564869055, Learning Rate - 9.765625e-05, magnitude of gradient - 1.14111603772311\n",
      "Step - 9661, Loss - 0.6248946541184507, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5867583534439664\n",
      "Step - 9662, Loss - 0.8143045497370074, Learning Rate - 9.765625e-05, magnitude of gradient - 1.326016040767676\n",
      "Step - 9663, Loss - 0.5081892531973373, Learning Rate - 9.765625e-05, magnitude of gradient - 0.46890199390318976\n",
      "Step - 9664, Loss - 0.7457548557598057, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0223487184678115\n",
      "Step - 9665, Loss - 0.6792961944964019, Learning Rate - 9.765625e-05, magnitude of gradient - 0.46699494721493656\n",
      "Step - 9666, Loss - 0.5690390080557024, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7149244071342938\n",
      "Step - 9667, Loss - 0.4962873137976219, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3715984818813416\n",
      "Step - 9668, Loss - 0.7837551832840516, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0093121124549171\n",
      "Step - 9669, Loss - 0.6199695648740333, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9805251354557574\n",
      "Step - 9670, Loss - 0.9348411328256178, Learning Rate - 9.765625e-05, magnitude of gradient - 2.1696935034792495\n",
      "Step - 9671, Loss - 0.833009578369949, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2673480257262355\n",
      "Step - 9672, Loss - 0.7660614914581078, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3286470325348143\n",
      "Step - 9673, Loss - 0.763563827489604, Learning Rate - 9.765625e-05, magnitude of gradient - 0.36116879368637067\n",
      "Step - 9674, Loss - 0.7424871695063099, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9505207303742091\n",
      "Step - 9675, Loss - 0.6227197730047329, Learning Rate - 9.765625e-05, magnitude of gradient - 1.523583680849874\n",
      "Step - 9676, Loss - 0.7202161185937379, Learning Rate - 9.765625e-05, magnitude of gradient - 1.263435197507209\n",
      "Step - 9677, Loss - 0.6973116524580335, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8371362613095295\n",
      "Step - 9678, Loss - 0.5951102361980958, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1212707942144977\n",
      "Step - 9679, Loss - 0.5549854492466008, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9255905275345462\n",
      "Step - 9680, Loss - 0.551473260432262, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3019785194798519\n",
      "Step - 9681, Loss - 0.6177240223334385, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6128243885702929\n",
      "Step - 9682, Loss - 0.6782459158847385, Learning Rate - 9.765625e-05, magnitude of gradient - 2.008227271348571\n",
      "Step - 9683, Loss - 0.6830725613175878, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0416707020917781\n",
      "Step - 9684, Loss - 0.8198023063266309, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5962046012671391\n",
      "Step - 9685, Loss - 0.7326009460251393, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3243621217085115\n",
      "Step - 9686, Loss - 0.5617958434121578, Learning Rate - 9.765625e-05, magnitude of gradient - 0.43172638647168127\n",
      "Step - 9687, Loss - 0.6640808692514072, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4935996729004273\n",
      "Step - 9688, Loss - 0.8024456574598451, Learning Rate - 9.765625e-05, magnitude of gradient - 1.042537572441359\n",
      "Step - 9689, Loss - 0.9663247027862711, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9518059855073147\n",
      "Step - 9690, Loss - 0.810710531037102, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3470250622532365\n",
      "Step - 9691, Loss - 0.6788171163167938, Learning Rate - 9.765625e-05, magnitude of gradient - 2.4583783268957564\n",
      "Step - 9692, Loss - 0.7194080572479041, Learning Rate - 9.765625e-05, magnitude of gradient - 2.5121761859293255\n",
      "Step - 9693, Loss - 0.6822174771084213, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6351649243144202\n",
      "Step - 9694, Loss - 0.6176581742922631, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6872155047136357\n",
      "Step - 9695, Loss - 0.8031556028871892, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8354324953488079\n",
      "Step - 9696, Loss - 0.9800517526335991, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9901749030640838\n",
      "Step - 9697, Loss - 0.6656744169466966, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5387629043425975\n",
      "Step - 9698, Loss - 0.7277921962045885, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9945158411161992\n",
      "Step - 9699, Loss - 0.6973836208774307, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1132149626759196\n",
      "Step - 9700, Loss - 0.5932262652482851, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1844537315829957\n",
      "Step - 9701, Loss - 0.8051950950679504, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1673800992840537\n",
      "Step - 9702, Loss - 0.6791210007327113, Learning Rate - 9.765625e-05, magnitude of gradient - 2.221319409969527\n",
      "Step - 9703, Loss - 0.7087871912440984, Learning Rate - 9.765625e-05, magnitude of gradient - 0.830876600387344\n",
      "Step - 9704, Loss - 0.734191701368766, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6336437468260397\n",
      "Step - 9705, Loss - 0.5451171165229876, Learning Rate - 9.765625e-05, magnitude of gradient - 0.36680698510543297\n",
      "Step - 9706, Loss - 0.6461493784732253, Learning Rate - 9.765625e-05, magnitude of gradient - 0.671997525808426\n",
      "Step - 9707, Loss - 0.6628156400516562, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1043884897712912\n",
      "Step - 9708, Loss - 0.5492333575451748, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0399902111280674\n",
      "Step - 9709, Loss - 0.6871934805777951, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9371293653462285\n",
      "Step - 9710, Loss - 0.6674913321096664, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7870224001584039\n",
      "Step - 9711, Loss - 0.7917063569349252, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5863455219047664\n",
      "Step - 9712, Loss - 0.8203651463295497, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4702411272058573\n",
      "Step - 9713, Loss - 0.6000899670981586, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7673537856871542\n",
      "Step - 9714, Loss - 0.7664123596122934, Learning Rate - 9.765625e-05, magnitude of gradient - 0.3777607506582123\n",
      "Step - 9715, Loss - 0.6569905734326906, Learning Rate - 9.765625e-05, magnitude of gradient - 2.4936553503525927\n",
      "Step - 9716, Loss - 0.7341442217659614, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6520095264216038\n",
      "Step - 9717, Loss - 0.9130990156675728, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4145008603571234\n",
      "Step - 9718, Loss - 0.6226485137118976, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5165920789150085\n",
      "Step - 9719, Loss - 0.6012560208858447, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4603263000530087\n",
      "Step - 9720, Loss - 0.6219938280106939, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9662370609190936\n",
      "Step - 9721, Loss - 0.692457619612519, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0222417010052298\n",
      "Step - 9722, Loss - 0.722440238914648, Learning Rate - 9.765625e-05, magnitude of gradient - 1.445242051914974\n",
      "Step - 9723, Loss - 0.5575872451163766, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3099218193010245\n",
      "Step - 9724, Loss - 0.7888707516963539, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0475236899485925\n",
      "Step - 9725, Loss - 0.6612166941824451, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9905587130917678\n",
      "Step - 9726, Loss - 0.6145824295976069, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3394645011227113\n",
      "Step - 9727, Loss - 0.6366405973958555, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5902272141273421\n",
      "Step - 9728, Loss - 0.8120086375483319, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6470489193762796\n",
      "Step - 9729, Loss - 0.6820658847447121, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6916988379917718\n",
      "Step - 9730, Loss - 0.626368542108827, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9529977791392347\n",
      "Step - 9731, Loss - 0.6578377092774176, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7841799825642803\n",
      "Step - 9732, Loss - 0.8868919681370517, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0554850273031624\n",
      "Step - 9733, Loss - 0.6612951584282639, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7184062202076028\n",
      "Step - 9734, Loss - 0.685238819660765, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2719383762001883\n",
      "Step - 9735, Loss - 0.7162332569564662, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7029361410537147\n",
      "Step - 9736, Loss - 0.5885534107069015, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1198094758186685\n",
      "Step - 9737, Loss - 0.6540484108624556, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7071916484462301\n",
      "Step - 9738, Loss - 0.7271882343884685, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5627176736755185\n",
      "Step - 9739, Loss - 0.7730317380524362, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8799922993220041\n",
      "Step - 9740, Loss - 0.7303816595237376, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2136989922372914\n",
      "Step - 9741, Loss - 0.6832042818072488, Learning Rate - 9.765625e-05, magnitude of gradient - 1.748931972281938\n",
      "Step - 9742, Loss - 0.6921139301839251, Learning Rate - 9.765625e-05, magnitude of gradient - 0.974252505107599\n",
      "Step - 9743, Loss - 0.8578660791491491, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6081997301953138\n",
      "Step - 9744, Loss - 0.5980133517129685, Learning Rate - 9.765625e-05, magnitude of gradient - 0.47440619039836085\n",
      "Step - 9745, Loss - 0.5880804399805032, Learning Rate - 9.765625e-05, magnitude of gradient - 2.919593165897807\n",
      "Step - 9746, Loss - 0.7300573887704407, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4821881017892575\n",
      "Step - 9747, Loss - 0.7547607681019998, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8534792499805308\n",
      "Step - 9748, Loss - 0.5278665535832215, Learning Rate - 9.765625e-05, magnitude of gradient - 1.844165948909711\n",
      "Step - 9749, Loss - 0.5824832901948891, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8030606886711257\n",
      "Step - 9750, Loss - 0.7741785729529517, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3318974112682742\n",
      "Step - 9751, Loss - 0.868460442652342, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2740105825519756\n",
      "Step - 9752, Loss - 0.6833723083806094, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1239429530303853\n",
      "Step - 9753, Loss - 0.8626903492124871, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7598318129269045\n",
      "Step - 9754, Loss - 0.578638020390861, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2483038725398994\n",
      "Step - 9755, Loss - 0.7171375333893099, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7498200531817255\n",
      "Step - 9756, Loss - 0.6648117198241172, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7028853586373639\n",
      "Step - 9757, Loss - 0.7990713080064469, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6937721962344146\n",
      "Step - 9758, Loss - 0.6465139969126502, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5414872589896614\n",
      "Step - 9759, Loss - 0.6678981674653313, Learning Rate - 9.765625e-05, magnitude of gradient - 2.0853607050484513\n",
      "Step - 9760, Loss - 0.744926503472646, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4211190880901317\n",
      "Step - 9761, Loss - 0.5098354880121972, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9671696811137622\n",
      "Step - 9762, Loss - 0.9452071499737045, Learning Rate - 9.765625e-05, magnitude of gradient - 1.684943702459595\n",
      "Step - 9763, Loss - 0.6186303815458816, Learning Rate - 9.765625e-05, magnitude of gradient - 2.587780745214704\n",
      "Step - 9764, Loss - 0.6472869606078073, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3915979356741324\n",
      "Step - 9765, Loss - 0.7054996740395599, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7693125799621543\n",
      "Step - 9766, Loss - 0.6743456894713538, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2975380808250623\n",
      "Step - 9767, Loss - 0.5197280721543168, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2349291259546178\n",
      "Step - 9768, Loss - 0.7585098818211097, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1744290583045371\n",
      "Step - 9769, Loss - 0.6310839051473218, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8669074097642929\n",
      "Step - 9770, Loss - 0.5595516885877152, Learning Rate - 9.765625e-05, magnitude of gradient - 1.067799384754394\n",
      "Step - 9771, Loss - 0.6389212418887651, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2280399064321639\n",
      "Step - 9772, Loss - 0.5050267295504905, Learning Rate - 9.765625e-05, magnitude of gradient - 1.078764712903778\n",
      "Step - 9773, Loss - 0.7555759366887408, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9771510163105472\n",
      "Step - 9774, Loss - 0.7935869611562587, Learning Rate - 9.765625e-05, magnitude of gradient - 0.716388645517906\n",
      "Step - 9775, Loss - 0.539949513465104, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9212494191768503\n",
      "Step - 9776, Loss - 0.7394077946188095, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7884129308972627\n",
      "Step - 9777, Loss - 0.7930005751497029, Learning Rate - 9.765625e-05, magnitude of gradient - 0.37780611015622617\n",
      "Step - 9778, Loss - 0.6280439024224429, Learning Rate - 9.765625e-05, magnitude of gradient - 1.242990973957472\n",
      "Step - 9779, Loss - 0.6911843576078085, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4682678789631496\n",
      "Step - 9780, Loss - 0.7433562158528809, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8482084945278764\n",
      "Step - 9781, Loss - 0.6927920029398339, Learning Rate - 9.765625e-05, magnitude of gradient - 1.10807407033358\n",
      "Step - 9782, Loss - 0.6849376755802337, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0106051382930732\n",
      "Step - 9783, Loss - 0.6760426712706983, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3744779188025653\n",
      "Step - 9784, Loss - 0.5443855581202303, Learning Rate - 9.765625e-05, magnitude of gradient - 1.36081928159402\n",
      "Step - 9785, Loss - 0.6208655594950468, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5108412491981611\n",
      "Step - 9786, Loss - 0.557016365985707, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3136387739004465\n",
      "Step - 9787, Loss - 0.8461147267624698, Learning Rate - 9.765625e-05, magnitude of gradient - 0.42444077223248533\n",
      "Step - 9788, Loss - 0.6797358118281748, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9955271252483242\n",
      "Step - 9789, Loss - 0.6998583574122929, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6809939185596963\n",
      "Step - 9790, Loss - 0.6053086930076966, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7301034872536221\n",
      "Step - 9791, Loss - 0.7016378008011067, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2743761414151673\n",
      "Step - 9792, Loss - 0.6685954621043455, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0799638779527325\n",
      "Step - 9793, Loss - 0.6268486924823746, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8718412372684569\n",
      "Step - 9794, Loss - 0.8266846242815822, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8481492839217717\n",
      "Step - 9795, Loss - 0.782295224059152, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0506831594009494\n",
      "Step - 9796, Loss - 0.7309540033969564, Learning Rate - 9.765625e-05, magnitude of gradient - 2.1648358749654486\n",
      "Step - 9797, Loss - 0.8379478138470443, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2788649013826738\n",
      "Step - 9798, Loss - 0.7515762899669326, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0938748429265956\n",
      "Step - 9799, Loss - 0.8247210252699468, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1103942816008532\n",
      "Step - 9800, Loss - 0.8148096906351895, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1784004003682564\n",
      "Step - 9801, Loss - 0.5693457580384388, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4050479252861232\n",
      "Step - 9802, Loss - 0.6897222541584496, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9867992720514762\n",
      "Step - 9803, Loss - 0.6231600357251864, Learning Rate - 9.765625e-05, magnitude of gradient - 2.1212982846379043\n",
      "Step - 9804, Loss - 0.8755066426702127, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2510575821364394\n",
      "Step - 9805, Loss - 0.5387704398526225, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8443267231228976\n",
      "Step - 9806, Loss - 0.7128089513470179, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5214383269449676\n",
      "Step - 9807, Loss - 0.8238181054170679, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7695573513292684\n",
      "Step - 9808, Loss - 0.7987783806110548, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0189391649289388\n",
      "Step - 9809, Loss - 0.584593160661129, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6641737332884783\n",
      "Step - 9810, Loss - 0.6208851891957673, Learning Rate - 9.765625e-05, magnitude of gradient - 2.664375664463478\n",
      "Step - 9811, Loss - 0.7211303143189829, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3613050643417022\n",
      "Step - 9812, Loss - 0.5615769547464158, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6551820709346465\n",
      "Step - 9813, Loss - 0.6991349169931486, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9598417534832415\n",
      "Step - 9814, Loss - 0.6690465996620646, Learning Rate - 9.765625e-05, magnitude of gradient - 1.573472592391779\n",
      "Step - 9815, Loss - 0.7726294736961149, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0007728072143875\n",
      "Step - 9816, Loss - 0.5530373594670652, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1147269476714088\n",
      "Step - 9817, Loss - 0.6858290607351781, Learning Rate - 9.765625e-05, magnitude of gradient - 1.100365360079634\n",
      "Step - 9818, Loss - 0.6038285188438389, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5201893239413723\n",
      "Step - 9819, Loss - 0.6945401960407735, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7830462093614625\n",
      "Step - 9820, Loss - 0.640101334943534, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5923216296808399\n",
      "Step - 9821, Loss - 0.5012039173871327, Learning Rate - 9.765625e-05, magnitude of gradient - 2.1479996887357564\n",
      "Step - 9822, Loss - 0.5752334289738902, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7647781563742806\n",
      "Step - 9823, Loss - 0.5926355967668252, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9279532985827782\n",
      "Step - 9824, Loss - 0.6983550234498499, Learning Rate - 9.765625e-05, magnitude of gradient - 1.444868456611255\n",
      "Step - 9825, Loss - 0.6775726407785879, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9247512035013825\n",
      "Step - 9826, Loss - 0.4572593632686238, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2126234019692226\n",
      "Step - 9827, Loss - 0.7396952062334974, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5269763377703618\n",
      "Step - 9828, Loss - 0.6267703093065407, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7763098831978245\n",
      "Step - 9829, Loss - 0.5448994659033788, Learning Rate - 9.765625e-05, magnitude of gradient - 2.07150955479658\n",
      "Step - 9830, Loss - 0.7511100916919091, Learning Rate - 9.765625e-05, magnitude of gradient - 2.1165271965258485\n",
      "Step - 9831, Loss - 0.6065848708561319, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1309255801926523\n",
      "Step - 9832, Loss - 0.633897834144867, Learning Rate - 9.765625e-05, magnitude of gradient - 1.231734255224416\n",
      "Step - 9833, Loss - 0.6014259857250699, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4074057674705898\n",
      "Step - 9834, Loss - 0.6683654132644411, Learning Rate - 9.765625e-05, magnitude of gradient - 0.576728369377368\n",
      "Step - 9835, Loss - 0.6519364804263375, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8401343529137225\n",
      "Step - 9836, Loss - 0.6446020844879262, Learning Rate - 9.765625e-05, magnitude of gradient - 1.971198668087014\n",
      "Step - 9837, Loss - 0.7222425587259622, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0062045979941259\n",
      "Step - 9838, Loss - 0.7023057003941824, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7955612235867661\n",
      "Step - 9839, Loss - 0.7912580543410648, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8655389537304019\n",
      "Step - 9840, Loss - 0.7534250209457982, Learning Rate - 9.765625e-05, magnitude of gradient - 0.795277234770886\n",
      "Step - 9841, Loss - 0.7114514363839207, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5323646727092903\n",
      "Step - 9842, Loss - 0.6952782259586621, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8258911869715313\n",
      "Step - 9843, Loss - 0.6339527811647355, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3105944189521133\n",
      "Step - 9844, Loss - 0.761973108827923, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5408735458637374\n",
      "Step - 9845, Loss - 0.8060233735855789, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8264026863521288\n",
      "Step - 9846, Loss - 0.8627201328301668, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7794241994873787\n",
      "Step - 9847, Loss - 0.6972067881757459, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8919841803773789\n",
      "Step - 9848, Loss - 1.0099324821535594, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2947076102373725\n",
      "Step - 9849, Loss - 0.8221431509312663, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0156976595891072\n",
      "Step - 9850, Loss - 0.7926742690833718, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6029259180772186\n",
      "Step - 9851, Loss - 0.7558985883362043, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7947594829425548\n",
      "Step - 9852, Loss - 0.780519313884629, Learning Rate - 9.765625e-05, magnitude of gradient - 0.20158147854872677\n",
      "Step - 9853, Loss - 0.8468132637102619, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0638248541940218\n",
      "Step - 9854, Loss - 0.8085605122164023, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3785147244759652\n",
      "Step - 9855, Loss - 0.7514117544370728, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8972114059425413\n",
      "Step - 9856, Loss - 0.675376875773692, Learning Rate - 9.765625e-05, magnitude of gradient - 1.370729851560194\n",
      "Step - 9857, Loss - 0.8557094398502296, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1249077067003959\n",
      "Step - 9858, Loss - 0.468180664939014, Learning Rate - 9.765625e-05, magnitude of gradient - 1.596423233725323\n",
      "Step - 9859, Loss - 0.7087490688978385, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7156045049844706\n",
      "Step - 9860, Loss - 0.6094867324516834, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0304012346920415\n",
      "Step - 9861, Loss - 0.5560335322257586, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7799089464726485\n",
      "Step - 9862, Loss - 0.5683498164552302, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7755678878344308\n",
      "Step - 9863, Loss - 0.6116781187898801, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7072726312208789\n",
      "Step - 9864, Loss - 0.7121948806612606, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0274883704572386\n",
      "Step - 9865, Loss - 0.8993204475437769, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9578401783722355\n",
      "Step - 9866, Loss - 0.6122453384906535, Learning Rate - 9.765625e-05, magnitude of gradient - 0.866882947995449\n",
      "Step - 9867, Loss - 0.8202616173799573, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1555040664398155\n",
      "Step - 9868, Loss - 1.0189592165129473, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8635218980299344\n",
      "Step - 9869, Loss - 0.6591662867596174, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9413696155258214\n",
      "Step - 9870, Loss - 0.5889229855625666, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6467238839135709\n",
      "Step - 9871, Loss - 0.7406383890767165, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2291397724912885\n",
      "Step - 9872, Loss - 0.6198692525499617, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8532722168872611\n",
      "Step - 9873, Loss - 0.7131220720330275, Learning Rate - 9.765625e-05, magnitude of gradient - 1.304537903318331\n",
      "Step - 9874, Loss - 0.8870125161279585, Learning Rate - 9.765625e-05, magnitude of gradient - 2.3053133767343277\n",
      "Step - 9875, Loss - 0.8384701689877316, Learning Rate - 9.765625e-05, magnitude of gradient - 1.283903724144499\n",
      "Step - 9876, Loss - 0.7692698008508173, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5146946840166757\n",
      "Step - 9877, Loss - 0.8094810575418359, Learning Rate - 9.765625e-05, magnitude of gradient - 0.4319572205808597\n",
      "Step - 9878, Loss - 0.617263399788332, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6768538059451468\n",
      "Step - 9879, Loss - 0.6248169930695141, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4117721851879264\n",
      "Step - 9880, Loss - 0.775123174693424, Learning Rate - 9.765625e-05, magnitude of gradient - 1.107232891080646\n",
      "Step - 9881, Loss - 0.7789994426926679, Learning Rate - 9.765625e-05, magnitude of gradient - 0.3865200269209585\n",
      "Step - 9882, Loss - 0.820467556401514, Learning Rate - 9.765625e-05, magnitude of gradient - 2.113629426559084\n",
      "Step - 9883, Loss - 0.7008184941248685, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3281047961606651\n",
      "Step - 9884, Loss - 0.6504760683302766, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0963803515070139\n",
      "Step - 9885, Loss - 0.5957897776652659, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8034024612040394\n",
      "Step - 9886, Loss - 0.6666976683395556, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1303828619013487\n",
      "Step - 9887, Loss - 0.7013597683603074, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5203542110506738\n",
      "Step - 9888, Loss - 0.7880903633746559, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5393102322686616\n",
      "Step - 9889, Loss - 0.6247825557773841, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6390099908686522\n",
      "Step - 9890, Loss - 0.6873734191899745, Learning Rate - 9.765625e-05, magnitude of gradient - 1.793480453866156\n",
      "Step - 9891, Loss - 0.6382704394895081, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4106419139313375\n",
      "Step - 9892, Loss - 0.6683919695949576, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2532631729626562\n",
      "Step - 9893, Loss - 0.7143819973813892, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9665961123976333\n",
      "Step - 9894, Loss - 0.7701937699401766, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5540748105061866\n",
      "Step - 9895, Loss - 0.6325704657849112, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5795455899709738\n",
      "Step - 9896, Loss - 0.7438390784905208, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9619173123107956\n",
      "Step - 9897, Loss - 0.7045118104571764, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9551716054906709\n",
      "Step - 9898, Loss - 0.8403932728101633, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8395154102202149\n",
      "Step - 9899, Loss - 0.6986834976288347, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9833828618367406\n",
      "Step - 9900, Loss - 0.5587425539608333, Learning Rate - 9.765625e-05, magnitude of gradient - 2.1511806521821826\n",
      "Step - 9901, Loss - 0.7741772643094249, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2456972123877428\n",
      "Step - 9902, Loss - 0.7649830541316903, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6852988381986527\n",
      "Step - 9903, Loss - 0.8424543323800652, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2010028527119447\n",
      "Step - 9904, Loss - 0.6617533466370122, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8077855542935367\n",
      "Step - 9905, Loss - 0.6436331677069489, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9318386947457147\n",
      "Step - 9906, Loss - 0.5677108419599388, Learning Rate - 9.765625e-05, magnitude of gradient - 1.32770370800864\n",
      "Step - 9907, Loss - 0.5103509878088797, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7849643768256095\n",
      "Step - 9908, Loss - 0.5636117913482722, Learning Rate - 9.765625e-05, magnitude of gradient - 0.893259354531206\n",
      "Step - 9909, Loss - 0.6473000518941913, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5819093874166065\n",
      "Step - 9910, Loss - 0.8106467830165941, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9118475854165411\n",
      "Step - 9911, Loss - 0.8059492686867946, Learning Rate - 9.765625e-05, magnitude of gradient - 2.3375390286678037\n",
      "Step - 9912, Loss - 0.6694349420278983, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8677031994662552\n",
      "Step - 9913, Loss - 0.7145875075044889, Learning Rate - 9.765625e-05, magnitude of gradient - 1.559182757117299\n",
      "Step - 9914, Loss - 0.784484376467004, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7903144856390771\n",
      "Step - 9915, Loss - 0.650275321219365, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7460032340671734\n",
      "Step - 9916, Loss - 0.6031207679289847, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3293890070636525\n",
      "Step - 9917, Loss - 0.8213374441658743, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2696423451927585\n",
      "Step - 9918, Loss - 0.5636466578046471, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4243416940310367\n",
      "Step - 9919, Loss - 0.8837011321065017, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7577454829332887\n",
      "Step - 9920, Loss - 0.8441259781975555, Learning Rate - 9.765625e-05, magnitude of gradient - 1.6859725388600673\n",
      "Step - 9921, Loss - 0.6507288708215049, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7458892641627\n",
      "Step - 9922, Loss - 0.86980955429633, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5767656864091982\n",
      "Step - 9923, Loss - 0.7032303543537752, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8992932716137024\n",
      "Step - 9924, Loss - 0.7170837105879573, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9387639709433475\n",
      "Step - 9925, Loss - 0.5003976067086581, Learning Rate - 9.765625e-05, magnitude of gradient - 2.632205526158957\n",
      "Step - 9926, Loss - 0.8454363708022525, Learning Rate - 9.765625e-05, magnitude of gradient - 1.327061140934143\n",
      "Step - 9927, Loss - 0.5337660479835273, Learning Rate - 9.765625e-05, magnitude of gradient - 1.8691710678478946\n",
      "Step - 9928, Loss - 0.5522671466175006, Learning Rate - 9.765625e-05, magnitude of gradient - 1.267183250843828\n",
      "Step - 9929, Loss - 0.7110149348273358, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1218738360916949\n",
      "Step - 9930, Loss - 0.7025953688592119, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3988755371161818\n",
      "Step - 9931, Loss - 0.49994188726121047, Learning Rate - 9.765625e-05, magnitude of gradient - 0.48905575234165954\n",
      "Step - 9932, Loss - 0.661205592629313, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1675447842844464\n",
      "Step - 9933, Loss - 0.7262477943792987, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0296143710130352\n",
      "Step - 9934, Loss - 0.7800633558834356, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5963626393695352\n",
      "Step - 9935, Loss - 0.6768669751157067, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9902411426790612\n",
      "Step - 9936, Loss - 0.6285867089202396, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7545252249966083\n",
      "Step - 9937, Loss - 0.6748190928436272, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5447834796524704\n",
      "Step - 9938, Loss - 0.6973364901794561, Learning Rate - 9.765625e-05, magnitude of gradient - 0.32334301829188683\n",
      "Step - 9939, Loss - 1.0108033986548295, Learning Rate - 9.765625e-05, magnitude of gradient - 2.3326986837106802\n",
      "Step - 9940, Loss - 0.6072023970766144, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3652515177592852\n",
      "Step - 9941, Loss - 0.7581415078986449, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2591464502021599\n",
      "Step - 9942, Loss - 0.9119338851079702, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9465210584360477\n",
      "Step - 9943, Loss - 0.4870223301939183, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7667014135892258\n",
      "Step - 9944, Loss - 0.5173569988240498, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6970375279669263\n",
      "Step - 9945, Loss - 0.5233152669586858, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3765152625303048\n",
      "Step - 9946, Loss - 0.7250112989611511, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6657942994808339\n",
      "Step - 9947, Loss - 0.557079397709397, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9183067493458159\n",
      "Step - 9948, Loss - 0.7995920368625175, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5022485196984676\n",
      "Step - 9949, Loss - 0.6716419187229763, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5742786154120403\n",
      "Step - 9950, Loss - 0.6284593302217615, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8585585841729596\n",
      "Step - 9951, Loss - 0.6829265892454042, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7942164305865385\n",
      "Step - 9952, Loss - 0.7584045942235254, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0745877356761389\n",
      "Step - 9953, Loss - 0.8902947028405199, Learning Rate - 9.765625e-05, magnitude of gradient - 2.5373584614094558\n",
      "Step - 9954, Loss - 0.8324852928501467, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8624079304537976\n",
      "Step - 9955, Loss - 0.7420468455197453, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9793744007027767\n",
      "Step - 9956, Loss - 0.6136565791130052, Learning Rate - 9.765625e-05, magnitude of gradient - 2.137487571711123\n",
      "Step - 9957, Loss - 0.6878021149371303, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5160227303294127\n",
      "Step - 9958, Loss - 0.88438393191105, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7564399067206715\n",
      "Step - 9959, Loss - 0.6333586039814262, Learning Rate - 9.765625e-05, magnitude of gradient - 1.126170329605512\n",
      "Step - 9960, Loss - 0.7244889298493151, Learning Rate - 9.765625e-05, magnitude of gradient - 1.400372650385387\n",
      "Step - 9961, Loss - 0.8139446069769903, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7124975653553249\n",
      "Step - 9962, Loss - 0.7785445818366753, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5830012870217822\n",
      "Step - 9963, Loss - 0.6612793115705354, Learning Rate - 9.765625e-05, magnitude of gradient - 0.5174561453568033\n",
      "Step - 9964, Loss - 0.6849576325259884, Learning Rate - 9.765625e-05, magnitude of gradient - 2.716930364346101\n",
      "Step - 9965, Loss - 0.9953635600132251, Learning Rate - 9.765625e-05, magnitude of gradient - 1.1013040059986543\n",
      "Step - 9966, Loss - 0.790742911025949, Learning Rate - 9.765625e-05, magnitude of gradient - 0.42904388546972794\n",
      "Step - 9967, Loss - 0.6991791939776383, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7982283157989\n",
      "Step - 9968, Loss - 0.744066513538241, Learning Rate - 9.765625e-05, magnitude of gradient - 1.2747552785320393\n",
      "Step - 9969, Loss - 0.586951480778468, Learning Rate - 9.765625e-05, magnitude of gradient - 1.021739820747394\n",
      "Step - 9970, Loss - 0.832752329011302, Learning Rate - 9.765625e-05, magnitude of gradient - 2.2699454430174106\n",
      "Step - 9971, Loss - 0.6140395254466811, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4852924056468697\n",
      "Step - 9972, Loss - 0.7645114976121156, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9782084259291138\n",
      "Step - 9973, Loss - 0.9821804465280262, Learning Rate - 9.765625e-05, magnitude of gradient - 2.7329111992096395\n",
      "Step - 9974, Loss - 0.8754471123218144, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9943681733940865\n",
      "Step - 9975, Loss - 0.6792363906382403, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7038040035380819\n",
      "Step - 9976, Loss - 0.5132542402006185, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7392526886523072\n",
      "Step - 9977, Loss - 0.5950623276507808, Learning Rate - 9.765625e-05, magnitude of gradient - 2.221593233163034\n",
      "Step - 9978, Loss - 0.5003624792415362, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8392471686232437\n",
      "Step - 9979, Loss - 0.6470656728622475, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9056713525522075\n",
      "Step - 9980, Loss - 0.6805454195093431, Learning Rate - 9.765625e-05, magnitude of gradient - 0.92871638259344\n",
      "Step - 9981, Loss - 0.6886567296181676, Learning Rate - 9.765625e-05, magnitude of gradient - 1.475778531209909\n",
      "Step - 9982, Loss - 0.7448113708842472, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8979753188314274\n",
      "Step - 9983, Loss - 0.6152936172313551, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7652032890333328\n",
      "Step - 9984, Loss - 0.68243458410687, Learning Rate - 9.765625e-05, magnitude of gradient - 1.3214882261114855\n",
      "Step - 9985, Loss - 0.621246172907045, Learning Rate - 9.765625e-05, magnitude of gradient - 2.08644116696533\n",
      "Step - 9986, Loss - 0.7507544899178791, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7794999965548047\n",
      "Step - 9987, Loss - 0.6729100194996216, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0376579993223354\n",
      "Step - 9988, Loss - 0.5610584843335562, Learning Rate - 9.765625e-05, magnitude of gradient - 2.406881679374931\n",
      "Step - 9989, Loss - 0.6086859121531182, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4189089330174236\n",
      "Step - 9990, Loss - 0.7113812555709279, Learning Rate - 9.765625e-05, magnitude of gradient - 1.372487658118471\n",
      "Step - 9991, Loss - 0.7385087217844973, Learning Rate - 9.765625e-05, magnitude of gradient - 0.9540867931984905\n",
      "Step - 9992, Loss - 0.6848321741769485, Learning Rate - 9.765625e-05, magnitude of gradient - 1.7474238406064897\n",
      "Step - 9993, Loss - 0.5116972897221913, Learning Rate - 9.765625e-05, magnitude of gradient - 0.29185176475759084\n",
      "Step - 9994, Loss - 0.6301066318987854, Learning Rate - 9.765625e-05, magnitude of gradient - 0.6593067287474033\n",
      "Step - 9995, Loss - 0.6779112459612048, Learning Rate - 9.765625e-05, magnitude of gradient - 0.8868456474012205\n",
      "Step - 9996, Loss - 0.7523613863680136, Learning Rate - 9.765625e-05, magnitude of gradient - 1.9691842399472856\n",
      "Step - 9997, Loss - 0.8520259506949581, Learning Rate - 9.765625e-05, magnitude of gradient - 1.0258082737462628\n",
      "Step - 9998, Loss - 0.6539400383719096, Learning Rate - 9.765625e-05, magnitude of gradient - 1.4838678650776813\n",
      "Step - 9999, Loss - 0.7039955130861522, Learning Rate - 9.765625e-05, magnitude of gradient - 0.7650344280574347\n",
      "Step - 10000, Loss - 0.8276351855534445, Learning Rate - 9.765625e-05, magnitude of gradient - 1.5051277720314047\n",
      "Step - 10001, Loss - 0.7727657745200943, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.453172347409864\n",
      "Step - 10002, Loss - 0.8721098739426905, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7098704125780173\n",
      "Step - 10003, Loss - 0.7411925700354185, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7504426174675508\n",
      "Step - 10004, Loss - 0.8221350427445955, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5238288164444748\n",
      "Step - 10005, Loss - 0.8273393837193259, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5595254402782736\n",
      "Step - 10006, Loss - 0.7449985735220538, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6733181122569408\n",
      "Step - 10007, Loss - 0.6077580750413845, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7059344578258104\n",
      "Step - 10008, Loss - 0.833893584616101, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6408652779007984\n",
      "Step - 10009, Loss - 0.7709662227224097, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.481208610383534\n",
      "Step - 10010, Loss - 0.6835712995558, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5978771826484102\n",
      "Step - 10011, Loss - 0.8347095766467569, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9496825271300793\n",
      "Step - 10012, Loss - 0.605146576839094, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5087224402908185\n",
      "Step - 10013, Loss - 0.4901366591856196, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5757558493342205\n",
      "Step - 10014, Loss - 0.796242544078546, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8682775344535767\n",
      "Step - 10015, Loss - 0.8600998621281175, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3597755988047655\n",
      "Step - 10016, Loss - 0.5767821405638879, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.8535136792379547\n",
      "Step - 10017, Loss - 0.5156666599060058, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.9065233606095873\n",
      "Step - 10018, Loss - 0.6132298437973482, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7815501970969885\n",
      "Step - 10019, Loss - 0.8037576491444999, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4091973783983558\n",
      "Step - 10020, Loss - 0.5980707469897593, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6536276831300982\n",
      "Step - 10021, Loss - 0.6925614893875494, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.4347138530017661\n",
      "Step - 10022, Loss - 0.7522569839433711, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9949470095737617\n",
      "Step - 10023, Loss - 0.6724705617704703, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.49665823446129526\n",
      "Step - 10024, Loss - 0.6741869042638023, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.629528569985065\n",
      "Step - 10025, Loss - 0.6318199465309148, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.423072739611281\n",
      "Step - 10026, Loss - 0.7028207992710922, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.283793464142723\n",
      "Step - 10027, Loss - 0.48166120233249576, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4976841905277005\n",
      "Step - 10028, Loss - 0.7761373423868306, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5407870706920763\n",
      "Step - 10029, Loss - 0.6814184664943457, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.594498062367751\n",
      "Step - 10030, Loss - 0.8978080967474551, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0745378237253111\n",
      "Step - 10031, Loss - 0.6391615572590889, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8325786401197943\n",
      "Step - 10032, Loss - 0.5842923701400049, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5880411016327054\n",
      "Step - 10033, Loss - 0.631761759266558, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7962552516080137\n",
      "Step - 10034, Loss - 0.6970684409391497, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7983135658007469\n",
      "Step - 10035, Loss - 0.607499998851461, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2593656909301973\n",
      "Step - 10036, Loss - 0.6799852593454229, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0217940290480656\n",
      "Step - 10037, Loss - 0.7203659352375424, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6408152685823355\n",
      "Step - 10038, Loss - 0.7988363275637593, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0376834012658716\n",
      "Step - 10039, Loss - 0.7584018441670131, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4284200451519342\n",
      "Step - 10040, Loss - 0.8289971339525065, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.323881911484468\n",
      "Step - 10041, Loss - 0.8353921381769459, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2750845355935694\n",
      "Step - 10042, Loss - 0.7454781701071863, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9195368645652474\n",
      "Step - 10043, Loss - 0.5422461443864673, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7493417086381784\n",
      "Step - 10044, Loss - 0.7201260676247387, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.423754464511558\n",
      "Step - 10045, Loss - 0.7211014062847281, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8625859546651078\n",
      "Step - 10046, Loss - 0.6601406009347308, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3386226699299262\n",
      "Step - 10047, Loss - 0.7506386932888672, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8843275551822357\n",
      "Step - 10048, Loss - 0.7315561117033923, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7477562475588944\n",
      "Step - 10049, Loss - 0.8066458566544517, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.72304362577311\n",
      "Step - 10050, Loss - 0.7614636803146418, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7837764434380523\n",
      "Step - 10051, Loss - 0.7235454877803922, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2326714147899218\n",
      "Step - 10052, Loss - 0.7231781171667417, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.079506776502897\n",
      "Step - 10053, Loss - 0.5687378272501082, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0405992834539228\n",
      "Step - 10054, Loss - 0.8860775159238505, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.408915312834445\n",
      "Step - 10055, Loss - 0.5975568458277547, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0488144603784089\n",
      "Step - 10056, Loss - 0.5627834360135029, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.181788717628331\n",
      "Step - 10057, Loss - 0.705250940037916, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6015022209449625\n",
      "Step - 10058, Loss - 0.768796232387501, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8120329330596481\n",
      "Step - 10059, Loss - 0.6770384272938751, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7950007337003336\n",
      "Step - 10060, Loss - 0.7095777525740896, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7531857295629276\n",
      "Step - 10061, Loss - 0.7790138590528889, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6977885430374744\n",
      "Step - 10062, Loss - 0.8122240587021838, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.296897348783951\n",
      "Step - 10063, Loss - 0.7282792003655263, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7845876294190942\n",
      "Step - 10064, Loss - 0.7235575334776935, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3448802556780504\n",
      "Step - 10065, Loss - 0.7646659033622267, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1400721257094781\n",
      "Step - 10066, Loss - 0.524989846466427, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0540309060226831\n",
      "Step - 10067, Loss - 0.7320612772291313, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6696929828287745\n",
      "Step - 10068, Loss - 0.6700164276529588, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9410766818390919\n",
      "Step - 10069, Loss - 0.5954612321472712, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5672932339632921\n",
      "Step - 10070, Loss - 0.6731744395774195, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7952702365823175\n",
      "Step - 10071, Loss - 0.5795507531086106, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1670967925561293\n",
      "Step - 10072, Loss - 0.7285515496110716, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3053445629362719\n",
      "Step - 10073, Loss - 0.7032927325493965, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5907616217025293\n",
      "Step - 10074, Loss - 0.9329594083613189, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8849334440595644\n",
      "Step - 10075, Loss - 0.5554771597308098, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0753211814157766\n",
      "Step - 10076, Loss - 0.747352368826925, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1254111930843873\n",
      "Step - 10077, Loss - 0.8768925100202873, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.49450032633760427\n",
      "Step - 10078, Loss - 0.5586995510970023, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.741089690819097\n",
      "Step - 10079, Loss - 0.505977804865354, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.15523599762735\n",
      "Step - 10080, Loss - 0.736418486342446, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8360599161095948\n",
      "Step - 10081, Loss - 0.7536530860166148, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4641866584725942\n",
      "Step - 10082, Loss - 0.6335822745180647, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2352284912229063\n",
      "Step - 10083, Loss - 0.6004613650230548, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5871013953199857\n",
      "Step - 10084, Loss - 0.6451116896590114, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1398391186695889\n",
      "Step - 10085, Loss - 0.9644847731632084, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.4673988114722483\n",
      "Step - 10086, Loss - 0.8433001653473803, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7905299877671463\n",
      "Step - 10087, Loss - 0.7513972461856028, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5854236300136827\n",
      "Step - 10088, Loss - 0.6758024917301122, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5862045475152278\n",
      "Step - 10089, Loss - 0.7089300232920409, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2658997432415018\n",
      "Step - 10090, Loss - 0.6369610953641913, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.870055861496346\n",
      "Step - 10091, Loss - 0.7909366760068575, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4415265902339325\n",
      "Step - 10092, Loss - 0.9066204887285907, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2617138329495656\n",
      "Step - 10093, Loss - 0.6709891003462782, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1202213509572332\n",
      "Step - 10094, Loss - 0.5941955587107886, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.455370690014412\n",
      "Step - 10095, Loss - 0.6965356575962335, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4352374655642504\n",
      "Step - 10096, Loss - 0.861742198246443, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.386241222562504\n",
      "Step - 10097, Loss - 0.6862597124947437, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5593425208318827\n",
      "Step - 10098, Loss - 0.6403946581084714, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0521788471833173\n",
      "Step - 10099, Loss - 0.554783025591015, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.8355780368779486\n",
      "Step - 10100, Loss - 0.6719503257746806, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7565639200132563\n",
      "Step - 10101, Loss - 0.7767603693600674, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.855597147982491\n",
      "Step - 10102, Loss - 0.5859186300091375, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.9090693126516862\n",
      "Step - 10103, Loss - 0.6992040283730698, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4921851258797147\n",
      "Step - 10104, Loss - 0.777679897146881, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2633375656771213\n",
      "Step - 10105, Loss - 0.6252590159305824, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.765379770549216\n",
      "Step - 10106, Loss - 0.7936385790093101, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6017710787210864\n",
      "Step - 10107, Loss - 0.7602125713787978, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.3416289600694507\n",
      "Step - 10108, Loss - 0.6040841677347727, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5792083041839635\n",
      "Step - 10109, Loss - 0.6117908701102158, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8827395389443188\n",
      "Step - 10110, Loss - 0.8116059071015552, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.7718371012365863\n",
      "Step - 10111, Loss - 0.6970495247100738, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3952963214107619\n",
      "Step - 10112, Loss - 0.6121160290503538, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6926435533594194\n",
      "Step - 10113, Loss - 0.5974668938145329, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7110245132427818\n",
      "Step - 10114, Loss - 0.7235708664669749, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.783744476750154\n",
      "Step - 10115, Loss - 0.6172226391423975, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.162209558761688\n",
      "Step - 10116, Loss - 0.8983499034713309, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.402387542594565\n",
      "Step - 10117, Loss - 0.7788794925585899, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.629953705255692\n",
      "Step - 10118, Loss - 0.49864058613111106, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.4516412560535817\n",
      "Step - 10119, Loss - 0.6047445446453992, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.895300667964968\n",
      "Step - 10120, Loss - 0.7229369731973492, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.9222529621045261\n",
      "Step - 10121, Loss - 0.5835998485059467, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0133944709556086\n",
      "Step - 10122, Loss - 0.7532555660918117, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5905934840243812\n",
      "Step - 10123, Loss - 0.7442421505299728, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6187030571262249\n",
      "Step - 10124, Loss - 0.6895421599500481, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3990173911854864\n",
      "Step - 10125, Loss - 0.6206929034959013, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6819329585603232\n",
      "Step - 10126, Loss - 0.8640259341686236, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4567783467218538\n",
      "Step - 10127, Loss - 0.7536303791583837, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6555929606893026\n",
      "Step - 10128, Loss - 0.7481413143969402, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3202497982424506\n",
      "Step - 10129, Loss - 0.4487737464689835, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8093464896918263\n",
      "Step - 10130, Loss - 0.6864317978781671, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8489282976006265\n",
      "Step - 10131, Loss - 0.5783404059772864, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6388548841226954\n",
      "Step - 10132, Loss - 0.7744451716443651, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2714308346378802\n",
      "Step - 10133, Loss - 0.5846552685277311, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9223843425745766\n",
      "Step - 10134, Loss - 0.8274238747945551, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9510238314870062\n",
      "Step - 10135, Loss - 0.7665199846420647, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1346142342715209\n",
      "Step - 10136, Loss - 0.653907602260458, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9015395001559543\n",
      "Step - 10137, Loss - 0.9165427328789008, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.552835135057843\n",
      "Step - 10138, Loss - 0.6999041287326275, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1088590239404275\n",
      "Step - 10139, Loss - 0.6440840004239886, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8684162552202326\n",
      "Step - 10140, Loss - 0.7878837486940813, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.208731381659256\n",
      "Step - 10141, Loss - 0.772436438180728, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.43610712176992933\n",
      "Step - 10142, Loss - 0.66457069079572, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0319112241760113\n",
      "Step - 10143, Loss - 0.6162419324032296, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.691429895660923\n",
      "Step - 10144, Loss - 0.7756084150686734, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.556277679077106\n",
      "Step - 10145, Loss - 0.5070008971793196, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3090004444971282\n",
      "Step - 10146, Loss - 0.8476338103585503, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2254121274555907\n",
      "Step - 10147, Loss - 0.6186359199226856, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.6506512009624443\n",
      "Step - 10148, Loss - 0.5891712129040978, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.3306070215011108\n",
      "Step - 10149, Loss - 0.7960597513545693, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.4706276073013074\n",
      "Step - 10150, Loss - 0.5662179647065647, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4137013816790716\n",
      "Step - 10151, Loss - 0.7014965698274238, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8892894734510476\n",
      "Step - 10152, Loss - 0.7074933354969498, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5825690628507076\n",
      "Step - 10153, Loss - 0.7453045574486609, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3561435766124017\n",
      "Step - 10154, Loss - 0.5996874205612753, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7525918932645548\n",
      "Step - 10155, Loss - 0.7047087824282079, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6285771219693529\n",
      "Step - 10156, Loss - 0.6819848310185644, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.742638738770625\n",
      "Step - 10157, Loss - 0.7800846594758637, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8206608965552065\n",
      "Step - 10158, Loss - 0.6536140437884219, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4522205654094076\n",
      "Step - 10159, Loss - 0.7890097963392221, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.102788557667703\n",
      "Step - 10160, Loss - 0.5571946524230613, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.2907978005639675\n",
      "Step - 10161, Loss - 0.6747469704061835, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.277949595154581\n",
      "Step - 10162, Loss - 0.7436878688324728, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7778917565571155\n",
      "Step - 10163, Loss - 0.7892814246937534, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3363031361852633\n",
      "Step - 10164, Loss - 0.7522893031106597, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7617234539523481\n",
      "Step - 10165, Loss - 0.7017908765226115, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8952879168122934\n",
      "Step - 10166, Loss - 0.7429853609096668, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.32800761115444\n",
      "Step - 10167, Loss - 0.7918276377495334, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7324364965742054\n",
      "Step - 10168, Loss - 0.7650339569232751, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2589576605244814\n",
      "Step - 10169, Loss - 0.7037698874487263, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.817518648342411\n",
      "Step - 10170, Loss - 0.685599802135514, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6840041120149793\n",
      "Step - 10171, Loss - 0.6637583920780005, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5998051549505881\n",
      "Step - 10172, Loss - 0.8470307266809138, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0269011283551097\n",
      "Step - 10173, Loss - 0.7682567875686911, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6431288070044723\n",
      "Step - 10174, Loss - 0.8556992197646038, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.0677962472916813\n",
      "Step - 10175, Loss - 0.6609368050194423, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.4043179270359258\n",
      "Step - 10176, Loss - 0.8899254541157516, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.163535386402565\n",
      "Step - 10177, Loss - 0.9583978849963418, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3457685111104374\n",
      "Step - 10178, Loss - 0.6925672413528599, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.3221919282594037\n",
      "Step - 10179, Loss - 0.5391681273649788, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.951714775897933\n",
      "Step - 10180, Loss - 0.4779558273165548, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3167494532056274\n",
      "Step - 10181, Loss - 0.6108253074394951, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1287158815628113\n",
      "Step - 10182, Loss - 0.465196210978344, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.634683527569016\n",
      "Step - 10183, Loss - 0.751592998415078, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.2708448193422122\n",
      "Step - 10184, Loss - 0.664623410509424, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9263238792650589\n",
      "Step - 10185, Loss - 0.7214202735964291, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1387556732315738\n",
      "Step - 10186, Loss - 0.6819570018093813, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5309326861544132\n",
      "Step - 10187, Loss - 0.8528296055850311, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8000851348308076\n",
      "Step - 10188, Loss - 0.6773745369303228, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3204422604608692\n",
      "Step - 10189, Loss - 0.5686167366409329, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6206920849094106\n",
      "Step - 10190, Loss - 0.7479767686555334, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8455008894806916\n",
      "Step - 10191, Loss - 0.7690545466760573, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.9089911485908289\n",
      "Step - 10192, Loss - 0.6278703243035734, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.566866842776147\n",
      "Step - 10193, Loss - 0.6637313113308364, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.080490070148659\n",
      "Step - 10194, Loss - 0.4586774804845729, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4009962902511497\n",
      "Step - 10195, Loss - 0.8120175070341425, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7331140129968273\n",
      "Step - 10196, Loss - 0.7384550032138565, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7477595070301153\n",
      "Step - 10197, Loss - 0.873373233889341, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.153412387146585\n",
      "Step - 10198, Loss - 0.5484487676264742, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1025810443747046\n",
      "Step - 10199, Loss - 0.691559459799858, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.569319407041042\n",
      "Step - 10200, Loss - 0.5766940991277862, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5710824188897047\n",
      "Step - 10201, Loss - 0.7682259386471072, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.44102248258565824\n",
      "Step - 10202, Loss - 0.8222018021770235, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2878874817667842\n",
      "Step - 10203, Loss - 0.44148704789398824, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.260643202318128\n",
      "Step - 10204, Loss - 0.6792230326424129, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.0056538502050434\n",
      "Step - 10205, Loss - 0.9163853461866558, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3878423735080403\n",
      "Step - 10206, Loss - 0.7545239854106128, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8530920440615204\n",
      "Step - 10207, Loss - 0.7944157429266948, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8090278777952489\n",
      "Step - 10208, Loss - 0.5353443834935719, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.3513186431900903\n",
      "Step - 10209, Loss - 0.6658315716856226, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.272733871676513\n",
      "Step - 10210, Loss - 0.6731686768840042, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5505053531783997\n",
      "Step - 10211, Loss - 0.7589932725688958, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.3257884516786779\n",
      "Step - 10212, Loss - 0.8910199793813074, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.3259341257698605\n",
      "Step - 10213, Loss - 0.7128298212423019, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2866488902416897\n",
      "Step - 10214, Loss - 0.537204111687484, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9030822021716103\n",
      "Step - 10215, Loss - 0.7861530296387387, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8138755278795716\n",
      "Step - 10216, Loss - 0.5958927004360943, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.172173525531651\n",
      "Step - 10217, Loss - 0.6486256226499123, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0996167296945671\n",
      "Step - 10218, Loss - 0.7706916756756346, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1862208440398339\n",
      "Step - 10219, Loss - 0.7560647563735049, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4447814916475734\n",
      "Step - 10220, Loss - 0.7979557558407607, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5329173220459893\n",
      "Step - 10221, Loss - 0.8279463494099861, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6007773990688643\n",
      "Step - 10222, Loss - 0.8406080650509413, Learning Rate - 4.8828125e-05, magnitude of gradient - 3.1358905880113452\n",
      "Step - 10223, Loss - 0.5957829971153155, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7755697467923905\n",
      "Step - 10224, Loss - 0.6294356913441769, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6985544984240009\n",
      "Step - 10225, Loss - 0.6744681760188519, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8563818497077705\n",
      "Step - 10226, Loss - 0.6789726029462608, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.504991891855343\n",
      "Step - 10227, Loss - 0.8712909911161593, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0368475494299183\n",
      "Step - 10228, Loss - 0.6371918585119336, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9958071243166458\n",
      "Step - 10229, Loss - 0.7985893814395114, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6636263968734922\n",
      "Step - 10230, Loss - 0.6985805553328409, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8076494963492409\n",
      "Step - 10231, Loss - 0.905561945383245, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.187430146480241\n",
      "Step - 10232, Loss - 0.8732570032185418, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0002577520608458\n",
      "Step - 10233, Loss - 0.5684115042658806, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5995974900042691\n",
      "Step - 10234, Loss - 0.5615549963194562, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5405695026924782\n",
      "Step - 10235, Loss - 0.7303062368081028, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.034150012827099\n",
      "Step - 10236, Loss - 0.7179818501179397, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2727263802199789\n",
      "Step - 10237, Loss - 0.6157769364215137, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.508404472327093\n",
      "Step - 10238, Loss - 0.6655597825703625, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6717870117187178\n",
      "Step - 10239, Loss - 0.8078175031894113, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0683818670023868\n",
      "Step - 10240, Loss - 0.5685790465846904, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5531930781717211\n",
      "Step - 10241, Loss - 0.74917037194017, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8190171107659907\n",
      "Step - 10242, Loss - 0.7809354224723196, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.178432094995425\n",
      "Step - 10243, Loss - 0.5603339851670415, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6666349134898677\n",
      "Step - 10244, Loss - 0.6243262333565287, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3030523240285843\n",
      "Step - 10245, Loss - 0.6417980816474369, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5810753543419384\n",
      "Step - 10246, Loss - 0.7224702762784495, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5156272477934905\n",
      "Step - 10247, Loss - 0.7581215646222255, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4269446936799268\n",
      "Step - 10248, Loss - 0.8671790189372997, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3805448800153384\n",
      "Step - 10249, Loss - 0.7314194018496618, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.0406521060994085\n",
      "Step - 10250, Loss - 0.6666589822519733, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0586904511523287\n",
      "Step - 10251, Loss - 0.5880711675592232, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2227614462026417\n",
      "Step - 10252, Loss - 0.7526855520769785, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0677700542224757\n",
      "Step - 10253, Loss - 0.6581438294491648, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0087158763648547\n",
      "Step - 10254, Loss - 0.7140574646423332, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.4562638698844164\n",
      "Step - 10255, Loss - 0.833913057580966, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8491514177233774\n",
      "Step - 10256, Loss - 0.6229466404427604, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.3683806312823136\n",
      "Step - 10257, Loss - 0.5716871136594082, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7904016966949337\n",
      "Step - 10258, Loss - 0.6468545989084946, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.062521502182163\n",
      "Step - 10259, Loss - 0.7047219470549829, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9643179614629976\n",
      "Step - 10260, Loss - 0.7017028813970365, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8204863766388273\n",
      "Step - 10261, Loss - 0.8345844295312517, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6513290709444657\n",
      "Step - 10262, Loss - 0.8711644931603044, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.61911513781499\n",
      "Step - 10263, Loss - 0.7288055509986582, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8751291309620002\n",
      "Step - 10264, Loss - 0.6935726003452449, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.920566947906742\n",
      "Step - 10265, Loss - 0.5052201201519455, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0947752472171428\n",
      "Step - 10266, Loss - 0.7864391558792405, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.009348582598905\n",
      "Step - 10267, Loss - 0.7212000867820019, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.41761401584041\n",
      "Step - 10268, Loss - 0.5280763446472259, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.965963664555627\n",
      "Step - 10269, Loss - 0.8686281576387053, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9330953378457412\n",
      "Step - 10270, Loss - 0.7284381074085495, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7552033727402228\n",
      "Step - 10271, Loss - 0.7639605915157092, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1053395474491325\n",
      "Step - 10272, Loss - 0.6686341253783538, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9715026949260168\n",
      "Step - 10273, Loss - 0.7533776210344184, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2652122799380636\n",
      "Step - 10274, Loss - 0.8141413664856261, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.47684624385983554\n",
      "Step - 10275, Loss - 0.6805871941360124, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.243992793847572\n",
      "Step - 10276, Loss - 0.7204547714464395, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.36874717113123595\n",
      "Step - 10277, Loss - 0.7599575238907957, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.130955997672971\n",
      "Step - 10278, Loss - 0.5506495204310964, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.083104948810732\n",
      "Step - 10279, Loss - 0.6140770636485651, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6213567915275604\n",
      "Step - 10280, Loss - 0.7196019442849049, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.91716091499125\n",
      "Step - 10281, Loss - 0.6560436368230478, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2842575643939007\n",
      "Step - 10282, Loss - 0.5033893821990811, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5946207596374639\n",
      "Step - 10283, Loss - 0.7036715820796855, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5967725988334882\n",
      "Step - 10284, Loss - 0.592849852873476, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4555026070077364\n",
      "Step - 10285, Loss - 0.8062796997882722, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0299635864919778\n",
      "Step - 10286, Loss - 0.6891849454192457, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9439907296878695\n",
      "Step - 10287, Loss - 0.7484544495252043, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6445586051171105\n",
      "Step - 10288, Loss - 0.5950058781043133, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.0676910314449253\n",
      "Step - 10289, Loss - 0.7242550824930656, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1497281098062366\n",
      "Step - 10290, Loss - 0.6750311087489324, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2935411501288374\n",
      "Step - 10291, Loss - 0.8749249810411205, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7055397462884092\n",
      "Step - 10292, Loss - 0.7365670718945033, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.801007085155057\n",
      "Step - 10293, Loss - 0.7831370531992137, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1383210465845022\n",
      "Step - 10294, Loss - 0.7681946960335821, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4312001906540277\n",
      "Step - 10295, Loss - 0.7579693161805778, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7039467098891481\n",
      "Step - 10296, Loss - 0.5461042477264069, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5406881655332025\n",
      "Step - 10297, Loss - 0.507531200383967, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.924904504134075\n",
      "Step - 10298, Loss - 0.550671632861994, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.2977744258579564\n",
      "Step - 10299, Loss - 0.7601711958766528, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7815503025106546\n",
      "Step - 10300, Loss - 0.7846122991779232, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2875205176898936\n",
      "Step - 10301, Loss - 0.6913387354154926, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.427280119694644\n",
      "Step - 10302, Loss - 0.6188164446713463, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7976527963951456\n",
      "Step - 10303, Loss - 0.6767812561494687, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7094340129416429\n",
      "Step - 10304, Loss - 0.9162890335936037, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9828780477981656\n",
      "Step - 10305, Loss - 0.8350406700687013, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8615734387076848\n",
      "Step - 10306, Loss - 0.8086405397678148, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.4359468893793537\n",
      "Step - 10307, Loss - 0.731930234229752, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5396497680826733\n",
      "Step - 10308, Loss - 0.7551234348926952, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6411450447633579\n",
      "Step - 10309, Loss - 0.9039507687362119, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.114852213684084\n",
      "Step - 10310, Loss - 0.8131567990325236, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6504317322325242\n",
      "Step - 10311, Loss - 0.5394837389853682, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8547412822677297\n",
      "Step - 10312, Loss - 0.592114854649293, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1759442142048948\n",
      "Step - 10313, Loss - 0.7358771153912472, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.766671580160967\n",
      "Step - 10314, Loss - 0.641762609611453, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5965210410972099\n",
      "Step - 10315, Loss - 0.75017493571368, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5725164442980577\n",
      "Step - 10316, Loss - 0.5643639278922984, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7679060850857538\n",
      "Step - 10317, Loss - 0.5686797103325257, Learning Rate - 4.8828125e-05, magnitude of gradient - 3.0809697105721003\n",
      "Step - 10318, Loss - 0.7980596436558381, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5666785324061183\n",
      "Step - 10319, Loss - 0.7115331495611565, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.067734393437828\n",
      "Step - 10320, Loss - 0.7302661923951983, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2246778985605258\n",
      "Step - 10321, Loss - 0.8226940194677692, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.4969097124506419\n",
      "Step - 10322, Loss - 0.6500551167888383, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3267632971661942\n",
      "Step - 10323, Loss - 0.5368865392914525, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.069455736558548\n",
      "Step - 10324, Loss - 0.7217324987059494, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9784965642252723\n",
      "Step - 10325, Loss - 0.47947744942762355, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.1849271417055958\n",
      "Step - 10326, Loss - 0.46099287130277644, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.1142571132209387\n",
      "Step - 10327, Loss - 0.7532354460008172, Learning Rate - 4.8828125e-05, magnitude of gradient - 3.2060331730675458\n",
      "Step - 10328, Loss - 0.5341841243531678, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.44949952086948275\n",
      "Step - 10329, Loss - 0.7350111919728007, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5298285753956313\n",
      "Step - 10330, Loss - 0.7490415706438487, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8296179621604736\n",
      "Step - 10331, Loss - 0.5662150972572373, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6137872640496147\n",
      "Step - 10332, Loss - 0.7028029076431521, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9399987648923522\n",
      "Step - 10333, Loss - 0.6760416792065332, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7933543723852423\n",
      "Step - 10334, Loss - 0.5600091675164106, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.1907398169175982\n",
      "Step - 10335, Loss - 0.7006492784853133, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3975653793653882\n",
      "Step - 10336, Loss - 0.8689268791899581, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6772415890205107\n",
      "Step - 10337, Loss - 0.7171315570434901, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.207482552144447\n",
      "Step - 10338, Loss - 0.5231590999624931, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5646257102156748\n",
      "Step - 10339, Loss - 0.595211231812099, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.357402504471538\n",
      "Step - 10340, Loss - 0.7052602473029352, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2984780278500652\n",
      "Step - 10341, Loss - 0.8235565560972055, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3897485414909543\n",
      "Step - 10342, Loss - 0.7768136630781787, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3015220009706916\n",
      "Step - 10343, Loss - 0.6959460055045344, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6503705960082573\n",
      "Step - 10344, Loss - 0.6700199692494941, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5988377879752853\n",
      "Step - 10345, Loss - 0.7080247594624147, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0561497702521483\n",
      "Step - 10346, Loss - 0.6172357484508615, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8840160908076967\n",
      "Step - 10347, Loss - 0.7973186029335242, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.908008073229242\n",
      "Step - 10348, Loss - 0.8480343861622228, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1913096572722282\n",
      "Step - 10349, Loss - 0.6105971478716808, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1006347291421754\n",
      "Step - 10350, Loss - 0.6830961076465016, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6945989886576277\n",
      "Step - 10351, Loss - 0.6819120852422145, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8703432571568288\n",
      "Step - 10352, Loss - 0.7302795583693888, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8215984759327537\n",
      "Step - 10353, Loss - 0.7524942553460999, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7575069926224622\n",
      "Step - 10354, Loss - 0.7006616474198051, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5410126136904905\n",
      "Step - 10355, Loss - 0.6876734967630455, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.4365478507090568\n",
      "Step - 10356, Loss - 0.5464536017563991, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5083224656628631\n",
      "Step - 10357, Loss - 0.8316742052583513, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.568804471472507\n",
      "Step - 10358, Loss - 0.7488017250280278, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9170849470311292\n",
      "Step - 10359, Loss - 0.7234522602096044, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5824940831129473\n",
      "Step - 10360, Loss - 0.5819715375814818, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5625905313142523\n",
      "Step - 10361, Loss - 0.7589255805683873, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.3861913907202906\n",
      "Step - 10362, Loss - 0.5739206665145222, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1792175062309953\n",
      "Step - 10363, Loss - 0.5650438563645791, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.35979092336004\n",
      "Step - 10364, Loss - 0.7798128154556494, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1235996275469389\n",
      "Step - 10365, Loss - 0.6050295607183088, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4180690364578672\n",
      "Step - 10366, Loss - 0.6436867615917843, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9688244771042288\n",
      "Step - 10367, Loss - 0.9440946645053696, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.9408473842286735\n",
      "Step - 10368, Loss - 0.7369865054649055, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5350633034618637\n",
      "Step - 10369, Loss - 0.7820835626420068, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3228101515042234\n",
      "Step - 10370, Loss - 0.8038992951226314, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3238631195661905\n",
      "Step - 10371, Loss - 0.5905805532695356, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.829608655006408\n",
      "Step - 10372, Loss - 0.7736471965289591, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0822375617111644\n",
      "Step - 10373, Loss - 0.7007610594735351, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0029867242950425\n",
      "Step - 10374, Loss - 0.547378351552916, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5468635018304255\n",
      "Step - 10375, Loss - 0.5288505680548437, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5336255202222607\n",
      "Step - 10376, Loss - 0.6153507675690884, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0980685581284593\n",
      "Step - 10377, Loss - 0.7378973248584103, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6090227071370853\n",
      "Step - 10378, Loss - 0.5326076542856765, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0651667945779542\n",
      "Step - 10379, Loss - 0.47919668983143776, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0313343784881737\n",
      "Step - 10380, Loss - 0.7417676771018434, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6507194775220936\n",
      "Step - 10381, Loss - 0.8094755159536604, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9016581340453511\n",
      "Step - 10382, Loss - 0.47270476626322666, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2622069929496098\n",
      "Step - 10383, Loss - 0.6822413985291751, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7578635074357734\n",
      "Step - 10384, Loss - 0.8483113904102514, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2142464500271348\n",
      "Step - 10385, Loss - 0.6158735175214618, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.0463096945110246\n",
      "Step - 10386, Loss - 0.7637022367860733, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8832959920302226\n",
      "Step - 10387, Loss - 0.7620533104224406, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2042751715160143\n",
      "Step - 10388, Loss - 0.6107552302370807, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8644430319158658\n",
      "Step - 10389, Loss - 0.6094583160010263, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3098519102152875\n",
      "Step - 10390, Loss - 0.6612445214381651, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.895175971842054\n",
      "Step - 10391, Loss - 0.6482867629439129, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6586838939570339\n",
      "Step - 10392, Loss - 0.629466478115413, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.35486772968136854\n",
      "Step - 10393, Loss - 0.7217072845432685, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8872052785298632\n",
      "Step - 10394, Loss - 0.6106355436925431, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4079051066174058\n",
      "Step - 10395, Loss - 0.7675430749714077, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6160933922078133\n",
      "Step - 10396, Loss - 0.630777930600783, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.012055476185767\n",
      "Step - 10397, Loss - 1.0069019880843515, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6454510865378427\n",
      "Step - 10398, Loss - 0.6502679881143743, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5722561648463862\n",
      "Step - 10399, Loss - 0.8556082326838366, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.47775243941226686\n",
      "Step - 10400, Loss - 0.7510468995106232, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9832560300039405\n",
      "Step - 10401, Loss - 0.7698099088271166, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5742899567912554\n",
      "Step - 10402, Loss - 0.628270034213908, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8596156892149189\n",
      "Step - 10403, Loss - 0.7113208549668508, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.495555002886896\n",
      "Step - 10404, Loss - 0.9215315800554049, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.960629311634937\n",
      "Step - 10405, Loss - 0.7026641469292719, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6038069316926752\n",
      "Step - 10406, Loss - 0.5333071608557117, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2413243634859357\n",
      "Step - 10407, Loss - 0.7257243265456147, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7824792035320567\n",
      "Step - 10408, Loss - 0.8627044029417441, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1003426070055153\n",
      "Step - 10409, Loss - 0.6105828632265675, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6356362186987161\n",
      "Step - 10410, Loss - 0.6717139829493013, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5954664630148295\n",
      "Step - 10411, Loss - 0.8019294387024796, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.5171319621789316\n",
      "Step - 10412, Loss - 0.5308377043827306, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.3421874040142446\n",
      "Step - 10413, Loss - 0.5846789439428836, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1326135475423769\n",
      "Step - 10414, Loss - 0.8649366042755133, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6997242705724582\n",
      "Step - 10415, Loss - 0.661841882534893, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.82534699825153\n",
      "Step - 10416, Loss - 0.7767566186199274, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1672482148076404\n",
      "Step - 10417, Loss - 0.6828258960504845, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.054448749108033\n",
      "Step - 10418, Loss - 0.8294125574703017, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2406086212354068\n",
      "Step - 10419, Loss - 0.8610726335071067, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5438506149150184\n",
      "Step - 10420, Loss - 0.7024457316774619, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7089404059160103\n",
      "Step - 10421, Loss - 0.4351318895937259, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.769638268398401\n",
      "Step - 10422, Loss - 0.4125372460122874, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9105132336399401\n",
      "Step - 10423, Loss - 0.661082428528835, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9624588319740903\n",
      "Step - 10424, Loss - 0.7916277294404244, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.472844195921531\n",
      "Step - 10425, Loss - 0.5777404776064882, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8617030487831748\n",
      "Step - 10426, Loss - 0.6439839108196199, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6759766286004886\n",
      "Step - 10427, Loss - 0.5321771556956626, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5832876684566207\n",
      "Step - 10428, Loss - 0.5786591914313192, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.582806545540613\n",
      "Step - 10429, Loss - 0.8599631213373071, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.537366985403828\n",
      "Step - 10430, Loss - 0.6062785501287993, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6350232122405067\n",
      "Step - 10431, Loss - 0.6589746749607258, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2386288824865332\n",
      "Step - 10432, Loss - 0.5104841954592086, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5126253716377995\n",
      "Step - 10433, Loss - 0.7385146028899963, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0786132201405758\n",
      "Step - 10434, Loss - 0.8508929857706515, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.571841079839196\n",
      "Step - 10435, Loss - 0.752935976006326, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8833610523881434\n",
      "Step - 10436, Loss - 0.6979877339602263, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2625364754516994\n",
      "Step - 10437, Loss - 0.5014693893131054, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.811329321625873\n",
      "Step - 10438, Loss - 0.8721128388398098, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.9541712169857213\n",
      "Step - 10439, Loss - 0.8993708280420611, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.634548571847585\n",
      "Step - 10440, Loss - 0.641317054418767, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.010409880598233\n",
      "Step - 10441, Loss - 0.5746473253101532, Learning Rate - 4.8828125e-05, magnitude of gradient - 3.757043619343394\n",
      "Step - 10442, Loss - 0.8582588041657899, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3145278485227216\n",
      "Step - 10443, Loss - 0.742389022732755, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7367433137825347\n",
      "Step - 10444, Loss - 0.8974126428307385, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6488185574930393\n",
      "Step - 10445, Loss - 0.6784611960613476, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.213994075728449\n",
      "Step - 10446, Loss - 0.7318781656594826, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.37150922364554567\n",
      "Step - 10447, Loss - 0.6980862712840039, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8706794205748694\n",
      "Step - 10448, Loss - 0.5983721089934267, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.162292435162504\n",
      "Step - 10449, Loss - 0.6566053792440032, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2248504224164822\n",
      "Step - 10450, Loss - 0.7913853006730963, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5143917346861069\n",
      "Step - 10451, Loss - 0.6844595050138013, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1398520559448866\n",
      "Step - 10452, Loss - 0.7694684677239397, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6822477769992832\n",
      "Step - 10453, Loss - 0.5796915310231325, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8163434990757567\n",
      "Step - 10454, Loss - 0.5880498140272811, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7215072563392172\n",
      "Step - 10455, Loss - 0.6833437959228552, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6131018250413438\n",
      "Step - 10456, Loss - 0.560673332740824, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.1100828208361686\n",
      "Step - 10457, Loss - 0.7716200629496154, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.4305851146328144\n",
      "Step - 10458, Loss - 0.8389502441725719, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9784111780109537\n",
      "Step - 10459, Loss - 0.680518959279312, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7975615297046335\n",
      "Step - 10460, Loss - 0.553360628258879, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8384918533260166\n",
      "Step - 10461, Loss - 0.6804218175785479, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4600464887787397\n",
      "Step - 10462, Loss - 0.6273871502718319, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.05300778778314\n",
      "Step - 10463, Loss - 0.8585319665127618, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.004716139120739\n",
      "Step - 10464, Loss - 0.7037217716708365, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7650354821605916\n",
      "Step - 10465, Loss - 0.7888038989272893, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0774006240638798\n",
      "Step - 10466, Loss - 0.7288047792250902, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9311091333835142\n",
      "Step - 10467, Loss - 0.8833518966793439, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2097660329697195\n",
      "Step - 10468, Loss - 0.6866362483462201, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.0210277844062965\n",
      "Step - 10469, Loss - 0.4941926646112508, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6371564324730614\n",
      "Step - 10470, Loss - 0.7294638876462971, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8936778250995637\n",
      "Step - 10471, Loss - 0.6425115735518371, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7860433595560808\n",
      "Step - 10472, Loss - 0.5670431612343836, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7029684275494891\n",
      "Step - 10473, Loss - 0.6110421638939947, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.99485202532183\n",
      "Step - 10474, Loss - 0.6416033633847061, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8856815051171254\n",
      "Step - 10475, Loss - 0.570731778654437, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1833716595890837\n",
      "Step - 10476, Loss - 0.7548792498336947, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.098462651133934\n",
      "Step - 10477, Loss - 0.7836141426104537, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0089409767160658\n",
      "Step - 10478, Loss - 0.6227895460176596, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.579508122935739\n",
      "Step - 10479, Loss - 0.6669016854261157, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.0875416685171526\n",
      "Step - 10480, Loss - 0.71346764602954, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.078431612083914\n",
      "Step - 10481, Loss - 0.7041028922632181, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5889313424425051\n",
      "Step - 10482, Loss - 0.7266457218584379, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.098418633738114\n",
      "Step - 10483, Loss - 0.7417109967860284, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6801814045791817\n",
      "Step - 10484, Loss - 0.8986827087133741, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6698787376756485\n",
      "Step - 10485, Loss - 0.7234224263564099, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6629461144410567\n",
      "Step - 10486, Loss - 0.6458452174466559, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.653529866895045\n",
      "Step - 10487, Loss - 0.5849188978902324, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.219540679558791\n",
      "Step - 10488, Loss - 0.7804298773421382, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0041256731924737\n",
      "Step - 10489, Loss - 0.7133853877943966, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6501420996241631\n",
      "Step - 10490, Loss - 0.670837026739473, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0638591052718065\n",
      "Step - 10491, Loss - 0.6395042296441802, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6335569089230358\n",
      "Step - 10492, Loss - 0.6407304885730577, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.628571520848012\n",
      "Step - 10493, Loss - 0.4290185316033636, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9577725976043064\n",
      "Step - 10494, Loss - 0.4943261802792697, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6620968650925705\n",
      "Step - 10495, Loss - 0.6957466329817058, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9319342495985238\n",
      "Step - 10496, Loss - 0.8860973760170281, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6395969597117184\n",
      "Step - 10497, Loss - 0.5756546460910468, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.179417479352746\n",
      "Step - 10498, Loss - 0.8360219495240663, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3880149622447473\n",
      "Step - 10499, Loss - 0.9011715873508557, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2298864223296286\n",
      "Step - 10500, Loss - 0.6678429969851216, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.63155149836751\n",
      "Step - 10501, Loss - 0.7013444497423769, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1265925838299362\n",
      "Step - 10502, Loss - 0.6529173892361958, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.945673368952589\n",
      "Step - 10503, Loss - 0.7004030123673671, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8225827632294398\n",
      "Step - 10504, Loss - 0.7023931823382479, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7279837410909755\n",
      "Step - 10505, Loss - 0.6422581687678047, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4646518813497444\n",
      "Step - 10506, Loss - 0.8267920800338147, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.110393480752386\n",
      "Step - 10507, Loss - 0.8062243488405998, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.618773850179471\n",
      "Step - 10508, Loss - 0.8880984041415482, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8481638822232154\n",
      "Step - 10509, Loss - 0.6979810344251018, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1510221202975477\n",
      "Step - 10510, Loss - 0.7777052509940914, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.669599399307766\n",
      "Step - 10511, Loss - 0.5017226235781052, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4206055188615254\n",
      "Step - 10512, Loss - 0.5918637020716176, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6122361315331186\n",
      "Step - 10513, Loss - 0.5659011441040999, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7587569497994742\n",
      "Step - 10514, Loss - 0.702086302343001, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1769970754307268\n",
      "Step - 10515, Loss - 0.6089954884909742, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7867916566752198\n",
      "Step - 10516, Loss - 0.859569305516829, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.0576201539858783\n",
      "Step - 10517, Loss - 0.5656004030950014, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2792925350340292\n",
      "Step - 10518, Loss - 0.5299235577708362, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.48696431495258974\n",
      "Step - 10519, Loss - 0.4756125554834809, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.0738432931983373\n",
      "Step - 10520, Loss - 0.7090043695522507, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2308224842091584\n",
      "Step - 10521, Loss - 0.6818024069107036, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3515465825691277\n",
      "Step - 10522, Loss - 0.581867373896217, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1005969529549946\n",
      "Step - 10523, Loss - 0.8585689153292527, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.661917787096909\n",
      "Step - 10524, Loss - 0.6710938927491057, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1090518031464929\n",
      "Step - 10525, Loss - 0.7898237806291644, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9983832070844822\n",
      "Step - 10526, Loss - 0.8226689860887235, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5644654324121259\n",
      "Step - 10527, Loss - 0.7480179705127086, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2500672349438524\n",
      "Step - 10528, Loss - 0.4704690035976177, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.3994108344040352\n",
      "Step - 10529, Loss - 0.8079418219438054, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3838197224790083\n",
      "Step - 10530, Loss - 0.6139409988481035, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4320605248362863\n",
      "Step - 10531, Loss - 0.818631701552272, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.9126725821326591\n",
      "Step - 10532, Loss - 0.7134055790477822, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2781609012433013\n",
      "Step - 10533, Loss - 0.5210026372804887, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.614701565861669\n",
      "Step - 10534, Loss - 0.5927785664762614, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.890415808081577\n",
      "Step - 10535, Loss - 0.8433884953425964, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3468422651295675\n",
      "Step - 10536, Loss - 0.6652234034497485, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2379958718442208\n",
      "Step - 10537, Loss - 0.7061228362833405, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7792161359243417\n",
      "Step - 10538, Loss - 0.6371001035908872, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.919041550264361\n",
      "Step - 10539, Loss - 0.692772252915472, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6334093852994533\n",
      "Step - 10540, Loss - 0.7478997255129516, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8254348697154936\n",
      "Step - 10541, Loss - 0.5657623162375569, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7864490218691482\n",
      "Step - 10542, Loss - 0.761727482811152, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.00590132832523\n",
      "Step - 10543, Loss - 0.7017993185392155, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1314408479316769\n",
      "Step - 10544, Loss - 0.6582860022837137, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8309815733743272\n",
      "Step - 10545, Loss - 0.6027291124924893, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7662090274611911\n",
      "Step - 10546, Loss - 0.9307160834484698, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.6954496017498495\n",
      "Step - 10547, Loss - 0.674921457245389, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4260151065579965\n",
      "Step - 10548, Loss - 0.5316016766984615, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5312859459981183\n",
      "Step - 10549, Loss - 0.7658202798509441, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7033480607383702\n",
      "Step - 10550, Loss - 0.6003161463031971, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.47541657666982523\n",
      "Step - 10551, Loss - 0.6120693138356799, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.239458775016944\n",
      "Step - 10552, Loss - 0.5971416149990376, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.503355333498254\n",
      "Step - 10553, Loss - 0.6522663665116641, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.3663289394854092\n",
      "Step - 10554, Loss - 0.8370814539022068, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4545220389628752\n",
      "Step - 10555, Loss - 0.6426982934167444, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5686632257456083\n",
      "Step - 10556, Loss - 0.8303567993856591, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6161675596856884\n",
      "Step - 10557, Loss - 0.5625960809152655, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5567096319515201\n",
      "Step - 10558, Loss - 0.5684602422469325, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.0066081219053418\n",
      "Step - 10559, Loss - 0.6263024877413434, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.00347278630861\n",
      "Step - 10560, Loss - 0.7817070491704338, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.1492665085091254\n",
      "Step - 10561, Loss - 0.45159188191587807, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9092355594363121\n",
      "Step - 10562, Loss - 0.9255665325691997, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.9286585317631542\n",
      "Step - 10563, Loss - 0.7293538643860398, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7558641502631713\n",
      "Step - 10564, Loss - 0.6792838767039265, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2546607884921168\n",
      "Step - 10565, Loss - 0.8470105946391653, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6027596224559622\n",
      "Step - 10566, Loss - 0.7665559261161222, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2232622528342503\n",
      "Step - 10567, Loss - 0.589396349171588, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1611518969910923\n",
      "Step - 10568, Loss - 0.8330451392066507, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.25292214482021763\n",
      "Step - 10569, Loss - 0.6495398097154634, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8698130789203852\n",
      "Step - 10570, Loss - 0.5576906911103758, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.4612846005964801\n",
      "Step - 10571, Loss - 0.7565225710844616, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5183554647307198\n",
      "Step - 10572, Loss - 0.8525802440827095, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.0712982487213094\n",
      "Step - 10573, Loss - 0.7834316110350041, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7601994277341129\n",
      "Step - 10574, Loss - 0.7472154767338867, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5952337224452762\n",
      "Step - 10575, Loss - 0.8460955068455422, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1740260001321203\n",
      "Step - 10576, Loss - 0.7329114990689616, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8916542338459466\n",
      "Step - 10577, Loss - 0.593162703669027, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9434891616137562\n",
      "Step - 10578, Loss - 0.7330363778653214, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.530354695996263\n",
      "Step - 10579, Loss - 0.6453474929031618, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7901712388478382\n",
      "Step - 10580, Loss - 0.6770247549701811, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4215726220461773\n",
      "Step - 10581, Loss - 0.7607526392419415, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.0467595571590387\n",
      "Step - 10582, Loss - 0.7236349252420904, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2318174050233315\n",
      "Step - 10583, Loss - 0.821294684698259, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.9196518782255623\n",
      "Step - 10584, Loss - 0.5904052621447795, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.3237397902385885\n",
      "Step - 10585, Loss - 0.8290963081064918, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1888123844056815\n",
      "Step - 10586, Loss - 0.6050123892842492, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4350641925156433\n",
      "Step - 10587, Loss - 0.6210618190774945, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6786624864568938\n",
      "Step - 10588, Loss - 0.6918747488109368, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8174296888567756\n",
      "Step - 10589, Loss - 0.7828894019375875, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5848807062580909\n",
      "Step - 10590, Loss - 0.6854095903608907, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8591666543894102\n",
      "Step - 10591, Loss - 0.7146507213101484, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8824932766490882\n",
      "Step - 10592, Loss - 0.5737588710797537, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.063609007937752\n",
      "Step - 10593, Loss - 0.8639155008428433, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.971841015115155\n",
      "Step - 10594, Loss - 0.7505571681293128, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.681873918072458\n",
      "Step - 10595, Loss - 0.8894288578137003, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6745999160721707\n",
      "Step - 10596, Loss - 0.6349233274623488, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9526271308432158\n",
      "Step - 10597, Loss - 0.6231456291880361, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.470108554514393\n",
      "Step - 10598, Loss - 0.794685080894774, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7666766914898953\n",
      "Step - 10599, Loss - 0.789260843188, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6062321433115918\n",
      "Step - 10600, Loss - 0.7467389353160132, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1599155727761514\n",
      "Step - 10601, Loss - 0.6358714438978587, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8449808370978922\n",
      "Step - 10602, Loss - 0.712807132053614, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8457414608102627\n",
      "Step - 10603, Loss - 0.5778973233491509, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5223387192127487\n",
      "Step - 10604, Loss - 0.6974089689273446, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8422993420639017\n",
      "Step - 10605, Loss - 0.809190413964793, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.986630877597792\n",
      "Step - 10606, Loss - 0.6521299185359384, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.699856744870911\n",
      "Step - 10607, Loss - 0.6629800539377102, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2721640011777302\n",
      "Step - 10608, Loss - 0.7857761034066477, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6790837542957164\n",
      "Step - 10609, Loss - 0.633053163009559, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8977262492429081\n",
      "Step - 10610, Loss - 0.7428603707485306, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2575772646679972\n",
      "Step - 10611, Loss - 0.6863079289793228, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5256315229684427\n",
      "Step - 10612, Loss - 0.7011077629106954, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8024962104457681\n",
      "Step - 10613, Loss - 0.6884611558612622, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.35887767809994625\n",
      "Step - 10614, Loss - 0.6876303814144001, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7857434784551031\n",
      "Step - 10615, Loss - 0.6386435788926501, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7554717516812933\n",
      "Step - 10616, Loss - 0.5880497760101187, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5576050411422597\n",
      "Step - 10617, Loss - 0.7133347537517849, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8446812951847398\n",
      "Step - 10618, Loss - 0.6079769917101415, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3311501572636308\n",
      "Step - 10619, Loss - 0.726486355607735, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3362969852192093\n",
      "Step - 10620, Loss - 0.5476852450507689, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.109177868263648\n",
      "Step - 10621, Loss - 0.6499062566796403, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6142905318971549\n",
      "Step - 10622, Loss - 0.48811609210006035, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1143740132560507\n",
      "Step - 10623, Loss - 0.5873920910695044, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.403420613868446\n",
      "Step - 10624, Loss - 0.7213179463643086, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.1554171096602524\n",
      "Step - 10625, Loss - 0.6820412051260919, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.298912226902159\n",
      "Step - 10626, Loss - 0.9178319377645412, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6136672145776116\n",
      "Step - 10627, Loss - 0.6358705581589277, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9926694243465151\n",
      "Step - 10628, Loss - 0.6127632604828992, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.70446817108134\n",
      "Step - 10629, Loss - 0.552744919749847, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8562337512764466\n",
      "Step - 10630, Loss - 0.6892116698193806, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8148993076161075\n",
      "Step - 10631, Loss - 0.6847488929051765, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6004526197916247\n",
      "Step - 10632, Loss - 0.8326763720517875, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7901718701172004\n",
      "Step - 10633, Loss - 0.47563888868413084, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3838445894019409\n",
      "Step - 10634, Loss - 0.5758244284740217, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6462466625199041\n",
      "Step - 10635, Loss - 0.8350568151142589, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9991298416142687\n",
      "Step - 10636, Loss - 0.61557486014004, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.0981804603982748\n",
      "Step - 10637, Loss - 0.8026931516166802, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.202465445356805\n",
      "Step - 10638, Loss - 0.7042733523731041, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4998229565793921\n",
      "Step - 10639, Loss - 0.7055388865598462, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.142885088917005\n",
      "Step - 10640, Loss - 0.607226001926919, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.4605088699013652\n",
      "Step - 10641, Loss - 0.7396473655746849, Learning Rate - 4.8828125e-05, magnitude of gradient - 3.010571246706696\n",
      "Step - 10642, Loss - 0.568389921022219, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8709948056476633\n",
      "Step - 10643, Loss - 0.6249372201214315, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1012377415888073\n",
      "Step - 10644, Loss - 0.6299493678173633, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7313831886533823\n",
      "Step - 10645, Loss - 0.8189256277966455, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7468564691434739\n",
      "Step - 10646, Loss - 0.665225458968575, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8530941259076279\n",
      "Step - 10647, Loss - 0.706306180572126, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.366650705201662\n",
      "Step - 10648, Loss - 0.7320816768889908, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.3786364479720077\n",
      "Step - 10649, Loss - 0.6614698949481854, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.9013318125720433\n",
      "Step - 10650, Loss - 0.7052476212130785, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6731427183530403\n",
      "Step - 10651, Loss - 0.556992563280029, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2359709743614777\n",
      "Step - 10652, Loss - 0.8468646148138786, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5178761971494025\n",
      "Step - 10653, Loss - 0.7918625265546381, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0324737814415468\n",
      "Step - 10654, Loss - 0.47604297201808926, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.3152063817306358\n",
      "Step - 10655, Loss - 0.8566871315755687, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6868048217020891\n",
      "Step - 10656, Loss - 0.6797634191276982, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.190521547985184\n",
      "Step - 10657, Loss - 0.7780430879781253, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3558344838762741\n",
      "Step - 10658, Loss - 0.6728911279893274, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.355504198622364\n",
      "Step - 10659, Loss - 0.6721965879113432, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0730915999908577\n",
      "Step - 10660, Loss - 0.8936344344132902, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1889305359999196\n",
      "Step - 10661, Loss - 0.7096469470018252, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9088479705430225\n",
      "Step - 10662, Loss - 0.7243463718856725, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.620131825331192\n",
      "Step - 10663, Loss - 0.5681351098663376, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1064451159469286\n",
      "Step - 10664, Loss - 0.8291932962361674, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2364190127926562\n",
      "Step - 10665, Loss - 0.5487084486382828, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7059108340164297\n",
      "Step - 10666, Loss - 0.5923107698930753, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0929267957538946\n",
      "Step - 10667, Loss - 0.7087680060071079, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.564553303379887\n",
      "Step - 10668, Loss - 0.7138179180976121, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2309924899238716\n",
      "Step - 10669, Loss - 0.7055803540819728, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.9674159890364833\n",
      "Step - 10670, Loss - 0.7242327708978393, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5548568500572648\n",
      "Step - 10671, Loss - 0.6953553430417757, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7828155546539008\n",
      "Step - 10672, Loss - 0.6666809844059465, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7900918772203795\n",
      "Step - 10673, Loss - 0.6770876383513615, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0264018844779097\n",
      "Step - 10674, Loss - 0.43121260382606064, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.4260447607520241\n",
      "Step - 10675, Loss - 0.6746249122665503, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5902890789289986\n",
      "Step - 10676, Loss - 0.814754348467257, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9977703900454009\n",
      "Step - 10677, Loss - 0.6405136558319757, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8959343646840323\n",
      "Step - 10678, Loss - 0.8483686272323441, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8553490178308678\n",
      "Step - 10679, Loss - 0.7786105434301404, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5730872301076644\n",
      "Step - 10680, Loss - 0.831926316475122, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9736051473239413\n",
      "Step - 10681, Loss - 0.7721199235962294, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.3279684977847896\n",
      "Step - 10682, Loss - 0.6312408229097822, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5901366907908311\n",
      "Step - 10683, Loss - 0.706794022978321, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.268469042707417\n",
      "Step - 10684, Loss - 0.6408723571857623, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.728718431986043\n",
      "Step - 10685, Loss - 0.7306368356935082, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4470382026763173\n",
      "Step - 10686, Loss - 0.6677156994247616, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3913237036004136\n",
      "Step - 10687, Loss - 0.6682378235690131, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5270013995901638\n",
      "Step - 10688, Loss - 0.6530399546780787, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9419788888117724\n",
      "Step - 10689, Loss - 0.7060987168614714, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6240705019083281\n",
      "Step - 10690, Loss - 0.6573118176374055, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.282289469450127\n",
      "Step - 10691, Loss - 0.5316272237331015, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9165343273723349\n",
      "Step - 10692, Loss - 0.8590939041870105, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6652178378833682\n",
      "Step - 10693, Loss - 0.7181306761105568, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8683345669912007\n",
      "Step - 10694, Loss - 0.6588170044152444, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2122442524916055\n",
      "Step - 10695, Loss - 0.9122925990763318, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.3901910490565235\n",
      "Step - 10696, Loss - 0.5841207284763796, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8176291868280177\n",
      "Step - 10697, Loss - 0.674986012654425, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7630717861252733\n",
      "Step - 10698, Loss - 0.6529189965577342, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1712035623651655\n",
      "Step - 10699, Loss - 0.656607022669356, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.0241248140959747\n",
      "Step - 10700, Loss - 0.5792905323204721, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.40977853042792656\n",
      "Step - 10701, Loss - 0.7148775273752465, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8627413619953112\n",
      "Step - 10702, Loss - 0.48212949733587906, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5056723594811576\n",
      "Step - 10703, Loss - 0.8847918957571755, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2255877927758954\n",
      "Step - 10704, Loss - 0.7356934118233656, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1770759825774495\n",
      "Step - 10705, Loss - 0.6798132253147741, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4305839853436533\n",
      "Step - 10706, Loss - 0.8175320561746, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9280480598599753\n",
      "Step - 10707, Loss - 0.5027398419183957, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.701372943214649\n",
      "Step - 10708, Loss - 0.6955156896840778, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0438505379471936\n",
      "Step - 10709, Loss - 0.5509793827170029, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0266351283352877\n",
      "Step - 10710, Loss - 0.628496904910532, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6850670158272072\n",
      "Step - 10711, Loss - 0.49154304876693394, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2592710518910397\n",
      "Step - 10712, Loss - 0.6153508467949517, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9756909142133016\n",
      "Step - 10713, Loss - 0.6471452671563557, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5084272986551057\n",
      "Step - 10714, Loss - 0.7890633606736845, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7741444258294318\n",
      "Step - 10715, Loss - 0.8175726569465488, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2511737875399238\n",
      "Step - 10716, Loss - 0.7361991583014178, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9087691094736986\n",
      "Step - 10717, Loss - 0.6181327170062743, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0980913552910467\n",
      "Step - 10718, Loss - 0.734238558693067, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2214186183571143\n",
      "Step - 10719, Loss - 0.6585830093895699, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9903525955386471\n",
      "Step - 10720, Loss - 0.5385038263297325, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3424252796231435\n",
      "Step - 10721, Loss - 0.8202934212230877, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6477057684190928\n",
      "Step - 10722, Loss - 0.7424240274015551, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.209126736564893\n",
      "Step - 10723, Loss - 0.9024870816758901, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.9756371610196692\n",
      "Step - 10724, Loss - 0.7676957091723846, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0823026335244232\n",
      "Step - 10725, Loss - 0.7801181268641704, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5876650120633882\n",
      "Step - 10726, Loss - 0.7151381739808619, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.751534620400093\n",
      "Step - 10727, Loss - 0.7907654119642359, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.36450927103290126\n",
      "Step - 10728, Loss - 0.5710608980235633, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.010511379198597\n",
      "Step - 10729, Loss - 0.8252169063753427, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.41901045237582574\n",
      "Step - 10730, Loss - 0.6732899173962866, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.084997175229676\n",
      "Step - 10731, Loss - 0.7745927990093816, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.50066033775819\n",
      "Step - 10732, Loss - 0.8336372708917636, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.785825607762456\n",
      "Step - 10733, Loss - 0.6675947615229033, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4663380258952614\n",
      "Step - 10734, Loss - 0.7543115858661573, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5102589608202939\n",
      "Step - 10735, Loss - 0.6881456495305326, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5561510613490225\n",
      "Step - 10736, Loss - 0.6910067779751499, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.9174579362230768\n",
      "Step - 10737, Loss - 0.5574197145805789, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2946514036064325\n",
      "Step - 10738, Loss - 0.5719326857376777, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8151300350902414\n",
      "Step - 10739, Loss - 0.4778753803697837, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1257837793324204\n",
      "Step - 10740, Loss - 0.6477510458140694, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5931890020375092\n",
      "Step - 10741, Loss - 0.8289952053164085, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.2323805519337971\n",
      "Step - 10742, Loss - 0.7382106994788198, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8305878565219091\n",
      "Step - 10743, Loss - 0.8024516357183235, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9665924085603872\n",
      "Step - 10744, Loss - 0.7491901756013742, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7143792318865098\n",
      "Step - 10745, Loss - 0.5112847881507495, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3192370724364595\n",
      "Step - 10746, Loss - 0.6194805093071538, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.45487388533756873\n",
      "Step - 10747, Loss - 0.46189020950963794, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4528124221348722\n",
      "Step - 10748, Loss - 0.7349327577629376, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8547739373026716\n",
      "Step - 10749, Loss - 0.7827524874688099, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.111433841148096\n",
      "Step - 10750, Loss - 0.5488739180938884, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9648119516862891\n",
      "Step - 10751, Loss - 0.6976276014831223, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.9024069836466277\n",
      "Step - 10752, Loss - 0.584094538889713, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1040116488027123\n",
      "Step - 10753, Loss - 0.6199943138242734, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7925935282167513\n",
      "Step - 10754, Loss - 0.722009879066442, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5504065023867365\n",
      "Step - 10755, Loss - 0.4056958613576306, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2568535857713858\n",
      "Step - 10756, Loss - 0.734987518210147, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9995055765614183\n",
      "Step - 10757, Loss - 0.6586367311794469, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.287552963151536\n",
      "Step - 10758, Loss - 0.6513350783167371, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.000046810032774\n",
      "Step - 10759, Loss - 0.6963885876084013, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.511160883625988\n",
      "Step - 10760, Loss - 0.6401477784296411, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.149256304618332\n",
      "Step - 10761, Loss - 0.79731234500762, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1743273011799338\n",
      "Step - 10762, Loss - 0.577855702862662, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.7798879727416597\n",
      "Step - 10763, Loss - 0.49065685930475256, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2262165109187178\n",
      "Step - 10764, Loss - 0.7502322464235305, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5098470126670828\n",
      "Step - 10765, Loss - 0.7306574471085399, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2947632748120474\n",
      "Step - 10766, Loss - 0.753029177924735, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.79428206649809\n",
      "Step - 10767, Loss - 0.5351917118094593, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5347381778622523\n",
      "Step - 10768, Loss - 0.843803753891893, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1580195061291598\n",
      "Step - 10769, Loss - 0.790842622622873, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.969606475299687\n",
      "Step - 10770, Loss - 0.6727771903998436, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6095627034308321\n",
      "Step - 10771, Loss - 0.5035470301984766, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.766362094208076\n",
      "Step - 10772, Loss - 0.7505360866533775, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9418846489137127\n",
      "Step - 10773, Loss - 0.7469215165440227, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7199081154970788\n",
      "Step - 10774, Loss - 0.6945362519975712, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.4104767071918617\n",
      "Step - 10775, Loss - 0.8923025808250322, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4888874302150668\n",
      "Step - 10776, Loss - 0.6189686331907102, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.172178418989988\n",
      "Step - 10777, Loss - 0.8013764660617977, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2972474576343687\n",
      "Step - 10778, Loss - 0.8788184962485736, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5390693156079411\n",
      "Step - 10779, Loss - 0.7339100034908077, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1911320095068234\n",
      "Step - 10780, Loss - 0.6794211010684255, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6661962539958095\n",
      "Step - 10781, Loss - 0.6852607607868171, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.4520269975725895\n",
      "Step - 10782, Loss - 0.7436791839033118, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.19613882504424052\n",
      "Step - 10783, Loss - 0.5537460480551606, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6655716182772119\n",
      "Step - 10784, Loss - 0.651963402665357, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.052735369664933\n",
      "Step - 10785, Loss - 0.580727561415999, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.3753199740108375\n",
      "Step - 10786, Loss - 0.8055679139258987, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9460838033782939\n",
      "Step - 10787, Loss - 0.6492729125388067, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7389064437677082\n",
      "Step - 10788, Loss - 0.6450342728013773, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.034567113315734\n",
      "Step - 10789, Loss - 0.7811316213498949, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4030872371779926\n",
      "Step - 10790, Loss - 0.7388375844548687, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7497571928722174\n",
      "Step - 10791, Loss - 0.6068266846760678, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.507543040826797\n",
      "Step - 10792, Loss - 0.7697717412052844, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.407872546177212\n",
      "Step - 10793, Loss - 0.743281642931358, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2440232677856307\n",
      "Step - 10794, Loss - 0.6253443064198302, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5715101732649561\n",
      "Step - 10795, Loss - 0.7467744796423472, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.46581090182937523\n",
      "Step - 10796, Loss - 0.743818208270499, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.591069664467282\n",
      "Step - 10797, Loss - 0.7516901468605667, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1000990785501643\n",
      "Step - 10798, Loss - 0.9521766183862364, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3483553766974097\n",
      "Step - 10799, Loss - 0.7082601734240246, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8860524401618909\n",
      "Step - 10800, Loss - 0.7052745621897016, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.249649167532705\n",
      "Step - 10801, Loss - 0.7160636630626522, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.838248027440471\n",
      "Step - 10802, Loss - 0.4909199560447654, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.689318165210559\n",
      "Step - 10803, Loss - 0.656322247803618, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7316387475190708\n",
      "Step - 10804, Loss - 0.8457676718439466, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5215531333777796\n",
      "Step - 10805, Loss - 0.6704063308799484, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1324215689621453\n",
      "Step - 10806, Loss - 0.744783385407029, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1005303224013714\n",
      "Step - 10807, Loss - 0.4839274482402923, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.738010999048992\n",
      "Step - 10808, Loss - 0.7994879265811472, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5106900447815235\n",
      "Step - 10809, Loss - 0.6944870209173668, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.36215483312268\n",
      "Step - 10810, Loss - 0.6211036982129281, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.335014627172727\n",
      "Step - 10811, Loss - 0.5333815986165468, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0186332521023245\n",
      "Step - 10812, Loss - 0.6919967880449958, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7489444556841585\n",
      "Step - 10813, Loss - 0.6701390739851643, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5332811611792561\n",
      "Step - 10814, Loss - 0.8075474021071876, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7211952710926502\n",
      "Step - 10815, Loss - 0.744123212513932, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7837588419515622\n",
      "Step - 10816, Loss - 0.7134259063862767, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.150973754951524\n",
      "Step - 10817, Loss - 0.7457642064443537, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7610616818378356\n",
      "Step - 10818, Loss - 0.5960649493047137, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8064220227758014\n",
      "Step - 10819, Loss - 0.7446983361695894, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.9214987638835108\n",
      "Step - 10820, Loss - 0.5939774317454818, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.476737732607225\n",
      "Step - 10821, Loss - 0.8747323138600333, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.9376142712590432\n",
      "Step - 10822, Loss - 0.7107402393100397, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8046858746614314\n",
      "Step - 10823, Loss - 0.70669009980974, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.1235628868097773\n",
      "Step - 10824, Loss - 0.6251057077231165, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.489337082098825\n",
      "Step - 10825, Loss - 0.7081353581204832, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8980217788937334\n",
      "Step - 10826, Loss - 0.7520427308503412, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4145580342328914\n",
      "Step - 10827, Loss - 0.679059908807435, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5407176915063783\n",
      "Step - 10828, Loss - 0.6254910815244985, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8516868448215869\n",
      "Step - 10829, Loss - 0.5367541982680173, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8122203678693292\n",
      "Step - 10830, Loss - 0.8808599035476283, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.2355297480180596\n",
      "Step - 10831, Loss - 0.932785856658217, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.5056293226808104\n",
      "Step - 10832, Loss - 0.38876649229993776, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9657941749443464\n",
      "Step - 10833, Loss - 0.5814331298366657, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8184449056617972\n",
      "Step - 10834, Loss - 0.7630145392959726, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.48767931322524327\n",
      "Step - 10835, Loss - 0.6711989846426153, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1063166877474424\n",
      "Step - 10836, Loss - 0.5082821376352825, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.292467216639946\n",
      "Step - 10837, Loss - 0.7421170995475608, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8874920779094029\n",
      "Step - 10838, Loss - 0.5940469182403982, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6987368863097259\n",
      "Step - 10839, Loss - 0.6126180184589797, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.32322359701897\n",
      "Step - 10840, Loss - 0.6086007759688553, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.216738880395231\n",
      "Step - 10841, Loss - 0.5616907358849751, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9173062352614004\n",
      "Step - 10842, Loss - 0.5378133347122109, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6830035596132793\n",
      "Step - 10843, Loss - 0.624863789742746, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3822474141279693\n",
      "Step - 10844, Loss - 0.6135426598611785, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0244068678028684\n",
      "Step - 10845, Loss - 0.5618004729053222, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8353622832163542\n",
      "Step - 10846, Loss - 0.682901540147303, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8942206914798003\n",
      "Step - 10847, Loss - 0.7976213602575623, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7820454207617624\n",
      "Step - 10848, Loss - 0.8927253912179948, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4237043243517762\n",
      "Step - 10849, Loss - 0.8845110635237057, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3288837008177237\n",
      "Step - 10850, Loss - 0.7774582447750751, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7494151825341\n",
      "Step - 10851, Loss - 0.6598425076054952, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.82493922650847\n",
      "Step - 10852, Loss - 0.7315542193242299, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6588495979354215\n",
      "Step - 10853, Loss - 0.8638245843189848, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.5327002933938108\n",
      "Step - 10854, Loss - 0.8410969366828336, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1810235406844674\n",
      "Step - 10855, Loss - 0.7599707914989948, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0579391932627642\n",
      "Step - 10856, Loss - 0.6705110730513231, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2043133084379372\n",
      "Step - 10857, Loss - 0.7328208937548706, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7535055599383934\n",
      "Step - 10858, Loss - 0.8300578211052883, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7340700129195508\n",
      "Step - 10859, Loss - 0.5848253074669166, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.2869761604691683\n",
      "Step - 10860, Loss - 0.6532121822237591, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7801327894605087\n",
      "Step - 10861, Loss - 0.7366439473229932, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.704987722587483\n",
      "Step - 10862, Loss - 0.7286257447459822, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.3790085001484233\n",
      "Step - 10863, Loss - 0.6240890406316835, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3866920343951135\n",
      "Step - 10864, Loss - 0.7227977972542241, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7152337416978094\n",
      "Step - 10865, Loss - 0.6408538654745183, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7658529066755753\n",
      "Step - 10866, Loss - 0.5819606311858184, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0772801755931245\n",
      "Step - 10867, Loss - 0.6182294165630959, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.082968477419089\n",
      "Step - 10868, Loss - 0.6671827705935088, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5034886148005713\n",
      "Step - 10869, Loss - 0.6235040579450781, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.1668754019027214\n",
      "Step - 10870, Loss - 0.7240377732996532, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.914009361281547\n",
      "Step - 10871, Loss - 0.5658449354726094, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.058758058592683\n",
      "Step - 10872, Loss - 0.5662163855629823, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7323577354193196\n",
      "Step - 10873, Loss - 0.7860574298373026, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2299158276431075\n",
      "Step - 10874, Loss - 0.454943208319105, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2167308660655747\n",
      "Step - 10875, Loss - 0.5665064359168046, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.7255683392272235\n",
      "Step - 10876, Loss - 0.6310805282998548, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1114378868738706\n",
      "Step - 10877, Loss - 0.6646055747070232, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.806688355926632\n",
      "Step - 10878, Loss - 0.9398879589641972, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5101716494313258\n",
      "Step - 10879, Loss - 0.6599125825183075, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6562797251840832\n",
      "Step - 10880, Loss - 0.6886978845075602, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6739656267280456\n",
      "Step - 10881, Loss - 0.6686556544050957, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3749440580583518\n",
      "Step - 10882, Loss - 0.7491145294644878, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.16901713808259\n",
      "Step - 10883, Loss - 0.6103598741329573, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4450474484364766\n",
      "Step - 10884, Loss - 0.6583965989122726, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.855204753147214\n",
      "Step - 10885, Loss - 0.7695403840427235, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7673061809174408\n",
      "Step - 10886, Loss - 0.6929787355783569, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5680468489993096\n",
      "Step - 10887, Loss - 0.588021943697361, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.059007526296538\n",
      "Step - 10888, Loss - 0.6361629638286118, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3297139596643954\n",
      "Step - 10889, Loss - 0.7884452341760577, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2557302493447373\n",
      "Step - 10890, Loss - 0.5593561720293843, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4373237453887668\n",
      "Step - 10891, Loss - 0.6175927507245712, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6738584345745118\n",
      "Step - 10892, Loss - 0.6860951553495517, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6458308868621094\n",
      "Step - 10893, Loss - 0.8194708014549643, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2319535150232437\n",
      "Step - 10894, Loss - 0.7678693565307956, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5837080168445272\n",
      "Step - 10895, Loss - 0.632706169347423, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7917422091171555\n",
      "Step - 10896, Loss - 0.7742632287160026, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.2010571165802015\n",
      "Step - 10897, Loss - 0.7466648971705256, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.48384577694392444\n",
      "Step - 10898, Loss - 0.7653854962312892, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8501104239830346\n",
      "Step - 10899, Loss - 0.5873252003319012, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.260922102105924\n",
      "Step - 10900, Loss - 0.6475955586122445, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6989969153767412\n",
      "Step - 10901, Loss - 0.6010306952235962, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2085376415705584\n",
      "Step - 10902, Loss - 0.9152199125611509, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1428284048375137\n",
      "Step - 10903, Loss - 0.8821628390762137, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.0694908402209156\n",
      "Step - 10904, Loss - 0.7816221487877087, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4248740950775027\n",
      "Step - 10905, Loss - 0.7534662216963265, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8905794670883057\n",
      "Step - 10906, Loss - 0.7272296851027277, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6450595883967256\n",
      "Step - 10907, Loss - 0.8925492343056047, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.688006732100969\n",
      "Step - 10908, Loss - 0.7706416887946295, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.6571296229249646\n",
      "Step - 10909, Loss - 0.6613491835042463, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6646266577886608\n",
      "Step - 10910, Loss - 0.8219178909709663, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4351774292633381\n",
      "Step - 10911, Loss - 0.5944535693296898, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.1452648984068206\n",
      "Step - 10912, Loss - 0.7593339873814066, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1677408523087882\n",
      "Step - 10913, Loss - 0.5698835068549113, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6741851017638467\n",
      "Step - 10914, Loss - 0.8281015807916001, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8961173529045183\n",
      "Step - 10915, Loss - 0.7639486786662606, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.33373308089369763\n",
      "Step - 10916, Loss - 0.8308914103893668, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1627139237358857\n",
      "Step - 10917, Loss - 0.566123289098095, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0695489176309523\n",
      "Step - 10918, Loss - 0.6620735700821475, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7205259059297515\n",
      "Step - 10919, Loss - 0.5416817850541161, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9976968340333913\n",
      "Step - 10920, Loss - 0.7779972804303521, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.40430346366212144\n",
      "Step - 10921, Loss - 0.814064906314579, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.158351789596847\n",
      "Step - 10922, Loss - 0.6066026192998433, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1274905473742303\n",
      "Step - 10923, Loss - 0.7448995901711044, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1724290221756768\n",
      "Step - 10924, Loss - 0.6794214849407112, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1183186249244157\n",
      "Step - 10925, Loss - 0.5884459329137641, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.675278970090619\n",
      "Step - 10926, Loss - 0.7500566491636311, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.524473889658065\n",
      "Step - 10927, Loss - 0.8136073053380783, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.3148351230448876\n",
      "Step - 10928, Loss - 0.7113507147868634, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.850129377481872\n",
      "Step - 10929, Loss - 0.8910853393853522, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.13869658084372\n",
      "Step - 10930, Loss - 0.8122837098916419, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.312636899827487\n",
      "Step - 10931, Loss - 0.560530592549048, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9894140019427519\n",
      "Step - 10932, Loss - 0.6307329661207172, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2037594476118805\n",
      "Step - 10933, Loss - 0.5567654858378387, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2771881436314856\n",
      "Step - 10934, Loss - 0.9164297103111186, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.44972804488245\n",
      "Step - 10935, Loss - 0.6427629065411554, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7347725444786264\n",
      "Step - 10936, Loss - 0.6793879796328433, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0520761175558841\n",
      "Step - 10937, Loss - 0.6896704450994774, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8467220911621965\n",
      "Step - 10938, Loss - 0.6280240173981253, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.767619017745644\n",
      "Step - 10939, Loss - 0.7540229305998091, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9337232312344736\n",
      "Step - 10940, Loss - 0.7441369461048193, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.604570713979696\n",
      "Step - 10941, Loss - 0.7187810561051444, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.8275766065686403\n",
      "Step - 10942, Loss - 0.7256945447550964, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.25984875669572566\n",
      "Step - 10943, Loss - 0.4976673455474222, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.8017401624270706\n",
      "Step - 10944, Loss - 0.5995553893032739, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9549237894065157\n",
      "Step - 10945, Loss - 0.6885079793175346, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9139673231418978\n",
      "Step - 10946, Loss - 0.8399054200695373, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.744919762355197\n",
      "Step - 10947, Loss - 0.6399244508745864, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2553023907941412\n",
      "Step - 10948, Loss - 0.92828085920936, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7990592459346164\n",
      "Step - 10949, Loss - 0.4708976377261163, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3000949950533993\n",
      "Step - 10950, Loss - 0.7806166576695983, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9529410853809462\n",
      "Step - 10951, Loss - 0.7803952140815249, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1459641473923592\n",
      "Step - 10952, Loss - 0.7946617924098408, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9266653122226034\n",
      "Step - 10953, Loss - 0.5723498374270308, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2968660963110403\n",
      "Step - 10954, Loss - 0.767786154370778, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7075105205552736\n",
      "Step - 10955, Loss - 0.6667288001772562, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5831253649627427\n",
      "Step - 10956, Loss - 0.5452556631329304, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.393253012610188\n",
      "Step - 10957, Loss - 0.7432228226984354, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.182739778907908\n",
      "Step - 10958, Loss - 0.6470181604241335, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1127106538764773\n",
      "Step - 10959, Loss - 0.7677185771885678, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.14406889610215\n",
      "Step - 10960, Loss - 0.8304433185790135, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4358946020502508\n",
      "Step - 10961, Loss - 0.9171621707895176, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.3343588867856593\n",
      "Step - 10962, Loss - 0.7967826738315844, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2397278927261972\n",
      "Step - 10963, Loss - 0.5119830998865121, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.2689149587856243\n",
      "Step - 10964, Loss - 0.9371682181115901, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.206110828085158\n",
      "Step - 10965, Loss - 0.6541797490371947, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.4096126809763014\n",
      "Step - 10966, Loss - 0.726229822767145, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0554620159691155\n",
      "Step - 10967, Loss - 0.585016026735754, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.111298586959224\n",
      "Step - 10968, Loss - 0.8439997650949517, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5722145730695599\n",
      "Step - 10969, Loss - 0.7405746245181043, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4547323663862135\n",
      "Step - 10970, Loss - 0.5497653351241887, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4635572396649965\n",
      "Step - 10971, Loss - 0.5957681717069565, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.53784532397596\n",
      "Step - 10972, Loss - 0.6664080609257657, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5002139198032453\n",
      "Step - 10973, Loss - 0.6062861884974552, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.0300900385414584\n",
      "Step - 10974, Loss - 0.6586879721200652, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0496157792673075\n",
      "Step - 10975, Loss - 0.8717190100109176, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.671096806256487\n",
      "Step - 10976, Loss - 0.7678112145894318, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.6151864244815253\n",
      "Step - 10977, Loss - 0.5370153577375092, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7071217424760405\n",
      "Step - 10978, Loss - 0.6343436426427326, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7732397693797002\n",
      "Step - 10979, Loss - 0.6374096680650013, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.4424723812847016\n",
      "Step - 10980, Loss - 0.7707585402402054, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.299193905952895\n",
      "Step - 10981, Loss - 0.6839466032142695, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.070435564047588\n",
      "Step - 10982, Loss - 0.6419276231688661, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.141201554787458\n",
      "Step - 10983, Loss - 0.8669400916289021, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.0855745164902335\n",
      "Step - 10984, Loss - 0.9267962613239035, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.324113342062422\n",
      "Step - 10985, Loss - 0.6405402882506483, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4171380748658555\n",
      "Step - 10986, Loss - 0.6714221427405418, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.46078015088081076\n",
      "Step - 10987, Loss - 0.6264773332193716, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.5857770103123836\n",
      "Step - 10988, Loss - 0.6232038675229459, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.5460589293736737\n",
      "Step - 10989, Loss - 0.817401973583046, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.7399584171727152\n",
      "Step - 10990, Loss - 0.6795467853426861, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8263045056614391\n",
      "Step - 10991, Loss - 0.7650456868938348, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1226106812665182\n",
      "Step - 10992, Loss - 0.8155961348743094, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.3140444880001099\n",
      "Step - 10993, Loss - 0.7915475182699708, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1694679030017539\n",
      "Step - 10994, Loss - 0.6558712823760056, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.0639992700524297\n",
      "Step - 10995, Loss - 0.6562873845080064, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.1115379026081356\n",
      "Step - 10996, Loss - 0.7544439799240384, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.9525277023068315\n",
      "Step - 10997, Loss - 0.6486884858544976, Learning Rate - 4.8828125e-05, magnitude of gradient - 0.8387260527804504\n",
      "Step - 10998, Loss - 0.5802849247960091, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.679123607138952\n",
      "Step - 10999, Loss - 1.0008269982072937, Learning Rate - 4.8828125e-05, magnitude of gradient - 1.4946000239916761\n",
      "Step - 11000, Loss - 0.5204638913196402, Learning Rate - 4.8828125e-05, magnitude of gradient - 2.002948126644738\n",
      "Step - 11001, Loss - 0.6579752395059362, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.0896975956875563\n",
      "Step - 11002, Loss - 0.6118367202704946, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9226834211888482\n",
      "Step - 11003, Loss - 0.8298975597682091, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0725761216532044\n",
      "Step - 11004, Loss - 0.5836951200917998, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8427623917147794\n",
      "Step - 11005, Loss - 0.7183939173186314, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5029586394728822\n",
      "Step - 11006, Loss - 0.7306056968259761, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5925805976429949\n",
      "Step - 11007, Loss - 0.8308835503818672, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8097675429998982\n",
      "Step - 11008, Loss - 0.7764151565352911, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.682965579745145\n",
      "Step - 11009, Loss - 0.6584696890059322, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.319323570117124\n",
      "Step - 11010, Loss - 0.5289591806736068, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3953978631396111\n",
      "Step - 11011, Loss - 0.9065980137195553, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6536163871717535\n",
      "Step - 11012, Loss - 0.5774831952944981, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.38188822501248026\n",
      "Step - 11013, Loss - 0.5669113225815965, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8770436577944654\n",
      "Step - 11014, Loss - 0.6976805721301231, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1330428793493177\n",
      "Step - 11015, Loss - 0.7211417323522498, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.85366958133421\n",
      "Step - 11016, Loss - 0.5226725342164065, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.929347089004181\n",
      "Step - 11017, Loss - 0.6530202224592033, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8238680014306122\n",
      "Step - 11018, Loss - 0.6752834979015008, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7233745226794058\n",
      "Step - 11019, Loss - 0.640849875482725, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5047541069434174\n",
      "Step - 11020, Loss - 0.5773939022978408, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.070311979046615\n",
      "Step - 11021, Loss - 0.8354684998001684, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7428574551435085\n",
      "Step - 11022, Loss - 0.5972498952167993, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3390195996878187\n",
      "Step - 11023, Loss - 0.654395467675959, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6550420925526392\n",
      "Step - 11024, Loss - 0.7549639416189764, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.060281990127593\n",
      "Step - 11025, Loss - 0.5636871119774793, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7406485593132063\n",
      "Step - 11026, Loss - 0.5261321238101359, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8714596301308294\n",
      "Step - 11027, Loss - 0.6152750593311538, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7537941058505282\n",
      "Step - 11028, Loss - 0.7055831904476862, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.962976185053118\n",
      "Step - 11029, Loss - 0.7097565192140164, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.468645902608017\n",
      "Step - 11030, Loss - 0.5559530279495428, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3859431087553424\n",
      "Step - 11031, Loss - 0.7151398255988493, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6689537138218542\n",
      "Step - 11032, Loss - 0.967532483281629, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6697949841515685\n",
      "Step - 11033, Loss - 0.7510200616608829, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8020967937178024\n",
      "Step - 11034, Loss - 0.6233262589012526, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8093982695116083\n",
      "Step - 11035, Loss - 0.5261141083580985, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9938754602487092\n",
      "Step - 11036, Loss - 0.7494834340305166, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3761392398226944\n",
      "Step - 11037, Loss - 0.9007648196102896, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7286704516449796\n",
      "Step - 11038, Loss - 0.6401347504473212, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.233391224307057\n",
      "Step - 11039, Loss - 0.6067258884641762, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5706169114061975\n",
      "Step - 11040, Loss - 0.5431894464860542, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4141243286925453\n",
      "Step - 11041, Loss - 0.5833638640306046, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2836899760607485\n",
      "Step - 11042, Loss - 0.8477045468774115, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3783767431953715\n",
      "Step - 11043, Loss - 0.5981430373801414, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.874290173979854\n",
      "Step - 11044, Loss - 0.8808648758023814, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1108983206437224\n",
      "Step - 11045, Loss - 0.6526309125606632, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6307858984004535\n",
      "Step - 11046, Loss - 0.5576368679750349, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6932814508089092\n",
      "Step - 11047, Loss - 0.5166494527519414, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3837738146678256\n",
      "Step - 11048, Loss - 0.5430780296338279, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4015897030444389\n",
      "Step - 11049, Loss - 0.8476199655199844, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.803345768101032\n",
      "Step - 11050, Loss - 0.7866679503566502, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0197120532323856\n",
      "Step - 11051, Loss - 0.7168949248413252, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1232050286168522\n",
      "Step - 11052, Loss - 0.6448237342766272, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2969673689865613\n",
      "Step - 11053, Loss - 0.752119201003266, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6263890733075462\n",
      "Step - 11054, Loss - 0.7096844386305201, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6177658176380247\n",
      "Step - 11055, Loss - 0.8179686431938353, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.0477604742667737\n",
      "Step - 11056, Loss - 0.9397218165831815, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.078851543115985\n",
      "Step - 11057, Loss - 0.6546157918057571, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.4916644759557259\n",
      "Step - 11058, Loss - 0.8622580046784483, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2565330168988893\n",
      "Step - 11059, Loss - 0.7387537223256712, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8083292655972691\n",
      "Step - 11060, Loss - 0.8779917787545919, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0553775100837175\n",
      "Step - 11061, Loss - 0.552995507740925, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2612901098896303\n",
      "Step - 11062, Loss - 0.715779253909285, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1754196470969207\n",
      "Step - 11063, Loss - 0.7991599900114434, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8393387345870863\n",
      "Step - 11064, Loss - 0.8383563560814018, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.123958760769627\n",
      "Step - 11065, Loss - 0.5613363866575506, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5206804321369805\n",
      "Step - 11066, Loss - 0.8280086528349501, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.000021323621721\n",
      "Step - 11067, Loss - 0.66158299574568, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7249249206009631\n",
      "Step - 11068, Loss - 0.7214443345084571, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.4296881530333203\n",
      "Step - 11069, Loss - 0.6580421461441806, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.182679849126919\n",
      "Step - 11070, Loss - 0.6708962967684053, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0569377711406323\n",
      "Step - 11071, Loss - 0.6020150340204381, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8195182601802498\n",
      "Step - 11072, Loss - 0.8669358528590774, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1790068787532288\n",
      "Step - 11073, Loss - 0.8364001220200918, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6155893548394136\n",
      "Step - 11074, Loss - 0.485386162552377, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5080370812859358\n",
      "Step - 11075, Loss - 0.6843004410677136, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.36287018627138695\n",
      "Step - 11076, Loss - 0.6437460168561753, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.158464398314728\n",
      "Step - 11077, Loss - 0.6919490055998792, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5309560166610626\n",
      "Step - 11078, Loss - 0.6245793467004828, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8438761821723367\n",
      "Step - 11079, Loss - 0.6300520095035718, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3191166987492595\n",
      "Step - 11080, Loss - 0.550025111026692, Learning Rate - 2.44140625e-05, magnitude of gradient - 3.2269183359842666\n",
      "Step - 11081, Loss - 0.6431854164812514, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0941319550455666\n",
      "Step - 11082, Loss - 0.8050816101057254, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0936397133225384\n",
      "Step - 11083, Loss - 0.6025666169415618, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.530850747929412\n",
      "Step - 11084, Loss - 0.5935177845976862, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.046361558241734\n",
      "Step - 11085, Loss - 0.6889621800080582, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.082339854244056\n",
      "Step - 11086, Loss - 0.8270451403001714, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6699946872738652\n",
      "Step - 11087, Loss - 0.8367070029004561, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9439413025593104\n",
      "Step - 11088, Loss - 0.8259933044117275, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.991072104076693\n",
      "Step - 11089, Loss - 0.583955072004439, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.0200364070000973\n",
      "Step - 11090, Loss - 0.940761565525327, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5634837736004634\n",
      "Step - 11091, Loss - 0.6013314719964827, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.0905759787980878\n",
      "Step - 11092, Loss - 0.6057793534768071, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1889957975277556\n",
      "Step - 11093, Loss - 0.897186778242142, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.3604085616378057\n",
      "Step - 11094, Loss - 0.6531697563300586, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6165698966797446\n",
      "Step - 11095, Loss - 0.6673477504471661, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6468764396180783\n",
      "Step - 11096, Loss - 0.8756041939575537, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6496938647647039\n",
      "Step - 11097, Loss - 0.843935058570636, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.55628008101748\n",
      "Step - 11098, Loss - 0.6571407195021794, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5343915721502147\n",
      "Step - 11099, Loss - 0.8940093219404064, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4556770661866745\n",
      "Step - 11100, Loss - 0.9342602534643105, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3441234393157395\n",
      "Step - 11101, Loss - 0.6359004628111644, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1816055958624356\n",
      "Step - 11102, Loss - 0.6596128081994692, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1805140102464438\n",
      "Step - 11103, Loss - 0.5813608471389371, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.245595540956143\n",
      "Step - 11104, Loss - 0.5749408172218824, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.527697987225437\n",
      "Step - 11105, Loss - 0.6845833081108532, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2402804932504865\n",
      "Step - 11106, Loss - 0.8078045381160461, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.99427667309387\n",
      "Step - 11107, Loss - 0.6066368884211639, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8758641149230406\n",
      "Step - 11108, Loss - 0.6319867941915546, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.20923165017966006\n",
      "Step - 11109, Loss - 0.7612381704383269, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4070459398564692\n",
      "Step - 11110, Loss - 0.8140569507193668, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0579371936271684\n",
      "Step - 11111, Loss - 0.6733102438386727, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.346461744054169\n",
      "Step - 11112, Loss - 0.8647200936022267, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.60554200511651\n",
      "Step - 11113, Loss - 0.6785651774717643, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.506271892991308\n",
      "Step - 11114, Loss - 0.8927070754575324, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4654048464031795\n",
      "Step - 11115, Loss - 0.7711289822070604, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3381863192982235\n",
      "Step - 11116, Loss - 0.5957723277741988, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6502285282649922\n",
      "Step - 11117, Loss - 0.8156399345685286, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6468308399103477\n",
      "Step - 11118, Loss - 0.7128634710213428, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.41658785559761985\n",
      "Step - 11119, Loss - 0.722380871050479, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9423366251829959\n",
      "Step - 11120, Loss - 0.5979898822692631, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7616273611825841\n",
      "Step - 11121, Loss - 0.7238218488272006, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.516126230994538\n",
      "Step - 11122, Loss - 0.5176094472285341, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.121382455024155\n",
      "Step - 11123, Loss - 0.713046795189686, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7511473025419468\n",
      "Step - 11124, Loss - 0.5576361550908329, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8940832401378416\n",
      "Step - 11125, Loss - 0.9749646412457544, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.760153215236495\n",
      "Step - 11126, Loss - 0.8315227355023127, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2637293362797444\n",
      "Step - 11127, Loss - 0.6955956765459351, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2445183811705516\n",
      "Step - 11128, Loss - 0.5258763272497233, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0621861656393152\n",
      "Step - 11129, Loss - 0.6481033034308588, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5666476503733835\n",
      "Step - 11130, Loss - 0.8061833881516037, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.4327545886898897\n",
      "Step - 11131, Loss - 0.739736962187494, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.2338750817010227\n",
      "Step - 11132, Loss - 0.6600098475839461, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1110555888262506\n",
      "Step - 11133, Loss - 0.8195756258488487, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3656941099227338\n",
      "Step - 11134, Loss - 0.7813513533050391, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.262313858640832\n",
      "Step - 11135, Loss - 0.6754158845863837, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.021305418850706\n",
      "Step - 11136, Loss - 0.7555589507837739, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8879042002250518\n",
      "Step - 11137, Loss - 0.7080806994285307, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5984772334392282\n",
      "Step - 11138, Loss - 0.8184747402308005, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8259633208218204\n",
      "Step - 11139, Loss - 0.5372052026817902, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9472642697704446\n",
      "Step - 11140, Loss - 0.6101079814016724, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2610496365213855\n",
      "Step - 11141, Loss - 0.8076566046222089, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1672124171987899\n",
      "Step - 11142, Loss - 0.6516768987831514, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1270335539617988\n",
      "Step - 11143, Loss - 0.6828085837798457, Learning Rate - 2.44140625e-05, magnitude of gradient - 3.2243082965709258\n",
      "Step - 11144, Loss - 0.5011532708378428, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7089626579300365\n",
      "Step - 11145, Loss - 0.7613438514080269, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.021462674204305\n",
      "Step - 11146, Loss - 0.9215530455292176, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.3555832329980919\n",
      "Step - 11147, Loss - 0.6324823923808403, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.808042346239795\n",
      "Step - 11148, Loss - 0.6315723455916259, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.262611627902018\n",
      "Step - 11149, Loss - 0.7608230657298237, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1907825121846536\n",
      "Step - 11150, Loss - 0.6228761387046395, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3559738997525035\n",
      "Step - 11151, Loss - 0.5666236363912578, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.188614445353332\n",
      "Step - 11152, Loss - 0.7752412733308688, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8265288636720414\n",
      "Step - 11153, Loss - 0.7088003612226971, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7519530889160424\n",
      "Step - 11154, Loss - 0.6131968461618885, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.69632309432932\n",
      "Step - 11155, Loss - 0.7804327563067233, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.031989843157943\n",
      "Step - 11156, Loss - 0.5864231754973398, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1654778076020333\n",
      "Step - 11157, Loss - 0.6590240076683802, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7880657303621107\n",
      "Step - 11158, Loss - 0.6322319144585096, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.246719503881925\n",
      "Step - 11159, Loss - 0.6122624205044156, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9523891731637769\n",
      "Step - 11160, Loss - 0.5969595633558027, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4093701509264949\n",
      "Step - 11161, Loss - 0.6117578351635167, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.211376635143325\n",
      "Step - 11162, Loss - 0.7120955768160321, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4777890523412485\n",
      "Step - 11163, Loss - 0.9498578935321139, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7110591801173171\n",
      "Step - 11164, Loss - 0.6623598738273393, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0507686429608665\n",
      "Step - 11165, Loss - 0.5446264558905577, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8840573036299766\n",
      "Step - 11166, Loss - 0.504697104925596, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5784108473968734\n",
      "Step - 11167, Loss - 0.723174528880076, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2585282753656142\n",
      "Step - 11168, Loss - 0.7281420525811049, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6647509128730845\n",
      "Step - 11169, Loss - 0.6898725448350592, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2016770787441893\n",
      "Step - 11170, Loss - 0.7524871439463386, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.486195096139716\n",
      "Step - 11171, Loss - 0.9763110688782717, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7413676672106788\n",
      "Step - 11172, Loss - 0.6501699253788683, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7949753738819448\n",
      "Step - 11173, Loss - 0.792728270395101, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6489152616190803\n",
      "Step - 11174, Loss - 0.942441587438511, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6881270131981547\n",
      "Step - 11175, Loss - 0.5257392663040592, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.4581465063017048\n",
      "Step - 11176, Loss - 0.8191042266068267, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.455782643751812\n",
      "Step - 11177, Loss - 0.8193692562043671, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5845878611167702\n",
      "Step - 11178, Loss - 0.8029863489019513, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.104258044497439\n",
      "Step - 11179, Loss - 0.7522001063705273, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8563885240192209\n",
      "Step - 11180, Loss - 0.7746242151580753, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0193116636208421\n",
      "Step - 11181, Loss - 0.6277769206009617, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9217229888014115\n",
      "Step - 11182, Loss - 0.775003569709098, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1825574334313211\n",
      "Step - 11183, Loss - 0.7100114232901744, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9545508503790359\n",
      "Step - 11184, Loss - 0.7033619702094828, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5660601736991787\n",
      "Step - 11185, Loss - 0.7317665509835862, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.783120083031059\n",
      "Step - 11186, Loss - 0.6516879655993554, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8234630307850622\n",
      "Step - 11187, Loss - 0.8638504692331561, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9673868731625009\n",
      "Step - 11188, Loss - 0.721432554477411, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0184955772362494\n",
      "Step - 11189, Loss - 0.6702636064837327, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5030894806969222\n",
      "Step - 11190, Loss - 0.6058217194832031, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2320646975298368\n",
      "Step - 11191, Loss - 0.8614756651303822, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.0958521383200064\n",
      "Step - 11192, Loss - 0.5921794932388293, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5924358669962421\n",
      "Step - 11193, Loss - 0.6484501468083945, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.46666809669591724\n",
      "Step - 11194, Loss - 0.7103455579400496, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8841998880554658\n",
      "Step - 11195, Loss - 0.6132123233254285, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7157758079363168\n",
      "Step - 11196, Loss - 0.6761609285765855, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5503364326088522\n",
      "Step - 11197, Loss - 0.6959153237251163, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6937665932926461\n",
      "Step - 11198, Loss - 0.8523916770477441, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6434590591409453\n",
      "Step - 11199, Loss - 0.7705062582372206, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4833590173550213\n",
      "Step - 11200, Loss - 0.6541251216447644, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.25726510632146593\n",
      "Step - 11201, Loss - 0.7581958564894733, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7292013029037733\n",
      "Step - 11202, Loss - 0.645338099891297, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6519526348606384\n",
      "Step - 11203, Loss - 0.8118391603765526, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8024031758897924\n",
      "Step - 11204, Loss - 0.8245243484308983, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8238559466633265\n",
      "Step - 11205, Loss - 0.6974567926596886, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6217953995750121\n",
      "Step - 11206, Loss - 0.5372185678190415, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0046679750247995\n",
      "Step - 11207, Loss - 0.6716102612112799, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.193690851989771\n",
      "Step - 11208, Loss - 0.7220449410591583, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.852880286218883\n",
      "Step - 11209, Loss - 0.6217977436082142, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3442369315734126\n",
      "Step - 11210, Loss - 0.7249968518293357, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4324572032045588\n",
      "Step - 11211, Loss - 0.6512506919151484, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8542554725535898\n",
      "Step - 11212, Loss - 0.6234589968112898, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5444595824491656\n",
      "Step - 11213, Loss - 0.750177898813122, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3044512121515124\n",
      "Step - 11214, Loss - 0.6541902915612464, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.4648849993343489\n",
      "Step - 11215, Loss - 0.47636200081643204, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4447141439765518\n",
      "Step - 11216, Loss - 0.5596584473935288, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1811555555671502\n",
      "Step - 11217, Loss - 0.8178047908685149, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.067080777255848\n",
      "Step - 11218, Loss - 0.4670484816716192, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1118684944666235\n",
      "Step - 11219, Loss - 0.672361651434987, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.5420467485834384\n",
      "Step - 11220, Loss - 0.6153690909501022, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7937497837026348\n",
      "Step - 11221, Loss - 0.671970707039007, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9877935965937861\n",
      "Step - 11222, Loss - 0.9322323693550166, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.949171281392789\n",
      "Step - 11223, Loss - 0.9382034176290912, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2457882441182797\n",
      "Step - 11224, Loss - 0.8014451633968256, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5455007283499221\n",
      "Step - 11225, Loss - 0.7862293592168222, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9400672717851768\n",
      "Step - 11226, Loss - 0.5638590085698593, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6431598045191768\n",
      "Step - 11227, Loss - 0.5904797348343824, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.40393249327558256\n",
      "Step - 11228, Loss - 0.8221601509332683, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7603201577650499\n",
      "Step - 11229, Loss - 0.7487955884387683, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6311697513638552\n",
      "Step - 11230, Loss - 0.6519941245438811, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8757500679488207\n",
      "Step - 11231, Loss - 0.8339830133040778, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6386632087481154\n",
      "Step - 11232, Loss - 0.810767666780224, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8101105344561277\n",
      "Step - 11233, Loss - 0.6327552114820252, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8419330767733924\n",
      "Step - 11234, Loss - 0.6403067807165972, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8162046304725176\n",
      "Step - 11235, Loss - 0.6894118143781417, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6807774356140771\n",
      "Step - 11236, Loss - 0.670686487603058, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.2888641310161066\n",
      "Step - 11237, Loss - 0.6176293499457917, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9460947424295745\n",
      "Step - 11238, Loss - 0.797692773859928, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7591814615531474\n",
      "Step - 11239, Loss - 0.5807812343219189, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.460496797008182\n",
      "Step - 11240, Loss - 0.7103908220966663, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7960950722445939\n",
      "Step - 11241, Loss - 0.6843860897517908, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.992696865793362\n",
      "Step - 11242, Loss - 0.6022148545950641, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1383005576451066\n",
      "Step - 11243, Loss - 0.6884896070681277, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8868304790105829\n",
      "Step - 11244, Loss - 0.7536992894882126, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7087433788660162\n",
      "Step - 11245, Loss - 0.4765984635630226, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2003799478967792\n",
      "Step - 11246, Loss - 0.3701655544901649, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7579938674192532\n",
      "Step - 11247, Loss - 0.6550776180658644, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8534559245229127\n",
      "Step - 11248, Loss - 0.7652939143606892, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.902639355990124\n",
      "Step - 11249, Loss - 0.5925648276123379, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5445857675225156\n",
      "Step - 11250, Loss - 0.8788323733628824, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.11248668159038\n",
      "Step - 11251, Loss - 0.7459350334655335, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.005081207986923\n",
      "Step - 11252, Loss - 0.685184052983657, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2725593695609374\n",
      "Step - 11253, Loss - 0.5513805547296713, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5669360833420278\n",
      "Step - 11254, Loss - 0.7170887701810139, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.48003838576847485\n",
      "Step - 11255, Loss - 0.7026174769493927, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9478537306555527\n",
      "Step - 11256, Loss - 0.7552447935461944, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4201258667875438\n",
      "Step - 11257, Loss - 0.7439253043142616, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9763983524915745\n",
      "Step - 11258, Loss - 0.7462439245699337, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4080013575883408\n",
      "Step - 11259, Loss - 0.8081548358492402, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.615235778277847\n",
      "Step - 11260, Loss - 0.6330895472175985, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6581160923597125\n",
      "Step - 11261, Loss - 0.6857560192988934, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6547184429381081\n",
      "Step - 11262, Loss - 0.865189370343953, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0550410475015655\n",
      "Step - 11263, Loss - 0.724234853969014, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8752582286891741\n",
      "Step - 11264, Loss - 0.5537909394688587, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.8433512630344695\n",
      "Step - 11265, Loss - 0.5630695229176423, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3052744680067907\n",
      "Step - 11266, Loss - 0.648811482007185, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8374531623875734\n",
      "Step - 11267, Loss - 0.9553549216290872, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.552735340952955\n",
      "Step - 11268, Loss - 0.7729081927691036, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0658243214013825\n",
      "Step - 11269, Loss - 0.5651199549869717, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9418306520037475\n",
      "Step - 11270, Loss - 0.7412867868588853, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9929840822164795\n",
      "Step - 11271, Loss - 0.6337875239306474, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8296471625856452\n",
      "Step - 11272, Loss - 0.7881683262026382, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4583734044577712\n",
      "Step - 11273, Loss - 0.8201758579347669, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7550444191586547\n",
      "Step - 11274, Loss - 0.7828479077175204, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6535684492438051\n",
      "Step - 11275, Loss - 0.6712084198740601, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3185752089850509\n",
      "Step - 11276, Loss - 0.8728789791773883, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.791076447992157\n",
      "Step - 11277, Loss - 0.5702098760641199, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8512473311529035\n",
      "Step - 11278, Loss - 0.7500286791216955, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8049037803927539\n",
      "Step - 11279, Loss - 0.6913483938752591, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9211328002571707\n",
      "Step - 11280, Loss - 0.7142038326793656, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3812281598477432\n",
      "Step - 11281, Loss - 0.5354388162381883, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.18611066780658148\n",
      "Step - 11282, Loss - 0.7430925356537321, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3262519524597822\n",
      "Step - 11283, Loss - 0.619409992480667, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8547710806575606\n",
      "Step - 11284, Loss - 0.584691039704772, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5488692077336236\n",
      "Step - 11285, Loss - 0.7595868649808409, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6390076992029273\n",
      "Step - 11286, Loss - 0.721633026274397, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.355225675096471\n",
      "Step - 11287, Loss - 0.6468868924730373, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5572738528728052\n",
      "Step - 11288, Loss - 0.6828214690208568, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9055313164189704\n",
      "Step - 11289, Loss - 0.7582411991670185, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7801432235368827\n",
      "Step - 11290, Loss - 0.5774203363875204, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.4967840526038601\n",
      "Step - 11291, Loss - 0.811294421292937, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.056492032841973\n",
      "Step - 11292, Loss - 0.7730117028756849, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6452552297829122\n",
      "Step - 11293, Loss - 0.5306899268770147, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.2698081856227392\n",
      "Step - 11294, Loss - 0.8682096420762796, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3608531180269439\n",
      "Step - 11295, Loss - 0.46350116247613726, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.475226234244429\n",
      "Step - 11296, Loss - 0.7572256416076848, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8662803606019812\n",
      "Step - 11297, Loss - 0.908099941739434, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.2979920703385046\n",
      "Step - 11298, Loss - 0.7318892786514106, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8117845899687214\n",
      "Step - 11299, Loss - 0.6566837658682965, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4277223094927662\n",
      "Step - 11300, Loss - 0.7997195457988364, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.2888102209493642\n",
      "Step - 11301, Loss - 0.5915657577471367, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.101119780252697\n",
      "Step - 11302, Loss - 0.7620960500786004, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9800836633609293\n",
      "Step - 11303, Loss - 0.6000301513918543, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6128147127387364\n",
      "Step - 11304, Loss - 0.6667283527525887, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7293650335444385\n",
      "Step - 11305, Loss - 0.6665053507861468, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.276177785655084\n",
      "Step - 11306, Loss - 0.6259908853684495, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.2388171907137413\n",
      "Step - 11307, Loss - 0.6069152193828482, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7300779295868001\n",
      "Step - 11308, Loss - 0.7375520499594883, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9579185450863198\n",
      "Step - 11309, Loss - 0.7663664505523128, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.249663811461831\n",
      "Step - 11310, Loss - 0.7482950975629332, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.908524432215347\n",
      "Step - 11311, Loss - 0.6978055746122841, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3958413835195884\n",
      "Step - 11312, Loss - 0.6983937928894135, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6910233330891423\n",
      "Step - 11313, Loss - 0.7252048902683061, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9550283932421619\n",
      "Step - 11314, Loss - 0.6173885026977264, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.38814167810050854\n",
      "Step - 11315, Loss - 0.7670782435186783, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.584571356650114\n",
      "Step - 11316, Loss - 0.7675652236823532, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5343322209974568\n",
      "Step - 11317, Loss - 0.6149682626287617, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8040989852710932\n",
      "Step - 11318, Loss - 0.7045576281068853, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4053150841168458\n",
      "Step - 11319, Loss - 0.6170602720242838, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0288095246349784\n",
      "Step - 11320, Loss - 0.8380768028352151, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2864684484137183\n",
      "Step - 11321, Loss - 0.7170268687620229, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1311242890673396\n",
      "Step - 11322, Loss - 0.7051021292533844, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5500562810639467\n",
      "Step - 11323, Loss - 0.6431737454569223, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8429995243176751\n",
      "Step - 11324, Loss - 0.7292655239864841, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3739743877936619\n",
      "Step - 11325, Loss - 0.6618979009522158, Learning Rate - 2.44140625e-05, magnitude of gradient - 3.4219769369442243\n",
      "Step - 11326, Loss - 0.8341294029615112, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.0174089837539455\n",
      "Step - 11327, Loss - 0.6587524103792929, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5636733446412283\n",
      "Step - 11328, Loss - 0.6837359262747529, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9399069985219508\n",
      "Step - 11329, Loss - 0.6541835475229719, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0453490056266992\n",
      "Step - 11330, Loss - 0.7335010838878525, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2968810227071248\n",
      "Step - 11331, Loss - 0.8159316314289684, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.647463877307114\n",
      "Step - 11332, Loss - 0.7136352332620496, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.387031918315216\n",
      "Step - 11333, Loss - 0.6425467217900585, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7745856073074768\n",
      "Step - 11334, Loss - 0.6110512991655643, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7319034063095327\n",
      "Step - 11335, Loss - 0.6983108306337042, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.903363351605295\n",
      "Step - 11336, Loss - 0.7072091182447678, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9545331967429359\n",
      "Step - 11337, Loss - 0.6782083363523708, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9958830699539637\n",
      "Step - 11338, Loss - 0.784494611219385, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6549930149564159\n",
      "Step - 11339, Loss - 0.5986382664426559, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7872755894209681\n",
      "Step - 11340, Loss - 0.5343663644139819, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0344524992501358\n",
      "Step - 11341, Loss - 0.6758638918329534, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7573632344013699\n",
      "Step - 11342, Loss - 0.9572070758249822, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.356484701467559\n",
      "Step - 11343, Loss - 0.635212218723296, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1618193562185046\n",
      "Step - 11344, Loss - 0.5972371042526207, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.697019807363449\n",
      "Step - 11345, Loss - 0.6190616797477642, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3988841602989042\n",
      "Step - 11346, Loss - 0.8159247935983083, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1946176480999158\n",
      "Step - 11347, Loss - 0.6025479672767748, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3804398884559859\n",
      "Step - 11348, Loss - 0.5528119166334426, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.3737881644059652\n",
      "Step - 11349, Loss - 0.7589731332216039, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6306048223680938\n",
      "Step - 11350, Loss - 0.7377964581513821, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9399318055762499\n",
      "Step - 11351, Loss - 0.8525334811632705, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.124172831963252\n",
      "Step - 11352, Loss - 0.7237775859392925, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1705098935243037\n",
      "Step - 11353, Loss - 0.7697049459017764, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.508766959643757\n",
      "Step - 11354, Loss - 0.5406445060290456, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6682442129534883\n",
      "Step - 11355, Loss - 0.6925117013473724, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3202786230153352\n",
      "Step - 11356, Loss - 0.7005757785384904, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8746465699081225\n",
      "Step - 11357, Loss - 0.8388951145906896, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.384361211389358\n",
      "Step - 11358, Loss - 0.5276731586817099, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2912580530428466\n",
      "Step - 11359, Loss - 0.7451423460139732, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.142887829417089\n",
      "Step - 11360, Loss - 0.7853330044828559, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5455829296479269\n",
      "Step - 11361, Loss - 0.5783523717488105, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9329706951646195\n",
      "Step - 11362, Loss - 0.8913731978304391, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7623648473983051\n",
      "Step - 11363, Loss - 0.8643322165486388, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2607630124938503\n",
      "Step - 11364, Loss - 0.6568023495787436, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9919919962129553\n",
      "Step - 11365, Loss - 0.6448703852947385, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1958563698873343\n",
      "Step - 11366, Loss - 0.6138106760941674, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9993546372852168\n",
      "Step - 11367, Loss - 0.716200655786364, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0224347972184415\n",
      "Step - 11368, Loss - 0.8786840532736125, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9794486950109464\n",
      "Step - 11369, Loss - 0.7514824345638818, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0707379800767276\n",
      "Step - 11370, Loss - 0.7183801869811948, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8888597222721707\n",
      "Step - 11371, Loss - 0.5130264687890318, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9755790793779967\n",
      "Step - 11372, Loss - 0.7771790266230341, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7740594259016892\n",
      "Step - 11373, Loss - 0.8325414424279753, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2537799970134555\n",
      "Step - 11374, Loss - 0.6155102128208082, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6889520407301755\n",
      "Step - 11375, Loss - 0.7063657306990484, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1691604207541342\n",
      "Step - 11376, Loss - 0.7207446148698415, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1677948397765043\n",
      "Step - 11377, Loss - 0.6290274009526557, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.579421309826041\n",
      "Step - 11378, Loss - 0.7511963658928109, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1930327519057948\n",
      "Step - 11379, Loss - 0.9040019924377607, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.189744398565489\n",
      "Step - 11380, Loss - 0.7828859393671919, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1921908151982676\n",
      "Step - 11381, Loss - 0.8616351616239712, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7513059260539185\n",
      "Step - 11382, Loss - 0.7177473680332983, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.37866460160638776\n",
      "Step - 11383, Loss - 0.6681208697371546, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4676380175913892\n",
      "Step - 11384, Loss - 0.9352813315499963, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1880721906829994\n",
      "Step - 11385, Loss - 0.5174011147726336, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.314908260779284\n",
      "Step - 11386, Loss - 0.6927696197933918, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.42938467996887103\n",
      "Step - 11387, Loss - 0.838686657321246, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9829196670987105\n",
      "Step - 11388, Loss - 0.7791827328105042, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9314427497319598\n",
      "Step - 11389, Loss - 0.7476449530162321, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.46780776369780147\n",
      "Step - 11390, Loss - 0.4473923709328018, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.28940621565212954\n",
      "Step - 11391, Loss - 0.7679781004016825, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.691422994955806\n",
      "Step - 11392, Loss - 0.5197627671807286, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4424524953943454\n",
      "Step - 11393, Loss - 0.656188203386939, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6572044739797372\n",
      "Step - 11394, Loss - 0.7549132374363751, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7678160153177411\n",
      "Step - 11395, Loss - 0.6705391879711166, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.484590797430227\n",
      "Step - 11396, Loss - 0.6489064482681378, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0738895022606967\n",
      "Step - 11397, Loss - 0.8453778351576481, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1302532296719663\n",
      "Step - 11398, Loss - 0.7645964826735969, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5627355497987275\n",
      "Step - 11399, Loss - 0.6720052277909682, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.310218719303332\n",
      "Step - 11400, Loss - 0.7200635145805424, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8387366374564036\n",
      "Step - 11401, Loss - 0.5150228237816472, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.424434142497692\n",
      "Step - 11402, Loss - 0.7052106262560355, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1860592556303564\n",
      "Step - 11403, Loss - 0.8565688289064592, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8238946748081647\n",
      "Step - 11404, Loss - 0.6141634931663741, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1545003625922774\n",
      "Step - 11405, Loss - 0.9072441617011188, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.717024768264842\n",
      "Step - 11406, Loss - 0.8905250853005542, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3947789682640548\n",
      "Step - 11407, Loss - 0.7529413220104898, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7243651000230933\n",
      "Step - 11408, Loss - 0.7203641561043591, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3032313642433686\n",
      "Step - 11409, Loss - 0.7892641851499387, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6872293736346167\n",
      "Step - 11410, Loss - 0.7838646257360409, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.050363906323827\n",
      "Step - 11411, Loss - 0.7793865447928923, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6047396206886397\n",
      "Step - 11412, Loss - 0.7298388291265977, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.528638834238678\n",
      "Step - 11413, Loss - 0.8453468452717177, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9053764462562389\n",
      "Step - 11414, Loss - 0.6555354853499272, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2324765803156563\n",
      "Step - 11415, Loss - 0.7281815368647502, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.556139143503502\n",
      "Step - 11416, Loss - 0.6710387977878765, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7018907823391344\n",
      "Step - 11417, Loss - 0.8165894956155921, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5480134105353388\n",
      "Step - 11418, Loss - 0.6773727698475691, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.48047092795751\n",
      "Step - 11419, Loss - 0.6953920891344018, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.4460082024839848\n",
      "Step - 11420, Loss - 0.7583255192671193, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.3661577156077005\n",
      "Step - 11421, Loss - 0.6156760254310403, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9532878751449217\n",
      "Step - 11422, Loss - 0.7277724758292906, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1730096056738053\n",
      "Step - 11423, Loss - 0.7921638126864828, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5198794364624391\n",
      "Step - 11424, Loss - 0.5919514936921481, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7870288423844414\n",
      "Step - 11425, Loss - 0.8337377663959208, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5975195223352237\n",
      "Step - 11426, Loss - 0.5826176035911846, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.34609114211273306\n",
      "Step - 11427, Loss - 0.5631691903029323, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.4764480808580668\n",
      "Step - 11428, Loss - 0.7068299701067782, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7204260745756919\n",
      "Step - 11429, Loss - 0.469559638983197, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9026795754703126\n",
      "Step - 11430, Loss - 0.7829222243297542, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8609307199011599\n",
      "Step - 11431, Loss - 0.7541690089489058, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.095469771421781\n",
      "Step - 11432, Loss - 0.703793961060641, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5909444429081773\n",
      "Step - 11433, Loss - 0.7732306890250452, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1791596961380304\n",
      "Step - 11434, Loss - 0.6307824081137907, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1240083957684412\n",
      "Step - 11435, Loss - 0.7916232034652523, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5229817734159864\n",
      "Step - 11436, Loss - 0.8222645795018414, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8752962244415727\n",
      "Step - 11437, Loss - 0.7201207602915387, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1343986033623454\n",
      "Step - 11438, Loss - 0.37308250092891604, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5543167077474294\n",
      "Step - 11439, Loss - 0.7135944702926278, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.573849332438202\n",
      "Step - 11440, Loss - 0.7517726237315656, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.534586516042046\n",
      "Step - 11441, Loss - 0.7183909116849025, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5244709119477379\n",
      "Step - 11442, Loss - 0.649111747397354, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7157226790404458\n",
      "Step - 11443, Loss - 0.7885351549884647, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.3859668672634998\n",
      "Step - 11444, Loss - 0.6230252764031143, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.018416222462872\n",
      "Step - 11445, Loss - 0.6385939874161294, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6187201728255289\n",
      "Step - 11446, Loss - 0.7020163267167092, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7648890488384349\n",
      "Step - 11447, Loss - 0.822767397375364, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6554310172672042\n",
      "Step - 11448, Loss - 0.6547379221007884, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.3992221619991893\n",
      "Step - 11449, Loss - 0.7936581808680976, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3989060718257824\n",
      "Step - 11450, Loss - 0.8282804802315361, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.849843087527706\n",
      "Step - 11451, Loss - 0.9269171940157955, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5826280737069676\n",
      "Step - 11452, Loss - 0.7112200966733168, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.911662462453769\n",
      "Step - 11453, Loss - 0.6947188782040322, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0763004937867628\n",
      "Step - 11454, Loss - 0.5545125457094092, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2398975315538108\n",
      "Step - 11455, Loss - 0.5011897429945767, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8982612338440643\n",
      "Step - 11456, Loss - 0.7274516693282174, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8704927182088784\n",
      "Step - 11457, Loss - 0.7086111667915086, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2083340128330544\n",
      "Step - 11458, Loss - 0.619460164465472, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7323652716055578\n",
      "Step - 11459, Loss - 0.8178484179317866, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3819518641688628\n",
      "Step - 11460, Loss - 1.0456843475099542, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.0372887051641038\n",
      "Step - 11461, Loss - 0.9223812688432371, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.03908480330326\n",
      "Step - 11462, Loss - 0.6717742386257818, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0659667288402477\n",
      "Step - 11463, Loss - 0.6610736415337207, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.603354974534473\n",
      "Step - 11464, Loss - 0.8829198919183314, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5457087153922806\n",
      "Step - 11465, Loss - 0.54480474185681, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.021448702164474\n",
      "Step - 11466, Loss - 0.8821559340528393, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5230326514549637\n",
      "Step - 11467, Loss - 0.6351812758700757, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.276215608996493\n",
      "Step - 11468, Loss - 0.6769422312176536, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.36536731760654073\n",
      "Step - 11469, Loss - 0.6621602058562114, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.2946027535838962\n",
      "Step - 11470, Loss - 0.7842438048956842, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1745318424910973\n",
      "Step - 11471, Loss - 0.5673828446462778, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1237161677016425\n",
      "Step - 11472, Loss - 0.6433028836027697, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.3068724221656167\n",
      "Step - 11473, Loss - 0.5892420462103222, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1843675851861823\n",
      "Step - 11474, Loss - 0.6178802307415667, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2578120410694227\n",
      "Step - 11475, Loss - 0.8140043914576679, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1048029305807596\n",
      "Step - 11476, Loss - 0.5644371196434202, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.796947697705517\n",
      "Step - 11477, Loss - 0.4017559218356241, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.194262842630335\n",
      "Step - 11478, Loss - 0.678530662577344, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6233428368269153\n",
      "Step - 11479, Loss - 0.8664498654972107, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6078540605268603\n",
      "Step - 11480, Loss - 0.7059439756265309, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.378846765816463\n",
      "Step - 11481, Loss - 0.7952748935585265, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.460937977229416\n",
      "Step - 11482, Loss - 0.7569580668307008, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.3725207244460647\n",
      "Step - 11483, Loss - 0.761904128629004, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0868515822573281\n",
      "Step - 11484, Loss - 0.7159929564257055, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.204922641589481\n",
      "Step - 11485, Loss - 0.6102243847932391, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6863802287531933\n",
      "Step - 11486, Loss - 0.6825932962447113, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.743215485924214\n",
      "Step - 11487, Loss - 0.6966095102103269, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.329423139463584\n",
      "Step - 11488, Loss - 0.6888349233043489, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0255258564360812\n",
      "Step - 11489, Loss - 0.5393683846658365, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.594284643812044\n",
      "Step - 11490, Loss - 0.8503582435898925, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1796818255417243\n",
      "Step - 11491, Loss - 1.0623405479432402, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.7329456098376057\n",
      "Step - 11492, Loss - 0.8858084296834485, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.447922978316026\n",
      "Step - 11493, Loss - 0.583432316673182, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9434412386696945\n",
      "Step - 11494, Loss - 0.5655322006533677, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.5401872890714334\n",
      "Step - 11495, Loss - 0.7561345504237638, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0711594129267037\n",
      "Step - 11496, Loss - 0.8213151401454182, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.162250560512574\n",
      "Step - 11497, Loss - 0.6165516290605517, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2491602300052962\n",
      "Step - 11498, Loss - 0.7123795828649352, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8021167782243936\n",
      "Step - 11499, Loss - 0.5039870254260741, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.526403745769433\n",
      "Step - 11500, Loss - 0.6114919902306899, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1749280040912877\n",
      "Step - 11501, Loss - 0.6350938050044542, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7530966735731939\n",
      "Step - 11502, Loss - 0.6524373467548674, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3909312001061243\n",
      "Step - 11503, Loss - 0.545717219510044, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0143963109670655\n",
      "Step - 11504, Loss - 0.5496876016719917, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.133627111447326\n",
      "Step - 11505, Loss - 0.7805778807024746, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9814845630118374\n",
      "Step - 11506, Loss - 0.6198013219258153, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1381545953204466\n",
      "Step - 11507, Loss - 0.8499536149479267, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.180969123914179\n",
      "Step - 11508, Loss - 0.6599560576763772, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6706846279842038\n",
      "Step - 11509, Loss - 0.6583324795334771, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9848236897698457\n",
      "Step - 11510, Loss - 0.660309397368851, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.85208982868894\n",
      "Step - 11511, Loss - 0.7177302477588929, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1301520210522589\n",
      "Step - 11512, Loss - 0.7978529539569995, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9990807826033556\n",
      "Step - 11513, Loss - 0.7289140252645094, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6911733345489866\n",
      "Step - 11514, Loss - 0.5467843651283415, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5941656026340977\n",
      "Step - 11515, Loss - 0.6181953715928401, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7197398808540532\n",
      "Step - 11516, Loss - 0.5030860031700073, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3269147700432091\n",
      "Step - 11517, Loss - 0.697857329333623, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.473987655395443\n",
      "Step - 11518, Loss - 0.6252288610260192, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.104620238910225\n",
      "Step - 11519, Loss - 0.6989852600262523, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9735871935039986\n",
      "Step - 11520, Loss - 0.7149632064455689, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1078324913131812\n",
      "Step - 11521, Loss - 0.473927464607816, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.060911826744036\n",
      "Step - 11522, Loss - 0.5207906513874038, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3839432382492782\n",
      "Step - 11523, Loss - 0.4474707781380198, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7640229784963016\n",
      "Step - 11524, Loss - 0.5977905760226403, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5480809363492898\n",
      "Step - 11525, Loss - 0.706192341332136, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.237411218833877\n",
      "Step - 11526, Loss - 0.6513333146659335, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6582470723852972\n",
      "Step - 11527, Loss - 1.0106754969264624, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1237389937284799\n",
      "Step - 11528, Loss - 0.7335533402679759, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.2980702477014172\n",
      "Step - 11529, Loss - 0.687906430131241, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.010132466678597\n",
      "Step - 11530, Loss - 0.5959629074067745, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3805118402135248\n",
      "Step - 11531, Loss - 0.7039642633716102, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9193111407867597\n",
      "Step - 11532, Loss - 0.8385286054189873, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9429373328807485\n",
      "Step - 11533, Loss - 0.8089834655109396, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.24510923108419272\n",
      "Step - 11534, Loss - 0.7570100756153552, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.3301676301756933\n",
      "Step - 11535, Loss - 0.732778213745019, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2085852816235036\n",
      "Step - 11536, Loss - 0.6764149461269622, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5873820385193042\n",
      "Step - 11537, Loss - 0.7522569991744752, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6348728051770576\n",
      "Step - 11538, Loss - 0.7591174654831133, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1902265440866588\n",
      "Step - 11539, Loss - 0.7863060900813734, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4012749763533903\n",
      "Step - 11540, Loss - 0.7252473670559094, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6473783297764971\n",
      "Step - 11541, Loss - 0.8259637686027438, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5116718024664955\n",
      "Step - 11542, Loss - 0.8202966637130708, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5301878517249337\n",
      "Step - 11543, Loss - 0.6694046345663424, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7334147075490418\n",
      "Step - 11544, Loss - 0.6792264086032916, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5076760057306686\n",
      "Step - 11545, Loss - 0.756568724057105, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0897579446481744\n",
      "Step - 11546, Loss - 0.8755545216803906, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.517645014879002\n",
      "Step - 11547, Loss - 0.6054281444838241, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2622228591692142\n",
      "Step - 11548, Loss - 0.6171211360643866, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6808675529877393\n",
      "Step - 11549, Loss - 0.5293013081072819, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8268893853811997\n",
      "Step - 11550, Loss - 0.658498218163293, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0145457390358708\n",
      "Step - 11551, Loss - 0.7537655491141843, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7290031617566226\n",
      "Step - 11552, Loss - 0.8018785418386423, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1565815052678126\n",
      "Step - 11553, Loss - 0.6937602569105994, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2033697486551307\n",
      "Step - 11554, Loss - 0.6944378804883682, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1012902708786343\n",
      "Step - 11555, Loss - 0.7636970505298106, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3927695097107793\n",
      "Step - 11556, Loss - 0.5721840857801337, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8557825998507957\n",
      "Step - 11557, Loss - 0.8442605401630441, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1960600126064236\n",
      "Step - 11558, Loss - 0.8052019434550904, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3492536955939032\n",
      "Step - 11559, Loss - 0.8283170481888241, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.556452264717312\n",
      "Step - 11560, Loss - 0.7632381415982982, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8575731419180379\n",
      "Step - 11561, Loss - 0.6446907845020134, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5337409333690424\n",
      "Step - 11562, Loss - 0.6523248558085326, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8922847807582437\n",
      "Step - 11563, Loss - 0.7419233738179003, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9700027873951701\n",
      "Step - 11564, Loss - 0.6828928998354177, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.754301536123318\n",
      "Step - 11565, Loss - 0.5069741028517594, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2086863291052647\n",
      "Step - 11566, Loss - 0.5128221547007992, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.040331330769165\n",
      "Step - 11567, Loss - 0.6396621751149455, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0403491173354862\n",
      "Step - 11568, Loss - 0.7364380368735556, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9314759178892861\n",
      "Step - 11569, Loss - 0.5113615943504629, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9871648403873402\n",
      "Step - 11570, Loss - 0.6129973204887791, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.25563216104862\n",
      "Step - 11571, Loss - 0.7674015859997929, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.0322909676168193\n",
      "Step - 11572, Loss - 0.629692872953837, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9547846694652864\n",
      "Step - 11573, Loss - 0.6449112448865122, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7814199197384577\n",
      "Step - 11574, Loss - 0.6614865628278144, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7974590756520696\n",
      "Step - 11575, Loss - 0.691262124027069, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6731954984828974\n",
      "Step - 11576, Loss - 0.9148238996448985, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.106948454520486\n",
      "Step - 11577, Loss - 0.646096651472142, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8111528241589419\n",
      "Step - 11578, Loss - 0.8277387299000664, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6049433756390323\n",
      "Step - 11579, Loss - 0.7457249730767893, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1545836992958218\n",
      "Step - 11580, Loss - 0.6510894613766319, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5291415079655529\n",
      "Step - 11581, Loss - 0.5580441883994092, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.073694242553253\n",
      "Step - 11582, Loss - 0.6057521369152254, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.3040158917037075\n",
      "Step - 11583, Loss - 0.6304944433564386, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8984468710747568\n",
      "Step - 11584, Loss - 0.7687868003285292, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.849716071657447\n",
      "Step - 11585, Loss - 0.5985861909296204, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.034961021688489\n",
      "Step - 11586, Loss - 0.7605928572874315, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5129211458021683\n",
      "Step - 11587, Loss - 0.8264552686138286, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.227788380971889\n",
      "Step - 11588, Loss - 0.56616359605298, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9022302103353992\n",
      "Step - 11589, Loss - 0.8750932801123243, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.3928501379713167\n",
      "Step - 11590, Loss - 0.5946637993359296, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.3441224956062581\n",
      "Step - 11591, Loss - 0.729675408188476, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4115412506764924\n",
      "Step - 11592, Loss - 0.8015501974147973, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7203707936876758\n",
      "Step - 11593, Loss - 0.6977570880406624, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6830277440200628\n",
      "Step - 11594, Loss - 0.7840074911869126, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7236898602062223\n",
      "Step - 11595, Loss - 0.8224054694314907, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2914448843748045\n",
      "Step - 11596, Loss - 0.6991743212437471, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.911623762899487\n",
      "Step - 11597, Loss - 0.7106997795381937, Learning Rate - 2.44140625e-05, magnitude of gradient - 3.4310686480446115\n",
      "Step - 11598, Loss - 0.5958686599031119, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3091507603176975\n",
      "Step - 11599, Loss - 0.519159079197696, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5297480234283294\n",
      "Step - 11600, Loss - 0.5613163010917097, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6771820325537022\n",
      "Step - 11601, Loss - 0.6497586714070815, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0459965606447772\n",
      "Step - 11602, Loss - 0.46024117969485495, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7006201308255315\n",
      "Step - 11603, Loss - 0.7765477660685733, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3710098020096195\n",
      "Step - 11604, Loss - 0.5354378293621062, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6540093617516844\n",
      "Step - 11605, Loss - 0.7970390195519691, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3481960620030446\n",
      "Step - 11606, Loss - 0.7414863143280879, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0918519342699566\n",
      "Step - 11607, Loss - 0.656181972943535, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5574632603407037\n",
      "Step - 11608, Loss - 0.6175850550456785, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4702888128811074\n",
      "Step - 11609, Loss - 0.7243958349547581, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9311614412024295\n",
      "Step - 11610, Loss - 0.612823535968194, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1550700769360274\n",
      "Step - 11611, Loss - 0.4955591064018712, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8039145223252837\n",
      "Step - 11612, Loss - 0.7278613460181951, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0942378428131627\n",
      "Step - 11613, Loss - 0.6465603625136496, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4023179064121558\n",
      "Step - 11614, Loss - 0.595139138603723, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8785725875763533\n",
      "Step - 11615, Loss - 0.5367490853008678, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5060582448960875\n",
      "Step - 11616, Loss - 0.7092624893537915, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7671957873640429\n",
      "Step - 11617, Loss - 0.7575555569309755, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1879270257193413\n",
      "Step - 11618, Loss - 0.6433511086156297, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1594590305654684\n",
      "Step - 11619, Loss - 0.5576661336305186, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0745333150578025\n",
      "Step - 11620, Loss - 0.628040980296704, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3181277474879163\n",
      "Step - 11621, Loss - 0.7089634486786869, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.685937397389547\n",
      "Step - 11622, Loss - 0.8329572214252214, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8665601485916774\n",
      "Step - 11623, Loss - 0.5601700683872334, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1487227009964707\n",
      "Step - 11624, Loss - 0.6001520364607061, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8364915163728928\n",
      "Step - 11625, Loss - 0.5490311031733922, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5977710177076637\n",
      "Step - 11626, Loss - 0.8548519216713296, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.4925998559035884\n",
      "Step - 11627, Loss - 0.719116843803679, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6270354860542566\n",
      "Step - 11628, Loss - 0.6286456735299619, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.49233570357220724\n",
      "Step - 11629, Loss - 0.7267340745074518, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0320960514648168\n",
      "Step - 11630, Loss - 0.7164380283128386, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3133713780804674\n",
      "Step - 11631, Loss - 0.8511884518226757, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5960967164292115\n",
      "Step - 11632, Loss - 0.7781660477893453, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.962430112700203\n",
      "Step - 11633, Loss - 0.702873361842515, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6715370558447703\n",
      "Step - 11634, Loss - 0.47761201847988205, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8112200943658083\n",
      "Step - 11635, Loss - 0.5229526795319235, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3709318792441276\n",
      "Step - 11636, Loss - 0.8999785355240114, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5200959748219531\n",
      "Step - 11637, Loss - 0.8197444963659033, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7647454190456606\n",
      "Step - 11638, Loss - 0.7285796263992238, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3071673322072503\n",
      "Step - 11639, Loss - 0.4983952935600763, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7802059125605258\n",
      "Step - 11640, Loss - 0.5468024230226964, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.898313498116916\n",
      "Step - 11641, Loss - 0.6154442882549649, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.665878303467831\n",
      "Step - 11642, Loss - 0.760766924061765, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5611886181140856\n",
      "Step - 11643, Loss - 0.7984837456924209, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.16242427393379\n",
      "Step - 11644, Loss - 0.5947818529948588, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7461765666253465\n",
      "Step - 11645, Loss - 0.9378704999405432, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9148136815637107\n",
      "Step - 11646, Loss - 0.6519196044642578, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9673217161454667\n",
      "Step - 11647, Loss - 0.7435304714739435, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5886992591042205\n",
      "Step - 11648, Loss - 0.46055440439123935, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4022150876322563\n",
      "Step - 11649, Loss - 0.7749008332259784, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4078211589131533\n",
      "Step - 11650, Loss - 0.7686213874866561, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2987323235038055\n",
      "Step - 11651, Loss - 0.9393796260353336, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1750434113394155\n",
      "Step - 11652, Loss - 0.6601566500042082, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.48470977935731385\n",
      "Step - 11653, Loss - 0.6328449162865291, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6098806676994857\n",
      "Step - 11654, Loss - 0.6949732589216108, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9827552646837344\n",
      "Step - 11655, Loss - 0.725055463392974, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9408960468735031\n",
      "Step - 11656, Loss - 0.7293888229515871, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2782995621542896\n",
      "Step - 11657, Loss - 0.609970937565991, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6758710087699519\n",
      "Step - 11658, Loss - 0.6735852571516157, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8135904865697865\n",
      "Step - 11659, Loss - 0.7395355160612613, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.030710955793813\n",
      "Step - 11660, Loss - 0.7430058272971595, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4242019427864845\n",
      "Step - 11661, Loss - 0.6465224490970519, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5698588691380804\n",
      "Step - 11662, Loss - 0.7676380896099618, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8115872950795368\n",
      "Step - 11663, Loss - 0.6801911606636436, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6618971113572168\n",
      "Step - 11664, Loss - 0.751417329145872, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4082264603681156\n",
      "Step - 11665, Loss - 0.5137795035206607, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8560185762053487\n",
      "Step - 11666, Loss - 0.8029701868840545, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1858765422292457\n",
      "Step - 11667, Loss - 0.9168163462907575, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3775745066197873\n",
      "Step - 11668, Loss - 0.6062357321050098, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4779025572499147\n",
      "Step - 11669, Loss - 0.7499029852776451, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.694404613277125\n",
      "Step - 11670, Loss - 0.8674661705924149, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6053499556411355\n",
      "Step - 11671, Loss - 0.7112862329450762, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1440948415487073\n",
      "Step - 11672, Loss - 0.6091535299641654, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4877105747284312\n",
      "Step - 11673, Loss - 0.7707322333056215, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8570842544522793\n",
      "Step - 11674, Loss - 0.7151300935755874, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9773958550039588\n",
      "Step - 11675, Loss - 0.8000742388080089, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8767051911557622\n",
      "Step - 11676, Loss - 0.8630846574953766, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0316605053079573\n",
      "Step - 11677, Loss - 0.8371638076938036, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8369197120014253\n",
      "Step - 11678, Loss - 0.9127371616186075, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6267214532239527\n",
      "Step - 11679, Loss - 0.657966362317125, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8711007979929389\n",
      "Step - 11680, Loss - 0.7992898877598065, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.4379907148010594\n",
      "Step - 11681, Loss - 0.7175662958013795, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0086510060836613\n",
      "Step - 11682, Loss - 0.9321133929282371, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1822028160237263\n",
      "Step - 11683, Loss - 0.6767176826099435, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0990331979282806\n",
      "Step - 11684, Loss - 0.7130844436807391, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4878100474183513\n",
      "Step - 11685, Loss - 0.6279249589331588, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.188467114071053\n",
      "Step - 11686, Loss - 0.8846282161720018, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.493902857217478\n",
      "Step - 11687, Loss - 0.7153947057278263, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7360218620380157\n",
      "Step - 11688, Loss - 0.6995081302384685, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7763309945795811\n",
      "Step - 11689, Loss - 0.7765603936062135, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.390157798762554\n",
      "Step - 11690, Loss - 0.6930257585525716, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7969322589748964\n",
      "Step - 11691, Loss - 0.7219095565328743, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.4172180507715692\n",
      "Step - 11692, Loss - 0.7191966094461943, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9580813832420015\n",
      "Step - 11693, Loss - 0.6327096308445184, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8232984347183087\n",
      "Step - 11694, Loss - 0.6485803085091658, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6019377122763567\n",
      "Step - 11695, Loss - 0.6495554185756787, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4265465764472796\n",
      "Step - 11696, Loss - 0.7468729265314187, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.2163234377753267\n",
      "Step - 11697, Loss - 0.5474462097481696, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6081760015148758\n",
      "Step - 11698, Loss - 0.6335007376307107, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9055053761159408\n",
      "Step - 11699, Loss - 0.8421591857875628, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.018673081417899\n",
      "Step - 11700, Loss - 0.4700857663935401, Learning Rate - 2.44140625e-05, magnitude of gradient - 3.0866143359461526\n",
      "Step - 11701, Loss - 0.7258388229456127, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.611849571051939\n",
      "Step - 11702, Loss - 0.6991173914979971, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9205640017294019\n",
      "Step - 11703, Loss - 0.6943870633760282, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8892965692156366\n",
      "Step - 11704, Loss - 0.6043726156464015, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3628805314826506\n",
      "Step - 11705, Loss - 0.6933893512276726, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7474588625554476\n",
      "Step - 11706, Loss - 0.7065082618000438, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9889795277396176\n",
      "Step - 11707, Loss - 0.6841928480725921, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0734860732497722\n",
      "Step - 11708, Loss - 0.5053498884146874, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.401741761311118\n",
      "Step - 11709, Loss - 0.682552765345517, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1422134450817554\n",
      "Step - 11710, Loss - 0.6457612078055381, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8635956140293815\n",
      "Step - 11711, Loss - 0.5102106407184418, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.353515617379527\n",
      "Step - 11712, Loss - 0.8056923986579836, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1599126801310131\n",
      "Step - 11713, Loss - 0.6045784645594264, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3394339960267914\n",
      "Step - 11714, Loss - 0.7160519640497789, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2582902942321688\n",
      "Step - 11715, Loss - 0.7163622946671742, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6217963507999795\n",
      "Step - 11716, Loss - 0.8168175969804043, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8365264058655142\n",
      "Step - 11717, Loss - 0.7555792186582966, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.3401537590266773\n",
      "Step - 11718, Loss - 0.5191645501600813, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.15612736990426\n",
      "Step - 11719, Loss - 0.6868918899002525, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9816551441920971\n",
      "Step - 11720, Loss - 0.8682052391015007, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.379567639391848\n",
      "Step - 11721, Loss - 0.5681894270642079, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5397008421001\n",
      "Step - 11722, Loss - 0.7193037299588, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1931413636935515\n",
      "Step - 11723, Loss - 0.7987366227797423, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5024901106642639\n",
      "Step - 11724, Loss - 0.8091881927736837, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.359070245651751\n",
      "Step - 11725, Loss - 0.6410663214674039, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.21431256201124\n",
      "Step - 11726, Loss - 0.8391032544519478, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7731746920175593\n",
      "Step - 11727, Loss - 0.6882575642653922, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3133487458921271\n",
      "Step - 11728, Loss - 0.7700110580385555, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5191743502344814\n",
      "Step - 11729, Loss - 0.5805667502666956, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3633409180153482\n",
      "Step - 11730, Loss - 0.9998999341211008, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4358735893131573\n",
      "Step - 11731, Loss - 0.5131037828130852, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7123487012154075\n",
      "Step - 11732, Loss - 0.47842329367056574, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.910248090642555\n",
      "Step - 11733, Loss - 0.9012455564006138, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.903315991375843\n",
      "Step - 11734, Loss - 0.6398399660743849, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7426188831592476\n",
      "Step - 11735, Loss - 0.7354428049135617, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9842111934687517\n",
      "Step - 11736, Loss - 0.5632413629431889, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6283435844665418\n",
      "Step - 11737, Loss - 0.7941958876936145, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9832516002990951\n",
      "Step - 11738, Loss - 0.8136230533149735, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.611933562305162\n",
      "Step - 11739, Loss - 0.6378116609432536, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2311120647349985\n",
      "Step - 11740, Loss - 0.6968003438118319, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2807192485818186\n",
      "Step - 11741, Loss - 0.7290845220347819, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5133586232574839\n",
      "Step - 11742, Loss - 0.643750674783962, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5083602279056565\n",
      "Step - 11743, Loss - 0.6564998657716151, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0760299283364143\n",
      "Step - 11744, Loss - 0.7291463806847052, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3103345564631785\n",
      "Step - 11745, Loss - 0.5906323511605787, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.7777494870617594\n",
      "Step - 11746, Loss - 0.7718304921307435, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6485643838093875\n",
      "Step - 11747, Loss - 0.7341195200803853, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2382750002275726\n",
      "Step - 11748, Loss - 0.7478857547091361, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8514923014220501\n",
      "Step - 11749, Loss - 0.6529787817408852, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.396212421330112\n",
      "Step - 11750, Loss - 0.5679745685470099, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0702064170011771\n",
      "Step - 11751, Loss - 0.564941583562068, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7223347781300956\n",
      "Step - 11752, Loss - 0.6171897340066557, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5253700653957047\n",
      "Step - 11753, Loss - 0.7044871376245327, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.30812682520367474\n",
      "Step - 11754, Loss - 0.5692812893696356, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5759056281470305\n",
      "Step - 11755, Loss - 0.8323259497675461, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2997909664240341\n",
      "Step - 11756, Loss - 0.41778927311969133, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6663982109809656\n",
      "Step - 11757, Loss - 0.6344757559857381, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.794575215588928\n",
      "Step - 11758, Loss - 0.5486522733645619, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0298822742113336\n",
      "Step - 11759, Loss - 0.789312341580075, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.0080091227852863\n",
      "Step - 11760, Loss - 0.6525294521724463, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9937073366632665\n",
      "Step - 11761, Loss - 0.6389931342385712, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5815939137845757\n",
      "Step - 11762, Loss - 0.739524892100423, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7238517889276825\n",
      "Step - 11763, Loss - 0.5197502681720304, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8436541089327504\n",
      "Step - 11764, Loss - 0.7540970308764531, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8425065810775374\n",
      "Step - 11765, Loss - 0.8397863645469161, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6417913490259863\n",
      "Step - 11766, Loss - 0.6072137144446255, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.458974044171424\n",
      "Step - 11767, Loss - 0.6732704542894881, Learning Rate - 2.44140625e-05, magnitude of gradient - 3.23045468925073\n",
      "Step - 11768, Loss - 0.6096645996223798, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1213781523541542\n",
      "Step - 11769, Loss - 0.632636952893443, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8401814530440612\n",
      "Step - 11770, Loss - 0.675620307533356, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5806914179734775\n",
      "Step - 11771, Loss - 0.6496549008810955, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3463282463469775\n",
      "Step - 11772, Loss - 0.6650241039069094, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0839864593922246\n",
      "Step - 11773, Loss - 0.6354351212731176, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7484761208159795\n",
      "Step - 11774, Loss - 0.8193417220845443, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7510407205061338\n",
      "Step - 11775, Loss - 0.8311396399353383, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.4073163389306451\n",
      "Step - 11776, Loss - 0.9367865564380851, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3043329169637832\n",
      "Step - 11777, Loss - 0.5814273126016621, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0800424379517795\n",
      "Step - 11778, Loss - 0.8379181565045666, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0456583663236307\n",
      "Step - 11779, Loss - 0.7458714327537248, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1256797966711725\n",
      "Step - 11780, Loss - 0.7212983461202994, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4130123091445876\n",
      "Step - 11781, Loss - 0.7512454429031633, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.47027253611594777\n",
      "Step - 11782, Loss - 0.6418318691338009, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.830497715895222\n",
      "Step - 11783, Loss - 0.7340749229664683, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7078094569938096\n",
      "Step - 11784, Loss - 0.6181622465167717, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2421211134813357\n",
      "Step - 11785, Loss - 0.42971010328241516, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.4195922393852967\n",
      "Step - 11786, Loss - 0.669634014876046, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5326084999144736\n",
      "Step - 11787, Loss - 0.6851203875889379, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2523648943919632\n",
      "Step - 11788, Loss - 0.7457541336384963, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2073008306337956\n",
      "Step - 11789, Loss - 0.6669231814525843, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3010627695620627\n",
      "Step - 11790, Loss - 0.8361029847004992, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8701576057614704\n",
      "Step - 11791, Loss - 0.6500191579994504, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9012947571171537\n",
      "Step - 11792, Loss - 0.8103329807888537, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1630473310287996\n",
      "Step - 11793, Loss - 0.7077135186722263, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8397234078496161\n",
      "Step - 11794, Loss - 0.6476271724014306, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1329627987387818\n",
      "Step - 11795, Loss - 0.6024984033187435, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.302135143274242\n",
      "Step - 11796, Loss - 0.8131485171700925, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7496634911856956\n",
      "Step - 11797, Loss - 0.9291239270043854, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.25954603549597\n",
      "Step - 11798, Loss - 0.8858948792429983, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9820988804357694\n",
      "Step - 11799, Loss - 0.78644592173685, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.928642546047489\n",
      "Step - 11800, Loss - 0.7892380162143385, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.377356395142485\n",
      "Step - 11801, Loss - 0.6771690353655125, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7726117329952797\n",
      "Step - 11802, Loss - 0.51716763648642, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4884479148311007\n",
      "Step - 11803, Loss - 0.6665468218706981, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.629819199008274\n",
      "Step - 11804, Loss - 0.6586486097542111, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3654377335178678\n",
      "Step - 11805, Loss - 0.7792723070559405, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0147908904880432\n",
      "Step - 11806, Loss - 1.0183691520685367, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3675310406653418\n",
      "Step - 11807, Loss - 0.7536995984041349, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5295486845225172\n",
      "Step - 11808, Loss - 0.6970102119831498, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2039394714540994\n",
      "Step - 11809, Loss - 0.5999049785884785, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.090928000407761\n",
      "Step - 11810, Loss - 0.6037378088337226, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.4329566520436683\n",
      "Step - 11811, Loss - 0.6780759669299861, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6875818108820081\n",
      "Step - 11812, Loss - 0.4980698196962885, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.3337278906338214\n",
      "Step - 11813, Loss - 0.7272823872274454, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7256216499521445\n",
      "Step - 11814, Loss - 0.7097410332961611, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1802081049552868\n",
      "Step - 11815, Loss - 0.555786169070841, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7861292563979838\n",
      "Step - 11816, Loss - 0.5337498355882488, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1730001880670446\n",
      "Step - 11817, Loss - 0.6101616666996, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1148453048028026\n",
      "Step - 11818, Loss - 0.6088569937434278, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.110083803088594\n",
      "Step - 11819, Loss - 0.5700742996797573, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3389746233353073\n",
      "Step - 11820, Loss - 0.5918743656552705, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.02241950188532\n",
      "Step - 11821, Loss - 0.9016288667104982, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3014117441029869\n",
      "Step - 11822, Loss - 0.8779805345212812, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.925014216490095\n",
      "Step - 11823, Loss - 0.7447473857249391, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6822656518985853\n",
      "Step - 11824, Loss - 0.9143388943192261, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.44354570264353144\n",
      "Step - 11825, Loss - 0.7051437841769286, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.660253890865031\n",
      "Step - 11826, Loss - 0.8176861641868324, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2551437698235968\n",
      "Step - 11827, Loss - 0.7051619027926497, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4739417438839033\n",
      "Step - 11828, Loss - 0.669692772855832, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.619152705255718\n",
      "Step - 11829, Loss - 0.6018033923342624, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1899057071549868\n",
      "Step - 11830, Loss - 0.9462826478257952, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.206579213462403\n",
      "Step - 11831, Loss - 0.41301298319144586, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6344538683392261\n",
      "Step - 11832, Loss - 0.7346517856161111, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0742682162599801\n",
      "Step - 11833, Loss - 0.7336002088360746, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.3925009911589625\n",
      "Step - 11834, Loss - 0.5779382469864056, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4768426538656396\n",
      "Step - 11835, Loss - 0.5683937310531109, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7629432368515763\n",
      "Step - 11836, Loss - 0.5399318224218125, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9941768254148132\n",
      "Step - 11837, Loss - 0.6176482439517246, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7703606736427371\n",
      "Step - 11838, Loss - 0.8569938630721358, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9532457397739438\n",
      "Step - 11839, Loss - 0.7351979094967263, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7012009100415908\n",
      "Step - 11840, Loss - 0.6814420387064434, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3787747666963561\n",
      "Step - 11841, Loss - 0.630909262268555, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8785045002495935\n",
      "Step - 11842, Loss - 0.8241947010096149, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5217076128181591\n",
      "Step - 11843, Loss - 0.8016655894863138, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7896784962470993\n",
      "Step - 11844, Loss - 0.5717833370285982, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0043010375174342\n",
      "Step - 11845, Loss - 0.5284745716706097, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.8535646173148668\n",
      "Step - 11846, Loss - 0.6111955316797707, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1465603135280564\n",
      "Step - 11847, Loss - 0.7024665445177128, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1368274105353646\n",
      "Step - 11848, Loss - 0.8479637123180315, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1145454635331284\n",
      "Step - 11849, Loss - 0.6687266272188539, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0088392779261088\n",
      "Step - 11850, Loss - 0.5535145718651685, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.799361693361468\n",
      "Step - 11851, Loss - 0.7240988126280316, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8082110692863543\n",
      "Step - 11852, Loss - 0.5599919498210859, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7175015493614344\n",
      "Step - 11853, Loss - 0.7383631374306622, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6740841654023864\n",
      "Step - 11854, Loss - 0.6507365519068582, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8781486366871454\n",
      "Step - 11855, Loss - 0.6378485335075947, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7470701368335226\n",
      "Step - 11856, Loss - 0.8051421222919435, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6034616934589601\n",
      "Step - 11857, Loss - 0.769096573431289, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9387523281140411\n",
      "Step - 11858, Loss - 0.6422700446053143, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1868933621275635\n",
      "Step - 11859, Loss - 0.5926146492527733, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9796555077825404\n",
      "Step - 11860, Loss - 0.7631067043189338, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9335993921350281\n",
      "Step - 11861, Loss - 0.5691474566038351, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7891493630345668\n",
      "Step - 11862, Loss - 0.6379276629480406, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3132953689820495\n",
      "Step - 11863, Loss - 0.7819763585868396, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7258573716360847\n",
      "Step - 11864, Loss - 0.6812114066040924, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5207319283695697\n",
      "Step - 11865, Loss - 0.6908373319711125, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0319744496073886\n",
      "Step - 11866, Loss - 0.7972338926933125, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5931982312682134\n",
      "Step - 11867, Loss - 0.8890029282093319, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7775297860568762\n",
      "Step - 11868, Loss - 0.6362686632787423, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5683006250159763\n",
      "Step - 11869, Loss - 0.6875764711209548, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6663787527684595\n",
      "Step - 11870, Loss - 0.8508055843878933, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4125221602540574\n",
      "Step - 11871, Loss - 0.6823770982611994, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7951885843671072\n",
      "Step - 11872, Loss - 0.6671358035602486, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4334024165516799\n",
      "Step - 11873, Loss - 0.5566889983768929, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7005470144167143\n",
      "Step - 11874, Loss - 0.5956501540113446, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4838011338890014\n",
      "Step - 11875, Loss - 0.6340515268002553, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3020353546441898\n",
      "Step - 11876, Loss - 0.7089107476142417, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1058561268380755\n",
      "Step - 11877, Loss - 0.5952167007801318, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5782388402659057\n",
      "Step - 11878, Loss - 0.7113403072385386, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7108458004838888\n",
      "Step - 11879, Loss - 0.697479859279094, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5557328133307599\n",
      "Step - 11880, Loss - 0.9649290862714567, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9066370447061274\n",
      "Step - 11881, Loss - 0.7592266962497771, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3104110908014952\n",
      "Step - 11882, Loss - 0.9124841083665551, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.6157288390963962\n",
      "Step - 11883, Loss - 0.78992377800868, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.017704599126449\n",
      "Step - 11884, Loss - 0.7831913778375494, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.602560891320934\n",
      "Step - 11885, Loss - 0.631870618361348, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2300277164817752\n",
      "Step - 11886, Loss - 0.7332396809493119, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.760622161784671\n",
      "Step - 11887, Loss - 0.6836139433676991, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1242673894468016\n",
      "Step - 11888, Loss - 0.6082463117078605, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.94002644910271\n",
      "Step - 11889, Loss - 0.7387237043151742, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2144497072609228\n",
      "Step - 11890, Loss - 0.5949710159055024, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6470533506795082\n",
      "Step - 11891, Loss - 0.7408275293704781, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6538847359302233\n",
      "Step - 11892, Loss - 0.638831499528304, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8014476559896\n",
      "Step - 11893, Loss - 0.6136485956130602, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4690367822371526\n",
      "Step - 11894, Loss - 0.8420901275508789, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5317435191120893\n",
      "Step - 11895, Loss - 0.7234958195131158, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.403151825736295\n",
      "Step - 11896, Loss - 0.7359941414156004, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.5413829893838447\n",
      "Step - 11897, Loss - 0.7781004916929953, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9479403361245938\n",
      "Step - 11898, Loss - 0.5889778867357687, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7420905464846594\n",
      "Step - 11899, Loss - 0.8909000569428789, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.5118763844917078\n",
      "Step - 11900, Loss - 0.735079823095725, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8355096110274068\n",
      "Step - 11901, Loss - 0.8778332012943583, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.595278296351831\n",
      "Step - 11902, Loss - 0.600345408316811, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5123886408600199\n",
      "Step - 11903, Loss - 0.613510097218701, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1363813079295861\n",
      "Step - 11904, Loss - 0.6493042149217778, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9693808823412635\n",
      "Step - 11905, Loss - 0.4977326571526725, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1084810373431133\n",
      "Step - 11906, Loss - 0.5845249405696862, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.147031400986226\n",
      "Step - 11907, Loss - 0.6575100147757325, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4833195421683592\n",
      "Step - 11908, Loss - 0.7197508041331513, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3137083729357553\n",
      "Step - 11909, Loss - 0.7871647726209575, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7477473929461892\n",
      "Step - 11910, Loss - 0.6665560713043744, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7216498753839805\n",
      "Step - 11911, Loss - 0.7237902971273311, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9578233383702357\n",
      "Step - 11912, Loss - 0.8074675474581863, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8149329287268857\n",
      "Step - 11913, Loss - 0.7548766002693477, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0321319901729653\n",
      "Step - 11914, Loss - 0.8143598833893533, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9643255280681389\n",
      "Step - 11915, Loss - 0.7650727088795439, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.8904153275191016\n",
      "Step - 11916, Loss - 0.6421095022923354, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3485199225740918\n",
      "Step - 11917, Loss - 0.7112728666177259, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.388612692677359\n",
      "Step - 11918, Loss - 0.8299186547641855, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4010479315848827\n",
      "Step - 11919, Loss - 0.8474450002998939, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1068968177797611\n",
      "Step - 11920, Loss - 0.7375584383320253, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1088227932483994\n",
      "Step - 11921, Loss - 0.6269450336629686, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6402693945234482\n",
      "Step - 11922, Loss - 0.7121831721760644, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7261576151327908\n",
      "Step - 11923, Loss - 0.8125374747364106, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7756724316445914\n",
      "Step - 11924, Loss - 0.7291167424794784, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4615343287260094\n",
      "Step - 11925, Loss - 0.6803530189802585, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1449707777735716\n",
      "Step - 11926, Loss - 0.612379038614169, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3648415839478327\n",
      "Step - 11927, Loss - 0.5873472009508088, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9847734853764875\n",
      "Step - 11928, Loss - 0.7002513361532663, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8487904966134581\n",
      "Step - 11929, Loss - 0.6125387269236746, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8456805655438484\n",
      "Step - 11930, Loss - 0.8135387251204724, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7206506695235961\n",
      "Step - 11931, Loss - 0.6924116295512709, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.9172790135325963\n",
      "Step - 11932, Loss - 0.6289158075684478, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9482698376206591\n",
      "Step - 11933, Loss - 0.8011407633711073, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7885596965981424\n",
      "Step - 11934, Loss - 0.7339017713188574, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.093252136205246\n",
      "Step - 11935, Loss - 0.6033479137020588, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.1391366404049302\n",
      "Step - 11936, Loss - 0.7549945435308841, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.420300241051132\n",
      "Step - 11937, Loss - 0.752122506891405, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.145397298426709\n",
      "Step - 11938, Loss - 1.0191447821937076, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.947430346139587\n",
      "Step - 11939, Loss - 0.6215448538623667, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6074582265749644\n",
      "Step - 11940, Loss - 0.5829895196215569, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.084085949077642\n",
      "Step - 11941, Loss - 0.6976347667269074, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.4849326091822258\n",
      "Step - 11942, Loss - 0.9448072371248708, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.749002156060983\n",
      "Step - 11943, Loss - 0.6050306055826751, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8943709876496483\n",
      "Step - 11944, Loss - 0.6657154304892405, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.597830868124407\n",
      "Step - 11945, Loss - 0.6549121788193711, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.5420671587629989\n",
      "Step - 11946, Loss - 0.7194020387498626, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.076456136049159\n",
      "Step - 11947, Loss - 0.6507503727326351, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.021756820873389\n",
      "Step - 11948, Loss - 0.5031544819465366, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3800191865488405\n",
      "Step - 11949, Loss - 0.664512187035204, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.3023837581355813\n",
      "Step - 11950, Loss - 0.7241066630979178, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.792988906912229\n",
      "Step - 11951, Loss - 0.6641144287515258, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.809531061655191\n",
      "Step - 11952, Loss - 0.8194483491638289, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.560376524484888\n",
      "Step - 11953, Loss - 0.5229083677275229, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.49547020539157627\n",
      "Step - 11954, Loss - 0.6414371582823244, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.2174350585020144\n",
      "Step - 11955, Loss - 0.7171176857017871, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4244092826990553\n",
      "Step - 11956, Loss - 0.8469950200489207, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.8918349117447573\n",
      "Step - 11957, Loss - 0.7285497062320544, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3522533192867539\n",
      "Step - 11958, Loss - 0.8730566030310642, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7361336695975078\n",
      "Step - 11959, Loss - 0.8743954662153491, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9808539331093508\n",
      "Step - 11960, Loss - 0.5226070236847264, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.2990612915443045\n",
      "Step - 11961, Loss - 0.6503351065006728, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.7762673193714331\n",
      "Step - 11962, Loss - 0.5967594762199764, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.6385020051477474\n",
      "Step - 11963, Loss - 0.7645675567881691, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.7529897554255863\n",
      "Step - 11964, Loss - 0.8312670667230961, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4934414836353131\n",
      "Step - 11965, Loss - 0.8190834441977998, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3269220463947582\n",
      "Step - 11966, Loss - 0.71837625296779, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4724190156190091\n",
      "Step - 11967, Loss - 0.560429394570396, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9445697689379005\n",
      "Step - 11968, Loss - 0.5617903133998263, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9146744815619026\n",
      "Step - 11969, Loss - 0.5396719936138133, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3809963676648365\n",
      "Step - 11970, Loss - 0.7391768604154059, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.616933572207768\n",
      "Step - 11971, Loss - 0.7564155988155135, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8730853347689866\n",
      "Step - 11972, Loss - 0.619876405124891, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.911016585799459\n",
      "Step - 11973, Loss - 0.5631822967235341, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0373593444077225\n",
      "Step - 11974, Loss - 0.5859538017531043, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.030912114543596\n",
      "Step - 11975, Loss - 0.8043619879868249, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.851663800535038\n",
      "Step - 11976, Loss - 0.7934728956860951, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.2402133284973585\n",
      "Step - 11977, Loss - 0.5327733117929249, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8955537175627729\n",
      "Step - 11978, Loss - 0.627020802868315, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1155858857120886\n",
      "Step - 11979, Loss - 0.6656157697242225, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1200897930652545\n",
      "Step - 11980, Loss - 0.6438198460689956, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3539584380123495\n",
      "Step - 11981, Loss - 0.8520387723675441, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.9078179274687272\n",
      "Step - 11982, Loss - 0.6051096661152798, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.655923178576806\n",
      "Step - 11983, Loss - 0.7546927532446616, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4468772570057216\n",
      "Step - 11984, Loss - 0.6063961634107523, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.167679829681834\n",
      "Step - 11985, Loss - 0.9059969458978989, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.051633709138611\n",
      "Step - 11986, Loss - 0.8362074830771065, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.1128516373755797\n",
      "Step - 11987, Loss - 0.5824309899703707, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.3341510629684348\n",
      "Step - 11988, Loss - 0.6838718284177477, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.2254921729385417\n",
      "Step - 11989, Loss - 0.47980763967598355, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.2026929959378405\n",
      "Step - 11990, Loss - 0.5361949677777914, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.1353489445341167\n",
      "Step - 11991, Loss - 0.5516275770456732, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.420099430544168\n",
      "Step - 11992, Loss - 0.6936363955197833, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.275619639810492\n",
      "Step - 11993, Loss - 0.5750997858327018, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.8829238329700743\n",
      "Step - 11994, Loss - 0.606868832376777, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.975185012752625\n",
      "Step - 11995, Loss - 0.5822149257328355, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.4077953881835318\n",
      "Step - 11996, Loss - 0.6486424220742784, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0000217169547365\n",
      "Step - 11997, Loss - 1.0195639769238773, Learning Rate - 2.44140625e-05, magnitude of gradient - 2.3124219332213456\n",
      "Step - 11998, Loss - 0.8048493190241831, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.4769343658981828\n",
      "Step - 11999, Loss - 0.597270466105477, Learning Rate - 2.44140625e-05, magnitude of gradient - 1.0401987181337602\n",
      "Step - 12000, Loss - 0.6057602624636228, Learning Rate - 2.44140625e-05, magnitude of gradient - 0.993253851140504\n",
      "Step - 12001, Loss - 0.7544538813719275, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6009340691750407\n",
      "Step - 12002, Loss - 0.7839901207162493, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4876339092793738\n",
      "Step - 12003, Loss - 0.7202578852536725, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.3921571801128003\n",
      "Step - 12004, Loss - 0.6864976860079692, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4340871066092342\n",
      "Step - 12005, Loss - 0.8262522651885267, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4995125879428659\n",
      "Step - 12006, Loss - 0.6176762967942211, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.793114234079221\n",
      "Step - 12007, Loss - 0.642130079029522, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1996839717519485\n",
      "Step - 12008, Loss - 0.571720903029587, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7689442615141389\n",
      "Step - 12009, Loss - 0.5050159799666667, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.895767791026452\n",
      "Step - 12010, Loss - 0.6176672313744228, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8265784550325214\n",
      "Step - 12011, Loss - 0.6441602492568845, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8312089572976467\n",
      "Step - 12012, Loss - 0.7013796416719249, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0038440416934298\n",
      "Step - 12013, Loss - 0.5422043568056454, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.909308203555468\n",
      "Step - 12014, Loss - 0.7727620094496273, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0920620905925842\n",
      "Step - 12015, Loss - 0.6847453348589263, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.06609266022811\n",
      "Step - 12016, Loss - 0.768169025784449, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6123456839452481\n",
      "Step - 12017, Loss - 0.8322653728724922, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5385682151720593\n",
      "Step - 12018, Loss - 0.6479770391383429, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5146993739645203\n",
      "Step - 12019, Loss - 0.7953458606638278, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8829878898194973\n",
      "Step - 12020, Loss - 0.7652072970656916, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1564613310364524\n",
      "Step - 12021, Loss - 0.9450976282368765, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4217426664386754\n",
      "Step - 12022, Loss - 0.6694109291312031, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5114432824288195\n",
      "Step - 12023, Loss - 0.766073585784957, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4275299956781569\n",
      "Step - 12024, Loss - 0.7771696452172399, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0633429681868056\n",
      "Step - 12025, Loss - 0.6551153585235199, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7150194171935373\n",
      "Step - 12026, Loss - 0.6443108856412051, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.8239216027645977\n",
      "Step - 12027, Loss - 0.7321281782317981, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4049536027667662\n",
      "Step - 12028, Loss - 0.6721519755249723, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.238956992689007\n",
      "Step - 12029, Loss - 0.8322891581514564, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8311118463323757\n",
      "Step - 12030, Loss - 0.6089981864912245, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7851837945828478\n",
      "Step - 12031, Loss - 1.0401804557725278, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7566257806303214\n",
      "Step - 12032, Loss - 0.7772457150622714, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.4035292158002424\n",
      "Step - 12033, Loss - 0.7365991449212409, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.3536115300236338\n",
      "Step - 12034, Loss - 0.6845641290004909, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.80518509552408\n",
      "Step - 12035, Loss - 0.918566308646662, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3098726993985217\n",
      "Step - 12036, Loss - 0.6639660364017563, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.706239496258622\n",
      "Step - 12037, Loss - 0.705667921709581, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6889257387544018\n",
      "Step - 12038, Loss - 0.5763139719739928, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.935970296174575\n",
      "Step - 12039, Loss - 0.7627497386265193, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6802788296769791\n",
      "Step - 12040, Loss - 0.6710017215888598, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9732660843811408\n",
      "Step - 12041, Loss - 0.6840391245623854, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.825253640110607\n",
      "Step - 12042, Loss - 0.5515947643881511, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0491605456432942\n",
      "Step - 12043, Loss - 1.0254469812768325, Learning Rate - 1.220703125e-05, magnitude of gradient - 3.0081818764192136\n",
      "Step - 12044, Loss - 0.5830321389078461, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7853467833601881\n",
      "Step - 12045, Loss - 0.7879396951322418, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1870220292461098\n",
      "Step - 12046, Loss - 0.6098134847558607, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5894165478444076\n",
      "Step - 12047, Loss - 0.8399256438785192, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7826949927680493\n",
      "Step - 12048, Loss - 0.5904425069428384, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.347135470369419\n",
      "Step - 12049, Loss - 0.7058499813966421, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0283516719202284\n",
      "Step - 12050, Loss - 0.5651095775246939, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.8814704039242116\n",
      "Step - 12051, Loss - 0.900854036036113, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.215716321117557\n",
      "Step - 12052, Loss - 0.6820563027272314, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9083158469267323\n",
      "Step - 12053, Loss - 0.6397548913701951, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8167708824392271\n",
      "Step - 12054, Loss - 0.709523503841434, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9959608615704547\n",
      "Step - 12055, Loss - 0.7850471029000337, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6242217962511234\n",
      "Step - 12056, Loss - 0.6737152004378165, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5902185325635056\n",
      "Step - 12057, Loss - 0.5918639622133501, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8678856783817486\n",
      "Step - 12058, Loss - 0.7288654708581124, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5941572989971131\n",
      "Step - 12059, Loss - 0.818259825183135, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6798482056046895\n",
      "Step - 12060, Loss - 0.7764287766052078, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.494851430505004\n",
      "Step - 12061, Loss - 0.9466277416788951, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.0452238003429253\n",
      "Step - 12062, Loss - 0.7256348288763089, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.72687327460909\n",
      "Step - 12063, Loss - 0.7939205852691841, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7203123664311896\n",
      "Step - 12064, Loss - 0.6560539266953388, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6685013881936049\n",
      "Step - 12065, Loss - 0.6137383165137824, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0774654599871147\n",
      "Step - 12066, Loss - 0.7059697983192798, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9521439963270177\n",
      "Step - 12067, Loss - 0.7138487885755556, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2128407230194076\n",
      "Step - 12068, Loss - 0.8309232765711909, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9675315258319661\n",
      "Step - 12069, Loss - 0.6805326085835643, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1285276123029826\n",
      "Step - 12070, Loss - 0.8157818196788686, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.151384963471213\n",
      "Step - 12071, Loss - 0.9695098027029486, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4998374528387908\n",
      "Step - 12072, Loss - 0.6635111459711491, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6708244069122367\n",
      "Step - 12073, Loss - 0.6829476967366773, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6480150707645365\n",
      "Step - 12074, Loss - 0.6914016683292374, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.990083505576941\n",
      "Step - 12075, Loss - 0.7003678315001235, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9104464343177305\n",
      "Step - 12076, Loss - 0.6362285726804615, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1728749643211909\n",
      "Step - 12077, Loss - 0.5908024418976398, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.102467909012839\n",
      "Step - 12078, Loss - 0.6549219745827641, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1099127373499116\n",
      "Step - 12079, Loss - 0.5579436931348343, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.732107221708902\n",
      "Step - 12080, Loss - 0.634490603059215, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.20270693654185476\n",
      "Step - 12081, Loss - 0.5751078579600084, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.74498309048603\n",
      "Step - 12082, Loss - 0.5620302541465806, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1593260864475383\n",
      "Step - 12083, Loss - 0.5660336603036686, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9657648029385897\n",
      "Step - 12084, Loss - 0.7783528540783183, Learning Rate - 1.220703125e-05, magnitude of gradient - 3.421346637215027\n",
      "Step - 12085, Loss - 0.6705630644455716, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5871436297518279\n",
      "Step - 12086, Loss - 0.71612847240929, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7772175526928424\n",
      "Step - 12087, Loss - 0.6745676237051107, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6034685378856934\n",
      "Step - 12088, Loss - 0.8094770001796143, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5901008933560804\n",
      "Step - 12089, Loss - 0.6509363146685161, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9713914489402079\n",
      "Step - 12090, Loss - 0.7403396864244278, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8604137182128403\n",
      "Step - 12091, Loss - 0.6935232710326655, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0352214647851006\n",
      "Step - 12092, Loss - 0.6357662395703431, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.2224692133481883\n",
      "Step - 12093, Loss - 0.5915011784178887, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4440599655135684\n",
      "Step - 12094, Loss - 0.7756191584376575, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.40344113574614143\n",
      "Step - 12095, Loss - 0.6900489741044433, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9648814139409392\n",
      "Step - 12096, Loss - 0.7548491780508096, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6246687631613678\n",
      "Step - 12097, Loss - 0.6477510208848042, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.739375402941489\n",
      "Step - 12098, Loss - 0.5557853961656573, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0368500888055812\n",
      "Step - 12099, Loss - 0.7866968367806199, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.086322224517544\n",
      "Step - 12100, Loss - 0.7640603290998167, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.2891683872405226\n",
      "Step - 12101, Loss - 0.6691779518225986, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7289430762726565\n",
      "Step - 12102, Loss - 0.9216111502319757, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1572030557868649\n",
      "Step - 12103, Loss - 0.7500874319127415, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9917072519889115\n",
      "Step - 12104, Loss - 0.7884915363555643, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.0414394983240567\n",
      "Step - 12105, Loss - 0.6300488107309559, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.9246963272252975\n",
      "Step - 12106, Loss - 0.8046282982731146, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2592853089849168\n",
      "Step - 12107, Loss - 0.7752619493188225, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6191084891121793\n",
      "Step - 12108, Loss - 0.5913229625508964, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1055708943221845\n",
      "Step - 12109, Loss - 0.5773160217689282, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8647268766318875\n",
      "Step - 12110, Loss - 0.7987834763548556, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6099392920724893\n",
      "Step - 12111, Loss - 0.7918470325638101, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8437778669774003\n",
      "Step - 12112, Loss - 0.7243791261031515, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1444622121669514\n",
      "Step - 12113, Loss - 0.6917983425047664, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.514166017864018\n",
      "Step - 12114, Loss - 0.6973952363287751, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0376917119501885\n",
      "Step - 12115, Loss - 0.7097581639519545, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8352954471211255\n",
      "Step - 12116, Loss - 0.7565147930599101, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.571318983086413\n",
      "Step - 12117, Loss - 0.7270500246674362, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4055491478437012\n",
      "Step - 12118, Loss - 0.7145606104765972, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1568543133861604\n",
      "Step - 12119, Loss - 0.7711808240881304, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.10309435641751\n",
      "Step - 12120, Loss - 0.7882982954186865, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0490524099971932\n",
      "Step - 12121, Loss - 0.7905707966380879, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.019728994487469\n",
      "Step - 12122, Loss - 0.7856427267565214, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.806432236498791\n",
      "Step - 12123, Loss - 0.6661755447173481, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.3934559337489016\n",
      "Step - 12124, Loss - 0.6915711333371202, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5646947199711417\n",
      "Step - 12125, Loss - 0.6816938653661575, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.307796984971179\n",
      "Step - 12126, Loss - 0.6564413295913054, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6805621210146268\n",
      "Step - 12127, Loss - 0.8251169592693672, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1580952322965996\n",
      "Step - 12128, Loss - 0.5904565908200725, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1692271280580875\n",
      "Step - 12129, Loss - 0.7294340951900031, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.31980057962261\n",
      "Step - 12130, Loss - 0.7683718418771848, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4476759293899535\n",
      "Step - 12131, Loss - 0.7500380727397042, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.373209529545067\n",
      "Step - 12132, Loss - 0.9571909047270344, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.0371043451843565\n",
      "Step - 12133, Loss - 0.7101178717360798, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2062508447753966\n",
      "Step - 12134, Loss - 0.6455744538259737, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0383029734932048\n",
      "Step - 12135, Loss - 0.6262888961169024, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6431271723594414\n",
      "Step - 12136, Loss - 0.8365934816763888, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.239701196328518\n",
      "Step - 12137, Loss - 0.6955629994380772, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1951288425257658\n",
      "Step - 12138, Loss - 0.7463653579995349, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7784587470834012\n",
      "Step - 12139, Loss - 0.6367194409801575, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.36112960063907346\n",
      "Step - 12140, Loss - 1.0113748827499496, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.5886659365209743\n",
      "Step - 12141, Loss - 0.5697749594742266, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4408601989293726\n",
      "Step - 12142, Loss - 0.7828159240506483, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5825992692351785\n",
      "Step - 12143, Loss - 0.786989646183747, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0795470729342285\n",
      "Step - 12144, Loss - 0.6831908849091634, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.47460551854275\n",
      "Step - 12145, Loss - 0.7107955327374191, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9676818606482968\n",
      "Step - 12146, Loss - 0.5700551100842208, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7389654317049017\n",
      "Step - 12147, Loss - 0.6924070943571151, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2157698170246758\n",
      "Step - 12148, Loss - 0.7382802122259003, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7565383340344234\n",
      "Step - 12149, Loss - 0.6669600411436599, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2368363989190694\n",
      "Step - 12150, Loss - 0.5587166333012521, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8584172170000985\n",
      "Step - 12151, Loss - 0.7482067173052463, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0111589764914248\n",
      "Step - 12152, Loss - 0.7374696963666147, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8655429323906279\n",
      "Step - 12153, Loss - 0.6645544945713868, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.288119072303731\n",
      "Step - 12154, Loss - 0.6516896280525102, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8921895617463504\n",
      "Step - 12155, Loss - 0.5886256672737928, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6471810676025248\n",
      "Step - 12156, Loss - 0.7733310895635922, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4366806495845659\n",
      "Step - 12157, Loss - 0.6983840817327065, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9764825464249596\n",
      "Step - 12158, Loss - 0.7654431565341175, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6458947767455133\n",
      "Step - 12159, Loss - 0.7901041733744953, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7124191024873258\n",
      "Step - 12160, Loss - 0.7828282929056262, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6173743346632463\n",
      "Step - 12161, Loss - 0.7644935254424706, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7206162299516043\n",
      "Step - 12162, Loss - 0.707441347701483, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1630319361200632\n",
      "Step - 12163, Loss - 0.9403181823549286, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9550886774102271\n",
      "Step - 12164, Loss - 0.6481228507381863, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9045994165562045\n",
      "Step - 12165, Loss - 0.6476688584567369, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9295410504603043\n",
      "Step - 12166, Loss - 0.727141792112, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5935506515241893\n",
      "Step - 12167, Loss - 0.693471379565642, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1409152519791723\n",
      "Step - 12168, Loss - 0.8822469235580412, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.3444563315019544\n",
      "Step - 12169, Loss - 0.6053077590668262, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9956268978139339\n",
      "Step - 12170, Loss - 0.6608235133007503, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0215125688230424\n",
      "Step - 12171, Loss - 0.8623905796516089, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2144666978437653\n",
      "Step - 12172, Loss - 0.7797427689391412, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6133991323155297\n",
      "Step - 12173, Loss - 0.8000391503930928, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.40401425782215517\n",
      "Step - 12174, Loss - 0.8014228351182772, Learning Rate - 1.220703125e-05, magnitude of gradient - 3.1890830831291965\n",
      "Step - 12175, Loss - 0.688483614563497, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9404547770921939\n",
      "Step - 12176, Loss - 0.9708117187118932, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1762268470501251\n",
      "Step - 12177, Loss - 0.7679629331247728, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.843750618861917\n",
      "Step - 12178, Loss - 0.7848534337137901, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6894464598430585\n",
      "Step - 12179, Loss - 0.7820252318754041, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4660463125838463\n",
      "Step - 12180, Loss - 0.4688881811098828, Learning Rate - 1.220703125e-05, magnitude of gradient - 3.0973496507319562\n",
      "Step - 12181, Loss - 0.7405592479244564, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7580071648497603\n",
      "Step - 12182, Loss - 0.8572622487136127, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5260338697229354\n",
      "Step - 12183, Loss - 0.8131726831195469, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5463431849873344\n",
      "Step - 12184, Loss - 0.6913102632018311, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5365959121468453\n",
      "Step - 12185, Loss - 0.7118778541648693, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.108399445349781\n",
      "Step - 12186, Loss - 0.6836059471074218, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5348528888616412\n",
      "Step - 12187, Loss - 0.6186381576544886, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7040005600361485\n",
      "Step - 12188, Loss - 0.868488505914172, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1341034668530885\n",
      "Step - 12189, Loss - 0.8199100628036071, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.166284629408416\n",
      "Step - 12190, Loss - 0.8244569748848859, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7331699264552884\n",
      "Step - 12191, Loss - 0.4473303848287684, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.4097759168736297\n",
      "Step - 12192, Loss - 0.7222605356571584, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5538531009866987\n",
      "Step - 12193, Loss - 0.5165693990659516, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6147945117779392\n",
      "Step - 12194, Loss - 0.7242035589760196, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.69045811898532\n",
      "Step - 12195, Loss - 0.49567110111446205, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9466569848604296\n",
      "Step - 12196, Loss - 0.6926158375195299, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.646071922821221\n",
      "Step - 12197, Loss - 0.518808657679261, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.06233929546184032\n",
      "Step - 12198, Loss - 0.7059362724816455, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7512507583677552\n",
      "Step - 12199, Loss - 0.3962416281062582, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.45633612249281996\n",
      "Step - 12200, Loss - 0.727350878987717, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9143959640030539\n",
      "Step - 12201, Loss - 0.6605231431387266, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7861584356112522\n",
      "Step - 12202, Loss - 0.6161548253552926, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2244210898803247\n",
      "Step - 12203, Loss - 0.5587528830489812, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6806411689266042\n",
      "Step - 12204, Loss - 0.602440286437741, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8038423592299452\n",
      "Step - 12205, Loss - 0.5977428486854776, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1674329719166745\n",
      "Step - 12206, Loss - 0.6067358450638389, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1643057042699407\n",
      "Step - 12207, Loss - 0.9028171385601227, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.333516885476865\n",
      "Step - 12208, Loss - 0.7105435314975177, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.394294185293652\n",
      "Step - 12209, Loss - 0.6412178016999196, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8065221483264466\n",
      "Step - 12210, Loss - 0.7392685782454171, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3694093263596445\n",
      "Step - 12211, Loss - 0.7705129869948901, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.29970916098150807\n",
      "Step - 12212, Loss - 0.8123583066641828, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1016984782863266\n",
      "Step - 12213, Loss - 0.7258447650374539, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8248220598305348\n",
      "Step - 12214, Loss - 0.7455481608629139, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7700278129397138\n",
      "Step - 12215, Loss - 0.9805190804283395, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.953343244011912\n",
      "Step - 12216, Loss - 0.553044561836384, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3159256420085401\n",
      "Step - 12217, Loss - 0.5909687809329975, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.48911384336762176\n",
      "Step - 12218, Loss - 0.8620977582408327, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.26478084500728993\n",
      "Step - 12219, Loss - 0.7324286841576817, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8869474972307902\n",
      "Step - 12220, Loss - 0.7623805765864784, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0502927826100439\n",
      "Step - 12221, Loss - 0.8547482206287315, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7645866189714912\n",
      "Step - 12222, Loss - 0.5289007595173605, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5864826030405187\n",
      "Step - 12223, Loss - 0.6062970070071664, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3772235214393889\n",
      "Step - 12224, Loss - 0.6705923690860831, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.156613446154511\n",
      "Step - 12225, Loss - 0.5972298818567677, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7166975151049542\n",
      "Step - 12226, Loss - 0.685922639099201, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5567263083347735\n",
      "Step - 12227, Loss - 0.6622767699154019, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.627002709121585\n",
      "Step - 12228, Loss - 0.841393907322864, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8708178767665768\n",
      "Step - 12229, Loss - 0.6937963462335413, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6596472163687783\n",
      "Step - 12230, Loss - 0.6804097972374072, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.2938797184038093\n",
      "Step - 12231, Loss - 0.6202178477316165, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6503732982354393\n",
      "Step - 12232, Loss - 0.8045665867199486, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4467755142434013\n",
      "Step - 12233, Loss - 0.7688851652773623, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9288485672366723\n",
      "Step - 12234, Loss - 0.8426917665596045, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1981004022305606\n",
      "Step - 12235, Loss - 0.6517893190659851, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1721656924868924\n",
      "Step - 12236, Loss - 0.8341148605838739, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5637957914048246\n",
      "Step - 12237, Loss - 0.7796379503956945, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4786378963056048\n",
      "Step - 12238, Loss - 0.7747527756738004, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0987223898533933\n",
      "Step - 12239, Loss - 0.8275733425627727, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4210750132510281\n",
      "Step - 12240, Loss - 0.7709469848714061, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9014748268826912\n",
      "Step - 12241, Loss - 0.6950036244569777, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4766042114598301\n",
      "Step - 12242, Loss - 0.8742880077645101, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5094069020621712\n",
      "Step - 12243, Loss - 0.8812189689673613, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2368440481304435\n",
      "Step - 12244, Loss - 0.6329311243486744, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7476089698498259\n",
      "Step - 12245, Loss - 0.6225407491332714, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2753669508130447\n",
      "Step - 12246, Loss - 0.709563883863173, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2351495569107531\n",
      "Step - 12247, Loss - 0.7679311892635117, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7657164412456692\n",
      "Step - 12248, Loss - 0.6524117243308719, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2487467114407533\n",
      "Step - 12249, Loss - 0.49386221981360495, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1541559631597338\n",
      "Step - 12250, Loss - 0.5430738945219739, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9351721689980437\n",
      "Step - 12251, Loss - 0.7258070155884122, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.655918105489854\n",
      "Step - 12252, Loss - 0.6134514012657388, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.424869896654104\n",
      "Step - 12253, Loss - 0.6639935260816966, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5550709617873302\n",
      "Step - 12254, Loss - 0.6175008958624822, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2213423594379165\n",
      "Step - 12255, Loss - 0.9380263925854159, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8600311265131481\n",
      "Step - 12256, Loss - 0.7090647659619447, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1954723808902712\n",
      "Step - 12257, Loss - 0.594672692886129, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9979313826625396\n",
      "Step - 12258, Loss - 0.6277167956332773, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7242438970836669\n",
      "Step - 12259, Loss - 0.7596871181842069, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4307661604628034\n",
      "Step - 12260, Loss - 0.6300099660985573, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7573106437564119\n",
      "Step - 12261, Loss - 0.6020037137208556, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4688465496256868\n",
      "Step - 12262, Loss - 0.6081833881668764, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8780029110927636\n",
      "Step - 12263, Loss - 0.8781663541342699, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9724262762488889\n",
      "Step - 12264, Loss - 0.6317110032333533, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7263708434123031\n",
      "Step - 12265, Loss - 0.6244808823031478, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.107942338886116\n",
      "Step - 12266, Loss - 0.6529796653905442, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.727808729073606\n",
      "Step - 12267, Loss - 0.6414604808899084, Learning Rate - 1.220703125e-05, magnitude of gradient - 3.800171605578643\n",
      "Step - 12268, Loss - 0.6734562105286301, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.0528217994760034\n",
      "Step - 12269, Loss - 0.5769112901467696, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8774256153877216\n",
      "Step - 12270, Loss - 0.724940386169277, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9345217879129634\n",
      "Step - 12271, Loss - 0.6111267509028548, Learning Rate - 1.220703125e-05, magnitude of gradient - 3.2858503267115413\n",
      "Step - 12272, Loss - 0.907227042679519, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.89302159596952\n",
      "Step - 12273, Loss - 0.9009507690567815, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0023272098481313\n",
      "Step - 12274, Loss - 0.7075880691104205, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8921116789956913\n",
      "Step - 12275, Loss - 0.6828554552006321, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7104154341782891\n",
      "Step - 12276, Loss - 0.8204675440930376, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9326257703169805\n",
      "Step - 12277, Loss - 0.4737190852896128, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.19367092760086\n",
      "Step - 12278, Loss - 0.8217670346050908, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4803668679617141\n",
      "Step - 12279, Loss - 0.6395657236893073, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5767242338999535\n",
      "Step - 12280, Loss - 0.8142045601540907, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8876017923061659\n",
      "Step - 12281, Loss - 0.7639105463119414, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.823474592106145\n",
      "Step - 12282, Loss - 0.8446600249664804, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.068109706207583\n",
      "Step - 12283, Loss - 0.6132746262221954, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.362870639131273\n",
      "Step - 12284, Loss - 0.7004282687206828, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.46865810356877546\n",
      "Step - 12285, Loss - 0.6980599454118636, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.3168716143129522\n",
      "Step - 12286, Loss - 0.687141345206347, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.94928832798024\n",
      "Step - 12287, Loss - 0.6683870577945372, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.129723341144827\n",
      "Step - 12288, Loss - 0.6170357496411885, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7388142689376568\n",
      "Step - 12289, Loss - 0.5949366637632114, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0591395775423311\n",
      "Step - 12290, Loss - 0.793979080243195, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.112116119693234\n",
      "Step - 12291, Loss - 0.7690652490367762, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.04766795044611\n",
      "Step - 12292, Loss - 0.5092229398895368, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.009847167898762\n",
      "Step - 12293, Loss - 0.65167843890078, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1136569813463493\n",
      "Step - 12294, Loss - 0.7362740816979588, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.369536686868938\n",
      "Step - 12295, Loss - 0.574149333041065, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0253452482185108\n",
      "Step - 12296, Loss - 0.9884570906538805, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9469069439605267\n",
      "Step - 12297, Loss - 0.6615682235035094, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.525644289206211\n",
      "Step - 12298, Loss - 0.7514054333415109, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8749492444948863\n",
      "Step - 12299, Loss - 0.7433944261757518, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0546094357109765\n",
      "Step - 12300, Loss - 0.580610277045879, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1765726529952905\n",
      "Step - 12301, Loss - 0.7318691230156744, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6115593776241404\n",
      "Step - 12302, Loss - 0.7692810341227422, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8503754528023773\n",
      "Step - 12303, Loss - 0.8736865646969699, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5858171248585828\n",
      "Step - 12304, Loss - 0.6964338317281207, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9074028531691402\n",
      "Step - 12305, Loss - 0.5015953330096709, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8492837929053427\n",
      "Step - 12306, Loss - 0.6989279181487835, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.675686374898909\n",
      "Step - 12307, Loss - 0.6461865538721672, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7246690282131871\n",
      "Step - 12308, Loss - 0.717062870235952, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0757204662333075\n",
      "Step - 12309, Loss - 0.6887934929660859, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6472694814697795\n",
      "Step - 12310, Loss - 0.6666540406964419, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.780900559803123\n",
      "Step - 12311, Loss - 0.81595157018365, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3216610613150748\n",
      "Step - 12312, Loss - 0.5178162946448749, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0589329765650148\n",
      "Step - 12313, Loss - 0.6366877385413979, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8036493601847985\n",
      "Step - 12314, Loss - 0.6925310403049894, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9769020466108754\n",
      "Step - 12315, Loss - 0.8546301329961719, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6868684687506421\n",
      "Step - 12316, Loss - 0.6344471865726086, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3205627466774936\n",
      "Step - 12317, Loss - 0.8019897791042798, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6455788724483048\n",
      "Step - 12318, Loss - 0.5747263396565465, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.47566536520427216\n",
      "Step - 12319, Loss - 0.938408040295653, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9833193847038045\n",
      "Step - 12320, Loss - 0.6842508472131559, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5172347394031453\n",
      "Step - 12321, Loss - 0.600114189308202, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7763071713244136\n",
      "Step - 12322, Loss - 0.5140894095513642, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.594155831210239\n",
      "Step - 12323, Loss - 0.9068452800285802, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5785564990289778\n",
      "Step - 12324, Loss - 0.7302175511243136, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9588240010492823\n",
      "Step - 12325, Loss - 0.8196220620189898, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.669038987028595\n",
      "Step - 12326, Loss - 0.8167833506828311, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.685701586144336\n",
      "Step - 12327, Loss - 0.752432358664534, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3635654327513345\n",
      "Step - 12328, Loss - 0.6558097944697049, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8102397360962196\n",
      "Step - 12329, Loss - 0.49026350058632223, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5460722141341842\n",
      "Step - 12330, Loss - 0.5660277875558766, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9836013161819726\n",
      "Step - 12331, Loss - 0.7754595794972838, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0600445489608594\n",
      "Step - 12332, Loss - 0.6527140327664676, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.831607178842583\n",
      "Step - 12333, Loss - 0.7687126144484389, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7012121873165948\n",
      "Step - 12334, Loss - 0.9178303266878344, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.5959200493312866\n",
      "Step - 12335, Loss - 0.5687694538804631, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9203361727521203\n",
      "Step - 12336, Loss - 0.6933723629952336, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.428776353786514\n",
      "Step - 12337, Loss - 0.5531723967045926, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8102058970824062\n",
      "Step - 12338, Loss - 0.9306085055508502, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3985070311304337\n",
      "Step - 12339, Loss - 0.7064112386844282, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.738724933988665\n",
      "Step - 12340, Loss - 0.6672816407855741, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.888719128228923\n",
      "Step - 12341, Loss - 0.5664421798936609, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3869216910549242\n",
      "Step - 12342, Loss - 0.7908694006722766, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4816198546393227\n",
      "Step - 12343, Loss - 0.6581441281527912, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.3677261755567003\n",
      "Step - 12344, Loss - 0.6004242929299518, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.6119439364512136\n",
      "Step - 12345, Loss - 0.7691500891705484, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8374958223739611\n",
      "Step - 12346, Loss - 0.7263711986398352, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2021774700409047\n",
      "Step - 12347, Loss - 0.7326060772924092, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4686254945456176\n",
      "Step - 12348, Loss - 0.7275364907232689, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.894367039578843\n",
      "Step - 12349, Loss - 0.5856148165719516, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9057897000134424\n",
      "Step - 12350, Loss - 0.5574311960583409, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.3425531231630352\n",
      "Step - 12351, Loss - 0.6933213801938588, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.301286861167365\n",
      "Step - 12352, Loss - 0.8983974042724105, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.149231975668519\n",
      "Step - 12353, Loss - 0.7722045700413657, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4758663829102903\n",
      "Step - 12354, Loss - 0.6640513042220062, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.210223748424536\n",
      "Step - 12355, Loss - 0.5773890425902157, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1030852420951824\n",
      "Step - 12356, Loss - 0.7200979856223665, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2924218645736476\n",
      "Step - 12357, Loss - 0.7609515415797242, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3670732735175835\n",
      "Step - 12358, Loss - 0.634390097072903, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6526357670834512\n",
      "Step - 12359, Loss - 0.51663871074745, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6687135919774352\n",
      "Step - 12360, Loss - 0.6679860503422757, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.831149839455726\n",
      "Step - 12361, Loss - 0.5313251046270625, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7183322852622265\n",
      "Step - 12362, Loss - 0.5642337099541537, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.027459742761357\n",
      "Step - 12363, Loss - 0.8038254410852759, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3583492127134085\n",
      "Step - 12364, Loss - 0.8041121043443492, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8165756212541596\n",
      "Step - 12365, Loss - 0.7661799121307284, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7896837604363984\n",
      "Step - 12366, Loss - 0.6424433348159606, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5426155498890156\n",
      "Step - 12367, Loss - 0.7096651720980512, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1726383157784452\n",
      "Step - 12368, Loss - 0.9842965412846318, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.884865672420429\n",
      "Step - 12369, Loss - 0.6886384874766058, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.179508591909978\n",
      "Step - 12370, Loss - 0.7239245742100905, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2306485399949272\n",
      "Step - 12371, Loss - 0.7057739437330139, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.49578655279599126\n",
      "Step - 12372, Loss - 0.7236108080247903, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3234410849717977\n",
      "Step - 12373, Loss - 0.7392151239898439, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.411270909414586\n",
      "Step - 12374, Loss - 0.6189608189762048, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2460206745641034\n",
      "Step - 12375, Loss - 0.7447960688295912, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9565131956040717\n",
      "Step - 12376, Loss - 0.7954592098579079, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4507039704814139\n",
      "Step - 12377, Loss - 0.6328974392799014, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7658205919654748\n",
      "Step - 12378, Loss - 0.7845862809303242, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6319080247188467\n",
      "Step - 12379, Loss - 0.5853717684096705, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7699013518540279\n",
      "Step - 12380, Loss - 0.5592302905251127, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.446912580389825\n",
      "Step - 12381, Loss - 0.6284481711342385, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9992582142665593\n",
      "Step - 12382, Loss - 0.6721618218705283, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3935222819871536\n",
      "Step - 12383, Loss - 0.6255205298318388, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5509237923301524\n",
      "Step - 12384, Loss - 0.7239940710342659, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5067072921855529\n",
      "Step - 12385, Loss - 0.7723333360137377, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9114986547237905\n",
      "Step - 12386, Loss - 0.7158764781540821, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5275225943871141\n",
      "Step - 12387, Loss - 0.6220108152437904, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6518875894839171\n",
      "Step - 12388, Loss - 0.7353982720435375, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0094888380693827\n",
      "Step - 12389, Loss - 0.5701183634895615, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7279772106168187\n",
      "Step - 12390, Loss - 0.7610058257657691, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.857953001879243\n",
      "Step - 12391, Loss - 0.618824505892354, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2959353731484393\n",
      "Step - 12392, Loss - 0.7097658880299184, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.2681474445492205\n",
      "Step - 12393, Loss - 0.8678135857002177, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7058980386367095\n",
      "Step - 12394, Loss - 0.6666818531769643, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.576522512698666\n",
      "Step - 12395, Loss - 0.7697538090640278, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.33840290810901386\n",
      "Step - 12396, Loss - 0.6174336679119148, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.226049424077247\n",
      "Step - 12397, Loss - 0.5434005734895421, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8237998348511566\n",
      "Step - 12398, Loss - 0.5570868278909438, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1960659636535138\n",
      "Step - 12399, Loss - 0.6562733381016644, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6575720135069268\n",
      "Step - 12400, Loss - 0.802727281760939, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1736446285342719\n",
      "Step - 12401, Loss - 0.7086082832764866, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5759690541475768\n",
      "Step - 12402, Loss - 0.5935627110681505, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4193569476359982\n",
      "Step - 12403, Loss - 0.7951756634523481, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1000955360332947\n",
      "Step - 12404, Loss - 0.6128952632108802, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.3892763835212561\n",
      "Step - 12405, Loss - 0.8612929205187361, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.3552209548555036\n",
      "Step - 12406, Loss - 0.6578823794021732, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.458739375519578\n",
      "Step - 12407, Loss - 0.8143245765908784, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2802339670976925\n",
      "Step - 12408, Loss - 0.7366542201568375, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1535360993592918\n",
      "Step - 12409, Loss - 0.8206485105388946, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3785934743331558\n",
      "Step - 12410, Loss - 0.6062958840680167, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.662935766669709\n",
      "Step - 12411, Loss - 0.8496224480779391, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8609298931719925\n",
      "Step - 12412, Loss - 0.6866409330472467, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6332238369442981\n",
      "Step - 12413, Loss - 0.6445556546936789, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4240032121378894\n",
      "Step - 12414, Loss - 0.8251373002367799, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1214126524478132\n",
      "Step - 12415, Loss - 0.5878894305884838, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2420431912840633\n",
      "Step - 12416, Loss - 0.5385804298093942, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.378639638787332\n",
      "Step - 12417, Loss - 0.6725156999443257, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9690199473694517\n",
      "Step - 12418, Loss - 0.7045925022369889, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8330115204644803\n",
      "Step - 12419, Loss - 0.8136457837767671, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1273928395612383\n",
      "Step - 12420, Loss - 0.5275870533066944, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.3073111594582141\n",
      "Step - 12421, Loss - 0.6700568655819908, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4915389739660156\n",
      "Step - 12422, Loss - 0.6403151386172289, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9923227261488067\n",
      "Step - 12423, Loss - 0.8567944184279987, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.32946023119730683\n",
      "Step - 12424, Loss - 0.6913270227527243, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.625678146977959\n",
      "Step - 12425, Loss - 0.5623822183376479, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7471240122756315\n",
      "Step - 12426, Loss - 0.776139178600808, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9669304059964087\n",
      "Step - 12427, Loss - 0.857357921265389, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.510162086841487\n",
      "Step - 12428, Loss - 0.8598585401908168, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2923649966418083\n",
      "Step - 12429, Loss - 0.6359779571954349, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7363646979519397\n",
      "Step - 12430, Loss - 0.6721612981650422, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.0046065168311293\n",
      "Step - 12431, Loss - 0.7857673594887886, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1673608692924744\n",
      "Step - 12432, Loss - 0.7677909207800824, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5087614668061545\n",
      "Step - 12433, Loss - 0.8862019547689532, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7069793251732255\n",
      "Step - 12434, Loss - 0.5991466692293984, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.067002497461602\n",
      "Step - 12435, Loss - 0.7193256555907667, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.111708176187718\n",
      "Step - 12436, Loss - 0.6150180459573202, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.5005472888024327\n",
      "Step - 12437, Loss - 0.7527978173948792, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.774050092026167\n",
      "Step - 12438, Loss - 0.5670217849689978, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9135923025595116\n",
      "Step - 12439, Loss - 0.6655026499004849, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.28530132398390856\n",
      "Step - 12440, Loss - 0.6634948523250666, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7020026919643706\n",
      "Step - 12441, Loss - 0.8570172933134654, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8291823151586033\n",
      "Step - 12442, Loss - 0.7340435330028211, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7618423681894073\n",
      "Step - 12443, Loss - 0.69913777412998, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.463939724725572\n",
      "Step - 12444, Loss - 0.6624702244242123, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.339363634459052\n",
      "Step - 12445, Loss - 0.7447096680225389, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5004206496118464\n",
      "Step - 12446, Loss - 0.4480069927111364, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8931627968561283\n",
      "Step - 12447, Loss - 0.7843141326093368, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.633160719966949\n",
      "Step - 12448, Loss - 0.7867924715375122, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1571294083235024\n",
      "Step - 12449, Loss - 0.5411692872965276, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3568017873893226\n",
      "Step - 12450, Loss - 0.5759755919198438, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1237609833089697\n",
      "Step - 12451, Loss - 0.6026535662584862, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6555235399587962\n",
      "Step - 12452, Loss - 0.7050923100261118, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9762262945158457\n",
      "Step - 12453, Loss - 0.7228500937973453, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5501362488558603\n",
      "Step - 12454, Loss - 0.731196694933336, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6906276440946894\n",
      "Step - 12455, Loss - 0.8418912316014704, Learning Rate - 1.220703125e-05, magnitude of gradient - 3.1553969719262334\n",
      "Step - 12456, Loss - 0.6960749331985567, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.319710741512356\n",
      "Step - 12457, Loss - 0.6427957513709821, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7749162868285118\n",
      "Step - 12458, Loss - 0.48230918754612007, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9419894284781529\n",
      "Step - 12459, Loss - 0.7743294541041055, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5941786346803369\n",
      "Step - 12460, Loss - 0.6015801630823528, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8608705640688356\n",
      "Step - 12461, Loss - 0.6613658019910567, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.866739822815152\n",
      "Step - 12462, Loss - 0.8360315188006299, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.388644877390173\n",
      "Step - 12463, Loss - 0.5556739959177353, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9310638169422374\n",
      "Step - 12464, Loss - 0.5512753250756075, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6530017545859184\n",
      "Step - 12465, Loss - 0.5963313858008599, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6208343755886239\n",
      "Step - 12466, Loss - 0.7342527479153487, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6777745376770877\n",
      "Step - 12467, Loss - 0.8753355046283693, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9491323849253537\n",
      "Step - 12468, Loss - 0.8142237184101294, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7777528248476043\n",
      "Step - 12469, Loss - 0.60590056200664, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6746428998199345\n",
      "Step - 12470, Loss - 0.7328656053464391, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9332723853767068\n",
      "Step - 12471, Loss - 0.6440254625888373, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6538159489469337\n",
      "Step - 12472, Loss - 0.4701639126054402, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.0300794118668466\n",
      "Step - 12473, Loss - 0.7402546054848221, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6143677206614833\n",
      "Step - 12474, Loss - 0.6972790276276164, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.9422528400996013\n",
      "Step - 12475, Loss - 0.8282998610524741, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.770228499143549\n",
      "Step - 12476, Loss - 0.8657457098906987, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2379472488739123\n",
      "Step - 12477, Loss - 0.8933374546136801, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0234818984926148\n",
      "Step - 12478, Loss - 0.7890180020932176, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8038065593800858\n",
      "Step - 12479, Loss - 0.8023519884544543, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7878262163150738\n",
      "Step - 12480, Loss - 0.6012382172977734, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.886476566977322\n",
      "Step - 12481, Loss - 0.6005133744571127, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1297932313114694\n",
      "Step - 12482, Loss - 0.5960908473040731, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6183267394875294\n",
      "Step - 12483, Loss - 0.819616319856006, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.107152776324225\n",
      "Step - 12484, Loss - 0.6181307985898237, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5632467922610023\n",
      "Step - 12485, Loss - 0.683188255572377, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8156729124327308\n",
      "Step - 12486, Loss - 0.7423076223535441, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.194082894822904\n",
      "Step - 12487, Loss - 0.8157124485187641, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.878351597571339\n",
      "Step - 12488, Loss - 0.6246353215417586, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6863504136891398\n",
      "Step - 12489, Loss - 0.7978273423347919, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.2510736199953422\n",
      "Step - 12490, Loss - 0.8051521783320719, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6695372739761045\n",
      "Step - 12491, Loss - 0.8256746562436031, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.156248321541192\n",
      "Step - 12492, Loss - 0.6707447983240646, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7638246463983192\n",
      "Step - 12493, Loss - 0.8022230398968144, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1431352176657865\n",
      "Step - 12494, Loss - 0.6454704563456786, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5443036019221028\n",
      "Step - 12495, Loss - 0.6356631714038001, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4090545408898085\n",
      "Step - 12496, Loss - 0.8470874673397242, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8015513313242763\n",
      "Step - 12497, Loss - 0.7129797646185514, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8682670173665525\n",
      "Step - 12498, Loss - 0.7735763692260477, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8531149909788502\n",
      "Step - 12499, Loss - 0.7566735469551737, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0431136373913963\n",
      "Step - 12500, Loss - 0.7537407761747579, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.623874971783784\n",
      "Step - 12501, Loss - 0.7128565771399596, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8425143198367808\n",
      "Step - 12502, Loss - 0.7539002521533832, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7940410530878554\n",
      "Step - 12503, Loss - 0.6069771883202016, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6435596450944127\n",
      "Step - 12504, Loss - 0.6920340850007157, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5652512279840725\n",
      "Step - 12505, Loss - 0.6903148277926471, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8800613373807693\n",
      "Step - 12506, Loss - 0.5638949181017169, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0717320719146186\n",
      "Step - 12507, Loss - 0.7456391332901106, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5091740399460438\n",
      "Step - 12508, Loss - 0.6612076854160307, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4930504547650434\n",
      "Step - 12509, Loss - 0.7298349515550562, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9033157757157225\n",
      "Step - 12510, Loss - 0.6946675965701362, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4553758434745927\n",
      "Step - 12511, Loss - 0.519307479808957, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0355997400348937\n",
      "Step - 12512, Loss - 0.5968582965042017, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8784898971718698\n",
      "Step - 12513, Loss - 1.0354205128524745, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7180254477837038\n",
      "Step - 12514, Loss - 0.5701111759060261, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9082206088856877\n",
      "Step - 12515, Loss - 0.9030370195738713, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3491593123104804\n",
      "Step - 12516, Loss - 0.7888010223721516, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.86632399545906\n",
      "Step - 12517, Loss - 0.6043034393055559, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.389105339735471\n",
      "Step - 12518, Loss - 0.6534782904104568, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4196731226635633\n",
      "Step - 12519, Loss - 0.6854029713321612, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4769391998543952\n",
      "Step - 12520, Loss - 0.606888745785614, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9987293641637971\n",
      "Step - 12521, Loss - 0.4515531613296532, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3371907136260712\n",
      "Step - 12522, Loss - 0.776184421721821, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8004841114301301\n",
      "Step - 12523, Loss - 0.7213294643727766, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.11003834080916\n",
      "Step - 12524, Loss - 0.5810087011212954, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.3453994672190028\n",
      "Step - 12525, Loss - 0.6531394750083034, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7744921416872588\n",
      "Step - 12526, Loss - 0.7130293653344262, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.3753471909030996\n",
      "Step - 12527, Loss - 0.9178448971275518, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1793092969967884\n",
      "Step - 12528, Loss - 0.5849007402331492, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7005181232506814\n",
      "Step - 12529, Loss - 0.49502539793007017, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9893706209245481\n",
      "Step - 12530, Loss - 0.8812152840717422, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1150821342189297\n",
      "Step - 12531, Loss - 0.643649223366313, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.274365240153955\n",
      "Step - 12532, Loss - 0.5606989706857888, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5171634308946547\n",
      "Step - 12533, Loss - 0.5186887000503969, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.726235729839393\n",
      "Step - 12534, Loss - 0.6532128087684135, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1611924786709868\n",
      "Step - 12535, Loss - 0.626934532619023, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.546481306951529\n",
      "Step - 12536, Loss - 0.7600264109277143, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2774107238558903\n",
      "Step - 12537, Loss - 0.8968289825829514, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.57906333196849\n",
      "Step - 12538, Loss - 0.6264818275471841, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8826677668889252\n",
      "Step - 12539, Loss - 0.6055343982566793, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9803882305713235\n",
      "Step - 12540, Loss - 0.9020245165835504, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.2418192163962622\n",
      "Step - 12541, Loss - 0.804242367874017, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1025640185463517\n",
      "Step - 12542, Loss - 0.6131249718071228, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8613652017700391\n",
      "Step - 12543, Loss - 0.6677848712047743, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0031339274312567\n",
      "Step - 12544, Loss - 0.7356696169636466, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0978710323971543\n",
      "Step - 12545, Loss - 0.6837905184061854, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5351229710665795\n",
      "Step - 12546, Loss - 0.7579962716352642, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9003224043425528\n",
      "Step - 12547, Loss - 0.8336574436779491, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4259547709280602\n",
      "Step - 12548, Loss - 0.8931268464147087, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.397864295955729\n",
      "Step - 12549, Loss - 0.5715090410659677, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3158619812109493\n",
      "Step - 12550, Loss - 0.8431218757523032, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1428467672850213\n",
      "Step - 12551, Loss - 0.7241267258174812, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9192147720409699\n",
      "Step - 12552, Loss - 0.7293785036630152, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0502610947988456\n",
      "Step - 12553, Loss - 0.5807312894795609, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3624825694778344\n",
      "Step - 12554, Loss - 0.6945519634703271, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.150336086957593\n",
      "Step - 12555, Loss - 0.8578986459390435, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8591069228622373\n",
      "Step - 12556, Loss - 0.7127012827762047, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6570814583899783\n",
      "Step - 12557, Loss - 0.7896132071829322, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3296296423916292\n",
      "Step - 12558, Loss - 0.7148478425347964, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1651493467999716\n",
      "Step - 12559, Loss - 0.7129084176258689, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.853556278985717\n",
      "Step - 12560, Loss - 0.8063590033290771, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9705482301646416\n",
      "Step - 12561, Loss - 0.696135957139164, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9501880632800694\n",
      "Step - 12562, Loss - 0.7264016571976653, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7052750520324238\n",
      "Step - 12563, Loss - 0.6893852727996863, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6373307360239108\n",
      "Step - 12564, Loss - 0.7619880863078126, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2079040874091111\n",
      "Step - 12565, Loss - 0.6911304016645026, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.211774540275258\n",
      "Step - 12566, Loss - 0.6006163789534738, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6713310372230028\n",
      "Step - 12567, Loss - 0.7310219721706233, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.601783113181826\n",
      "Step - 12568, Loss - 0.7644368985961181, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5494212765890328\n",
      "Step - 12569, Loss - 0.6189313660263991, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9045924017904325\n",
      "Step - 12570, Loss - 0.83106432965717, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0942873649416691\n",
      "Step - 12571, Loss - 0.7757195265438652, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.770409030349231\n",
      "Step - 12572, Loss - 0.6503911666048743, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.389052824076471\n",
      "Step - 12573, Loss - 0.6792819115533247, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.281497403890953\n",
      "Step - 12574, Loss - 0.8874867238643978, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.615616859327854\n",
      "Step - 12575, Loss - 0.612887363792286, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.16534793051302482\n",
      "Step - 12576, Loss - 0.7456846284695223, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8309599393965266\n",
      "Step - 12577, Loss - 0.6328086220875295, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2546451227854654\n",
      "Step - 12578, Loss - 0.7258307724303268, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3724546681974579\n",
      "Step - 12579, Loss - 0.6115143810726276, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9585219776756975\n",
      "Step - 12580, Loss - 0.886592121639005, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3161860064247592\n",
      "Step - 12581, Loss - 0.5568291064415697, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.366991473993338\n",
      "Step - 12582, Loss - 0.5991857667546973, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8352859583711576\n",
      "Step - 12583, Loss - 0.598489640126972, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.24363399289118262\n",
      "Step - 12584, Loss - 0.8255337599738055, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.662387665310314\n",
      "Step - 12585, Loss - 0.7407436897495432, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4046852819874145\n",
      "Step - 12586, Loss - 0.7383946167580944, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9068083356164074\n",
      "Step - 12587, Loss - 0.5382102488464411, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.644253445729684\n",
      "Step - 12588, Loss - 0.6501708401087454, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2008636624041873\n",
      "Step - 12589, Loss - 0.7326178737455763, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.092994978397399\n",
      "Step - 12590, Loss - 0.8779811841385449, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5688477873220334\n",
      "Step - 12591, Loss - 0.6146199662008429, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7084922387781699\n",
      "Step - 12592, Loss - 0.7283651251186616, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2210505118485477\n",
      "Step - 12593, Loss - 0.6365575476530154, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.183686379155842\n",
      "Step - 12594, Loss - 0.5612134739666328, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.953376657536096\n",
      "Step - 12595, Loss - 0.844740140141914, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.885778160107183\n",
      "Step - 12596, Loss - 0.6331408018621366, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8226677283158813\n",
      "Step - 12597, Loss - 0.9172511409944445, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4031711152780626\n",
      "Step - 12598, Loss - 0.6681026755097202, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7069271544135561\n",
      "Step - 12599, Loss - 0.6785738336893147, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8672862252801902\n",
      "Step - 12600, Loss - 0.5447745651341609, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.351066035982851\n",
      "Step - 12601, Loss - 0.6599327268561265, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8225841692511769\n",
      "Step - 12602, Loss - 0.6912566784281828, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8749330609621124\n",
      "Step - 12603, Loss - 0.8381169463755136, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8910099739212457\n",
      "Step - 12604, Loss - 0.852601641741061, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.453946629057526\n",
      "Step - 12605, Loss - 0.524188098119028, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9878252106038026\n",
      "Step - 12606, Loss - 0.6495384947936962, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9031625124502102\n",
      "Step - 12607, Loss - 0.5272944528435921, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7374260410068582\n",
      "Step - 12608, Loss - 0.6438473045580486, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1563932407825785\n",
      "Step - 12609, Loss - 0.7094494356367977, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7413964810958905\n",
      "Step - 12610, Loss - 0.5660627694412579, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9956676495325409\n",
      "Step - 12611, Loss - 0.7633278478712846, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0564715806369676\n",
      "Step - 12612, Loss - 0.7880325198303273, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3913401697472298\n",
      "Step - 12613, Loss - 0.8466178699515631, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.31696132990442155\n",
      "Step - 12614, Loss - 0.5230669096150926, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4731779318963385\n",
      "Step - 12615, Loss - 0.7835952885076573, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.215341318836421\n",
      "Step - 12616, Loss - 0.7050316777265979, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5430463784811632\n",
      "Step - 12617, Loss - 0.7355338047720633, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6923592756874244\n",
      "Step - 12618, Loss - 0.8882502879918066, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3048331356744574\n",
      "Step - 12619, Loss - 0.42730080502041323, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.15340520760919\n",
      "Step - 12620, Loss - 0.7508332360974016, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0354228036997029\n",
      "Step - 12621, Loss - 0.560949910167665, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0211221564830273\n",
      "Step - 12622, Loss - 0.6979052782575533, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0318414755701049\n",
      "Step - 12623, Loss - 0.6341235179118739, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9409756779561111\n",
      "Step - 12624, Loss - 0.7333664548556984, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2787160775194744\n",
      "Step - 12625, Loss - 0.6883865616058952, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.068074909110009\n",
      "Step - 12626, Loss - 0.843757622504364, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6706197910498828\n",
      "Step - 12627, Loss - 0.7103962304434795, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7324755088525786\n",
      "Step - 12628, Loss - 0.5008758347031144, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.472403421587403\n",
      "Step - 12629, Loss - 0.5931760224691343, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8384575294325731\n",
      "Step - 12630, Loss - 0.7544084495050766, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1826899448517265\n",
      "Step - 12631, Loss - 0.6720427357578626, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.275132952368758\n",
      "Step - 12632, Loss - 0.7155115609269311, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.3762215474372332\n",
      "Step - 12633, Loss - 0.4512391724508107, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.607431650053273\n",
      "Step - 12634, Loss - 0.7467597462902763, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.356102837288187\n",
      "Step - 12635, Loss - 0.6815937510063977, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3728625603684783\n",
      "Step - 12636, Loss - 0.5837044897437184, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5582001069042763\n",
      "Step - 12637, Loss - 0.779349187332392, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6067083963889974\n",
      "Step - 12638, Loss - 1.0650497154308332, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.964575595542182\n",
      "Step - 12639, Loss - 0.7551317935349464, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.807257256703069\n",
      "Step - 12640, Loss - 0.7792116740801732, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9524422723140435\n",
      "Step - 12641, Loss - 0.7614602187755808, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.3519628561381683\n",
      "Step - 12642, Loss - 0.7222348253249352, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1350229953829516\n",
      "Step - 12643, Loss - 0.847721547910744, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6199088134202271\n",
      "Step - 12644, Loss - 0.7792188237220086, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9831091687927835\n",
      "Step - 12645, Loss - 0.5687095341848701, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3295131260145954\n",
      "Step - 12646, Loss - 0.531370224241605, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5398405950998237\n",
      "Step - 12647, Loss - 0.6360444894600594, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5896872989595077\n",
      "Step - 12648, Loss - 0.6172410923834876, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4483704382452633\n",
      "Step - 12649, Loss - 0.5289945054743483, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.534675254609808\n",
      "Step - 12650, Loss - 0.6311857863054354, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6099755633962926\n",
      "Step - 12651, Loss - 0.7037606712413953, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9317295364125414\n",
      "Step - 12652, Loss - 0.6205008392759527, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.2932976769401856\n",
      "Step - 12653, Loss - 0.8686795922626726, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.676367905621604\n",
      "Step - 12654, Loss - 0.8780482405137441, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8561899463799437\n",
      "Step - 12655, Loss - 0.7654380028101254, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8042107901347774\n",
      "Step - 12656, Loss - 0.7531834975342314, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4103561551547277\n",
      "Step - 12657, Loss - 0.6899334302561375, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2445349288245733\n",
      "Step - 12658, Loss - 0.7025478690763391, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9849307098511029\n",
      "Step - 12659, Loss - 0.8047751851078329, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.748280832266408\n",
      "Step - 12660, Loss - 0.7973060388497364, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6236964200530025\n",
      "Step - 12661, Loss - 0.7509365676808992, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5156343601488075\n",
      "Step - 12662, Loss - 0.55946840990918, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7907022474524854\n",
      "Step - 12663, Loss - 0.7115731080352885, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3370001544441317\n",
      "Step - 12664, Loss - 0.7970290731789054, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.502658231628156\n",
      "Step - 12665, Loss - 0.4670337319308716, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9227494866951338\n",
      "Step - 12666, Loss - 0.8731999157347762, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.715854424565182\n",
      "Step - 12667, Loss - 0.45524752644116423, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3288971564078318\n",
      "Step - 12668, Loss - 0.6138926464558339, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7280527848736599\n",
      "Step - 12669, Loss - 0.7232689528765852, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7662353110258117\n",
      "Step - 12670, Loss - 0.7274704360778017, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6529661519751646\n",
      "Step - 12671, Loss - 0.642707307198821, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1801936577518204\n",
      "Step - 12672, Loss - 0.696275069259226, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5248074645919076\n",
      "Step - 12673, Loss - 0.6157172000218227, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7146373490286368\n",
      "Step - 12674, Loss - 0.7064681304849483, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7790836284616688\n",
      "Step - 12675, Loss - 0.5218164997358431, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.48460715018932704\n",
      "Step - 12676, Loss - 0.7491121196027308, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6497800906818663\n",
      "Step - 12677, Loss - 0.6515026988859693, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.759249697165326\n",
      "Step - 12678, Loss - 0.826338462139254, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4672127581799344\n",
      "Step - 12679, Loss - 0.6825157896096963, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6312553935792227\n",
      "Step - 12680, Loss - 0.7189596129733743, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3591756730763285\n",
      "Step - 12681, Loss - 0.8070391317345396, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6093853175676838\n",
      "Step - 12682, Loss - 0.7046939964675163, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3040758418672729\n",
      "Step - 12683, Loss - 0.6160346794098474, Learning Rate - 1.220703125e-05, magnitude of gradient - 3.576946781399304\n",
      "Step - 12684, Loss - 0.6195816045554284, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.0126651016299553\n",
      "Step - 12685, Loss - 0.5338030642974783, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.354214082519336\n",
      "Step - 12686, Loss - 0.48885159237948805, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6670195271309247\n",
      "Step - 12687, Loss - 0.8002062058275421, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5252158622463032\n",
      "Step - 12688, Loss - 0.730295780232366, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.2229984055822785\n",
      "Step - 12689, Loss - 0.655193512348428, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4220341461629895\n",
      "Step - 12690, Loss - 0.8338915768121561, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6495324052509006\n",
      "Step - 12691, Loss - 0.6863989617938342, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0311234312005917\n",
      "Step - 12692, Loss - 0.807521727072758, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.4160547208011716\n",
      "Step - 12693, Loss - 0.7614377055366981, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0435826236026267\n",
      "Step - 12694, Loss - 0.5938258021459746, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.54080080495456\n",
      "Step - 12695, Loss - 0.5590664839473319, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.914489534397207\n",
      "Step - 12696, Loss - 0.7055927849995864, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2579402195927984\n",
      "Step - 12697, Loss - 0.6143898466355661, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5326212601248967\n",
      "Step - 12698, Loss - 0.7103609789233284, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9727175336121803\n",
      "Step - 12699, Loss - 0.5839777634727216, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1053213616084372\n",
      "Step - 12700, Loss - 0.7182422319810494, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0302947609490387\n",
      "Step - 12701, Loss - 0.8375524797280922, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2570728081124942\n",
      "Step - 12702, Loss - 0.5252552242313617, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4432028319648733\n",
      "Step - 12703, Loss - 0.7245845795425272, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0807957665611434\n",
      "Step - 12704, Loss - 0.3902048635221525, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3507867917642395\n",
      "Step - 12705, Loss - 0.982826548795036, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.182019393413694\n",
      "Step - 12706, Loss - 0.6642157987108761, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6164630122651216\n",
      "Step - 12707, Loss - 0.5406507655577035, Learning Rate - 1.220703125e-05, magnitude of gradient - 3.6486113124301855\n",
      "Step - 12708, Loss - 0.6937189677478632, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1643834019478077\n",
      "Step - 12709, Loss - 0.7958605392172196, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.3012558148756756\n",
      "Step - 12710, Loss - 0.7901600883381502, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2432479127916565\n",
      "Step - 12711, Loss - 0.617452940043529, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3430021309596143\n",
      "Step - 12712, Loss - 0.5401432646263613, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1211424394325227\n",
      "Step - 12713, Loss - 0.738181645644431, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6027417698824503\n",
      "Step - 12714, Loss - 0.7320547102869528, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7209426828996983\n",
      "Step - 12715, Loss - 0.7514311275403792, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7188079480309782\n",
      "Step - 12716, Loss - 0.6041704311287912, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0843800187092971\n",
      "Step - 12717, Loss - 0.5416634879519544, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.25462397044816\n",
      "Step - 12718, Loss - 0.6286600328723685, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.578180701980423\n",
      "Step - 12719, Loss - 0.7234437162569934, Learning Rate - 1.220703125e-05, magnitude of gradient - 3.2883127189679615\n",
      "Step - 12720, Loss - 0.825342056714532, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1171937536377277\n",
      "Step - 12721, Loss - 0.5930473095064213, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7721562074918034\n",
      "Step - 12722, Loss - 0.552932814824645, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4511484687287357\n",
      "Step - 12723, Loss - 0.7112378434134312, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.422552559683566\n",
      "Step - 12724, Loss - 0.6943858229861877, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5359648014491805\n",
      "Step - 12725, Loss - 0.8523221612746633, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7831734613154333\n",
      "Step - 12726, Loss - 0.4987832603363321, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2302977909843646\n",
      "Step - 12727, Loss - 0.693851642163226, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8237835545825964\n",
      "Step - 12728, Loss - 0.5614871582160705, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9802555811129258\n",
      "Step - 12729, Loss - 0.8093609403572456, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5148216995499895\n",
      "Step - 12730, Loss - 0.6413796892349604, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.069505195603412\n",
      "Step - 12731, Loss - 0.6427384442544783, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2899354922327684\n",
      "Step - 12732, Loss - 0.7281577249398126, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.3114555439911837\n",
      "Step - 12733, Loss - 0.9458577194740788, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6539491611752265\n",
      "Step - 12734, Loss - 0.9077390461852064, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4849126035385647\n",
      "Step - 12735, Loss - 0.6374648297296444, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7941820191473191\n",
      "Step - 12736, Loss - 1.0026190688982253, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.2683315787435188\n",
      "Step - 12737, Loss - 0.8160891496192341, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.2066276109525167\n",
      "Step - 12738, Loss - 0.41408650527947327, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4483179193034932\n",
      "Step - 12739, Loss - 0.6646184830850201, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2633577250384473\n",
      "Step - 12740, Loss - 0.7341016862864675, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.0211283646939124\n",
      "Step - 12741, Loss - 0.7267200685999895, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8260515999802882\n",
      "Step - 12742, Loss - 0.881646863262681, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0420556700166885\n",
      "Step - 12743, Loss - 0.5771862019794373, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.45151895251098373\n",
      "Step - 12744, Loss - 0.8243508387578666, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.085158457685812\n",
      "Step - 12745, Loss - 0.9134215637887426, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6246436319315614\n",
      "Step - 12746, Loss - 0.5570912288949386, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4137308869322775\n",
      "Step - 12747, Loss - 0.780943323884945, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6445106385221555\n",
      "Step - 12748, Loss - 0.47970298110057175, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5481965371007413\n",
      "Step - 12749, Loss - 0.6693684627018939, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0812800838488883\n",
      "Step - 12750, Loss - 0.574002612908286, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.25137255255258917\n",
      "Step - 12751, Loss - 0.6214214580789101, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.3678726755857981\n",
      "Step - 12752, Loss - 0.49249909692929716, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.831851364512614\n",
      "Step - 12753, Loss - 0.6112256359778573, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4232567852090308\n",
      "Step - 12754, Loss - 0.5586709398821629, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.464161318610585\n",
      "Step - 12755, Loss - 0.6362445816803316, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.791292112106486\n",
      "Step - 12756, Loss - 0.8175107810695629, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.48054664611632014\n",
      "Step - 12757, Loss - 0.566891474139457, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5068951337400271\n",
      "Step - 12758, Loss - 0.8544612906161616, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.005047683315673\n",
      "Step - 12759, Loss - 0.9459838534491681, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5666004030791445\n",
      "Step - 12760, Loss - 0.7670835688356641, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6905375605207822\n",
      "Step - 12761, Loss - 0.5945440571589462, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8694780761078025\n",
      "Step - 12762, Loss - 0.8903125445750499, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.480002873722964\n",
      "Step - 12763, Loss - 0.7641761786539444, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6833879436681284\n",
      "Step - 12764, Loss - 0.6590094242865155, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5994219213787753\n",
      "Step - 12765, Loss - 0.8616976782007721, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8373533145036387\n",
      "Step - 12766, Loss - 0.6398370821572572, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1725668277454073\n",
      "Step - 12767, Loss - 0.5280891372661768, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0292996935575138\n",
      "Step - 12768, Loss - 0.8470809056316326, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8374705211021225\n",
      "Step - 12769, Loss - 0.6083567456913195, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.23008551412792946\n",
      "Step - 12770, Loss - 0.9346209619658352, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5462884912669828\n",
      "Step - 12771, Loss - 0.6841968918396202, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7837572270717805\n",
      "Step - 12772, Loss - 0.8733986977061324, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2601556174932833\n",
      "Step - 12773, Loss - 0.7569763093823361, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4187932219556031\n",
      "Step - 12774, Loss - 0.7174015634141128, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1454145068580566\n",
      "Step - 12775, Loss - 0.7015709349596896, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.22891169182456073\n",
      "Step - 12776, Loss - 0.7536446732143478, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6748585552374704\n",
      "Step - 12777, Loss - 0.8634416552633751, Learning Rate - 1.220703125e-05, magnitude of gradient - 3.033656539894879\n",
      "Step - 12778, Loss - 0.7365022290990118, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1428000767020063\n",
      "Step - 12779, Loss - 0.7885217418950541, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5934928767791581\n",
      "Step - 12780, Loss - 0.6106045666197384, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2569856021708312\n",
      "Step - 12781, Loss - 0.526267664822502, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.872200495072266\n",
      "Step - 12782, Loss - 0.7802622065309859, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2858403594316528\n",
      "Step - 12783, Loss - 0.4876532670552567, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.2914837333350935\n",
      "Step - 12784, Loss - 0.46262810789945247, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9113631326444305\n",
      "Step - 12785, Loss - 1.0146554324209462, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9225301584171073\n",
      "Step - 12786, Loss - 0.8056190344661368, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9258261249183625\n",
      "Step - 12787, Loss - 0.7041810896545431, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7983594986313594\n",
      "Step - 12788, Loss - 0.6734637174158554, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2101364047539243\n",
      "Step - 12789, Loss - 0.6113391308999638, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7077196422861118\n",
      "Step - 12790, Loss - 0.5135473123149111, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8144682226282488\n",
      "Step - 12791, Loss - 0.5134292595533358, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4516127857499638\n",
      "Step - 12792, Loss - 0.7179650927371163, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9747243989693175\n",
      "Step - 12793, Loss - 0.7741562682295756, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0438284927994914\n",
      "Step - 12794, Loss - 0.8073166358284746, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8995963516338943\n",
      "Step - 12795, Loss - 0.874554021571434, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6467968994990279\n",
      "Step - 12796, Loss - 0.7774398739016327, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.405667729681841\n",
      "Step - 12797, Loss - 0.7216818807641858, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1606767953179253\n",
      "Step - 12798, Loss - 0.6564442676118248, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1525996124335236\n",
      "Step - 12799, Loss - 0.616517420432257, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3813108857596434\n",
      "Step - 12800, Loss - 0.7006358317340838, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2727023770286625\n",
      "Step - 12801, Loss - 0.7817403535676701, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.7999206042990763\n",
      "Step - 12802, Loss - 0.8306595695522061, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6254351802674968\n",
      "Step - 12803, Loss - 0.6768258251537443, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6124654999756118\n",
      "Step - 12804, Loss - 0.71518718037376, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8338036400196758\n",
      "Step - 12805, Loss - 0.8195171340826547, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6584025775096305\n",
      "Step - 12806, Loss - 0.7663839650236293, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9403583833527269\n",
      "Step - 12807, Loss - 0.6466206759599407, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.320514721437443\n",
      "Step - 12808, Loss - 0.6783191217530092, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.984286815252143\n",
      "Step - 12809, Loss - 0.7356646496084738, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8647044729021527\n",
      "Step - 12810, Loss - 0.6800214877156552, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.942872233038528\n",
      "Step - 12811, Loss - 0.8546099136151777, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.099860039887181\n",
      "Step - 12812, Loss - 0.8037091143690706, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4699351947500106\n",
      "Step - 12813, Loss - 0.8180831318472113, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.432471557613198\n",
      "Step - 12814, Loss - 0.5900352241748303, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5961112442324947\n",
      "Step - 12815, Loss - 0.8291247620162159, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6677748947603892\n",
      "Step - 12816, Loss - 0.5889256616770246, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5813422635869987\n",
      "Step - 12817, Loss - 0.5599923650031181, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5524208463890576\n",
      "Step - 12818, Loss - 0.5538727033791375, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5045284024997377\n",
      "Step - 12819, Loss - 0.5493098339137451, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8837277227192789\n",
      "Step - 12820, Loss - 0.8818698617287211, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3360194902993598\n",
      "Step - 12821, Loss - 0.6714959179872586, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1120055903818413\n",
      "Step - 12822, Loss - 0.5557099429324425, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8457034332230846\n",
      "Step - 12823, Loss - 0.6867136407395641, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7445093251651033\n",
      "Step - 12824, Loss - 0.7299050841606637, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.15248941748558\n",
      "Step - 12825, Loss - 0.8404611489456779, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7164315961668752\n",
      "Step - 12826, Loss - 0.8245379581030324, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.156331408592277\n",
      "Step - 12827, Loss - 0.6599895060659167, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.140087190664701\n",
      "Step - 12828, Loss - 0.5831248385580914, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.712484728718528\n",
      "Step - 12829, Loss - 0.7995499865976045, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5985449903127458\n",
      "Step - 12830, Loss - 0.6829067397188789, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.102310069717182\n",
      "Step - 12831, Loss - 0.6736632383270419, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.40761403595475093\n",
      "Step - 12832, Loss - 0.6768340885201978, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9552348607331972\n",
      "Step - 12833, Loss - 0.6985458177744333, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7693270288988604\n",
      "Step - 12834, Loss - 0.719575998496996, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7239087557143945\n",
      "Step - 12835, Loss - 0.6777009802846081, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0871869854840255\n",
      "Step - 12836, Loss - 0.6042596828203818, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.203764346213098\n",
      "Step - 12837, Loss - 0.6469315942373176, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.343669073305231\n",
      "Step - 12838, Loss - 0.935152456526595, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7989830276421619\n",
      "Step - 12839, Loss - 0.6909330301269548, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8715145174690041\n",
      "Step - 12840, Loss - 0.7797552802911083, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9204435206007072\n",
      "Step - 12841, Loss - 0.5470034192826639, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2667953617000358\n",
      "Step - 12842, Loss - 0.7468326617138539, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2099959158439868\n",
      "Step - 12843, Loss - 0.6635720499798715, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8229271729334281\n",
      "Step - 12844, Loss - 0.6855066983375815, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.856840316164989\n",
      "Step - 12845, Loss - 0.6366417649446892, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.625558937267091\n",
      "Step - 12846, Loss - 0.6962447257238067, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4104998106103974\n",
      "Step - 12847, Loss - 0.7415957086338683, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.983815437146154\n",
      "Step - 12848, Loss - 0.6707138170204703, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8506339842284729\n",
      "Step - 12849, Loss - 0.7040846783989143, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1039733218751702\n",
      "Step - 12850, Loss - 0.6563636550231899, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.316446030277548\n",
      "Step - 12851, Loss - 0.7972528598039246, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.241213289769291\n",
      "Step - 12852, Loss - 0.8314457738815978, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.4729017156263957\n",
      "Step - 12853, Loss - 0.764340064208539, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5361568440250037\n",
      "Step - 12854, Loss - 0.6698877045215945, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0966169860941102\n",
      "Step - 12855, Loss - 0.68278621143435, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.014695228344376\n",
      "Step - 12856, Loss - 0.5653064127026902, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.124295291206107\n",
      "Step - 12857, Loss - 0.6463783097459851, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.986997106326126\n",
      "Step - 12858, Loss - 0.7574520918744099, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1615399673051705\n",
      "Step - 12859, Loss - 0.8969815883603862, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5362628827908187\n",
      "Step - 12860, Loss - 0.7825929055732962, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8183147293315869\n",
      "Step - 12861, Loss - 0.7821229975008319, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.602246627407324\n",
      "Step - 12862, Loss - 0.8153656023718528, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4566367690905462\n",
      "Step - 12863, Loss - 0.9315824584349714, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.0360727068386613\n",
      "Step - 12864, Loss - 0.5174913187463567, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2523326481866675\n",
      "Step - 12865, Loss - 0.5760337176922335, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0787258178006978\n",
      "Step - 12866, Loss - 0.7043351059818173, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2186172457214317\n",
      "Step - 12867, Loss - 0.7848137810362673, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0050230150006703\n",
      "Step - 12868, Loss - 0.6275883692118406, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6519472834066703\n",
      "Step - 12869, Loss - 0.8463763566053304, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8844793613510362\n",
      "Step - 12870, Loss - 0.6686200347028957, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9780509938656005\n",
      "Step - 12871, Loss - 0.7984434460617388, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.037094681737145\n",
      "Step - 12872, Loss - 1.0005096662888788, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4725158920002956\n",
      "Step - 12873, Loss - 0.6253308322071954, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.44478706462169504\n",
      "Step - 12874, Loss - 0.8133975379189613, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7974487456154616\n",
      "Step - 12875, Loss - 0.6546839891346125, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.233173410722288\n",
      "Step - 12876, Loss - 0.7987552042527236, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9862477515063974\n",
      "Step - 12877, Loss - 0.7922024882425396, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8714709116378656\n",
      "Step - 12878, Loss - 0.8904821283323312, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3095900685530526\n",
      "Step - 12879, Loss - 0.8031218798333353, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9930600045505361\n",
      "Step - 12880, Loss - 0.6763723437745637, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9911045818051849\n",
      "Step - 12881, Loss - 0.5312663273658709, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8105400264622469\n",
      "Step - 12882, Loss - 0.6052028695863173, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.001759703847063\n",
      "Step - 12883, Loss - 0.6079206453487717, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3664367914284543\n",
      "Step - 12884, Loss - 0.9128800367458717, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.482967094017948\n",
      "Step - 12885, Loss - 0.6170097818110402, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.638528194520653\n",
      "Step - 12886, Loss - 0.8432500367571596, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.4919040637916928\n",
      "Step - 12887, Loss - 0.8846795959517995, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6094666977783334\n",
      "Step - 12888, Loss - 0.5419343350559055, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.10528069016002\n",
      "Step - 12889, Loss - 0.813895259404325, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5075828619101701\n",
      "Step - 12890, Loss - 0.7125286093351548, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5963320076087596\n",
      "Step - 12891, Loss - 0.729104153812973, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3958170395511713\n",
      "Step - 12892, Loss - 0.8253348598237478, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.49118152735024756\n",
      "Step - 12893, Loss - 0.7927677117275804, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.105691513476739\n",
      "Step - 12894, Loss - 0.6165555759326996, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.877678370354098\n",
      "Step - 12895, Loss - 0.7141411945117577, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8148248798084927\n",
      "Step - 12896, Loss - 0.7391621108874222, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.6235663741240893\n",
      "Step - 12897, Loss - 0.5851793040979297, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.459538940465128\n",
      "Step - 12898, Loss - 0.8077745714392124, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5873199801312696\n",
      "Step - 12899, Loss - 0.8750037696481291, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1204437530585256\n",
      "Step - 12900, Loss - 0.5692560731597924, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1955580525011482\n",
      "Step - 12901, Loss - 0.65843080996273, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8070429756924453\n",
      "Step - 12902, Loss - 0.6178164185575367, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.808354889437095\n",
      "Step - 12903, Loss - 0.7272772204439197, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2960650765196915\n",
      "Step - 12904, Loss - 0.7235091317021627, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.390915383262563\n",
      "Step - 12905, Loss - 0.7770999187256229, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1033635308284186\n",
      "Step - 12906, Loss - 0.7375476803862306, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.40449193118545684\n",
      "Step - 12907, Loss - 0.8469663322211041, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0865773941849812\n",
      "Step - 12908, Loss - 0.5675262453360721, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.2950945681004664\n",
      "Step - 12909, Loss - 0.7656954828476721, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0958732002564067\n",
      "Step - 12910, Loss - 0.6926381306754206, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.5797910559316377\n",
      "Step - 12911, Loss - 0.8090496126424629, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8062531091041489\n",
      "Step - 12912, Loss - 0.5007106490559655, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8976024636995542\n",
      "Step - 12913, Loss - 0.8182262052699232, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8520159067727663\n",
      "Step - 12914, Loss - 0.8257367638914579, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.9757713962649759\n",
      "Step - 12915, Loss - 0.6870749901756842, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1426921777344365\n",
      "Step - 12916, Loss - 0.7813777896849586, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8748636683793434\n",
      "Step - 12917, Loss - 0.716730919776908, Learning Rate - 1.220703125e-05, magnitude of gradient - 3.089316400940579\n",
      "Step - 12918, Loss - 0.6183665656333277, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.320550478286598\n",
      "Step - 12919, Loss - 0.7829318302396763, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7946373054511834\n",
      "Step - 12920, Loss - 0.6455230796819522, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5992249577304904\n",
      "Step - 12921, Loss - 0.772251641772937, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5798923835928921\n",
      "Step - 12922, Loss - 0.48790104767514164, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2877613404025121\n",
      "Step - 12923, Loss - 0.6402647935536816, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.530409569535258\n",
      "Step - 12924, Loss - 0.7305706862047141, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1561085380066627\n",
      "Step - 12925, Loss - 0.5047423247298741, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4267979205097807\n",
      "Step - 12926, Loss - 0.8568901159294976, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0067492408714125\n",
      "Step - 12927, Loss - 0.6352088312498335, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7649459321257215\n",
      "Step - 12928, Loss - 0.7215225804038135, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8898992669545895\n",
      "Step - 12929, Loss - 0.6521554510525195, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8360409433107214\n",
      "Step - 12930, Loss - 0.6235875468948149, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.611193032715782\n",
      "Step - 12931, Loss - 0.5150549753963182, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6365047483753086\n",
      "Step - 12932, Loss - 0.6132618830757809, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3306857896512243\n",
      "Step - 12933, Loss - 0.7021484246197484, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3442299299455684\n",
      "Step - 12934, Loss - 0.7110958093929101, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9441594653929879\n",
      "Step - 12935, Loss - 0.7838857280041606, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2934812210925117\n",
      "Step - 12936, Loss - 0.6421195284617082, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1880211265284726\n",
      "Step - 12937, Loss - 0.6693742390459723, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.930437820382738\n",
      "Step - 12938, Loss - 0.5747994232375633, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3655101897200819\n",
      "Step - 12939, Loss - 0.6104358443775401, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1479562786516493\n",
      "Step - 12940, Loss - 0.678244348171968, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7808657587804773\n",
      "Step - 12941, Loss - 0.8438275813166677, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3352150614301865\n",
      "Step - 12942, Loss - 0.6833376029106848, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.106761501482278\n",
      "Step - 12943, Loss - 0.6557437413020382, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8649082148513746\n",
      "Step - 12944, Loss - 0.7514550673640441, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8223314653616571\n",
      "Step - 12945, Loss - 0.7251336649741277, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5326806830770443\n",
      "Step - 12946, Loss - 0.6885552680741733, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0608230093214643\n",
      "Step - 12947, Loss - 0.7752506553663834, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2706264680686623\n",
      "Step - 12948, Loss - 0.7670335823743729, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9310829778408427\n",
      "Step - 12949, Loss - 0.6531038745726543, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8902910673825792\n",
      "Step - 12950, Loss - 0.6312441479332376, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.6326771159624291\n",
      "Step - 12951, Loss - 0.6378543048529584, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0210607621080001\n",
      "Step - 12952, Loss - 0.6993543650068068, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8408080211541243\n",
      "Step - 12953, Loss - 0.6222663873506463, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7992072740530638\n",
      "Step - 12954, Loss - 0.7826318192227412, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1407289572733037\n",
      "Step - 12955, Loss - 0.8117230602118002, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.37440845605957823\n",
      "Step - 12956, Loss - 0.5535004525637882, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.659835089232438\n",
      "Step - 12957, Loss - 0.5708976551072944, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0694340669535563\n",
      "Step - 12958, Loss - 0.6298514295873214, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0324703951160243\n",
      "Step - 12959, Loss - 0.7728067149996362, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0278381663250304\n",
      "Step - 12960, Loss - 0.8274695656067629, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.8283634476371091\n",
      "Step - 12961, Loss - 0.6997280373046928, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.0825251548135992\n",
      "Step - 12962, Loss - 0.6382249267057858, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9636860172305899\n",
      "Step - 12963, Loss - 0.6393989929377606, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2786419579932602\n",
      "Step - 12964, Loss - 0.5528046419445878, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.121918239083184\n",
      "Step - 12965, Loss - 0.5892066635938256, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0944553217076356\n",
      "Step - 12966, Loss - 0.7071836952738899, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2621530798687282\n",
      "Step - 12967, Loss - 0.6539343917895276, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.48776740333848384\n",
      "Step - 12968, Loss - 0.801282338193116, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5018602510443366\n",
      "Step - 12969, Loss - 0.6159598380165762, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.543806939888104\n",
      "Step - 12970, Loss - 0.8223453476190062, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.5680535041801464\n",
      "Step - 12971, Loss - 0.7385297243537782, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.7812388555706761\n",
      "Step - 12972, Loss - 0.6164732035461902, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6815520699523081\n",
      "Step - 12973, Loss - 0.6564050556498148, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.1081473586958297\n",
      "Step - 12974, Loss - 0.5485801512895917, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1927122018256364\n",
      "Step - 12975, Loss - 0.6404940222432312, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.3336851709881063\n",
      "Step - 12976, Loss - 0.7682113327152298, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2342640836568826\n",
      "Step - 12977, Loss - 0.5228429656550826, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8006425770658345\n",
      "Step - 12978, Loss - 0.7001401593703583, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.3028267032821481\n",
      "Step - 12979, Loss - 0.7088226455328925, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.4130693408793658\n",
      "Step - 12980, Loss - 0.9733434145099074, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4835319618579925\n",
      "Step - 12981, Loss - 0.9576413642813122, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.7445257933459457\n",
      "Step - 12982, Loss - 0.6463111063142173, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.697492058609546\n",
      "Step - 12983, Loss - 0.5582828666807412, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5084943660623867\n",
      "Step - 12984, Loss - 0.7585546119659756, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2823456033207887\n",
      "Step - 12985, Loss - 0.6897769144120786, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.6125037169523299\n",
      "Step - 12986, Loss - 0.6281711114013331, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9304032402673174\n",
      "Step - 12987, Loss - 0.970831995930574, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8406644598262687\n",
      "Step - 12988, Loss - 0.5520124497191029, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.2056601531494757\n",
      "Step - 12989, Loss - 0.7614955239393502, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.1141689646364101\n",
      "Step - 12990, Loss - 0.6649425869394341, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.0571233341537847\n",
      "Step - 12991, Loss - 0.6241670666431753, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.632048860597306\n",
      "Step - 12992, Loss - 0.8294576686929362, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.7401338887485847\n",
      "Step - 12993, Loss - 0.6173760654514782, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.107861125296632\n",
      "Step - 12994, Loss - 0.8594988569303503, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.8463942556192414\n",
      "Step - 12995, Loss - 0.6890823172041726, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.5864773406135738\n",
      "Step - 12996, Loss - 0.814588223016707, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.189356246504455\n",
      "Step - 12997, Loss - 0.5106004507395686, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.787397475562282\n",
      "Step - 12998, Loss - 0.6617184717035973, Learning Rate - 1.220703125e-05, magnitude of gradient - 0.9315905036829992\n",
      "Step - 12999, Loss - 0.801391500844618, Learning Rate - 1.220703125e-05, magnitude of gradient - 1.4987876165797824\n",
      "Step - 13000, Loss - 0.6325318764970446, Learning Rate - 1.220703125e-05, magnitude of gradient - 2.9887717015724022\n",
      "Step - 13001, Loss - 0.7053053289740644, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.4435978358774285\n",
      "Step - 13002, Loss - 0.789808766271077, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2824436043374499\n",
      "Step - 13003, Loss - 0.610864194373845, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8934691398689313\n",
      "Step - 13004, Loss - 0.6583703401407633, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0282884107568597\n",
      "Step - 13005, Loss - 0.6710386644244577, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.0402603519187017\n",
      "Step - 13006, Loss - 0.7031350269701185, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3423602401043393\n",
      "Step - 13007, Loss - 0.7056921614189055, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.2907784878471786\n",
      "Step - 13008, Loss - 0.6034909708203366, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7172445763748243\n",
      "Step - 13009, Loss - 0.6779568110405462, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1837343096436213\n",
      "Step - 13010, Loss - 0.6058196154713867, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.186157897285126\n",
      "Step - 13011, Loss - 0.5531009448966648, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1981192657314381\n",
      "Step - 13012, Loss - 0.6544663942823519, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0167273851153609\n",
      "Step - 13013, Loss - 0.6605873374379143, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2591944630248935\n",
      "Step - 13014, Loss - 0.6710436814628092, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9836269011830678\n",
      "Step - 13015, Loss - 0.7452858578733252, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.872643947110989\n",
      "Step - 13016, Loss - 0.5777581479085048, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8030476036712857\n",
      "Step - 13017, Loss - 0.6512729056041588, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.040717120254934\n",
      "Step - 13018, Loss - 0.6917244754279538, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.364215928689183\n",
      "Step - 13019, Loss - 0.7567525058491243, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4779480930754747\n",
      "Step - 13020, Loss - 0.8446940171536343, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0697466852332544\n",
      "Step - 13021, Loss - 0.6826864878799566, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6671433209987899\n",
      "Step - 13022, Loss - 0.8297056156909625, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.1230373485784773\n",
      "Step - 13023, Loss - 0.8628039426477766, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9068831243056483\n",
      "Step - 13024, Loss - 0.8474333009682963, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.3291344255876663\n",
      "Step - 13025, Loss - 0.5543351713990579, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.149515075005735\n",
      "Step - 13026, Loss - 0.5484783389174883, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9165014418792508\n",
      "Step - 13027, Loss - 0.7328788510914224, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6831775444109423\n",
      "Step - 13028, Loss - 0.6995070756298195, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5456610884981699\n",
      "Step - 13029, Loss - 0.5336505580977073, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8785581483949394\n",
      "Step - 13030, Loss - 0.7925456698113972, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3189075579384786\n",
      "Step - 13031, Loss - 0.7454521835603257, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.2273487303652633\n",
      "Step - 13032, Loss - 0.8388094689715775, Learning Rate - 6.103515625e-06, magnitude of gradient - 3.1517343691860877\n",
      "Step - 13033, Loss - 0.6048170071657512, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.032384129723089\n",
      "Step - 13034, Loss - 0.597495135628672, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9682601957524005\n",
      "Step - 13035, Loss - 0.5605189221011421, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.633496755585668\n",
      "Step - 13036, Loss - 0.6114536804969475, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7704574302451276\n",
      "Step - 13037, Loss - 0.6459592476178496, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0826540711997734\n",
      "Step - 13038, Loss - 0.6240568920754821, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8420550740106447\n",
      "Step - 13039, Loss - 0.6902407022226575, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3660268469837482\n",
      "Step - 13040, Loss - 0.6854314227790039, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6422417364062747\n",
      "Step - 13041, Loss - 0.7213207195557098, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3661109406004814\n",
      "Step - 13042, Loss - 0.699804252312879, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.665142227838518\n",
      "Step - 13043, Loss - 0.5934059948145123, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.1856620062367322\n",
      "Step - 13044, Loss - 0.6551684010561635, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7174834615480636\n",
      "Step - 13045, Loss - 0.5395796466340144, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8970026763060026\n",
      "Step - 13046, Loss - 0.8042755544750777, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0573423026733204\n",
      "Step - 13047, Loss - 0.7043176553479158, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9760924797420205\n",
      "Step - 13048, Loss - 0.6245200606776211, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9619357450517139\n",
      "Step - 13049, Loss - 0.6083174949551392, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.174104017750538\n",
      "Step - 13050, Loss - 0.7535645447836982, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7843523752225546\n",
      "Step - 13051, Loss - 0.8413195613424062, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6803615321659381\n",
      "Step - 13052, Loss - 0.686458182467638, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5503522082068509\n",
      "Step - 13053, Loss - 0.7150405019861317, Learning Rate - 6.103515625e-06, magnitude of gradient - 3.339983496322455\n",
      "Step - 13054, Loss - 0.8566276177128588, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4515913910775526\n",
      "Step - 13055, Loss - 0.7218524883909626, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.934560988542184\n",
      "Step - 13056, Loss - 0.8064558626473233, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8637984061079037\n",
      "Step - 13057, Loss - 0.8366577337560036, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6155871228302665\n",
      "Step - 13058, Loss - 0.809618036950557, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2501448770151222\n",
      "Step - 13059, Loss - 0.825984355297556, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8744634612391212\n",
      "Step - 13060, Loss - 0.6789830772571623, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.4581585470193673\n",
      "Step - 13061, Loss - 0.8222122543183138, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7346658845348052\n",
      "Step - 13062, Loss - 0.5494355131886892, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.404988049128071\n",
      "Step - 13063, Loss - 0.6473671362666732, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7326436561510866\n",
      "Step - 13064, Loss - 0.7453533903846512, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.933376127182269\n",
      "Step - 13065, Loss - 0.5084117091638523, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6357130356618872\n",
      "Step - 13066, Loss - 0.7181752715033622, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6536523340550253\n",
      "Step - 13067, Loss - 0.692367406286639, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1444067583448425\n",
      "Step - 13068, Loss - 0.6977834027801495, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4354035761338682\n",
      "Step - 13069, Loss - 0.5747989933273081, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5933556959611948\n",
      "Step - 13070, Loss - 0.6121530900132729, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2262626837796398\n",
      "Step - 13071, Loss - 0.7428954849541132, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2427660599031642\n",
      "Step - 13072, Loss - 0.6856228886112254, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1039823668832347\n",
      "Step - 13073, Loss - 0.4772878548140793, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2785235453676578\n",
      "Step - 13074, Loss - 0.7981447193644935, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4180549898641752\n",
      "Step - 13075, Loss - 0.727640695677725, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.26565827055417274\n",
      "Step - 13076, Loss - 0.8395110731995328, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9991996101181198\n",
      "Step - 13077, Loss - 0.6910967028968187, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2410358162829564\n",
      "Step - 13078, Loss - 0.9271493350126065, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2774048718391038\n",
      "Step - 13079, Loss - 0.5928982299474238, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.963872383344834\n",
      "Step - 13080, Loss - 0.7533125814876905, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9493353446652079\n",
      "Step - 13081, Loss - 0.5890351985681422, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7338702675248003\n",
      "Step - 13082, Loss - 0.6808766271235541, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.113661550107354\n",
      "Step - 13083, Loss - 0.6199452081528231, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9267503037784979\n",
      "Step - 13084, Loss - 0.6176318386528822, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1418809925911193\n",
      "Step - 13085, Loss - 0.5152334800610765, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6773638911659876\n",
      "Step - 13086, Loss - 0.635190408963191, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7835622132265518\n",
      "Step - 13087, Loss - 0.9298994802791958, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.850389260382491\n",
      "Step - 13088, Loss - 0.8157661338389541, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7928882995950395\n",
      "Step - 13089, Loss - 0.8057084340301207, Learning Rate - 6.103515625e-06, magnitude of gradient - 3.0025550927082545\n",
      "Step - 13090, Loss - 0.6230898316481567, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6958911896065211\n",
      "Step - 13091, Loss - 0.7167362251714883, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.319131291857352\n",
      "Step - 13092, Loss - 0.6942812468568758, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3048071416450973\n",
      "Step - 13093, Loss - 0.8322407068187956, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.491569263228049\n",
      "Step - 13094, Loss - 0.80549622656644, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2081683151375524\n",
      "Step - 13095, Loss - 0.7572381788943489, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7874696678654935\n",
      "Step - 13096, Loss - 0.556773321181447, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1588879216525079\n",
      "Step - 13097, Loss - 0.8858067434024193, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.469658563799077\n",
      "Step - 13098, Loss - 0.8663151288507047, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.855970424645634\n",
      "Step - 13099, Loss - 0.6466520847846216, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9621522866475205\n",
      "Step - 13100, Loss - 0.7336931125102777, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7979963346253827\n",
      "Step - 13101, Loss - 0.7603248406505448, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.24085156904577829\n",
      "Step - 13102, Loss - 0.641668463283105, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9064348788331222\n",
      "Step - 13103, Loss - 0.6818175888526655, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1906200384537828\n",
      "Step - 13104, Loss - 0.7165386149628968, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.050724963690081\n",
      "Step - 13105, Loss - 0.849447796592167, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8244216222699771\n",
      "Step - 13106, Loss - 0.8370294729495449, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9919479787296438\n",
      "Step - 13107, Loss - 0.686054762763336, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1399330820945577\n",
      "Step - 13108, Loss - 0.6629926658518818, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5168291190250212\n",
      "Step - 13109, Loss - 0.6451433233532966, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5179493664164109\n",
      "Step - 13110, Loss - 0.6972168728447383, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.44166837173283235\n",
      "Step - 13111, Loss - 0.8740779679005692, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.25171004293816357\n",
      "Step - 13112, Loss - 0.9042944416318159, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5536348488651683\n",
      "Step - 13113, Loss - 0.6870673436720427, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.655311377878038\n",
      "Step - 13114, Loss - 0.6478210381611036, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.313813041298815\n",
      "Step - 13115, Loss - 0.9044655752658247, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9494699486893675\n",
      "Step - 13116, Loss - 0.6895292509735856, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3387131560822378\n",
      "Step - 13117, Loss - 0.7059649914735064, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5949974469219104\n",
      "Step - 13118, Loss - 0.8208801027314246, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9905133434623271\n",
      "Step - 13119, Loss - 0.7027511308427421, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.43481246949824026\n",
      "Step - 13120, Loss - 0.8170498756928422, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6384261631618753\n",
      "Step - 13121, Loss - 0.604722599703767, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.273796883757226\n",
      "Step - 13122, Loss - 0.8038327899391261, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.653166686729802\n",
      "Step - 13123, Loss - 0.6440474906106117, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.65926188802481\n",
      "Step - 13124, Loss - 0.677890799344145, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4396523460052093\n",
      "Step - 13125, Loss - 0.7816671502509439, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.801625952775728\n",
      "Step - 13126, Loss - 0.4946575801308001, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2613493272841712\n",
      "Step - 13127, Loss - 0.7134676214418922, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1341408041392558\n",
      "Step - 13128, Loss - 0.5897514353460943, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5041506150583616\n",
      "Step - 13129, Loss - 0.8544874472134183, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2030036159168447\n",
      "Step - 13130, Loss - 0.7826948195697635, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5631372732397482\n",
      "Step - 13131, Loss - 0.6088239856959226, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.37678689235610774\n",
      "Step - 13132, Loss - 0.7191066196479626, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.912066995091302\n",
      "Step - 13133, Loss - 0.5858063081255778, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.120877168944773\n",
      "Step - 13134, Loss - 0.7535214985340228, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.834962116034414\n",
      "Step - 13135, Loss - 0.7042497719313816, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4834895974294497\n",
      "Step - 13136, Loss - 0.6812633691763755, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0105099547011802\n",
      "Step - 13137, Loss - 0.7219865477424773, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7704742427764957\n",
      "Step - 13138, Loss - 0.49911147675392875, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.0094918158078197\n",
      "Step - 13139, Loss - 0.5446551378195829, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.405667738019625\n",
      "Step - 13140, Loss - 0.5768017445634925, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5946463419214298\n",
      "Step - 13141, Loss - 0.49107639534088765, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2226038999201752\n",
      "Step - 13142, Loss - 0.5755250585594482, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0338929032606594\n",
      "Step - 13143, Loss - 0.6889272476468483, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.222489529349085\n",
      "Step - 13144, Loss - 0.8043379549700013, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9929414153525051\n",
      "Step - 13145, Loss - 0.9157587942801962, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5628972397283045\n",
      "Step - 13146, Loss - 0.7711526351122464, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8893721739293681\n",
      "Step - 13147, Loss - 0.6515375862660224, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3569370094441926\n",
      "Step - 13148, Loss - 0.6444284541144782, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4406226287985104\n",
      "Step - 13149, Loss - 0.7406547449797565, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.46314591638366026\n",
      "Step - 13150, Loss - 0.661903665788909, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3042332686716072\n",
      "Step - 13151, Loss - 0.6727457483370055, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.36253582414431723\n",
      "Step - 13152, Loss - 0.6824143294803036, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.464173850877706\n",
      "Step - 13153, Loss - 0.7389332196289767, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.2263089107685734\n",
      "Step - 13154, Loss - 0.8257256026719486, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.4115530049097529\n",
      "Step - 13155, Loss - 0.5777693238954502, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8233760558132839\n",
      "Step - 13156, Loss - 0.592946189336732, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.570065297651424\n",
      "Step - 13157, Loss - 0.7372501439221159, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6387664170671794\n",
      "Step - 13158, Loss - 0.5754442699306809, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5289883804503107\n",
      "Step - 13159, Loss - 0.7015757762971988, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.368409816316154\n",
      "Step - 13160, Loss - 0.7368658453791275, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7955814325628995\n",
      "Step - 13161, Loss - 0.5941127806011883, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.06541403963473\n",
      "Step - 13162, Loss - 0.6315581037115542, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5111420458319924\n",
      "Step - 13163, Loss - 0.7273445989278922, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8616836751964563\n",
      "Step - 13164, Loss - 0.7110954658733982, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.701015965720101\n",
      "Step - 13165, Loss - 0.6487794785604097, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.872514292615945\n",
      "Step - 13166, Loss - 0.7197610903067096, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8181886733988383\n",
      "Step - 13167, Loss - 0.5615151154200961, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2886281741811765\n",
      "Step - 13168, Loss - 0.5152785687641307, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.39837212455349\n",
      "Step - 13169, Loss - 0.6941811393418406, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.177915859677205\n",
      "Step - 13170, Loss - 0.5598174288388225, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.0290234407397905\n",
      "Step - 13171, Loss - 0.4911603995725343, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6245445558621947\n",
      "Step - 13172, Loss - 0.848619689881855, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2095107797462108\n",
      "Step - 13173, Loss - 0.8112051562478628, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2060278572134433\n",
      "Step - 13174, Loss - 0.6622054707415627, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4194372089396148\n",
      "Step - 13175, Loss - 0.639402451592058, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4073713410746287\n",
      "Step - 13176, Loss - 0.634418546555908, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6236083473853778\n",
      "Step - 13177, Loss - 0.7451394482814994, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8628675969469507\n",
      "Step - 13178, Loss - 0.6181605730187762, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1292668036711293\n",
      "Step - 13179, Loss - 0.7765298531576075, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.0267388007441522\n",
      "Step - 13180, Loss - 0.7231540645028263, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5013979645390556\n",
      "Step - 13181, Loss - 0.6664733810287028, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5651459836351408\n",
      "Step - 13182, Loss - 0.595198267253447, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5234353102272007\n",
      "Step - 13183, Loss - 0.6114390323691994, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8687932243851737\n",
      "Step - 13184, Loss - 0.71006812531169, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.62846322665364\n",
      "Step - 13185, Loss - 0.7870565433413002, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7453688490773573\n",
      "Step - 13186, Loss - 0.8181857613777984, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2327141060528826\n",
      "Step - 13187, Loss - 0.8058813295142655, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.058864465528547\n",
      "Step - 13188, Loss - 0.6551418804659521, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6534504819917628\n",
      "Step - 13189, Loss - 0.5567623768177192, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.734536482211915\n",
      "Step - 13190, Loss - 0.7387515379177946, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0885619409833371\n",
      "Step - 13191, Loss - 0.6721839877798648, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5423764423302336\n",
      "Step - 13192, Loss - 0.8383288695244796, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6030232430469404\n",
      "Step - 13193, Loss - 0.6632167402559194, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0545713922005757\n",
      "Step - 13194, Loss - 0.8037931110687608, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.39446032357854155\n",
      "Step - 13195, Loss - 0.8707363771952191, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7168198180480053\n",
      "Step - 13196, Loss - 0.825673526579185, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2591313867535985\n",
      "Step - 13197, Loss - 0.7046407642307437, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0973879189652247\n",
      "Step - 13198, Loss - 0.678160928971322, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0424224490382217\n",
      "Step - 13199, Loss - 0.6950888408180143, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4313618104069366\n",
      "Step - 13200, Loss - 0.6458936237289814, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.806414891390844\n",
      "Step - 13201, Loss - 0.8292099083978575, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9917637864444702\n",
      "Step - 13202, Loss - 0.5871997490374854, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.482293768775004\n",
      "Step - 13203, Loss - 0.6273845346803518, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.0800915955181467\n",
      "Step - 13204, Loss - 0.7076187986279232, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3401622904116504\n",
      "Step - 13205, Loss - 0.7150593363277102, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.323984170023273\n",
      "Step - 13206, Loss - 0.6195284745766046, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8088576963024456\n",
      "Step - 13207, Loss - 0.8554697357226848, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7225142265242099\n",
      "Step - 13208, Loss - 0.49891521108787373, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9822278019046233\n",
      "Step - 13209, Loss - 0.6086505416526287, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6974977199071019\n",
      "Step - 13210, Loss - 0.5343195202686585, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1341903748631268\n",
      "Step - 13211, Loss - 0.7949680171138246, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6555133970050651\n",
      "Step - 13212, Loss - 0.5954334231746015, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6464228748594507\n",
      "Step - 13213, Loss - 0.6095843335814437, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.924026642575551\n",
      "Step - 13214, Loss - 0.4835612466097793, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8287586158721407\n",
      "Step - 13215, Loss - 0.6070977512130884, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.635458883722903\n",
      "Step - 13216, Loss - 0.6405695547661542, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.178842557930523\n",
      "Step - 13217, Loss - 0.8951984305616151, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.051111539196106\n",
      "Step - 13218, Loss - 0.5969684903903043, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.167984726465774\n",
      "Step - 13219, Loss - 0.8092979567180272, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.075979946654074\n",
      "Step - 13220, Loss - 0.7624932702461675, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6150937300226708\n",
      "Step - 13221, Loss - 0.6771440785020486, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0565692999174279\n",
      "Step - 13222, Loss - 0.6969266188455914, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0153285501583187\n",
      "Step - 13223, Loss - 0.8199771234329366, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0963138385115971\n",
      "Step - 13224, Loss - 0.6734965281031935, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0780728577912506\n",
      "Step - 13225, Loss - 0.5840340485442773, Learning Rate - 6.103515625e-06, magnitude of gradient - 3.59515352858975\n",
      "Step - 13226, Loss - 0.7850241795545138, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6100038986940336\n",
      "Step - 13227, Loss - 0.4887089043146321, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.841446623084779\n",
      "Step - 13228, Loss - 0.8634236853360122, Learning Rate - 6.103515625e-06, magnitude of gradient - 3.0378705677837337\n",
      "Step - 13229, Loss - 0.8764047234245101, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7362970415662866\n",
      "Step - 13230, Loss - 0.6516014765798965, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.749129383719484\n",
      "Step - 13231, Loss - 0.788711714753416, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8526403091323648\n",
      "Step - 13232, Loss - 0.5679638404704833, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5348866482014367\n",
      "Step - 13233, Loss - 0.683655948975606, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6952181820644109\n",
      "Step - 13234, Loss - 0.49679898486811247, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8694443295590188\n",
      "Step - 13235, Loss - 0.853149948834077, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8610707770972916\n",
      "Step - 13236, Loss - 0.613542003221736, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.385979993195564\n",
      "Step - 13237, Loss - 0.6899656498159038, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.45841952291077587\n",
      "Step - 13238, Loss - 0.8650422796290568, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.17165202851679\n",
      "Step - 13239, Loss - 0.6889296430690427, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8149601292912797\n",
      "Step - 13240, Loss - 0.7202541781461501, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2075261058249194\n",
      "Step - 13241, Loss - 0.7598740696978019, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3495358586482762\n",
      "Step - 13242, Loss - 0.7775523571040366, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.46989248598925787\n",
      "Step - 13243, Loss - 0.6380377565681742, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7598271388136253\n",
      "Step - 13244, Loss - 0.8199988755270176, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0391143505672946\n",
      "Step - 13245, Loss - 0.7976880438083885, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.904232158592348\n",
      "Step - 13246, Loss - 0.5296850396790348, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9940563243618568\n",
      "Step - 13247, Loss - 0.7813519381652316, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9012443991578901\n",
      "Step - 13248, Loss - 0.8514270022799239, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9170320113599775\n",
      "Step - 13249, Loss - 0.6170780774474078, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.809861377685079\n",
      "Step - 13250, Loss - 0.6865392884608275, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.12937745115617\n",
      "Step - 13251, Loss - 0.9269622629720106, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.666477878005153\n",
      "Step - 13252, Loss - 0.5956374689107871, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.536802296498129\n",
      "Step - 13253, Loss - 0.6613265029296931, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.755784374560693\n",
      "Step - 13254, Loss - 0.8201015581632531, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8218495481548881\n",
      "Step - 13255, Loss - 0.7498174459983822, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6428199044129375\n",
      "Step - 13256, Loss - 0.8881672351549226, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4014925492060872\n",
      "Step - 13257, Loss - 0.72819303565059, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8029063154185637\n",
      "Step - 13258, Loss - 0.7349974881374824, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9099503682344665\n",
      "Step - 13259, Loss - 0.7288231692814213, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4043826250764933\n",
      "Step - 13260, Loss - 0.6558217312043959, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9643362386024272\n",
      "Step - 13261, Loss - 0.7511302362310616, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.059980586263532\n",
      "Step - 13262, Loss - 0.7841353000227855, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1842481492341494\n",
      "Step - 13263, Loss - 0.5450321574852651, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6614654185832064\n",
      "Step - 13264, Loss - 0.713617411875987, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9229674211785064\n",
      "Step - 13265, Loss - 0.6875400997042507, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7732854955161954\n",
      "Step - 13266, Loss - 0.8136761586096337, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8493444931686901\n",
      "Step - 13267, Loss - 0.6429235338580646, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1106592912949875\n",
      "Step - 13268, Loss - 0.551148007777486, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5350588761075539\n",
      "Step - 13269, Loss - 0.8842130167872903, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.253034384876795\n",
      "Step - 13270, Loss - 0.6888502268674663, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.69842166667117\n",
      "Step - 13271, Loss - 0.6525171935891064, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0656237484004945\n",
      "Step - 13272, Loss - 0.6671787845609122, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.531123573256305\n",
      "Step - 13273, Loss - 0.6745646424682938, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0750702495382092\n",
      "Step - 13274, Loss - 0.6264862924756491, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6538324852206443\n",
      "Step - 13275, Loss - 0.5735197348571561, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1357110221322138\n",
      "Step - 13276, Loss - 0.5434379778193071, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9837788699292604\n",
      "Step - 13277, Loss - 0.5851721342000266, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.92517933208832\n",
      "Step - 13278, Loss - 0.6808868434363293, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6079456489739085\n",
      "Step - 13279, Loss - 0.6365775718937083, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3624058864586992\n",
      "Step - 13280, Loss - 0.7728528095528527, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.41274244505442476\n",
      "Step - 13281, Loss - 0.6727120106077425, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2351887402863684\n",
      "Step - 13282, Loss - 0.7479674256940063, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1232974004374743\n",
      "Step - 13283, Loss - 0.4948277559017847, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9524278343666785\n",
      "Step - 13284, Loss - 0.8876560334571921, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.343172563966618\n",
      "Step - 13285, Loss - 0.7043213546474298, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3614362721117221\n",
      "Step - 13286, Loss - 0.5563286041213186, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6229208536935059\n",
      "Step - 13287, Loss - 0.6792946462478852, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.299066679134818\n",
      "Step - 13288, Loss - 0.5225503798031931, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6277258382461024\n",
      "Step - 13289, Loss - 0.7773418614768013, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6310966965417288\n",
      "Step - 13290, Loss - 0.774685895839325, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.698868482936352\n",
      "Step - 13291, Loss - 0.6943953871235437, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7910811747674131\n",
      "Step - 13292, Loss - 0.7590015718720908, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.4062598598511573\n",
      "Step - 13293, Loss - 0.49485879864258564, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.2111033120818872\n",
      "Step - 13294, Loss - 0.7330310349342222, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6222984094593597\n",
      "Step - 13295, Loss - 0.7991252470666261, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5709607842463174\n",
      "Step - 13296, Loss - 0.7367232676644867, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.4493304309004873\n",
      "Step - 13297, Loss - 0.8955673670110645, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.22896888261539\n",
      "Step - 13298, Loss - 0.7162176369509256, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6420646491296695\n",
      "Step - 13299, Loss - 0.7654280214071648, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9021958184061635\n",
      "Step - 13300, Loss - 0.7368902788581682, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4452410353271934\n",
      "Step - 13301, Loss - 0.6842141713694746, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.24420378416543467\n",
      "Step - 13302, Loss - 0.6834922016060018, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.055931285101424\n",
      "Step - 13303, Loss - 0.6428898875972963, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.817391158165127\n",
      "Step - 13304, Loss - 0.7261146878665038, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7537882453150799\n",
      "Step - 13305, Loss - 0.7585015699248085, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3238418573670223\n",
      "Step - 13306, Loss - 0.7551880612187846, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3174973580243103\n",
      "Step - 13307, Loss - 0.6685188684654653, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2676463394094957\n",
      "Step - 13308, Loss - 0.7093341104824157, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8814401259890793\n",
      "Step - 13309, Loss - 0.7164657725094199, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.41545561548743304\n",
      "Step - 13310, Loss - 0.8820045290145945, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9907564900109678\n",
      "Step - 13311, Loss - 0.5708152039358829, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.944411308175019\n",
      "Step - 13312, Loss - 0.622156957529886, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8018915603908812\n",
      "Step - 13313, Loss - 0.627279451352689, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1141669177682745\n",
      "Step - 13314, Loss - 0.903590669101971, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9175771265627128\n",
      "Step - 13315, Loss - 0.8078488823089264, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4713895065560407\n",
      "Step - 13316, Loss - 0.7364433794707514, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.766835467596995\n",
      "Step - 13317, Loss - 0.6284205964312652, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.010952482606785\n",
      "Step - 13318, Loss - 0.7937946937633935, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7911111126961657\n",
      "Step - 13319, Loss - 0.8634507014930668, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.847734176598079\n",
      "Step - 13320, Loss - 0.5860172676347116, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7697718346892934\n",
      "Step - 13321, Loss - 0.7325610025958149, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6612832079637466\n",
      "Step - 13322, Loss - 0.4923389886190057, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.116562198490054\n",
      "Step - 13323, Loss - 0.71398783151592, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1312070038139768\n",
      "Step - 13324, Loss - 0.48888936841415964, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.10424402307791\n",
      "Step - 13325, Loss - 0.5242354922274947, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8928859775324043\n",
      "Step - 13326, Loss - 1.053447649219637, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.4192420837822826\n",
      "Step - 13327, Loss - 0.8798542341816972, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4485139681328851\n",
      "Step - 13328, Loss - 0.7482972072687742, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6878966312520598\n",
      "Step - 13329, Loss - 0.7113544696012222, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5700343595577815\n",
      "Step - 13330, Loss - 0.6860540686974866, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9165571024077793\n",
      "Step - 13331, Loss - 0.7218754846502851, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1081227014394746\n",
      "Step - 13332, Loss - 0.7538658574490089, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.843205942040626\n",
      "Step - 13333, Loss - 0.7088694393380393, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4140404645576565\n",
      "Step - 13334, Loss - 0.7819295228210934, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0874673601407174\n",
      "Step - 13335, Loss - 0.8667309668767006, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0319093427238417\n",
      "Step - 13336, Loss - 0.6654230246211862, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2965034150742143\n",
      "Step - 13337, Loss - 0.5632035699896021, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7106459617919496\n",
      "Step - 13338, Loss - 0.6721829521481932, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.539282639964906\n",
      "Step - 13339, Loss - 0.7761550919447388, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5562325796027356\n",
      "Step - 13340, Loss - 0.8199566713739228, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9354079405906732\n",
      "Step - 13341, Loss - 0.5662738433512108, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7668833167032387\n",
      "Step - 13342, Loss - 0.7544313158681705, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3923615373963603\n",
      "Step - 13343, Loss - 0.6898004064431953, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.5648810406654436\n",
      "Step - 13344, Loss - 0.5318938159671003, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.30857334919669344\n",
      "Step - 13345, Loss - 0.5742149595682636, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.251376228607468\n",
      "Step - 13346, Loss - 0.6075851345913125, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0459839152447734\n",
      "Step - 13347, Loss - 0.7748658872308323, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6076841696024595\n",
      "Step - 13348, Loss - 0.7560740545542999, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5458332632847225\n",
      "Step - 13349, Loss - 0.8659413792105429, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8066000997514757\n",
      "Step - 13350, Loss - 0.8896948602105939, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6156166973608439\n",
      "Step - 13351, Loss - 0.5695119628474477, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9512332988567601\n",
      "Step - 13352, Loss - 0.676306291622825, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5282750374206815\n",
      "Step - 13353, Loss - 0.6083559290229202, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.150592622706873\n",
      "Step - 13354, Loss - 0.6970155276129796, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.5096516960923045\n",
      "Step - 13355, Loss - 0.7065909817164494, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.056744717849695\n",
      "Step - 13356, Loss - 0.6427913085371126, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6641208536250293\n",
      "Step - 13357, Loss - 0.6675900508844913, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.26035947649932\n",
      "Step - 13358, Loss - 0.5229863092989253, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8677221994182622\n",
      "Step - 13359, Loss - 0.7141686953107987, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6665570438054501\n",
      "Step - 13360, Loss - 0.6651156212593168, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8997540657677466\n",
      "Step - 13361, Loss - 0.5402522695820491, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7900760029706041\n",
      "Step - 13362, Loss - 0.626497964834791, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8594955747799384\n",
      "Step - 13363, Loss - 0.7006983752345881, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8176960161284763\n",
      "Step - 13364, Loss - 0.743622168633751, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.509424117304209\n",
      "Step - 13365, Loss - 0.812574775342152, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.5327321327053745\n",
      "Step - 13366, Loss - 0.7965985650984168, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6492764716226886\n",
      "Step - 13367, Loss - 0.5885342736731529, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.373196565954575\n",
      "Step - 13368, Loss - 0.6926946446696586, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5778405722010571\n",
      "Step - 13369, Loss - 0.8259034860874239, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.453300827234238\n",
      "Step - 13370, Loss - 0.7888147363233434, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2430065474609415\n",
      "Step - 13371, Loss - 0.6712007005588553, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.415111692286417\n",
      "Step - 13372, Loss - 0.7977495617224304, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7678161792540577\n",
      "Step - 13373, Loss - 0.6351542773336699, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6861125257949718\n",
      "Step - 13374, Loss - 0.7979098504068153, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.200771790480076\n",
      "Step - 13375, Loss - 0.5258187519000961, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9234765470017844\n",
      "Step - 13376, Loss - 0.7992910607065817, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.244469472289855\n",
      "Step - 13377, Loss - 0.916527903732973, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3216836973070567\n",
      "Step - 13378, Loss - 0.8395841319769702, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.471248247236654\n",
      "Step - 13379, Loss - 0.750340523134751, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7178276042046685\n",
      "Step - 13380, Loss - 0.702602075489142, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3443804826563652\n",
      "Step - 13381, Loss - 0.7211334421027158, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5231728131382593\n",
      "Step - 13382, Loss - 0.7086815737447638, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6783699106927416\n",
      "Step - 13383, Loss - 0.5828509515317405, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9211464605539181\n",
      "Step - 13384, Loss - 0.7038644443338019, Learning Rate - 6.103515625e-06, magnitude of gradient - 3.5974753315707897\n",
      "Step - 13385, Loss - 0.7463939213717475, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9885893518811898\n",
      "Step - 13386, Loss - 0.7861446219499799, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7789831636395528\n",
      "Step - 13387, Loss - 0.6548771883349154, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.153845081177537\n",
      "Step - 13388, Loss - 0.731607780714985, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9884685031036343\n",
      "Step - 13389, Loss - 0.8363150572825468, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.08061085051427\n",
      "Step - 13390, Loss - 0.7499978166825447, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2230458014508399\n",
      "Step - 13391, Loss - 0.6518976814267291, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3079455390605346\n",
      "Step - 13392, Loss - 0.5695596867422915, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0607778211761982\n",
      "Step - 13393, Loss - 0.9799852971098203, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2333959627206932\n",
      "Step - 13394, Loss - 0.686194208476165, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6878156063056624\n",
      "Step - 13395, Loss - 0.7956205843882533, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5179506618775025\n",
      "Step - 13396, Loss - 0.4567479269614938, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.251626564791228\n",
      "Step - 13397, Loss - 0.8508572182370837, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4812166917428637\n",
      "Step - 13398, Loss - 0.783630441427106, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3834370707562562\n",
      "Step - 13399, Loss - 0.838226138727487, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5706333034820406\n",
      "Step - 13400, Loss - 0.7428189584386141, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0117668264603785\n",
      "Step - 13401, Loss - 0.7009736456318252, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6820700133035961\n",
      "Step - 13402, Loss - 0.5525306759730635, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8127201553719603\n",
      "Step - 13403, Loss - 0.826806736519873, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.356079284149938\n",
      "Step - 13404, Loss - 0.6607428365252275, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6743182927239117\n",
      "Step - 13405, Loss - 0.8470410864312985, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5852395738966278\n",
      "Step - 13406, Loss - 0.9322094365433296, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2363271698997007\n",
      "Step - 13407, Loss - 0.6807750511581564, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.0500782284381245\n",
      "Step - 13408, Loss - 0.860865995981561, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7546947532607688\n",
      "Step - 13409, Loss - 0.5471502657130525, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3953165675144574\n",
      "Step - 13410, Loss - 0.5288266644993733, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2620330488453546\n",
      "Step - 13411, Loss - 0.8106409047846856, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0933785220720758\n",
      "Step - 13412, Loss - 0.6909911118800678, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5677541810236908\n",
      "Step - 13413, Loss - 0.7348621799378344, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9296631920746017\n",
      "Step - 13414, Loss - 0.7434407531938266, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.966320640192373\n",
      "Step - 13415, Loss - 0.7098652003152044, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.42949315066291904\n",
      "Step - 13416, Loss - 0.6799660502306981, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.831953539078069\n",
      "Step - 13417, Loss - 0.9068673959583003, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.817718041103665\n",
      "Step - 13418, Loss - 0.8357443679509069, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3002030863570448\n",
      "Step - 13419, Loss - 0.7455144880600282, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0163857312098208\n",
      "Step - 13420, Loss - 0.7388118563282647, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5089067272793417\n",
      "Step - 13421, Loss - 0.8892366345174374, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.25336967290407314\n",
      "Step - 13422, Loss - 0.7783023209912455, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2499328520947994\n",
      "Step - 13423, Loss - 0.8649162193282008, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.602314814661374\n",
      "Step - 13424, Loss - 0.7825489404588685, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0221963708413062\n",
      "Step - 13425, Loss - 0.48468640006851677, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2268340215098594\n",
      "Step - 13426, Loss - 0.8385577201466758, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0497848570131645\n",
      "Step - 13427, Loss - 0.6520679121481328, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0122931115031282\n",
      "Step - 13428, Loss - 0.7078352662196069, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.1263227828936615\n",
      "Step - 13429, Loss - 0.841389065622933, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5395685025786668\n",
      "Step - 13430, Loss - 0.7671189772768657, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8881619904648147\n",
      "Step - 13431, Loss - 0.7373192644346299, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9774650704325328\n",
      "Step - 13432, Loss - 0.889681313867594, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.595312457685314\n",
      "Step - 13433, Loss - 0.6702351623231935, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.031966081358443\n",
      "Step - 13434, Loss - 0.6253068546659534, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6068251550276518\n",
      "Step - 13435, Loss - 0.7418097806055841, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6573521058159443\n",
      "Step - 13436, Loss - 0.7921951337977556, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.038999480345446\n",
      "Step - 13437, Loss - 0.6009798442282819, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6513899240880305\n",
      "Step - 13438, Loss - 0.7034644293830121, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8567029383350749\n",
      "Step - 13439, Loss - 0.8087297949067918, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2890026938587746\n",
      "Step - 13440, Loss - 0.5430096530062928, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5130479686214269\n",
      "Step - 13441, Loss - 0.7031921941404095, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.4227270649949455\n",
      "Step - 13442, Loss - 0.7358348654912924, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.0580720104969585\n",
      "Step - 13443, Loss - 0.5011815517887699, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7387450965333633\n",
      "Step - 13444, Loss - 0.6836559051776603, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9486911160513631\n",
      "Step - 13445, Loss - 0.6765427136722775, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1543474487866032\n",
      "Step - 13446, Loss - 0.5426038234006395, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5759229826492556\n",
      "Step - 13447, Loss - 0.7273046541371951, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7941808381967486\n",
      "Step - 13448, Loss - 0.6770427728483341, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0036128643777387\n",
      "Step - 13449, Loss - 0.7646473112302152, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7241778903959257\n",
      "Step - 13450, Loss - 0.6979627675431259, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2832317371225637\n",
      "Step - 13451, Loss - 0.772905614156792, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.251576440288076\n",
      "Step - 13452, Loss - 0.6202800433403577, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.763501479703284\n",
      "Step - 13453, Loss - 0.6181598682160065, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8367935890323984\n",
      "Step - 13454, Loss - 0.6149838324098962, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.0853949375682506\n",
      "Step - 13455, Loss - 0.8764559460496617, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.471808744043896\n",
      "Step - 13456, Loss - 0.7936727903156816, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7935905008669313\n",
      "Step - 13457, Loss - 0.7940733234143902, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.685299923145512\n",
      "Step - 13458, Loss - 0.9089904932078221, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3860636713579064\n",
      "Step - 13459, Loss - 0.7536158589991788, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5422900735536845\n",
      "Step - 13460, Loss - 0.7955503786175685, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5391038274028764\n",
      "Step - 13461, Loss - 0.6262114876676518, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1167208649300995\n",
      "Step - 13462, Loss - 0.5850337161068702, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8164408700203194\n",
      "Step - 13463, Loss - 0.7786421383673562, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6621742152197132\n",
      "Step - 13464, Loss - 0.7092357465574827, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1745398681587984\n",
      "Step - 13465, Loss - 0.8513907325686817, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.0980865629967607\n",
      "Step - 13466, Loss - 0.8411943508423629, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.90249460553584\n",
      "Step - 13467, Loss - 0.6417245501204943, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9334010986629886\n",
      "Step - 13468, Loss - 0.6469684378567346, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4252124021359114\n",
      "Step - 13469, Loss - 0.6134356862891642, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1880672877520257\n",
      "Step - 13470, Loss - 0.7638327604687487, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8901359291160654\n",
      "Step - 13471, Loss - 0.7046783098357541, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1920629538305851\n",
      "Step - 13472, Loss - 0.6869898128784896, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9171174799198508\n",
      "Step - 13473, Loss - 0.7906078067033854, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.930101862543803\n",
      "Step - 13474, Loss - 0.7314379261019816, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7858619036149195\n",
      "Step - 13475, Loss - 0.8181126352147692, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1065457469340625\n",
      "Step - 13476, Loss - 0.6990072350082579, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.857404191588226\n",
      "Step - 13477, Loss - 0.5389444595325099, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7530246015889449\n",
      "Step - 13478, Loss - 0.6348148407170867, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7008030301080416\n",
      "Step - 13479, Loss - 0.6239289863510538, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6946461760073049\n",
      "Step - 13480, Loss - 0.7551988301568331, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6489232319022864\n",
      "Step - 13481, Loss - 0.8579116318367348, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1865516915005647\n",
      "Step - 13482, Loss - 0.6119019119197328, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5550874193618719\n",
      "Step - 13483, Loss - 0.5625159934081227, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.010462665224218\n",
      "Step - 13484, Loss - 0.713741217779522, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7644159556041806\n",
      "Step - 13485, Loss - 0.7292886822376181, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9708374685984676\n",
      "Step - 13486, Loss - 0.6554250374900121, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7191111320240688\n",
      "Step - 13487, Loss - 0.6974902589397316, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8951706961147063\n",
      "Step - 13488, Loss - 0.6991644399952118, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5108630761770256\n",
      "Step - 13489, Loss - 0.49045161161119755, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8725326863205818\n",
      "Step - 13490, Loss - 0.7568297281583266, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.86729434928646\n",
      "Step - 13491, Loss - 0.8674938044840006, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1850614272393798\n",
      "Step - 13492, Loss - 0.6744956191208488, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5541069069410577\n",
      "Step - 13493, Loss - 0.75111044382981, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.48874016322472247\n",
      "Step - 13494, Loss - 0.8766806253286252, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1171847121071623\n",
      "Step - 13495, Loss - 0.6775733631916283, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6598589496304976\n",
      "Step - 13496, Loss - 0.7438553917965189, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.3738104778372709\n",
      "Step - 13497, Loss - 0.4823767145695075, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.733980892885558\n",
      "Step - 13498, Loss - 0.6372619701099885, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.525166979808055\n",
      "Step - 13499, Loss - 0.6682857754031208, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3669321498580391\n",
      "Step - 13500, Loss - 0.6420592888861597, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3636337610494151\n",
      "Step - 13501, Loss - 0.7221141085879866, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5950513141176292\n",
      "Step - 13502, Loss - 0.7237844978147583, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5874506030941757\n",
      "Step - 13503, Loss - 0.9241300772166745, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.349936951038607\n",
      "Step - 13504, Loss - 0.6507706853504374, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8082255870927917\n",
      "Step - 13505, Loss - 0.7904795170904863, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7070343029939237\n",
      "Step - 13506, Loss - 0.6303878395506908, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8859337814088878\n",
      "Step - 13507, Loss - 0.8047119789414227, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4054777707632002\n",
      "Step - 13508, Loss - 0.7758169131649707, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5364401657404358\n",
      "Step - 13509, Loss - 0.5481037990534091, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7119334840711191\n",
      "Step - 13510, Loss - 0.7787385843357436, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7152384416106036\n",
      "Step - 13511, Loss - 0.6063978222318442, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1954063107407689\n",
      "Step - 13512, Loss - 0.796563292619515, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.851180983150398\n",
      "Step - 13513, Loss - 0.9374515636832432, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6189016643175281\n",
      "Step - 13514, Loss - 0.7619744172168571, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.3909527346842907\n",
      "Step - 13515, Loss - 1.1043858305119485, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9772596179220846\n",
      "Step - 13516, Loss - 0.6650367534798793, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2922681229426565\n",
      "Step - 13517, Loss - 0.5829813568441505, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0838067368350783\n",
      "Step - 13518, Loss - 0.6205141948804311, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7888894080541655\n",
      "Step - 13519, Loss - 0.7412236719737851, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1707038204389792\n",
      "Step - 13520, Loss - 0.7896586395662551, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5403668460992046\n",
      "Step - 13521, Loss - 0.5433264274267172, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7036930252046762\n",
      "Step - 13522, Loss - 0.7194180090224152, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6841313439368928\n",
      "Step - 13523, Loss - 0.669145765388698, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.858464862607697\n",
      "Step - 13524, Loss - 0.8084391091600596, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1820978544649017\n",
      "Step - 13525, Loss - 0.8680496840744782, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.1806548982750846\n",
      "Step - 13526, Loss - 0.7837296769768172, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4925164980856913\n",
      "Step - 13527, Loss - 0.8667361313534591, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7812943038246238\n",
      "Step - 13528, Loss - 0.7983156249002216, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7544153826230754\n",
      "Step - 13529, Loss - 0.7634837490165083, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8560328509797831\n",
      "Step - 13530, Loss - 0.5764038807578101, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9672354508708118\n",
      "Step - 13531, Loss - 0.636898552595906, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8815949443265859\n",
      "Step - 13532, Loss - 0.6332593247408038, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3114379919760895\n",
      "Step - 13533, Loss - 0.6579350670223146, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1099937239149662\n",
      "Step - 13534, Loss - 0.6144956004495964, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5875668899160003\n",
      "Step - 13535, Loss - 0.87328073200938, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.0948786836051707\n",
      "Step - 13536, Loss - 0.5179612610817312, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1981633081707497\n",
      "Step - 13537, Loss - 0.6017062119131815, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8781033222677395\n",
      "Step - 13538, Loss - 0.6035274826125777, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8866915247434901\n",
      "Step - 13539, Loss - 0.5462404553342626, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8065975175933406\n",
      "Step - 13540, Loss - 0.771014877378035, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7087156876229502\n",
      "Step - 13541, Loss - 0.46537441046322814, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7425494031630794\n",
      "Step - 13542, Loss - 0.7133975094712565, Learning Rate - 6.103515625e-06, magnitude of gradient - 3.1598662162839717\n",
      "Step - 13543, Loss - 0.7214615036843799, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5914297168972374\n",
      "Step - 13544, Loss - 0.7514844333866213, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8622667469518983\n",
      "Step - 13545, Loss - 0.785496616071298, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8539322436231348\n",
      "Step - 13546, Loss - 0.5753912983418091, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.135005313482282\n",
      "Step - 13547, Loss - 0.6839708688892333, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9050518798824285\n",
      "Step - 13548, Loss - 0.6455910258006337, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8206840149823778\n",
      "Step - 13549, Loss - 0.8155029406040586, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.481690464561973\n",
      "Step - 13550, Loss - 0.7473888484336433, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9756969749460647\n",
      "Step - 13551, Loss - 0.6480356979012724, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.4528498713798556\n",
      "Step - 13552, Loss - 0.7472425868794799, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.241616736904624\n",
      "Step - 13553, Loss - 0.6566904351307821, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3662346975012094\n",
      "Step - 13554, Loss - 0.8286050291998986, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4648119489886426\n",
      "Step - 13555, Loss - 0.572682131810405, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1622584229416026\n",
      "Step - 13556, Loss - 0.5838855158785605, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7611787819113386\n",
      "Step - 13557, Loss - 0.8724895879238174, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.4430563033023094\n",
      "Step - 13558, Loss - 0.7220292415329459, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.41699860640573005\n",
      "Step - 13559, Loss - 0.8552775890517519, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9650005837260506\n",
      "Step - 13560, Loss - 0.7917937619221503, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.564476631585956\n",
      "Step - 13561, Loss - 0.6461872559986739, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1122409323690035\n",
      "Step - 13562, Loss - 0.7394126862424223, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4069494470717505\n",
      "Step - 13563, Loss - 0.7292930300106106, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1249621861420496\n",
      "Step - 13564, Loss - 0.9451937403333522, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8783895221139747\n",
      "Step - 13565, Loss - 0.5930911966649037, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.027851024692274\n",
      "Step - 13566, Loss - 0.7935188468807068, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8164159027079627\n",
      "Step - 13567, Loss - 0.6326547026571365, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0558186393619038\n",
      "Step - 13568, Loss - 0.7251043282308753, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.49989486031788255\n",
      "Step - 13569, Loss - 0.646613683145923, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6900974467277673\n",
      "Step - 13570, Loss - 0.7646579874160582, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.7657261351262936\n",
      "Step - 13571, Loss - 0.7552296196796747, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1034095038987657\n",
      "Step - 13572, Loss - 0.5925786376131836, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.23659078637395986\n",
      "Step - 13573, Loss - 0.5637683907947653, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5275417687783084\n",
      "Step - 13574, Loss - 0.6132686430786982, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5370120476723628\n",
      "Step - 13575, Loss - 0.9180435561803059, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.690099348861974\n",
      "Step - 13576, Loss - 0.7733156534447378, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0050740654481636\n",
      "Step - 13577, Loss - 0.8532907715440561, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3347095582683584\n",
      "Step - 13578, Loss - 0.7370612029979016, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5065639586025719\n",
      "Step - 13579, Loss - 0.6908290587982588, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6455305579253403\n",
      "Step - 13580, Loss - 0.7770486992494843, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.4685140424603348\n",
      "Step - 13581, Loss - 0.7609978095966625, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6424024932446152\n",
      "Step - 13582, Loss - 0.8532031856816141, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.249059876495268\n",
      "Step - 13583, Loss - 0.5951865919862098, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6165527175315957\n",
      "Step - 13584, Loss - 0.6855028861929552, Learning Rate - 6.103515625e-06, magnitude of gradient - 3.123451932055617\n",
      "Step - 13585, Loss - 0.8067365214842777, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.5163904582632908\n",
      "Step - 13586, Loss - 0.7659050063357483, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3861306511217422\n",
      "Step - 13587, Loss - 0.7239093318362136, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1693099680318895\n",
      "Step - 13588, Loss - 0.817583817902571, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.9282841473254777\n",
      "Step - 13589, Loss - 0.7743382368802083, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2521313146915305\n",
      "Step - 13590, Loss - 0.7303201271840318, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.775052648532343\n",
      "Step - 13591, Loss - 0.7678948539289858, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.201784626202385\n",
      "Step - 13592, Loss - 0.8111992880144734, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2545773728086163\n",
      "Step - 13593, Loss - 0.7406641092539711, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.36010834940676756\n",
      "Step - 13594, Loss - 0.8105863161159641, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2693296167477286\n",
      "Step - 13595, Loss - 0.7878483903190647, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9733780966912845\n",
      "Step - 13596, Loss - 0.5058895483557955, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.735714421281562\n",
      "Step - 13597, Loss - 0.7001661256822096, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.3798871485671361\n",
      "Step - 13598, Loss - 0.7137846302612149, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6920005064639798\n",
      "Step - 13599, Loss - 0.6218163373183604, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7521978209804379\n",
      "Step - 13600, Loss - 0.5546523695547371, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5990251842060552\n",
      "Step - 13601, Loss - 0.6063677398635019, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.428084159305256\n",
      "Step - 13602, Loss - 0.6780076636395185, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9807082391995231\n",
      "Step - 13603, Loss - 0.5832077698598699, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7269106361373214\n",
      "Step - 13604, Loss - 0.6863430198757579, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1258970328439577\n",
      "Step - 13605, Loss - 0.707244114327489, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8966549473349873\n",
      "Step - 13606, Loss - 0.8533618250309908, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8628228623049059\n",
      "Step - 13607, Loss - 0.9438043066437141, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8790511042614656\n",
      "Step - 13608, Loss - 0.6482050765093207, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.963891152589312\n",
      "Step - 13609, Loss - 0.5878898707617574, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8924207114912529\n",
      "Step - 13610, Loss - 0.5957915263871402, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7831251115773266\n",
      "Step - 13611, Loss - 0.8131185256258253, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8981637669914289\n",
      "Step - 13612, Loss - 0.672805236604966, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.49279529953773504\n",
      "Step - 13613, Loss - 0.6554695461840616, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6331466140677218\n",
      "Step - 13614, Loss - 0.7980211234160812, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5084424599614346\n",
      "Step - 13615, Loss - 0.5825811415876851, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7716183989322665\n",
      "Step - 13616, Loss - 0.5186242965392909, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.27051915265351373\n",
      "Step - 13617, Loss - 0.9213125842445238, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.1368197448662136\n",
      "Step - 13618, Loss - 0.8504954142094197, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3319982655030564\n",
      "Step - 13619, Loss - 0.6863729593319006, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8995860562353651\n",
      "Step - 13620, Loss - 0.8155225068957969, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7831848875365766\n",
      "Step - 13621, Loss - 0.7834604731567016, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.99514202368274\n",
      "Step - 13622, Loss - 0.6230349458440874, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.373878171814445\n",
      "Step - 13623, Loss - 0.596281565226837, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7802065660611192\n",
      "Step - 13624, Loss - 0.8113519360611503, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8245678673374712\n",
      "Step - 13625, Loss - 0.45877675714684396, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.003115603771772\n",
      "Step - 13626, Loss - 0.7838795194710656, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.986171432248657\n",
      "Step - 13627, Loss - 0.6618114097426633, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.637194149231299\n",
      "Step - 13628, Loss - 0.926228967056727, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8811861173372485\n",
      "Step - 13629, Loss - 0.6196281896413747, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1745030078323453\n",
      "Step - 13630, Loss - 0.6542292262383372, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.3055055895559606\n",
      "Step - 13631, Loss - 0.7340269690197198, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.556403725771202\n",
      "Step - 13632, Loss - 0.5100381543448279, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2596333731520415\n",
      "Step - 13633, Loss - 0.7112092398559651, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2708194219389592\n",
      "Step - 13634, Loss - 0.6946535930455587, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1440486693072576\n",
      "Step - 13635, Loss - 0.5989501254384731, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6947777349014567\n",
      "Step - 13636, Loss - 0.7478518101400792, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8831540322717764\n",
      "Step - 13637, Loss - 0.831067552244436, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.17837165048141\n",
      "Step - 13638, Loss - 0.7051043094134785, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9764088819202319\n",
      "Step - 13639, Loss - 0.7092408693792498, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5096975255964373\n",
      "Step - 13640, Loss - 0.6849847738721986, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7265147826793418\n",
      "Step - 13641, Loss - 0.6910522934279368, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.328848685687767\n",
      "Step - 13642, Loss - 0.7449105328280298, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6752216190641179\n",
      "Step - 13643, Loss - 0.817985714971787, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6042880615136523\n",
      "Step - 13644, Loss - 0.8098442056006001, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6948428503286035\n",
      "Step - 13645, Loss - 0.6655910464589094, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4391887166438697\n",
      "Step - 13646, Loss - 0.6178514619994474, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9858296505945663\n",
      "Step - 13647, Loss - 0.8446552133588433, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0995040155911129\n",
      "Step - 13648, Loss - 0.8178112309263565, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.10227350191428\n",
      "Step - 13649, Loss - 0.7034468781864402, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6074570791731723\n",
      "Step - 13650, Loss - 0.5199492230585853, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7585854492643317\n",
      "Step - 13651, Loss - 0.4894467458134623, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.343863799611273\n",
      "Step - 13652, Loss - 0.7976786287998155, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.705792485422216\n",
      "Step - 13653, Loss - 0.7765594427696465, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9567952653314938\n",
      "Step - 13654, Loss - 0.6019553139880449, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7544408293618748\n",
      "Step - 13655, Loss - 0.7531385398903967, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8526256622659807\n",
      "Step - 13656, Loss - 0.5923960450744985, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6691550999059142\n",
      "Step - 13657, Loss - 0.7173154613611263, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.255703141498685\n",
      "Step - 13658, Loss - 0.5843356219328696, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4813167485664855\n",
      "Step - 13659, Loss - 0.8722481009106005, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.1167985992724705\n",
      "Step - 13660, Loss - 0.7417986667212589, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5484626394422611\n",
      "Step - 13661, Loss - 0.44296207238053303, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4031203962717118\n",
      "Step - 13662, Loss - 0.6135250657303841, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.590144285225297\n",
      "Step - 13663, Loss - 0.6810626947335537, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.6999729514962385\n",
      "Step - 13664, Loss - 0.8278962776820962, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.702360270735705\n",
      "Step - 13665, Loss - 0.5670620539282709, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.5557639730994777\n",
      "Step - 13666, Loss - 0.7035649560459409, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.1967516652004617\n",
      "Step - 13667, Loss - 0.4879721137798797, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0497574348342835\n",
      "Step - 13668, Loss - 0.7264707098439969, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.670854019366183\n",
      "Step - 13669, Loss - 0.6440725171987605, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9013905832275507\n",
      "Step - 13670, Loss - 0.7065197937770396, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.26262892592919757\n",
      "Step - 13671, Loss - 0.6854773923829152, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.167576750007752\n",
      "Step - 13672, Loss - 0.7054780330318082, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8103498162497993\n",
      "Step - 13673, Loss - 0.8054406759708402, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6021492997817246\n",
      "Step - 13674, Loss - 0.6129030873430823, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6790384480405169\n",
      "Step - 13675, Loss - 0.8699178249698688, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.83860287254795\n",
      "Step - 13676, Loss - 0.8814076691567654, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.1747346080552874\n",
      "Step - 13677, Loss - 0.7280655205605516, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3880148439012205\n",
      "Step - 13678, Loss - 0.7224031107582773, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8254119802296594\n",
      "Step - 13679, Loss - 0.7247181747987085, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8729297492730909\n",
      "Step - 13680, Loss - 0.8163276401858295, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8797122569471343\n",
      "Step - 13681, Loss - 0.8164384664250703, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.184623289264641\n",
      "Step - 13682, Loss - 0.7944467846739638, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.002790274132396\n",
      "Step - 13683, Loss - 0.5278342442570698, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7920430404463474\n",
      "Step - 13684, Loss - 0.7943667824790814, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3405034172491428\n",
      "Step - 13685, Loss - 0.6085124676059048, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2413402937180336\n",
      "Step - 13686, Loss - 0.6779382865951079, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7196481515062237\n",
      "Step - 13687, Loss - 0.8834916288517128, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9702922624190937\n",
      "Step - 13688, Loss - 0.44454963794377417, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1911587228940255\n",
      "Step - 13689, Loss - 0.6379035668186327, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.4437509318925481\n",
      "Step - 13690, Loss - 0.7728416993534305, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3679786661861493\n",
      "Step - 13691, Loss - 0.7464016350542576, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5495285486833438\n",
      "Step - 13692, Loss - 0.6433524701457384, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7248791407967096\n",
      "Step - 13693, Loss - 0.8311675325003697, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5454957120725077\n",
      "Step - 13694, Loss - 0.49905905011967455, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9540918877169873\n",
      "Step - 13695, Loss - 0.7113898946389492, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7935236317744854\n",
      "Step - 13696, Loss - 0.6966225806958917, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5856770483567719\n",
      "Step - 13697, Loss - 0.43376198393525245, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9247047933102924\n",
      "Step - 13698, Loss - 0.6203624959570283, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1632539514985778\n",
      "Step - 13699, Loss - 0.7587458825841474, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8432823150018636\n",
      "Step - 13700, Loss - 0.7476177868160483, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1255996788458846\n",
      "Step - 13701, Loss - 0.6108124837623582, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7061810105225139\n",
      "Step - 13702, Loss - 0.5234515627458218, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7946159273197384\n",
      "Step - 13703, Loss - 0.6982779381636037, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4524718958349474\n",
      "Step - 13704, Loss - 0.6007430476766044, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5147738079815545\n",
      "Step - 13705, Loss - 0.6586284002882827, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6205060350155643\n",
      "Step - 13706, Loss - 0.8227029009805352, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4504436626530566\n",
      "Step - 13707, Loss - 0.5487758067937055, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0945799496730142\n",
      "Step - 13708, Loss - 0.5488947786748317, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6110477173302566\n",
      "Step - 13709, Loss - 0.6323233272083018, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6927030204561482\n",
      "Step - 13710, Loss - 0.5989259816799819, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8854125581818855\n",
      "Step - 13711, Loss - 0.6722492418636274, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0804987458309012\n",
      "Step - 13712, Loss - 0.6654901975203678, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5978244659914208\n",
      "Step - 13713, Loss - 0.8943026010359167, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.384827682222417\n",
      "Step - 13714, Loss - 0.728861509503782, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6327907203161559\n",
      "Step - 13715, Loss - 0.6519898526558191, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.4663890312714742\n",
      "Step - 13716, Loss - 0.6206154375919746, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6071900027568748\n",
      "Step - 13717, Loss - 0.78218859128634, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8158314129508353\n",
      "Step - 13718, Loss - 0.8542690731112554, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9489506879167977\n",
      "Step - 13719, Loss - 0.8958155017240015, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2440121174765042\n",
      "Step - 13720, Loss - 0.7742899767630465, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0313659183746071\n",
      "Step - 13721, Loss - 0.7168183298881197, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5275740800251109\n",
      "Step - 13722, Loss - 0.5243408673095853, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.0293818688134624\n",
      "Step - 13723, Loss - 0.6472647380703318, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0630120023923306\n",
      "Step - 13724, Loss - 0.7789573317854227, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.285204997213821\n",
      "Step - 13725, Loss - 0.761473472331748, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.374391678005312\n",
      "Step - 13726, Loss - 0.7152708838198379, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7932976584753875\n",
      "Step - 13727, Loss - 0.4723623655737429, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8164742056402208\n",
      "Step - 13728, Loss - 0.8262894586046088, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9741529050273533\n",
      "Step - 13729, Loss - 0.6812512999034891, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.433231996086958\n",
      "Step - 13730, Loss - 1.0606677276191345, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6474059235960834\n",
      "Step - 13731, Loss - 0.7354562498189913, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.407891062443028\n",
      "Step - 13732, Loss - 0.7761356283618464, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.601812033420475\n",
      "Step - 13733, Loss - 0.7721262735510531, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2521945555183591\n",
      "Step - 13734, Loss - 0.7964350991890132, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1743739207501147\n",
      "Step - 13735, Loss - 0.6678234066503883, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8346735122146458\n",
      "Step - 13736, Loss - 0.7442107615524645, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8231466519955553\n",
      "Step - 13737, Loss - 0.6083441504874474, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8110302220176561\n",
      "Step - 13738, Loss - 0.6293030543877642, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1180941850311725\n",
      "Step - 13739, Loss - 0.7158192392286661, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4970767585028535\n",
      "Step - 13740, Loss - 0.6467571183232278, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2438446442815003\n",
      "Step - 13741, Loss - 0.7933768149230729, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.563300260595411\n",
      "Step - 13742, Loss - 0.8876792317919195, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2040337169786144\n",
      "Step - 13743, Loss - 0.6790866618782894, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.4524871971208198\n",
      "Step - 13744, Loss - 0.8628795661946069, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.3228289265173847\n",
      "Step - 13745, Loss - 0.8039121982493473, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.100917595958982\n",
      "Step - 13746, Loss - 0.7997771835768883, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6436521852325854\n",
      "Step - 13747, Loss - 0.5940259979342116, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7096544426771694\n",
      "Step - 13748, Loss - 0.8625279500909556, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2821252385828186\n",
      "Step - 13749, Loss - 0.7573218990266535, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3097066203950554\n",
      "Step - 13750, Loss - 0.5828739634953282, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.5918991644162896\n",
      "Step - 13751, Loss - 0.6774704601719843, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3411075900889253\n",
      "Step - 13752, Loss - 0.7607309182391534, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.4212834049717929\n",
      "Step - 13753, Loss - 0.6135949644714593, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2092605036664994\n",
      "Step - 13754, Loss - 0.45648127835149777, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3480609825516103\n",
      "Step - 13755, Loss - 0.6937479610894662, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2376339683446198\n",
      "Step - 13756, Loss - 0.7507305465773103, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.33567165625085\n",
      "Step - 13757, Loss - 0.7733747080233495, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0864676953922034\n",
      "Step - 13758, Loss - 0.6357561768192344, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8899568577611834\n",
      "Step - 13759, Loss - 0.877690221233314, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5670466435632208\n",
      "Step - 13760, Loss - 0.7121786734911835, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1194378127223346\n",
      "Step - 13761, Loss - 0.7102714389390512, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.4341935719810219\n",
      "Step - 13762, Loss - 0.6878942075104901, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.065332921711701\n",
      "Step - 13763, Loss - 0.5309013307787828, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9460219794531517\n",
      "Step - 13764, Loss - 0.8390448220118218, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2187432557367959\n",
      "Step - 13765, Loss - 0.7120216283667231, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9514651892044912\n",
      "Step - 13766, Loss - 0.7037965621353921, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8731343004301835\n",
      "Step - 13767, Loss - 0.6796549034294198, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8155409736883006\n",
      "Step - 13768, Loss - 0.6326876175484346, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8376381843188305\n",
      "Step - 13769, Loss - 0.769933103250782, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4192060813785443\n",
      "Step - 13770, Loss - 0.68214453897042, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9231901826275026\n",
      "Step - 13771, Loss - 0.7001173346402982, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9502604319558072\n",
      "Step - 13772, Loss - 0.6382430631290332, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.4236181785069455\n",
      "Step - 13773, Loss - 0.6332546715072342, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.935294147503184\n",
      "Step - 13774, Loss - 0.9434103440168866, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1025059852152908\n",
      "Step - 13775, Loss - 0.7830090149954974, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7210370650442821\n",
      "Step - 13776, Loss - 0.6903292293657555, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8677715126983562\n",
      "Step - 13777, Loss - 0.6837624803218456, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.4433297145479646\n",
      "Step - 13778, Loss - 0.5193554865334322, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1225251539388166\n",
      "Step - 13779, Loss - 0.6326915210437386, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9451712544605602\n",
      "Step - 13780, Loss - 0.6191611834099576, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6878043216062825\n",
      "Step - 13781, Loss - 0.7812630886598119, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0850349859391828\n",
      "Step - 13782, Loss - 0.8785004373053864, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9730421668198967\n",
      "Step - 13783, Loss - 0.5474536437968692, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1375581889850839\n",
      "Step - 13784, Loss - 0.6812517684934398, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4929725699290646\n",
      "Step - 13785, Loss - 0.7664793329581026, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5936848606308954\n",
      "Step - 13786, Loss - 0.747532083146515, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8725479939863892\n",
      "Step - 13787, Loss - 0.7242547807199963, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8047182658843769\n",
      "Step - 13788, Loss - 0.5599507414161767, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6297179061252524\n",
      "Step - 13789, Loss - 0.6562086264954087, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7260670054636145\n",
      "Step - 13790, Loss - 0.7280348597797694, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8725081430782937\n",
      "Step - 13791, Loss - 0.6998897773485507, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9056858308387016\n",
      "Step - 13792, Loss - 0.6373061998499421, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9924493055241416\n",
      "Step - 13793, Loss - 0.6841296711927518, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.089369366551338\n",
      "Step - 13794, Loss - 0.6213794015924349, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6552593978405707\n",
      "Step - 13795, Loss - 0.7496204590800066, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5567153570352986\n",
      "Step - 13796, Loss - 0.7757792490210578, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.138905408963935\n",
      "Step - 13797, Loss - 0.6660815629254138, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.42773988497771653\n",
      "Step - 13798, Loss - 0.6298927641122886, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.3114458129118374\n",
      "Step - 13799, Loss - 0.6791041616438636, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9494596337115774\n",
      "Step - 13800, Loss - 0.6181866486430605, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9406738455945607\n",
      "Step - 13801, Loss - 0.5072618061396611, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.848877113261987\n",
      "Step - 13802, Loss - 0.793696751362351, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.818154551638988\n",
      "Step - 13803, Loss - 0.6952478380176491, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8883533065751783\n",
      "Step - 13804, Loss - 0.5316205259503076, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3171730679162401\n",
      "Step - 13805, Loss - 0.9753123161150672, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6613141160714976\n",
      "Step - 13806, Loss - 0.5588600969116546, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7832798425303196\n",
      "Step - 13807, Loss - 0.7892781363227107, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.888878573940514\n",
      "Step - 13808, Loss - 0.6644865599192998, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7524848424152326\n",
      "Step - 13809, Loss - 0.6899523236124598, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0954523629460537\n",
      "Step - 13810, Loss - 0.693410908707033, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8794032642094982\n",
      "Step - 13811, Loss - 0.6205775959920516, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.1742136904996086\n",
      "Step - 13812, Loss - 0.5079632198269187, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.514284143974194\n",
      "Step - 13813, Loss - 0.6594283033350546, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5132767410914545\n",
      "Step - 13814, Loss - 0.7903268125710945, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4897691272236968\n",
      "Step - 13815, Loss - 0.8111594547893223, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.217199288712034\n",
      "Step - 13816, Loss - 0.7585510883303056, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.838715435213383\n",
      "Step - 13817, Loss - 0.7724094072268909, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6231148528873007\n",
      "Step - 13818, Loss - 0.66678016816099, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0837410432851151\n",
      "Step - 13819, Loss - 0.580590962725881, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7213761232698992\n",
      "Step - 13820, Loss - 0.7829458059934399, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9014081950913813\n",
      "Step - 13821, Loss - 0.76963894047702, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.768292972970122\n",
      "Step - 13822, Loss - 0.6525393660980823, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7942939127527266\n",
      "Step - 13823, Loss - 0.8135558556696478, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.02835284094995\n",
      "Step - 13824, Loss - 0.6688410662399247, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0825190345843048\n",
      "Step - 13825, Loss - 0.8829764275864289, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.270139975484807\n",
      "Step - 13826, Loss - 0.6073280291634082, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1345414215297924\n",
      "Step - 13827, Loss - 0.7065475269762307, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.908627053976671\n",
      "Step - 13828, Loss - 0.6476061630981412, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9879960619999516\n",
      "Step - 13829, Loss - 0.5236477178982545, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5627120120137485\n",
      "Step - 13830, Loss - 0.6818262107706058, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5686384910149533\n",
      "Step - 13831, Loss - 0.7955860016119048, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0665062404156596\n",
      "Step - 13832, Loss - 0.7481757832317478, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3034284600264523\n",
      "Step - 13833, Loss - 0.9425246628255524, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4285519773633335\n",
      "Step - 13834, Loss - 0.7031085497982517, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8951219165743418\n",
      "Step - 13835, Loss - 0.6763171360535289, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5072176150060879\n",
      "Step - 13836, Loss - 0.6029969294936294, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8757248400585683\n",
      "Step - 13837, Loss - 0.5373995347510393, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6318965899019395\n",
      "Step - 13838, Loss - 0.705311660856341, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6126915083284097\n",
      "Step - 13839, Loss - 0.5820000901237868, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.4879888314568968\n",
      "Step - 13840, Loss - 0.6888472657576032, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8745639496503984\n",
      "Step - 13841, Loss - 0.7939969452857754, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5956466049329263\n",
      "Step - 13842, Loss - 0.7510528614783897, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0565980938769202\n",
      "Step - 13843, Loss - 0.6915644212840732, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.3937648616973353\n",
      "Step - 13844, Loss - 0.7697372778692694, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.068945391977156\n",
      "Step - 13845, Loss - 0.7609182857095307, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1587674850845553\n",
      "Step - 13846, Loss - 0.6731097585634102, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.140546507319447\n",
      "Step - 13847, Loss - 0.6896744103567494, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.929567223951959\n",
      "Step - 13848, Loss - 0.7325806656184615, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.4536176852655465\n",
      "Step - 13849, Loss - 0.7712713844737911, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6470169609104277\n",
      "Step - 13850, Loss - 0.6325460733715927, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4710777384559988\n",
      "Step - 13851, Loss - 0.5700000515726193, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2958762764125047\n",
      "Step - 13852, Loss - 0.9241402806954109, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0368921005386975\n",
      "Step - 13853, Loss - 0.5988628470144747, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.366594095149161\n",
      "Step - 13854, Loss - 0.6492369010777456, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7917739433107287\n",
      "Step - 13855, Loss - 0.45779614475901004, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5147861653552266\n",
      "Step - 13856, Loss - 0.8174254873946751, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6094811064085748\n",
      "Step - 13857, Loss - 0.5152938308915094, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5055126950618563\n",
      "Step - 13858, Loss - 0.722800405676002, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.4571855024790071\n",
      "Step - 13859, Loss - 0.5919681945829184, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8467372478757317\n",
      "Step - 13860, Loss - 0.7166105202161671, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0372653633684816\n",
      "Step - 13861, Loss - 0.7144067974355399, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5895707346858314\n",
      "Step - 13862, Loss - 0.6553358493084698, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7233434548065525\n",
      "Step - 13863, Loss - 1.0062660864659032, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3397434731166005\n",
      "Step - 13864, Loss - 0.6689254527491637, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.27873201527500285\n",
      "Step - 13865, Loss - 0.7614000352749931, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4912907148248231\n",
      "Step - 13866, Loss - 0.6362236512352377, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.37592705723045955\n",
      "Step - 13867, Loss - 0.6738786167043261, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.7602945720026466\n",
      "Step - 13868, Loss - 0.7024024033861606, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.965704728198066\n",
      "Step - 13869, Loss - 0.7815639730512698, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0219811900278966\n",
      "Step - 13870, Loss - 0.8476629740670092, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.138866085318796\n",
      "Step - 13871, Loss - 0.8421507626441705, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8903146703399336\n",
      "Step - 13872, Loss - 0.7156957253645873, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5723626047682855\n",
      "Step - 13873, Loss - 0.692033538684138, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.714777661034421\n",
      "Step - 13874, Loss - 0.6581563370771975, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5321028020992389\n",
      "Step - 13875, Loss - 0.8015027883472756, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8335154220380816\n",
      "Step - 13876, Loss - 0.7084259031547854, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7553742429164744\n",
      "Step - 13877, Loss - 0.8185259208682296, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.499686372274156\n",
      "Step - 13878, Loss - 0.5652373404408954, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2581548650275194\n",
      "Step - 13879, Loss - 0.7520740340242528, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7886625243647374\n",
      "Step - 13880, Loss - 0.7419055089991211, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9994754854207516\n",
      "Step - 13881, Loss - 0.542803611947864, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.0380745881546787\n",
      "Step - 13882, Loss - 0.6256790904685943, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5463956865720917\n",
      "Step - 13883, Loss - 0.684291254242093, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7741744056187978\n",
      "Step - 13884, Loss - 0.7703598744478583, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.85676120485225\n",
      "Step - 13885, Loss - 0.7189405195132345, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8227342296824554\n",
      "Step - 13886, Loss - 0.6283479028308403, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7734849228252049\n",
      "Step - 13887, Loss - 0.6156818625723797, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2916428114010992\n",
      "Step - 13888, Loss - 0.684787219364114, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2696989239659318\n",
      "Step - 13889, Loss - 0.65481680052194, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.0074664188580877\n",
      "Step - 13890, Loss - 0.6820582287134255, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6779819749788301\n",
      "Step - 13891, Loss - 0.6442934708465465, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.1740838698514096\n",
      "Step - 13892, Loss - 0.5739035573649989, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7188394608583492\n",
      "Step - 13893, Loss - 0.8315900279272171, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.2642937672749026\n",
      "Step - 13894, Loss - 0.4820360055252755, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.085552052154253\n",
      "Step - 13895, Loss - 0.7392872524832506, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8231341479068982\n",
      "Step - 13896, Loss - 0.6582680218224419, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5497482766939016\n",
      "Step - 13897, Loss - 0.5240662315212261, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1754857026898264\n",
      "Step - 13898, Loss - 0.8240590926698613, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1184095889788421\n",
      "Step - 13899, Loss - 0.7766194420255692, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2208542253292323\n",
      "Step - 13900, Loss - 0.720911467169081, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7243379767258474\n",
      "Step - 13901, Loss - 0.7537234845624861, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9341946261938885\n",
      "Step - 13902, Loss - 0.8561899201709798, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2843163791089003\n",
      "Step - 13903, Loss - 0.46425206087349774, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.39795563178635623\n",
      "Step - 13904, Loss - 0.7862167008779878, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9020665740699725\n",
      "Step - 13905, Loss - 0.6646299177839521, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9882373663513717\n",
      "Step - 13906, Loss - 0.712041966046441, Learning Rate - 6.103515625e-06, magnitude of gradient - 3.0126161000426643\n",
      "Step - 13907, Loss - 0.7453208465837804, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6501092539645887\n",
      "Step - 13908, Loss - 0.7047111979547721, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7066198631009013\n",
      "Step - 13909, Loss - 0.6920522766486624, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7634045138061704\n",
      "Step - 13910, Loss - 1.0121093754640222, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.6474597923005672\n",
      "Step - 13911, Loss - 0.7452439892977674, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.842177730931593\n",
      "Step - 13912, Loss - 0.5493948233937399, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1120286970332325\n",
      "Step - 13913, Loss - 0.6821323498347093, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3178122977813664\n",
      "Step - 13914, Loss - 0.5975085130790234, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5144419352269887\n",
      "Step - 13915, Loss - 0.6420726956732296, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3010131541951069\n",
      "Step - 13916, Loss - 0.6082881881626111, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.428552289875369\n",
      "Step - 13917, Loss - 0.6907312952346893, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6477557879947038\n",
      "Step - 13918, Loss - 0.7504064101578669, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.719396268912738\n",
      "Step - 13919, Loss - 0.7856501124804784, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9575051331017692\n",
      "Step - 13920, Loss - 0.7080123586687643, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.4971092317272606\n",
      "Step - 13921, Loss - 0.6998734901069924, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8058159479711783\n",
      "Step - 13922, Loss - 0.5995188031844622, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4936080501610487\n",
      "Step - 13923, Loss - 0.8060878950953615, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.301655981067785\n",
      "Step - 13924, Loss - 0.6389049058315599, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.307125462280742\n",
      "Step - 13925, Loss - 0.730885391617019, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4967082667437854\n",
      "Step - 13926, Loss - 0.5868335435831343, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9662298625677972\n",
      "Step - 13927, Loss - 0.6731916051020096, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.3674708835153884\n",
      "Step - 13928, Loss - 0.8146353894794225, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6038479235139131\n",
      "Step - 13929, Loss - 0.7930621134018647, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9025138743732235\n",
      "Step - 13930, Loss - 0.6953631607314391, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6562197193402152\n",
      "Step - 13931, Loss - 0.7337098739614278, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9401048304938272\n",
      "Step - 13932, Loss - 0.6973499492595956, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4096658541751743\n",
      "Step - 13933, Loss - 0.7214543128108294, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8760405908488635\n",
      "Step - 13934, Loss - 0.6784156602300399, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5930037382688945\n",
      "Step - 13935, Loss - 0.7929702168604769, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0770235822428318\n",
      "Step - 13936, Loss - 0.5667385860195057, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.34251044007206555\n",
      "Step - 13937, Loss - 0.7191597959802354, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1104054578802736\n",
      "Step - 13938, Loss - 0.4338895641712566, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.5262177071797\n",
      "Step - 13939, Loss - 0.7914880161919244, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9611969547792358\n",
      "Step - 13940, Loss - 0.6654431593588376, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6392438944801675\n",
      "Step - 13941, Loss - 0.5794801518877792, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1714449368064683\n",
      "Step - 13942, Loss - 0.6274588759225374, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1225894572637871\n",
      "Step - 13943, Loss - 0.596753846383206, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9457137062141755\n",
      "Step - 13944, Loss - 0.6263396303740872, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0940993283136051\n",
      "Step - 13945, Loss - 0.5813950741249463, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5793400622946085\n",
      "Step - 13946, Loss - 0.7739429011856899, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3568726844194006\n",
      "Step - 13947, Loss - 0.7086423275747433, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1676254296249284\n",
      "Step - 13948, Loss - 0.7158938443162829, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.45690311064937345\n",
      "Step - 13949, Loss - 0.6159468635447867, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6587462588676702\n",
      "Step - 13950, Loss - 0.7464980427640725, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0623172740515776\n",
      "Step - 13951, Loss - 0.5848685832499476, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.730764969187079\n",
      "Step - 13952, Loss - 0.717381259403683, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0728811049992153\n",
      "Step - 13953, Loss - 0.8905176726646195, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.442849451480742\n",
      "Step - 13954, Loss - 0.5661421130545071, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5943424853256811\n",
      "Step - 13955, Loss - 0.5358423234823965, Learning Rate - 6.103515625e-06, magnitude of gradient - 3.0043473502348728\n",
      "Step - 13956, Loss - 0.7254716829074404, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7099565052904269\n",
      "Step - 13957, Loss - 0.6793349322910636, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7682227524268161\n",
      "Step - 13958, Loss - 0.8566577575454373, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7168668718015252\n",
      "Step - 13959, Loss - 0.5909738524217704, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.6234213611828242\n",
      "Step - 13960, Loss - 0.745999842799603, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.7043596426121216\n",
      "Step - 13961, Loss - 0.5879253750704472, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.528337360038323\n",
      "Step - 13962, Loss - 0.784124695512751, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9655518390965359\n",
      "Step - 13963, Loss - 0.8029360161709733, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3127673016129222\n",
      "Step - 13964, Loss - 0.6932739184529308, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0119697451883876\n",
      "Step - 13965, Loss - 0.6257881244398834, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.424187263689266\n",
      "Step - 13966, Loss - 0.8090649413432522, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9675931806885174\n",
      "Step - 13967, Loss - 0.771771809658867, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0530407447748167\n",
      "Step - 13968, Loss - 0.6108799888303672, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3267111213410874\n",
      "Step - 13969, Loss - 0.706953607752342, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5755051913263667\n",
      "Step - 13970, Loss - 0.7861034328876434, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.4358309732659247\n",
      "Step - 13971, Loss - 0.6599626987344225, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.38801878373731574\n",
      "Step - 13972, Loss - 0.5786952525573008, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1912200502897823\n",
      "Step - 13973, Loss - 0.768898417904142, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.5172156998270276\n",
      "Step - 13974, Loss - 0.6003353775463609, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8372728736117799\n",
      "Step - 13975, Loss - 0.8041850461981127, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.438378879979792\n",
      "Step - 13976, Loss - 0.753517805870118, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3195436211481515\n",
      "Step - 13977, Loss - 0.6504778782738257, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.699059378580168\n",
      "Step - 13978, Loss - 0.6995357197417073, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2279370457567829\n",
      "Step - 13979, Loss - 0.6452642902753574, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.7030343116636983\n",
      "Step - 13980, Loss - 0.9407031668270723, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2998051805724107\n",
      "Step - 13981, Loss - 0.8738260780763737, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.857396022577259\n",
      "Step - 13982, Loss - 0.5469723216477679, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0134364147536359\n",
      "Step - 13983, Loss - 0.935186300820163, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.1578259494807672\n",
      "Step - 13984, Loss - 0.9111671851669367, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.730698922665162\n",
      "Step - 13985, Loss - 0.7890114684546645, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.9308320893292875\n",
      "Step - 13986, Loss - 0.5919724448440493, Learning Rate - 6.103515625e-06, magnitude of gradient - 0.8069735601756696\n",
      "Step - 13987, Loss - 0.7659254126880434, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2977680703051828\n",
      "Step - 13988, Loss - 0.571052217437005, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.0314197026579468\n",
      "Step - 13989, Loss - 0.7324468681868513, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.221118748494052\n",
      "Step - 13990, Loss - 1.0695060755936876, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.7163893579082443\n",
      "Step - 13991, Loss - 0.5263375654186098, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.1274059788208093\n",
      "Step - 13992, Loss - 0.8541767129725905, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.5979242519563053\n",
      "Step - 13993, Loss - 0.5526651532601858, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2679923063582048\n",
      "Step - 13994, Loss - 0.5695498670362327, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.9369876507626211\n",
      "Step - 13995, Loss - 0.5071366682690449, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.2399849448630162\n",
      "Step - 13996, Loss - 1.017153194938281, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.8493973727083928\n",
      "Step - 13997, Loss - 0.6771234089139978, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.7287775739795093\n",
      "Step - 13998, Loss - 0.5946657928929759, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.676880028617001\n",
      "Step - 13999, Loss - 0.7195620741024168, Learning Rate - 6.103515625e-06, magnitude of gradient - 1.3640737012248985\n",
      "Step - 14000, Loss - 0.7823562341950571, Learning Rate - 6.103515625e-06, magnitude of gradient - 2.0241700476059865\n",
      "Step - 14001, Loss - 0.6068306481499681, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9830394096657671\n",
      "Step - 14002, Loss - 0.7395260198788324, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2696410068275437\n",
      "Step - 14003, Loss - 0.6035114613259909, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.4859055984235069\n",
      "Step - 14004, Loss - 0.6446261071897365, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.830708629561032\n",
      "Step - 14005, Loss - 0.5501555064489899, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7148910511237245\n",
      "Step - 14006, Loss - 0.9531357501833764, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7166127011364044\n",
      "Step - 14007, Loss - 0.5534982852127265, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6868160746001386\n",
      "Step - 14008, Loss - 0.6368988592387533, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.891848091738372\n",
      "Step - 14009, Loss - 0.6796510278875229, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6739145644839577\n",
      "Step - 14010, Loss - 0.7438335963291911, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.988704130141766\n",
      "Step - 14011, Loss - 0.7209168387950841, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2527256159227018\n",
      "Step - 14012, Loss - 0.5680925187658995, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.227632414574753\n",
      "Step - 14013, Loss - 0.6421414467983988, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6899495415806514\n",
      "Step - 14014, Loss - 0.5995657415485469, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2063205209748982\n",
      "Step - 14015, Loss - 0.7119597610832589, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.247331037843055\n",
      "Step - 14016, Loss - 0.8972363503675151, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3103001019298113\n",
      "Step - 14017, Loss - 0.7117783950224402, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.4243811283267427\n",
      "Step - 14018, Loss - 0.5362460054314543, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6347005810852472\n",
      "Step - 14019, Loss - 0.7188638239578357, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4834648302819173\n",
      "Step - 14020, Loss - 0.7275557300170127, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7184856997503748\n",
      "Step - 14021, Loss - 0.7609294205082124, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.671775195571083\n",
      "Step - 14022, Loss - 0.8319491133122735, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5759374155351276\n",
      "Step - 14023, Loss - 0.722156295223617, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7808448006469078\n",
      "Step - 14024, Loss - 0.6706044464572214, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.414339335362625\n",
      "Step - 14025, Loss - 0.643746840660969, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.41398681243191954\n",
      "Step - 14026, Loss - 0.6597461699826273, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9142709534841453\n",
      "Step - 14027, Loss - 0.6379576897842298, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.546220300044299\n",
      "Step - 14028, Loss - 0.6630204019699861, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0183451454178059\n",
      "Step - 14029, Loss - 0.7367954278428118, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6691940933326603\n",
      "Step - 14030, Loss - 0.9171518754392081, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9785762972055443\n",
      "Step - 14031, Loss - 0.7779214550779893, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1615922515504848\n",
      "Step - 14032, Loss - 0.8565483227451232, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8237704016016634\n",
      "Step - 14033, Loss - 0.54960162809159, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3616461519811387\n",
      "Step - 14034, Loss - 0.49730053792657597, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1799854627128805\n",
      "Step - 14035, Loss - 0.650478227665559, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0886734248859835\n",
      "Step - 14036, Loss - 0.6897763263629679, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5246967490966188\n",
      "Step - 14037, Loss - 0.6496381763613083, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7652561058033303\n",
      "Step - 14038, Loss - 0.8476573387323542, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.118316764380224\n",
      "Step - 14039, Loss - 0.6580472709805858, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.28967293799052124\n",
      "Step - 14040, Loss - 0.6692208876869898, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.41861036844528615\n",
      "Step - 14041, Loss - 0.8154802016393384, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.1497615489857895\n",
      "Step - 14042, Loss - 0.7070413204439867, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7087904338521298\n",
      "Step - 14043, Loss - 0.6440817717003234, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.116272258967761\n",
      "Step - 14044, Loss - 0.4483293018735145, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7810818544919675\n",
      "Step - 14045, Loss - 0.741354385118657, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.307199614977666\n",
      "Step - 14046, Loss - 0.6498895352101739, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7259211559073517\n",
      "Step - 14047, Loss - 0.7167672530532082, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.5193032681246077\n",
      "Step - 14048, Loss - 0.6505442427899538, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6252855839509606\n",
      "Step - 14049, Loss - 0.8703545363294783, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8998567836663112\n",
      "Step - 14050, Loss - 0.8872903769299695, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3402913703091228\n",
      "Step - 14051, Loss - 0.8464335066597543, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5207310681230166\n",
      "Step - 14052, Loss - 0.927212351222346, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4452458755024062\n",
      "Step - 14053, Loss - 0.7028380835499306, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5131967549867552\n",
      "Step - 14054, Loss - 0.7238738770189526, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7821515462564868\n",
      "Step - 14055, Loss - 0.7621917100644612, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.37605508614193023\n",
      "Step - 14056, Loss - 0.6256898636885282, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9755987429473058\n",
      "Step - 14057, Loss - 0.7341272026315789, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.264741447299249\n",
      "Step - 14058, Loss - 0.8799505756148854, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0998311380018402\n",
      "Step - 14059, Loss - 0.6602931272525252, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7232024780216572\n",
      "Step - 14060, Loss - 0.7797678121680732, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3608584443265268\n",
      "Step - 14061, Loss - 0.6665478864149061, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5358579583035336\n",
      "Step - 14062, Loss - 0.7753189695368835, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3213387726553751\n",
      "Step - 14063, Loss - 0.5568258160556044, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6789231884361384\n",
      "Step - 14064, Loss - 0.7892336989310502, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.87248239226008\n",
      "Step - 14065, Loss - 0.8714363309130914, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.14471496070438\n",
      "Step - 14066, Loss - 0.6658460721493642, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.876405387135558\n",
      "Step - 14067, Loss - 0.6930211392166942, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2089715759848432\n",
      "Step - 14068, Loss - 0.6416190179560755, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9918585804375356\n",
      "Step - 14069, Loss - 0.6868278754366891, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.8110987966402279\n",
      "Step - 14070, Loss - 0.6031131274525634, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2185297263201587\n",
      "Step - 14071, Loss - 0.7643135783454059, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.193362588175983\n",
      "Step - 14072, Loss - 0.8885898940715957, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6635835580724887\n",
      "Step - 14073, Loss - 0.5877261290279427, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.038065259829603\n",
      "Step - 14074, Loss - 0.7659526500199074, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9251679057050615\n",
      "Step - 14075, Loss - 0.5659186440958586, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.3503280461640268\n",
      "Step - 14076, Loss - 0.438514230223934, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9419115483240872\n",
      "Step - 14077, Loss - 0.602152181094776, Learning Rate - 3.0517578125e-06, magnitude of gradient - 3.3596394401971135\n",
      "Step - 14078, Loss - 0.5805683334445577, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.179267725943705\n",
      "Step - 14079, Loss - 0.8468146545784971, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1611165341527325\n",
      "Step - 14080, Loss - 0.7424137522758679, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7223532102133134\n",
      "Step - 14081, Loss - 0.7035745702716042, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7861197799780983\n",
      "Step - 14082, Loss - 0.6026399618927368, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6033238094512317\n",
      "Step - 14083, Loss - 0.8536097531245648, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9915018271061604\n",
      "Step - 14084, Loss - 0.7363872039596339, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7438017119874523\n",
      "Step - 14085, Loss - 0.5879895959607266, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.33035194379693544\n",
      "Step - 14086, Loss - 0.5802431085452907, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5650389915806286\n",
      "Step - 14087, Loss - 0.7942302578612778, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9907907956796805\n",
      "Step - 14088, Loss - 0.9513473533102922, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.8792925584894562\n",
      "Step - 14089, Loss - 0.6504824936273579, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2811019241763582\n",
      "Step - 14090, Loss - 0.6959827816357091, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.621793056872005\n",
      "Step - 14091, Loss - 0.5176580741980573, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.996102104373687\n",
      "Step - 14092, Loss - 0.7317214497628591, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9510211764080729\n",
      "Step - 14093, Loss - 0.6337264232082823, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6116115601419934\n",
      "Step - 14094, Loss - 0.5594434883685997, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7853772688394167\n",
      "Step - 14095, Loss - 0.5866762467797203, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.162888495259942\n",
      "Step - 14096, Loss - 0.7300741906589384, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.549014539402453\n",
      "Step - 14097, Loss - 0.6889434243032958, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5613076542493857\n",
      "Step - 14098, Loss - 0.675749032851962, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0132329397960689\n",
      "Step - 14099, Loss - 0.7720649642561122, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0477241494185847\n",
      "Step - 14100, Loss - 0.6927390064512862, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1036541490639618\n",
      "Step - 14101, Loss - 0.7663367093895899, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.53433197974565\n",
      "Step - 14102, Loss - 0.6026471628375866, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8189637751067268\n",
      "Step - 14103, Loss - 0.757921828567292, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1433034225064145\n",
      "Step - 14104, Loss - 0.7249205908239746, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.610963816574676\n",
      "Step - 14105, Loss - 0.7716762558624148, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.173728842310391\n",
      "Step - 14106, Loss - 0.7404025450116924, Learning Rate - 3.0517578125e-06, magnitude of gradient - 3.897782613621342\n",
      "Step - 14107, Loss - 0.6537649191582678, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8400095654020044\n",
      "Step - 14108, Loss - 0.8176337786595286, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.192516963515089\n",
      "Step - 14109, Loss - 0.667571546001449, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.829519391468825\n",
      "Step - 14110, Loss - 0.7456229178036877, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.528488699292606\n",
      "Step - 14111, Loss - 0.6814711273368859, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9677388248676745\n",
      "Step - 14112, Loss - 0.7327065270812211, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.094498463012102\n",
      "Step - 14113, Loss - 0.5961486837053696, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1688847129905442\n",
      "Step - 14114, Loss - 0.6895448135596547, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.444835635549773\n",
      "Step - 14115, Loss - 0.5445937781434411, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7871912117610728\n",
      "Step - 14116, Loss - 0.8989291616114622, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.0085648337435753\n",
      "Step - 14117, Loss - 0.7333890001476453, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9866152394941478\n",
      "Step - 14118, Loss - 0.6427595898675973, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.437979944488668\n",
      "Step - 14119, Loss - 0.7614166294261879, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7833960020006682\n",
      "Step - 14120, Loss - 0.811212082514756, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.005072341443491\n",
      "Step - 14121, Loss - 0.6713507761385755, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3628792214038548\n",
      "Step - 14122, Loss - 0.8982636480536602, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7434554613887663\n",
      "Step - 14123, Loss - 0.7726455455695621, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7770898393248175\n",
      "Step - 14124, Loss - 0.708558645794581, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.322255240701796\n",
      "Step - 14125, Loss - 0.6842943122442199, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5646783752145356\n",
      "Step - 14126, Loss - 1.149042336740788, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.4457561259961427\n",
      "Step - 14127, Loss - 0.6358509083488647, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8608806948475815\n",
      "Step - 14128, Loss - 0.7593181418550996, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.8432971339838204\n",
      "Step - 14129, Loss - 0.6336221832668394, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.249846497033463\n",
      "Step - 14130, Loss - 0.6658822533934501, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.46270202629980434\n",
      "Step - 14131, Loss - 0.6887924520432901, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.0601478864253124\n",
      "Step - 14132, Loss - 0.7238011687751165, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.4115562774596892\n",
      "Step - 14133, Loss - 0.6263508093491839, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.4921305893838421\n",
      "Step - 14134, Loss - 0.8480080753218626, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.7801479522875687\n",
      "Step - 14135, Loss - 0.7409564543691803, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1864003440437672\n",
      "Step - 14136, Loss - 0.693788486692076, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0485039879397253\n",
      "Step - 14137, Loss - 0.767359921811726, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.557693316586691\n",
      "Step - 14138, Loss - 0.7613110313547773, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4371420215473463\n",
      "Step - 14139, Loss - 0.6255294074994262, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9223754139967734\n",
      "Step - 14140, Loss - 0.7370882216304747, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8805376847318671\n",
      "Step - 14141, Loss - 0.7231712144575375, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.865922151759623\n",
      "Step - 14142, Loss - 0.7538892158063288, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.389160967211012\n",
      "Step - 14143, Loss - 0.7002547157107715, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.300659951287063\n",
      "Step - 14144, Loss - 0.8024674706821471, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9576380964535662\n",
      "Step - 14145, Loss - 0.7534313237522006, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8284735026199757\n",
      "Step - 14146, Loss - 0.9296322746828758, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7953708714389676\n",
      "Step - 14147, Loss - 0.6685696437915354, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7622019071034382\n",
      "Step - 14148, Loss - 0.7023372347285514, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6756722479980402\n",
      "Step - 14149, Loss - 0.563716889249889, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6385957744458979\n",
      "Step - 14150, Loss - 1.1233605656302454, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6290258127083457\n",
      "Step - 14151, Loss - 0.6336856527789761, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0458030456334448\n",
      "Step - 14152, Loss - 0.8112580577146056, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9089663499708203\n",
      "Step - 14153, Loss - 0.8205887701160675, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.38367830085613097\n",
      "Step - 14154, Loss - 0.7531092820610332, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2003776851645842\n",
      "Step - 14155, Loss - 0.4671181010816919, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5930906876725621\n",
      "Step - 14156, Loss - 0.7796890622116386, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1494224616621322\n",
      "Step - 14157, Loss - 0.7487198169770165, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9327862886499262\n",
      "Step - 14158, Loss - 0.9788151983344757, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0007337989237857\n",
      "Step - 14159, Loss - 0.5796793695304905, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.3989647725220101\n",
      "Step - 14160, Loss - 0.6173723616015674, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3137164162390071\n",
      "Step - 14161, Loss - 0.8191331390159601, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8366786591481227\n",
      "Step - 14162, Loss - 0.6317735700348323, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3453061257139487\n",
      "Step - 14163, Loss - 0.6087936698129681, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9615972707472221\n",
      "Step - 14164, Loss - 0.6388033959865831, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.4842919591059712\n",
      "Step - 14165, Loss - 0.5517972455478894, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.45569091964601094\n",
      "Step - 14166, Loss - 0.8593989269207609, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9657637515475503\n",
      "Step - 14167, Loss - 0.4874829059058284, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7929120872141058\n",
      "Step - 14168, Loss - 0.5219610362565694, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0386485536501227\n",
      "Step - 14169, Loss - 0.6591929918302937, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4121327079488701\n",
      "Step - 14170, Loss - 0.5625107754361152, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5178518409067243\n",
      "Step - 14171, Loss - 0.8045293433172014, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5987761948980811\n",
      "Step - 14172, Loss - 0.6694855074222636, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3696765407453657\n",
      "Step - 14173, Loss - 0.850827052852086, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5286666775999596\n",
      "Step - 14174, Loss - 0.9464625320585834, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.469849173948666\n",
      "Step - 14175, Loss - 0.7229488119944402, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1680093066318515\n",
      "Step - 14176, Loss - 0.7102079059947154, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7473219887539038\n",
      "Step - 14177, Loss - 0.7530478060882154, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0486143056686033\n",
      "Step - 14178, Loss - 0.7857087143199469, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2702859919960499\n",
      "Step - 14179, Loss - 0.702375469164692, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.104097688934553\n",
      "Step - 14180, Loss - 0.5773072050650435, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.033594732145274\n",
      "Step - 14181, Loss - 0.6287756362833143, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6212422093476426\n",
      "Step - 14182, Loss - 0.8518008192287877, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.0391649906985827\n",
      "Step - 14183, Loss - 0.8435645183009494, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9527681753232963\n",
      "Step - 14184, Loss - 0.5801866111516442, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1466282016158542\n",
      "Step - 14185, Loss - 0.5890498295555342, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8411656799294921\n",
      "Step - 14186, Loss - 0.6133494843385983, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0493781521658394\n",
      "Step - 14187, Loss - 0.7161962435055167, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9458777927935883\n",
      "Step - 14188, Loss - 0.5203064414787457, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.870973186391826\n",
      "Step - 14189, Loss - 0.7824915352057435, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8576059208537052\n",
      "Step - 14190, Loss - 0.7121064246001525, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.682701239308828\n",
      "Step - 14191, Loss - 0.7472683317803549, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.419497492247056\n",
      "Step - 14192, Loss - 0.5786874957562775, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0674848012914782\n",
      "Step - 14193, Loss - 0.5905821052092594, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6631214449070346\n",
      "Step - 14194, Loss - 0.7144318984522302, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.3509768620394962\n",
      "Step - 14195, Loss - 0.5179549560231351, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1149761131373792\n",
      "Step - 14196, Loss - 0.8013465620040491, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9414296611485503\n",
      "Step - 14197, Loss - 0.6981826523724597, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0824634907476165\n",
      "Step - 14198, Loss - 0.6872380591588007, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.4094084436410168\n",
      "Step - 14199, Loss - 0.728124744433052, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2872214414909795\n",
      "Step - 14200, Loss - 0.8831355045421411, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9535690091892273\n",
      "Step - 14201, Loss - 0.8451521936802575, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.288381163808058\n",
      "Step - 14202, Loss - 0.6164607636212691, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.4667385890264989\n",
      "Step - 14203, Loss - 0.6273232078525297, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0791782823883656\n",
      "Step - 14204, Loss - 0.6856450910838812, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8531922476362477\n",
      "Step - 14205, Loss - 0.7459000368089129, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9891212264951713\n",
      "Step - 14206, Loss - 0.9258726286213681, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5189138784973922\n",
      "Step - 14207, Loss - 0.7288825235589576, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7560655980212547\n",
      "Step - 14208, Loss - 0.5813226650746193, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8796090872765442\n",
      "Step - 14209, Loss - 0.5703124933908933, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.689055285867787\n",
      "Step - 14210, Loss - 0.528109091411018, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9184275823589487\n",
      "Step - 14211, Loss - 0.5894171542168648, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.273427087813007\n",
      "Step - 14212, Loss - 0.5751023455199795, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2388052596576231\n",
      "Step - 14213, Loss - 0.7304078565291071, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.097799014260979\n",
      "Step - 14214, Loss - 0.7130315426776186, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8704188175205554\n",
      "Step - 14215, Loss - 0.6925491810862359, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9855307025568203\n",
      "Step - 14216, Loss - 0.5797900490430064, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7786963189361604\n",
      "Step - 14217, Loss - 0.642983040181243, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0916679321538405\n",
      "Step - 14218, Loss - 0.4908766919072545, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.607864286816966\n",
      "Step - 14219, Loss - 0.6600995643811718, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.3139857417363578\n",
      "Step - 14220, Loss - 0.5155360277183348, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.42104895788067515\n",
      "Step - 14221, Loss - 0.5914310525980433, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5108666595190751\n",
      "Step - 14222, Loss - 0.6720677240047577, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6809251881132932\n",
      "Step - 14223, Loss - 0.8779032784499243, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8245957022644845\n",
      "Step - 14224, Loss - 0.8697729715796237, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.6307099092966157\n",
      "Step - 14225, Loss - 0.7237743435409858, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0902464836800938\n",
      "Step - 14226, Loss - 0.7767934095608598, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9177988267587073\n",
      "Step - 14227, Loss - 0.7203488575047973, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.3288081098838931\n",
      "Step - 14228, Loss - 0.9360946911851732, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9622872873125272\n",
      "Step - 14229, Loss - 0.7181980452381125, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7868309432283065\n",
      "Step - 14230, Loss - 0.7674406617892988, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9136816541366939\n",
      "Step - 14231, Loss - 0.54714421758268, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.052025682126264\n",
      "Step - 14232, Loss - 0.6903651796233069, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.315949442428134\n",
      "Step - 14233, Loss - 0.7823580129680042, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8097811055293512\n",
      "Step - 14234, Loss - 0.47587195603277477, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6700937966161549\n",
      "Step - 14235, Loss - 0.7729089270067878, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.255688107684149\n",
      "Step - 14236, Loss - 0.7713747004036386, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.43121854555426\n",
      "Step - 14237, Loss - 0.7950715539322561, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.500796575226882\n",
      "Step - 14238, Loss - 0.7114536007080696, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8563555993430447\n",
      "Step - 14239, Loss - 0.5409453800488045, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9803466794581011\n",
      "Step - 14240, Loss - 0.6811261878122941, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5328306264954772\n",
      "Step - 14241, Loss - 0.6974543530623971, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7629368568985485\n",
      "Step - 14242, Loss - 0.812737850124289, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6163112447846235\n",
      "Step - 14243, Loss - 0.6513546736605053, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7501045586370935\n",
      "Step - 14244, Loss - 0.7558422641876636, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.265490541935958\n",
      "Step - 14245, Loss - 0.7300540444678897, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.32553977694272707\n",
      "Step - 14246, Loss - 0.7593275611329986, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6027931371901862\n",
      "Step - 14247, Loss - 0.7500941981571214, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3534684910355061\n",
      "Step - 14248, Loss - 0.7323343598426806, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9430799592221957\n",
      "Step - 14249, Loss - 0.6850205634748855, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.873033314783864\n",
      "Step - 14250, Loss - 0.650458863063583, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6597023567157113\n",
      "Step - 14251, Loss - 0.7306430613602003, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6341392098992695\n",
      "Step - 14252, Loss - 0.9671971664036938, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0889562950749214\n",
      "Step - 14253, Loss - 0.804935529155834, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8682581771563125\n",
      "Step - 14254, Loss - 0.8327508977469908, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6726161266605074\n",
      "Step - 14255, Loss - 0.6495725736238028, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3350197697354949\n",
      "Step - 14256, Loss - 0.8848017988394647, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9956194129467804\n",
      "Step - 14257, Loss - 0.8424704740349593, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.013882221898901\n",
      "Step - 14258, Loss - 0.73062660430686, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.0840381938362893\n",
      "Step - 14259, Loss - 0.793180652593056, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4454816634223107\n",
      "Step - 14260, Loss - 0.7601477819076126, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1454259425581759\n",
      "Step - 14261, Loss - 0.789931287198967, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1724952111241143\n",
      "Step - 14262, Loss - 0.7829993894771223, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9716898294922773\n",
      "Step - 14263, Loss - 0.7895982010484244, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.358925506471914\n",
      "Step - 14264, Loss - 0.6091539770204755, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3349127699910448\n",
      "Step - 14265, Loss - 0.7525350526700726, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2331881163673002\n",
      "Step - 14266, Loss - 0.64308544706276, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.431485593638144\n",
      "Step - 14267, Loss - 0.6608047007308504, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0421802682403254\n",
      "Step - 14268, Loss - 0.5940927899195595, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1310165180953413\n",
      "Step - 14269, Loss - 0.6201828277726824, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0218842956538896\n",
      "Step - 14270, Loss - 0.7841716732484416, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.158583945039423\n",
      "Step - 14271, Loss - 0.6749476485631325, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9184061138643977\n",
      "Step - 14272, Loss - 0.6823127054731739, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7541152258921288\n",
      "Step - 14273, Loss - 0.5493023613018986, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9538944032774282\n",
      "Step - 14274, Loss - 0.6655683974653039, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5241478917037161\n",
      "Step - 14275, Loss - 0.7365261026813944, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.2850921950922673\n",
      "Step - 14276, Loss - 0.6821605540710429, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9118381251661174\n",
      "Step - 14277, Loss - 0.5484756668046995, Learning Rate - 3.0517578125e-06, magnitude of gradient - 3.095473554734654\n",
      "Step - 14278, Loss - 0.7401367144129206, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5155481723526878\n",
      "Step - 14279, Loss - 0.7090621048250807, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0970322841894897\n",
      "Step - 14280, Loss - 0.6744123015065464, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8963868806885995\n",
      "Step - 14281, Loss - 0.6579666660792596, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0408640916312992\n",
      "Step - 14282, Loss - 0.7159828805431971, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.905822214955452\n",
      "Step - 14283, Loss - 0.6151139094761783, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.138580021617215\n",
      "Step - 14284, Loss - 0.6951714034924886, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8289962737589666\n",
      "Step - 14285, Loss - 0.5784140635829741, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.4827500938717018\n",
      "Step - 14286, Loss - 0.9213103722446052, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4173284291981412\n",
      "Step - 14287, Loss - 0.5725307415810673, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2540690127831282\n",
      "Step - 14288, Loss - 0.5725266414760795, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.3786122538245932\n",
      "Step - 14289, Loss - 0.6320157633337006, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0577208324884564\n",
      "Step - 14290, Loss - 0.8129984626014746, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7805635355430929\n",
      "Step - 14291, Loss - 0.8331865760041554, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.8716012783458367\n",
      "Step - 14292, Loss - 0.6870178406706297, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7983963197919713\n",
      "Step - 14293, Loss - 0.5458073380256246, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.606631185392186\n",
      "Step - 14294, Loss - 0.7445374796801846, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8630528221572594\n",
      "Step - 14295, Loss - 0.5768544119349042, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2061752261617256\n",
      "Step - 14296, Loss - 0.7492731224111568, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0712002772587972\n",
      "Step - 14297, Loss - 0.692379235381406, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7042938577836556\n",
      "Step - 14298, Loss - 0.5625126542676422, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.651434349739705\n",
      "Step - 14299, Loss - 0.7683913676960425, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8196309307429563\n",
      "Step - 14300, Loss - 0.5945827198626683, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.3500530547365538\n",
      "Step - 14301, Loss - 0.7747589033632161, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.189201234796702\n",
      "Step - 14302, Loss - 0.8772183908472626, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5243830037427747\n",
      "Step - 14303, Loss - 0.7104260116003639, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.4799785745494057\n",
      "Step - 14304, Loss - 0.7045036861305951, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9160113951846958\n",
      "Step - 14305, Loss - 0.6897335250935956, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.4683658382053455\n",
      "Step - 14306, Loss - 0.6679420290281405, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.895386789560918\n",
      "Step - 14307, Loss - 0.7723652427673913, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1670663151396397\n",
      "Step - 14308, Loss - 0.6625009329823807, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2197905423790376\n",
      "Step - 14309, Loss - 0.8600783927233607, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7710651538347937\n",
      "Step - 14310, Loss - 0.620704896161527, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.302999978199785\n",
      "Step - 14311, Loss - 0.774414255967531, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.450695295838959\n",
      "Step - 14312, Loss - 0.6486076707007062, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.1347690218903814\n",
      "Step - 14313, Loss - 0.858555446118514, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4187545503756696\n",
      "Step - 14314, Loss - 0.6011436825282425, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.2812565945448169\n",
      "Step - 14315, Loss - 0.6770914961084742, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5471232682904579\n",
      "Step - 14316, Loss - 0.7127963424244081, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1048257461525897\n",
      "Step - 14317, Loss - 0.7441838099333615, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6229188815692435\n",
      "Step - 14318, Loss - 0.5067689453184886, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0488642727619961\n",
      "Step - 14319, Loss - 0.7238841482214775, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.4034730349239033\n",
      "Step - 14320, Loss - 0.6139191123373104, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4515064552371564\n",
      "Step - 14321, Loss - 0.7152537832897874, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.8265441912208795\n",
      "Step - 14322, Loss - 0.9297097273266832, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.2603618575755253\n",
      "Step - 14323, Loss - 0.6508112155734458, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9452604836430192\n",
      "Step - 14324, Loss - 0.4931681018611999, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5592969236352752\n",
      "Step - 14325, Loss - 0.8319332443440326, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8573236885197233\n",
      "Step - 14326, Loss - 0.7820983471912821, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.064631820792157\n",
      "Step - 14327, Loss - 0.8896281495073515, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7861046463318528\n",
      "Step - 14328, Loss - 0.7402798175948283, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5646726431032262\n",
      "Step - 14329, Loss - 0.7343483444248877, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6216028874761124\n",
      "Step - 14330, Loss - 0.5449371857068829, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8577702893893027\n",
      "Step - 14331, Loss - 0.69587232641247, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9177176723336897\n",
      "Step - 14332, Loss - 0.8343285644321279, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4274955776368345\n",
      "Step - 14333, Loss - 0.8325655857790155, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8637028911164788\n",
      "Step - 14334, Loss - 0.6747714776921151, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.553184342444805\n",
      "Step - 14335, Loss - 0.5567278235804369, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.0985205874929194\n",
      "Step - 14336, Loss - 0.6400721132878628, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8582671407430332\n",
      "Step - 14337, Loss - 0.6805734635273291, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0366259497685817\n",
      "Step - 14338, Loss - 0.5904131349566215, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7815079251494215\n",
      "Step - 14339, Loss - 0.5673790146791452, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4777098147723071\n",
      "Step - 14340, Loss - 0.650016218455208, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9467599245239242\n",
      "Step - 14341, Loss - 0.7839335647651453, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.883818582465407\n",
      "Step - 14342, Loss - 0.680440066559011, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8338939006836437\n",
      "Step - 14343, Loss - 0.6449292429288855, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.41658304218901837\n",
      "Step - 14344, Loss - 0.5887039397916908, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.402141070768058\n",
      "Step - 14345, Loss - 0.6791682507754937, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1919638513444162\n",
      "Step - 14346, Loss - 0.664416573110906, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5515180996857918\n",
      "Step - 14347, Loss - 0.5581132056385621, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4076293684119898\n",
      "Step - 14348, Loss - 0.6663973353827953, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.2685789240857703\n",
      "Step - 14349, Loss - 0.7278180461025048, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.1558692426306982\n",
      "Step - 14350, Loss - 0.6728393562619096, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7509122378223363\n",
      "Step - 14351, Loss - 0.9093594445253511, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4462599380826264\n",
      "Step - 14352, Loss - 0.7557905265985602, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2887240832086422\n",
      "Step - 14353, Loss - 0.6503573164468959, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.15818307882376628\n",
      "Step - 14354, Loss - 0.9510437720243912, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.777785744162979\n",
      "Step - 14355, Loss - 0.6426224387533864, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9891655111779118\n",
      "Step - 14356, Loss - 0.8628156509210502, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.22205513319412\n",
      "Step - 14357, Loss - 0.7188237891666514, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8813211461604734\n",
      "Step - 14358, Loss - 0.7290319596350399, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5942334411326569\n",
      "Step - 14359, Loss - 0.8477685168645326, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5679998513333526\n",
      "Step - 14360, Loss - 0.6536403833371862, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0001685151465354\n",
      "Step - 14361, Loss - 0.6325936371967487, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7955477901801857\n",
      "Step - 14362, Loss - 0.7192172633032039, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9534632133131843\n",
      "Step - 14363, Loss - 0.8363019566415992, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.043812183728286\n",
      "Step - 14364, Loss - 0.6735809424525541, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.0263805259153584\n",
      "Step - 14365, Loss - 0.6310524566460443, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3641853623550413\n",
      "Step - 14366, Loss - 0.5209748211623985, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6542212552348294\n",
      "Step - 14367, Loss - 0.7018782558281902, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1517211865276609\n",
      "Step - 14368, Loss - 0.5908751010015525, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.41127884022583505\n",
      "Step - 14369, Loss - 0.7380390039649452, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.760802373601034\n",
      "Step - 14370, Loss - 0.5795636899158062, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.507095337301314\n",
      "Step - 14371, Loss - 0.639379892657975, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1076826838791278\n",
      "Step - 14372, Loss - 0.7184834947958791, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.021744826116115\n",
      "Step - 14373, Loss - 0.7948476351045547, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6438150117475989\n",
      "Step - 14374, Loss - 0.7893366200036612, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.36553102955377\n",
      "Step - 14375, Loss - 0.9107195024823012, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.270716838722651\n",
      "Step - 14376, Loss - 0.6972770418787477, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5812862894437005\n",
      "Step - 14377, Loss - 0.668402328655518, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.414410322895467\n",
      "Step - 14378, Loss - 0.5074607293129207, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.48469613104388976\n",
      "Step - 14379, Loss - 0.5974185167909102, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9780453394457902\n",
      "Step - 14380, Loss - 0.9053734189835081, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.3976741212625831\n",
      "Step - 14381, Loss - 0.8116219951951509, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9627213980896168\n",
      "Step - 14382, Loss - 0.6883803704635265, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6063571993574877\n",
      "Step - 14383, Loss - 0.8395363488119314, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2360178459694566\n",
      "Step - 14384, Loss - 0.8395851159622485, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5667525793197592\n",
      "Step - 14385, Loss - 0.7351576813619867, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4021844388305034\n",
      "Step - 14386, Loss - 0.7178211991730232, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6719360268264517\n",
      "Step - 14387, Loss - 0.6875597085422738, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.59579489840378\n",
      "Step - 14388, Loss - 0.7115394064225564, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6022892153411972\n",
      "Step - 14389, Loss - 0.7038297552769073, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.0603378088928506\n",
      "Step - 14390, Loss - 0.6408077451142604, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5384062658203324\n",
      "Step - 14391, Loss - 0.6084107080675245, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3308086891211546\n",
      "Step - 14392, Loss - 0.8395610673217162, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2847065611816486\n",
      "Step - 14393, Loss - 0.8248500268510152, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.40402582085779387\n",
      "Step - 14394, Loss - 0.5486095339752171, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.48213209219193787\n",
      "Step - 14395, Loss - 0.6575654980951036, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0718276478420334\n",
      "Step - 14396, Loss - 0.5747479052305644, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.49610573981239553\n",
      "Step - 14397, Loss - 0.903299624379408, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.8322312727419772\n",
      "Step - 14398, Loss - 0.6952774702461177, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7284656411015967\n",
      "Step - 14399, Loss - 0.6801168562296529, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.034464539875207\n",
      "Step - 14400, Loss - 0.5858708462326273, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4294730987443531\n",
      "Step - 14401, Loss - 0.68432483331435, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.638982733745256\n",
      "Step - 14402, Loss - 0.7954739940358438, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.834285113406558\n",
      "Step - 14403, Loss - 0.7920662188842951, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.159397032832441\n",
      "Step - 14404, Loss - 0.5877017265405295, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1387277362995236\n",
      "Step - 14405, Loss - 0.8947614080651104, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5893541368471307\n",
      "Step - 14406, Loss - 0.7018388357548471, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.856604378816334\n",
      "Step - 14407, Loss - 0.8204429392212608, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1355940718374355\n",
      "Step - 14408, Loss - 0.5404105383388466, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9581858825946108\n",
      "Step - 14409, Loss - 0.6358998736462673, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8895363835330211\n",
      "Step - 14410, Loss - 0.7192915335395933, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2236908714548833\n",
      "Step - 14411, Loss - 0.6981228503297404, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.321690580401906\n",
      "Step - 14412, Loss - 0.8969183023297107, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.619557912868675\n",
      "Step - 14413, Loss - 0.9173972481625331, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.7275931989842013\n",
      "Step - 14414, Loss - 0.734998338557123, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.3076226790320852\n",
      "Step - 14415, Loss - 0.882730233419996, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.373972144255465\n",
      "Step - 14416, Loss - 0.7330380013763548, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8398311312253107\n",
      "Step - 14417, Loss - 0.7821393722354899, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5452711134846568\n",
      "Step - 14418, Loss - 0.5820190919296863, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.2562637860514433\n",
      "Step - 14419, Loss - 0.589100458838936, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9148418304012984\n",
      "Step - 14420, Loss - 0.7195034684780719, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7499703001794148\n",
      "Step - 14421, Loss - 0.7931585966093178, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4293462925264355\n",
      "Step - 14422, Loss - 0.6044179865558321, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5661133742832557\n",
      "Step - 14423, Loss - 0.7035080128277291, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8550450351105835\n",
      "Step - 14424, Loss - 0.7605324376069842, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.754809342494867\n",
      "Step - 14425, Loss - 0.8291139488433609, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.143030022089834\n",
      "Step - 14426, Loss - 0.6423514882438284, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7239043995999281\n",
      "Step - 14427, Loss - 0.7493733984154024, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.778864275392888\n",
      "Step - 14428, Loss - 0.7987862710932705, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6189644352411567\n",
      "Step - 14429, Loss - 0.7417122288446665, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3994230533653733\n",
      "Step - 14430, Loss - 0.5457267703603785, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.3222966706051341\n",
      "Step - 14431, Loss - 0.6270137472263533, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.063030763668034\n",
      "Step - 14432, Loss - 0.6496047447134835, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0654606207369066\n",
      "Step - 14433, Loss - 0.7798310048086103, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.601240846544706\n",
      "Step - 14434, Loss - 0.6338528868679643, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3047348840552366\n",
      "Step - 14435, Loss - 0.7323432159254011, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4057123290351354\n",
      "Step - 14436, Loss - 0.6824809147926758, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7499204576905104\n",
      "Step - 14437, Loss - 0.4456900987459088, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7052802639371429\n",
      "Step - 14438, Loss - 0.6804207271454503, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7032871524984237\n",
      "Step - 14439, Loss - 0.631160308109971, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9672963518175123\n",
      "Step - 14440, Loss - 0.8529547074068926, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8732360927213988\n",
      "Step - 14441, Loss - 0.4406791998597183, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5253621207670085\n",
      "Step - 14442, Loss - 0.73515156245062, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4339952552540165\n",
      "Step - 14443, Loss - 0.6422392410803857, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6564676315792025\n",
      "Step - 14444, Loss - 0.6488075661460573, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2765162494679172\n",
      "Step - 14445, Loss - 0.5140868306299157, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.279698240320663\n",
      "Step - 14446, Loss - 0.67097663437924, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6197410214555874\n",
      "Step - 14447, Loss - 0.8184787647891009, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.026712228991615\n",
      "Step - 14448, Loss - 0.5476380297360104, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1629403787622574\n",
      "Step - 14449, Loss - 0.38927057106098434, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6082652243028515\n",
      "Step - 14450, Loss - 0.6979998815750171, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5791427982536129\n",
      "Step - 14451, Loss - 0.5915502750875791, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9212580791852021\n",
      "Step - 14452, Loss - 0.5751995437901322, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1736974011980439\n",
      "Step - 14453, Loss - 0.7595942242743702, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.749918256333286\n",
      "Step - 14454, Loss - 0.6788742473728461, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9812901512545397\n",
      "Step - 14455, Loss - 0.8051512480888061, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.1959980767989475\n",
      "Step - 14456, Loss - 0.725815266455967, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5492447128714034\n",
      "Step - 14457, Loss - 0.8122284441407369, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5995738267443199\n",
      "Step - 14458, Loss - 0.7411458483472446, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.1827706763136008\n",
      "Step - 14459, Loss - 0.7087666081590763, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7900231620008382\n",
      "Step - 14460, Loss - 0.666255444487497, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.329911656018481\n",
      "Step - 14461, Loss - 0.7044982941449278, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.515248737175431\n",
      "Step - 14462, Loss - 0.7457224135060211, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9598303123479086\n",
      "Step - 14463, Loss - 0.6513619582240293, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4381574847764498\n",
      "Step - 14464, Loss - 0.6808624870063883, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1097594697288387\n",
      "Step - 14465, Loss - 0.4790266555284773, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5600624984849387\n",
      "Step - 14466, Loss - 0.8347497724885655, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3895954657302303\n",
      "Step - 14467, Loss - 0.6384855889990149, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9297833459673653\n",
      "Step - 14468, Loss - 0.9259012127703947, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.772664434219043\n",
      "Step - 14469, Loss - 0.8022381158115924, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3045754955056874\n",
      "Step - 14470, Loss - 0.6767363459389145, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2238964477412422\n",
      "Step - 14471, Loss - 0.7505594769758874, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7481339282088215\n",
      "Step - 14472, Loss - 0.6257353536311124, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3862748271258583\n",
      "Step - 14473, Loss - 0.780450265637229, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8970657092704509\n",
      "Step - 14474, Loss - 0.7451765997742393, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.060931411542146\n",
      "Step - 14475, Loss - 0.7674773618677719, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.40148948002985\n",
      "Step - 14476, Loss - 0.7391205439754917, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5367307117055048\n",
      "Step - 14477, Loss - 0.5590878330980011, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.4901224873089083\n",
      "Step - 14478, Loss - 0.6937133604523971, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0318484257744245\n",
      "Step - 14479, Loss - 0.7094552486914666, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.35028025719107\n",
      "Step - 14480, Loss - 0.7480240127139091, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0110499928013865\n",
      "Step - 14481, Loss - 0.6307302052680547, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0378466945355456\n",
      "Step - 14482, Loss - 0.6820874198416804, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.380510004301646\n",
      "Step - 14483, Loss - 0.7145345908930154, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8727410570535409\n",
      "Step - 14484, Loss - 0.5812617441233622, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.575013131627579\n",
      "Step - 14485, Loss - 0.5302461817553842, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8769104901164653\n",
      "Step - 14486, Loss - 0.635253036713312, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3019163620974434\n",
      "Step - 14487, Loss - 0.646380439967789, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4490434524458282\n",
      "Step - 14488, Loss - 0.6112344417542466, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5882145238783925\n",
      "Step - 14489, Loss - 0.6607724695442759, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.4824116314460846\n",
      "Step - 14490, Loss - 0.7720551907515338, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.362402208559667\n",
      "Step - 14491, Loss - 0.8271329692014397, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3669923305374603\n",
      "Step - 14492, Loss - 0.6496333246192703, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3868976580729389\n",
      "Step - 14493, Loss - 0.665824184551236, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.328558641296082\n",
      "Step - 14494, Loss - 0.7372509460245397, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3108197505592474\n",
      "Step - 14495, Loss - 0.5898106691923334, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.46496702336705636\n",
      "Step - 14496, Loss - 0.7471774475042126, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.25943534108996613\n",
      "Step - 14497, Loss - 0.6789478629874199, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8445168345325624\n",
      "Step - 14498, Loss - 0.8630236010711031, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.2619225283298365\n",
      "Step - 14499, Loss - 0.6799342750007105, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1012996723093345\n",
      "Step - 14500, Loss - 0.5353412716971092, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8157915921121062\n",
      "Step - 14501, Loss - 0.7275766178943738, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3339847548261776\n",
      "Step - 14502, Loss - 0.672749633165662, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4934900605385362\n",
      "Step - 14503, Loss - 0.7997960005384785, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.551749039006986\n",
      "Step - 14504, Loss - 0.7858609176089598, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1223982001538575\n",
      "Step - 14505, Loss - 0.7097634268266053, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5085277715387881\n",
      "Step - 14506, Loss - 0.6698518456916163, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.960888727921328\n",
      "Step - 14507, Loss - 0.6217827453657228, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.32696651384792547\n",
      "Step - 14508, Loss - 0.6121723413212659, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9213627466495021\n",
      "Step - 14509, Loss - 0.6521718409358094, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.3552679031630324\n",
      "Step - 14510, Loss - 0.7258865673030312, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9518122003485137\n",
      "Step - 14511, Loss - 0.7558143423762171, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.3245141214359304\n",
      "Step - 14512, Loss - 0.5893173586360027, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3039234968521913\n",
      "Step - 14513, Loss - 0.7109371275275659, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6515533887199494\n",
      "Step - 14514, Loss - 0.6438587396503984, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1850270194641765\n",
      "Step - 14515, Loss - 0.7865387695943429, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3344952662001175\n",
      "Step - 14516, Loss - 0.7329189859254532, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.42512633386767285\n",
      "Step - 14517, Loss - 0.6280535298048533, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.4876619318666356\n",
      "Step - 14518, Loss - 0.7735787083055057, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.4487434126959507\n",
      "Step - 14519, Loss - 0.8599853193101312, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5581624073379972\n",
      "Step - 14520, Loss - 0.7538695520031822, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0260966570482797\n",
      "Step - 14521, Loss - 0.7159527221189351, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.116199994533081\n",
      "Step - 14522, Loss - 0.810006757475979, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7782506005390122\n",
      "Step - 14523, Loss - 0.6572345915681714, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7718862582376896\n",
      "Step - 14524, Loss - 0.7208586092277876, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.391203359329664\n",
      "Step - 14525, Loss - 0.6905402128292356, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9263094922406074\n",
      "Step - 14526, Loss - 0.4936305961478618, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5573826556539779\n",
      "Step - 14527, Loss - 0.7642159513320766, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6018559858972607\n",
      "Step - 14528, Loss - 0.6305961584466064, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4999223911051718\n",
      "Step - 14529, Loss - 0.6962060126472545, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.8431767324632415\n",
      "Step - 14530, Loss - 0.8170184651999534, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6220425849127108\n",
      "Step - 14531, Loss - 0.8277043163748753, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.085788164704538\n",
      "Step - 14532, Loss - 0.795302905111674, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3553428777960952\n",
      "Step - 14533, Loss - 0.5893528510550177, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9447169069775447\n",
      "Step - 14534, Loss - 0.398749586758448, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.139211718624194\n",
      "Step - 14535, Loss - 0.7118589688947443, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0502744921419114\n",
      "Step - 14536, Loss - 0.7128394494236139, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.4573390035736162\n",
      "Step - 14537, Loss - 0.8165356472903071, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.4593929904948806\n",
      "Step - 14538, Loss - 0.7611207786348508, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.994858988232305\n",
      "Step - 14539, Loss - 0.9288527358383255, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9801418564045108\n",
      "Step - 14540, Loss - 0.5572099835702113, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1006038247418524\n",
      "Step - 14541, Loss - 0.4627553374461334, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5066415946747571\n",
      "Step - 14542, Loss - 0.6810022997360222, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1377609399656432\n",
      "Step - 14543, Loss - 0.5449164333202391, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6393074100546727\n",
      "Step - 14544, Loss - 0.6377601189172917, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1137080829451824\n",
      "Step - 14545, Loss - 0.6447866281879883, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5650077176809905\n",
      "Step - 14546, Loss - 0.6887645157406628, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9465181011899527\n",
      "Step - 14547, Loss - 0.658670227263735, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4644977790848044\n",
      "Step - 14548, Loss - 0.47582916468136227, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9970691384371904\n",
      "Step - 14549, Loss - 0.6362861636504252, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6985045651596901\n",
      "Step - 14550, Loss - 0.5952561856172938, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.760916470853382\n",
      "Step - 14551, Loss - 0.597905261405494, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3934384724592748\n",
      "Step - 14552, Loss - 0.7174640195105209, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8890747715800409\n",
      "Step - 14553, Loss - 0.7063295335529217, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6145825363944248\n",
      "Step - 14554, Loss - 0.6874077897840457, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0461806557036235\n",
      "Step - 14555, Loss - 0.698686489283363, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9952347210746584\n",
      "Step - 14556, Loss - 0.5957929542292134, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7058135972695131\n",
      "Step - 14557, Loss - 0.5830669330116541, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9774052179173568\n",
      "Step - 14558, Loss - 0.5344377880172172, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5910910991073346\n",
      "Step - 14559, Loss - 0.801148484074861, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9851664297652586\n",
      "Step - 14560, Loss - 0.6723555245932467, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.402518114032281\n",
      "Step - 14561, Loss - 0.7514756225145373, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.3760588074935533\n",
      "Step - 14562, Loss - 0.6359547400529664, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9996651625763077\n",
      "Step - 14563, Loss - 0.7717731346187733, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.544651258622265\n",
      "Step - 14564, Loss - 0.5845203596941537, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0477411715418745\n",
      "Step - 14565, Loss - 0.6898748465414115, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7797635867334707\n",
      "Step - 14566, Loss - 0.7539386737429026, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3724625137994306\n",
      "Step - 14567, Loss - 0.7437517328939911, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7376572493345367\n",
      "Step - 14568, Loss - 0.558601388691196, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.2889918082701295\n",
      "Step - 14569, Loss - 0.7715029751715067, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9510660697368555\n",
      "Step - 14570, Loss - 0.5634951884113426, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5667022413172789\n",
      "Step - 14571, Loss - 0.6573651133756993, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0350677139124713\n",
      "Step - 14572, Loss - 0.6586080521742981, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.042884738437804\n",
      "Step - 14573, Loss - 0.7600927926428767, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0490030082440305\n",
      "Step - 14574, Loss - 0.6613254751139573, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6677311837386696\n",
      "Step - 14575, Loss - 0.5544121668405584, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.413227594057659\n",
      "Step - 14576, Loss - 0.7121562669419526, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.241936238808013\n",
      "Step - 14577, Loss - 0.8446825100936033, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.5778921714823286\n",
      "Step - 14578, Loss - 0.7466827460563852, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7720437755179255\n",
      "Step - 14579, Loss - 0.5685323191755961, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5945399169479824\n",
      "Step - 14580, Loss - 0.7943818422446605, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.878825291948544\n",
      "Step - 14581, Loss - 0.5751741628942986, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.22631633244755767\n",
      "Step - 14582, Loss - 0.5755051550446385, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.002619930427963\n",
      "Step - 14583, Loss - 0.6948383593284921, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2563672770096546\n",
      "Step - 14584, Loss - 0.5309996506834055, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0146256656834192\n",
      "Step - 14585, Loss - 0.7604924703969008, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4341950711811382\n",
      "Step - 14586, Loss - 0.7917334264766638, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6743101461861223\n",
      "Step - 14587, Loss - 0.6866972739736461, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.917468801635608\n",
      "Step - 14588, Loss - 0.8211635705901436, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.2229356167561765\n",
      "Step - 14589, Loss - 0.6925938003338503, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1244729343798967\n",
      "Step - 14590, Loss - 0.7228982598365791, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.4479437175064334\n",
      "Step - 14591, Loss - 0.7698394324876707, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6498570246046358\n",
      "Step - 14592, Loss - 1.048638983802739, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.0219086725195057\n",
      "Step - 14593, Loss - 0.6988964482738714, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9667048025505826\n",
      "Step - 14594, Loss - 0.5851348955749789, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.0006854334487376\n",
      "Step - 14595, Loss - 0.7440334784243617, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.4848982819797625\n",
      "Step - 14596, Loss - 0.6651672831226985, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4044151222768362\n",
      "Step - 14597, Loss - 0.7551347278886476, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1272368614213693\n",
      "Step - 14598, Loss - 0.8337123728653053, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.0283454213505134\n",
      "Step - 14599, Loss - 0.8630692805320259, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1552119502413587\n",
      "Step - 14600, Loss - 0.7759251592240723, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.98065899883662\n",
      "Step - 14601, Loss - 0.654331412532107, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.8208449010993493\n",
      "Step - 14602, Loss - 0.6382736540066405, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.399779035422378\n",
      "Step - 14603, Loss - 0.7486595639605118, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5019352292210887\n",
      "Step - 14604, Loss - 0.8149317262965834, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5767094724813142\n",
      "Step - 14605, Loss - 0.6784779525153191, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.034901280247032\n",
      "Step - 14606, Loss - 0.6070387793700527, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7174928423491096\n",
      "Step - 14607, Loss - 0.6751016689668977, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.0061769490921546\n",
      "Step - 14608, Loss - 0.7735709605208665, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2939890325887853\n",
      "Step - 14609, Loss - 0.6737211005187498, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.982611989319825\n",
      "Step - 14610, Loss - 0.8180570577992289, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4470952787712201\n",
      "Step - 14611, Loss - 0.7944399614825317, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7824100022827604\n",
      "Step - 14612, Loss - 0.9386577278443132, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.388304184145019\n",
      "Step - 14613, Loss - 0.6798198053050761, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0473531374119995\n",
      "Step - 14614, Loss - 0.6233282277683683, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9751737584444687\n",
      "Step - 14615, Loss - 0.8344432052786007, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4869685841891016\n",
      "Step - 14616, Loss - 0.48534702506514227, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4143857726467428\n",
      "Step - 14617, Loss - 0.6406065726289294, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2071062729235313\n",
      "Step - 14618, Loss - 0.8229771577169325, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5167909592048683\n",
      "Step - 14619, Loss - 0.6584102394397722, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7536891200955792\n",
      "Step - 14620, Loss - 0.6721003688296396, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2018176231822209\n",
      "Step - 14621, Loss - 0.42703896440779543, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.1720855434905606\n",
      "Step - 14622, Loss - 0.7028492540080864, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8919184076680801\n",
      "Step - 14623, Loss - 0.7089170077226953, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7624166858885736\n",
      "Step - 14624, Loss - 0.6231800830630081, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9447842784519411\n",
      "Step - 14625, Loss - 0.6972331914496381, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2548187352267066\n",
      "Step - 14626, Loss - 0.9104210120624074, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4347531748664986\n",
      "Step - 14627, Loss - 0.8054237220543545, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1273030917774476\n",
      "Step - 14628, Loss - 0.6569156255538873, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.3462957952594725\n",
      "Step - 14629, Loss - 0.7638324390376174, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3348426804979174\n",
      "Step - 14630, Loss - 0.8718038763669236, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9840220545501753\n",
      "Step - 14631, Loss - 0.643822801349959, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.3034651900472071\n",
      "Step - 14632, Loss - 0.8160594388697224, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.503215439862365\n",
      "Step - 14633, Loss - 0.9106281323140775, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2345294265221591\n",
      "Step - 14634, Loss - 0.6130139046553902, Learning Rate - 3.0517578125e-06, magnitude of gradient - 3.020492486033646\n",
      "Step - 14635, Loss - 0.6567455122124948, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5628413439580432\n",
      "Step - 14636, Loss - 0.6355382683032724, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.307044931466078\n",
      "Step - 14637, Loss - 1.0111716084978337, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.5283399481011095\n",
      "Step - 14638, Loss - 0.616201513345198, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0873170049942158\n",
      "Step - 14639, Loss - 0.8136248814996913, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6323776085390342\n",
      "Step - 14640, Loss - 0.6117766338004232, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7625487516481269\n",
      "Step - 14641, Loss - 0.7494193050948907, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7004687949324193\n",
      "Step - 14642, Loss - 0.8263763458435843, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.4776804734344211\n",
      "Step - 14643, Loss - 0.6444947599822946, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.082035775106184\n",
      "Step - 14644, Loss - 0.6335273167725484, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8268164338757953\n",
      "Step - 14645, Loss - 0.6295532222203392, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7757501222347521\n",
      "Step - 14646, Loss - 0.7797572885150301, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8678545947829858\n",
      "Step - 14647, Loss - 0.6833280989269499, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4826543762849782\n",
      "Step - 14648, Loss - 0.8013591309232286, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5748864672787892\n",
      "Step - 14649, Loss - 0.4840701723906998, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.2338368008525696\n",
      "Step - 14650, Loss - 0.702667318976237, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7266647172802211\n",
      "Step - 14651, Loss - 0.6711622532580486, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7785581172674323\n",
      "Step - 14652, Loss - 0.6261889779437636, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6245994866929689\n",
      "Step - 14653, Loss - 0.657228279734625, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7371876925000048\n",
      "Step - 14654, Loss - 0.5958358491467862, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5272689535039157\n",
      "Step - 14655, Loss - 0.7409003349779251, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.3519060857059436\n",
      "Step - 14656, Loss - 0.6499925846144696, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9056602225647223\n",
      "Step - 14657, Loss - 0.5519616344822385, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4503835042607485\n",
      "Step - 14658, Loss - 0.6920938102243512, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.725951126623954\n",
      "Step - 14659, Loss - 0.7768663382167547, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9714975956162096\n",
      "Step - 14660, Loss - 0.7831526153417966, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8103663730415755\n",
      "Step - 14661, Loss - 0.6636292584150691, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6244229176793283\n",
      "Step - 14662, Loss - 0.5671059924850609, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.0775047765016383\n",
      "Step - 14663, Loss - 0.6146057210441452, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2623182898419607\n",
      "Step - 14664, Loss - 0.49443036136494406, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4423662063061624\n",
      "Step - 14665, Loss - 0.656577839584337, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9975201594792444\n",
      "Step - 14666, Loss - 0.6589182026008085, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9652271756345995\n",
      "Step - 14667, Loss - 0.7208807645395046, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8224985939579278\n",
      "Step - 14668, Loss - 0.7909859954460907, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7698868940313082\n",
      "Step - 14669, Loss - 0.6542697964672026, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5070078768330655\n",
      "Step - 14670, Loss - 0.5802099231561875, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7971770009713512\n",
      "Step - 14671, Loss - 0.8003972151174725, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3615108297504972\n",
      "Step - 14672, Loss - 0.5741879347854031, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.281316419033983\n",
      "Step - 14673, Loss - 0.5150909709857867, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.185506540832259\n",
      "Step - 14674, Loss - 0.8863947362163934, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5478844523957636\n",
      "Step - 14675, Loss - 0.6874853102863892, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2164511034886822\n",
      "Step - 14676, Loss - 0.676458203016266, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1038421287285523\n",
      "Step - 14677, Loss - 0.6357690084475678, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9286458290821697\n",
      "Step - 14678, Loss - 0.5174141617074259, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.384352449644069\n",
      "Step - 14679, Loss - 0.61977975020553, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6920389160442286\n",
      "Step - 14680, Loss - 0.9330157076355713, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5509585222947966\n",
      "Step - 14681, Loss - 0.5376671500166135, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1191369446487627\n",
      "Step - 14682, Loss - 0.7986114380554369, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7956300305278399\n",
      "Step - 14683, Loss - 0.8248093794469284, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.179395432434712\n",
      "Step - 14684, Loss - 0.6473792059742718, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6152474709722837\n",
      "Step - 14685, Loss - 0.924827297051463, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.0774748791602753\n",
      "Step - 14686, Loss - 0.6854092633436871, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.27764000252432275\n",
      "Step - 14687, Loss - 0.7452528681921223, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.786107897892787\n",
      "Step - 14688, Loss - 0.6870142187432615, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9742607897470926\n",
      "Step - 14689, Loss - 0.8760033964079907, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.8911739862174037\n",
      "Step - 14690, Loss - 0.6960426890903944, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.237483735241288\n",
      "Step - 14691, Loss - 0.684919006866272, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8639198983661878\n",
      "Step - 14692, Loss - 0.7937093667716801, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4793353526027717\n",
      "Step - 14693, Loss - 0.6829430354552042, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9033042083810711\n",
      "Step - 14694, Loss - 0.7152491363601268, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9708153511906146\n",
      "Step - 14695, Loss - 0.7940566392165256, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.86439075072884\n",
      "Step - 14696, Loss - 0.8757891359694558, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2119268348251198\n",
      "Step - 14697, Loss - 0.8498474765323106, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5734250198634282\n",
      "Step - 14698, Loss - 0.7570653132014097, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6132530864308394\n",
      "Step - 14699, Loss - 0.5315165133027769, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3437844042574154\n",
      "Step - 14700, Loss - 0.8348133231955158, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.0384180111024093\n",
      "Step - 14701, Loss - 0.7783734925101858, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.913891312813978\n",
      "Step - 14702, Loss - 0.7250547131033246, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5126168933155226\n",
      "Step - 14703, Loss - 0.6509026908691607, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.544284997882385\n",
      "Step - 14704, Loss - 0.9510790581843275, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.846283802590718\n",
      "Step - 14705, Loss - 0.7985725819680152, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1949666032361135\n",
      "Step - 14706, Loss - 0.5961913891408002, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2953335982470746\n",
      "Step - 14707, Loss - 0.677541410926847, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9839895179537382\n",
      "Step - 14708, Loss - 0.6536457876673778, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3489381795285278\n",
      "Step - 14709, Loss - 0.664503122681098, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.21061637120524\n",
      "Step - 14710, Loss - 0.529493851092748, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.45969194384358225\n",
      "Step - 14711, Loss - 0.8729013196939848, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.050233000648138\n",
      "Step - 14712, Loss - 0.7334068863008472, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8533628979765876\n",
      "Step - 14713, Loss - 0.6643158051192194, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7645810774355966\n",
      "Step - 14714, Loss - 0.64557260414236, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.452931381583001\n",
      "Step - 14715, Loss - 0.756612911783675, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6529322091915586\n",
      "Step - 14716, Loss - 0.6752419718551658, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9103595187952092\n",
      "Step - 14717, Loss - 0.6439991801874461, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8966134996053722\n",
      "Step - 14718, Loss - 0.6242252870195919, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7071561075777204\n",
      "Step - 14719, Loss - 0.6012103979018562, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8213125435224232\n",
      "Step - 14720, Loss - 0.6487641017044553, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.5783141505486324\n",
      "Step - 14721, Loss - 0.8549175738205561, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5272453492700399\n",
      "Step - 14722, Loss - 0.7658955232902227, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9548230253241943\n",
      "Step - 14723, Loss - 0.555547589244733, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7196062279873547\n",
      "Step - 14724, Loss - 0.702818096997839, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0994115739702126\n",
      "Step - 14725, Loss - 0.5737142294065305, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4273991136769821\n",
      "Step - 14726, Loss - 0.9373816440492785, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2490786467495703\n",
      "Step - 14727, Loss - 0.6412026919686891, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8460687479990031\n",
      "Step - 14728, Loss - 0.7723919552401031, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2183589589468218\n",
      "Step - 14729, Loss - 0.7994479208890549, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.5828147003482043\n",
      "Step - 14730, Loss - 0.8376849647952778, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.502564872094146\n",
      "Step - 14731, Loss - 0.7841204152245987, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8981328463310247\n",
      "Step - 14732, Loss - 0.6744135427604995, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8914496189047159\n",
      "Step - 14733, Loss - 0.5575732520396861, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5325217927438248\n",
      "Step - 14734, Loss - 0.7934845250378312, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.09662080149926\n",
      "Step - 14735, Loss - 0.6300497721082351, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5673534585159976\n",
      "Step - 14736, Loss - 0.6050378919617899, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.22907400788095342\n",
      "Step - 14737, Loss - 0.6747249048117425, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7585106460431723\n",
      "Step - 14738, Loss - 0.7068934976777757, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9039985637134009\n",
      "Step - 14739, Loss - 0.7596619311379718, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0757596276887629\n",
      "Step - 14740, Loss - 0.5298556917821385, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3169084489456298\n",
      "Step - 14741, Loss - 0.6771428736507843, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.800876974883725\n",
      "Step - 14742, Loss - 0.881695334395677, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6844234794638004\n",
      "Step - 14743, Loss - 0.5845807496792069, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9222563552915721\n",
      "Step - 14744, Loss - 0.7056186615279556, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.219479913792908\n",
      "Step - 14745, Loss - 0.5702097008200738, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1089416515562394\n",
      "Step - 14746, Loss - 0.7395927529748495, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2736413093955001\n",
      "Step - 14747, Loss - 0.6573446727159054, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7894348811920873\n",
      "Step - 14748, Loss - 0.47636643036859566, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.878837909203525\n",
      "Step - 14749, Loss - 0.6956386977320067, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2012976440390275\n",
      "Step - 14750, Loss - 0.5508958539639575, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.736592926867433\n",
      "Step - 14751, Loss - 0.3476505472240786, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1925580567955554\n",
      "Step - 14752, Loss - 0.7095760269636694, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.1439240823884194\n",
      "Step - 14753, Loss - 0.5848671961705714, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5135015780012516\n",
      "Step - 14754, Loss - 0.6246810495140414, Learning Rate - 3.0517578125e-06, magnitude of gradient - 3.2165410374648116\n",
      "Step - 14755, Loss - 0.49013085295488995, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2319088267782108\n",
      "Step - 14756, Loss - 0.7425170767485283, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6020725287337776\n",
      "Step - 14757, Loss - 0.6971626102035708, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7092086269803228\n",
      "Step - 14758, Loss - 0.7574672771581861, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0662073032528518\n",
      "Step - 14759, Loss - 0.6407525331579518, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0067593783594149\n",
      "Step - 14760, Loss - 0.5569631913590279, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5388859928801863\n",
      "Step - 14761, Loss - 0.7404086456789718, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7961853995758135\n",
      "Step - 14762, Loss - 0.6440743500344858, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1252971301177794\n",
      "Step - 14763, Loss - 0.7314445349629817, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6469089201069115\n",
      "Step - 14764, Loss - 0.6293273270027294, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5299741313944685\n",
      "Step - 14765, Loss - 0.9238195481627454, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3680805971375403\n",
      "Step - 14766, Loss - 0.737604204770731, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0693916956265213\n",
      "Step - 14767, Loss - 0.6284187660815438, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.126845330536559\n",
      "Step - 14768, Loss - 0.7921803232001583, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.325714430733223\n",
      "Step - 14769, Loss - 0.748113637602319, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7302759372474017\n",
      "Step - 14770, Loss - 0.635516532275617, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.930801136214894\n",
      "Step - 14771, Loss - 0.5494758876708835, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5398042241274184\n",
      "Step - 14772, Loss - 0.7898639020697376, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9188085124184961\n",
      "Step - 14773, Loss - 0.6593179077815711, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4579543193177087\n",
      "Step - 14774, Loss - 0.7450269972709538, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8192822342418093\n",
      "Step - 14775, Loss - 0.5089910441389413, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.410831263951361\n",
      "Step - 14776, Loss - 0.640972534405305, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.8216433392055054\n",
      "Step - 14777, Loss - 0.7340326113436375, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9718802766623031\n",
      "Step - 14778, Loss - 0.7454770842058724, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5223478107758448\n",
      "Step - 14779, Loss - 0.6521695869869885, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7326564925068431\n",
      "Step - 14780, Loss - 0.6441998377826654, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3417891277455491\n",
      "Step - 14781, Loss - 0.572413539017351, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0269896763620734\n",
      "Step - 14782, Loss - 0.5989920037555494, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.2388121460792805\n",
      "Step - 14783, Loss - 0.6368711657328989, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9837186445618116\n",
      "Step - 14784, Loss - 0.7439282478606102, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.1442201122200286\n",
      "Step - 14785, Loss - 0.7022304273969192, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0892730373978856\n",
      "Step - 14786, Loss - 0.7634595413004626, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.073309920183321\n",
      "Step - 14787, Loss - 0.6500583264499612, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9171089402307868\n",
      "Step - 14788, Loss - 0.5907018431033776, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8168004409509831\n",
      "Step - 14789, Loss - 0.5973435236213447, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.554223911227159\n",
      "Step - 14790, Loss - 0.5833957529861773, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2472552230083713\n",
      "Step - 14791, Loss - 0.7770964510256915, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.326238664009915\n",
      "Step - 14792, Loss - 0.8031565025975833, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1819329613419747\n",
      "Step - 14793, Loss - 0.635957756509498, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.150837748217504\n",
      "Step - 14794, Loss - 0.6053784133371676, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6452650033582719\n",
      "Step - 14795, Loss - 0.967936357736682, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.1802825524002545\n",
      "Step - 14796, Loss - 0.7404387512448787, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.893720077840156\n",
      "Step - 14797, Loss - 0.7211162049094302, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9093865353184178\n",
      "Step - 14798, Loss - 0.4816827091079802, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.753559952999787\n",
      "Step - 14799, Loss - 0.6397916335618339, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8158744852501219\n",
      "Step - 14800, Loss - 0.6813571546116058, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.382162523476879\n",
      "Step - 14801, Loss - 0.6840248735326149, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5763496134049872\n",
      "Step - 14802, Loss - 0.790701163762932, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9825922867179921\n",
      "Step - 14803, Loss - 0.7069533805426951, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0410876510407867\n",
      "Step - 14804, Loss - 0.7121195790522794, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2604292941959458\n",
      "Step - 14805, Loss - 0.5601485435871388, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.1009546323571784\n",
      "Step - 14806, Loss - 0.6478356519281386, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6195011969404844\n",
      "Step - 14807, Loss - 0.5251052499914054, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7414233808522637\n",
      "Step - 14808, Loss - 0.6335251959988439, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2550356111862138\n",
      "Step - 14809, Loss - 0.7174423438047184, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7895179593841124\n",
      "Step - 14810, Loss - 0.7285135041381885, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7539044950076603\n",
      "Step - 14811, Loss - 0.6833131649940499, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2311450725856679\n",
      "Step - 14812, Loss - 0.7330061743527452, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7058718343353092\n",
      "Step - 14813, Loss - 0.886255320812248, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.942915059614887\n",
      "Step - 14814, Loss - 0.8806778414123551, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.5105980000787347\n",
      "Step - 14815, Loss - 0.7609957661478386, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3110415979878687\n",
      "Step - 14816, Loss - 0.884426017843888, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1757734330176919\n",
      "Step - 14817, Loss - 0.811730515562658, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5524137453403881\n",
      "Step - 14818, Loss - 0.7442648815890971, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8501049290287717\n",
      "Step - 14819, Loss - 0.6050226665144972, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6057782142570823\n",
      "Step - 14820, Loss - 0.6054708915833348, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2564245621398105\n",
      "Step - 14821, Loss - 0.6396962467453511, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5268287855564664\n",
      "Step - 14822, Loss - 0.5894788484032224, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.51261510460866\n",
      "Step - 14823, Loss - 0.5549056370016145, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.22576550457855205\n",
      "Step - 14824, Loss - 0.5690155800448867, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3110565906580984\n",
      "Step - 14825, Loss - 0.7583265413430706, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.8604049371425093\n",
      "Step - 14826, Loss - 0.7365956202576383, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.8640207456226234\n",
      "Step - 14827, Loss - 0.7334379599487291, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0127129440602143\n",
      "Step - 14828, Loss - 0.6861493629729273, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5662338008122574\n",
      "Step - 14829, Loss - 0.7200100844684068, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0124628238201985\n",
      "Step - 14830, Loss - 0.6177224098777053, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6010896716675359\n",
      "Step - 14831, Loss - 0.7190695639063244, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6536762551596019\n",
      "Step - 14832, Loss - 0.8611387583392178, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9635752856382207\n",
      "Step - 14833, Loss - 0.8129086016935663, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1247526894617081\n",
      "Step - 14834, Loss - 0.8586316102247111, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.36952066146986284\n",
      "Step - 14835, Loss - 0.793736046726456, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5634185090595591\n",
      "Step - 14836, Loss - 0.727710654539375, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.0067724259756017\n",
      "Step - 14837, Loss - 0.7003181650289177, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7041922435873275\n",
      "Step - 14838, Loss - 0.7664899336559836, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.231238051148414\n",
      "Step - 14839, Loss - 0.7248442086344723, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5430151904446189\n",
      "Step - 14840, Loss - 0.7601801300137785, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.26840946594794807\n",
      "Step - 14841, Loss - 0.702560213494831, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0332124185793206\n",
      "Step - 14842, Loss - 0.7514204037537141, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5202138577298445\n",
      "Step - 14843, Loss - 0.7338337124050286, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3179590382879889\n",
      "Step - 14844, Loss - 0.7464217323639375, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6017177179915607\n",
      "Step - 14845, Loss - 0.7363360593571993, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9140103425066015\n",
      "Step - 14846, Loss - 0.8532952800867846, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1590528971193952\n",
      "Step - 14847, Loss - 0.4363855190921565, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.8149295625710067\n",
      "Step - 14848, Loss - 0.6810053274023826, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5613518470234231\n",
      "Step - 14849, Loss - 0.8343948847172927, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.8970592874747054\n",
      "Step - 14850, Loss - 0.8272323480897925, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2558207197499147\n",
      "Step - 14851, Loss - 0.6136367754627303, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1280689238799786\n",
      "Step - 14852, Loss - 0.7979002036052729, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2881860963223215\n",
      "Step - 14853, Loss - 0.6722656265469712, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5769137562687354\n",
      "Step - 14854, Loss - 0.6228228311867081, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.16653176778208764\n",
      "Step - 14855, Loss - 0.8279189957138192, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.247245390614485\n",
      "Step - 14856, Loss - 0.7344774842702074, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.446276274392579\n",
      "Step - 14857, Loss - 0.7964178016342882, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7434503695475396\n",
      "Step - 14858, Loss - 0.7632937333373475, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2271470151480282\n",
      "Step - 14859, Loss - 0.6774574200444551, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.471519873282166\n",
      "Step - 14860, Loss - 0.6913298470700101, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.0164638435094187\n",
      "Step - 14861, Loss - 0.6991498026506966, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0837756127606448\n",
      "Step - 14862, Loss - 0.6933097857052308, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.43773268692114997\n",
      "Step - 14863, Loss - 0.6697265930692277, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9419967740130073\n",
      "Step - 14864, Loss - 0.4867228878955225, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1457051316553664\n",
      "Step - 14865, Loss - 0.9131549771975243, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3043127021761078\n",
      "Step - 14866, Loss - 0.639987752861327, Learning Rate - 3.0517578125e-06, magnitude of gradient - 3.652662706299617\n",
      "Step - 14867, Loss - 0.7162937619214741, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7044529834592628\n",
      "Step - 14868, Loss - 0.8469155972183884, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.910339674027782\n",
      "Step - 14869, Loss - 0.9667887992300912, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.549859506729759\n",
      "Step - 14870, Loss - 0.8010358390803513, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.195312096415253\n",
      "Step - 14871, Loss - 0.6847337402955064, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2973398729321244\n",
      "Step - 14872, Loss - 0.7163436524653426, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2669130879913666\n",
      "Step - 14873, Loss - 0.6153806592682989, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.19866275357245214\n",
      "Step - 14874, Loss - 0.5653217065139782, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3101465187724755\n",
      "Step - 14875, Loss - 0.44064837491339526, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2064975598897794\n",
      "Step - 14876, Loss - 0.8771188387475111, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5604898934276636\n",
      "Step - 14877, Loss - 0.47256872515166365, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0424103040142607\n",
      "Step - 14878, Loss - 0.7257952850945454, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.398504480601071\n",
      "Step - 14879, Loss - 0.4967837620090136, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2243021601727224\n",
      "Step - 14880, Loss - 0.6888990337274772, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7163194449634916\n",
      "Step - 14881, Loss - 0.7118389332295443, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7978865957617525\n",
      "Step - 14882, Loss - 0.811007653627797, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9002618239270281\n",
      "Step - 14883, Loss - 0.6481847194775726, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5943489258064514\n",
      "Step - 14884, Loss - 0.704329382772181, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6263934525870754\n",
      "Step - 14885, Loss - 0.8318128348128107, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6448569625576548\n",
      "Step - 14886, Loss - 0.7849080232458678, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0861130209905174\n",
      "Step - 14887, Loss - 0.5707022060928345, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.144970750529319\n",
      "Step - 14888, Loss - 0.818502546690949, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2656215621058586\n",
      "Step - 14889, Loss - 0.654242021218306, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0529459509280839\n",
      "Step - 14890, Loss - 0.7356429565178797, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7822013232481507\n",
      "Step - 14891, Loss - 0.7851782655818146, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3819809861620649\n",
      "Step - 14892, Loss - 0.784324232723394, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6477980001262383\n",
      "Step - 14893, Loss - 0.6932547658675191, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8119425515984048\n",
      "Step - 14894, Loss - 0.6547571122557384, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.8516385840281053\n",
      "Step - 14895, Loss - 0.7626649927245599, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7718704726394904\n",
      "Step - 14896, Loss - 0.5707008654480447, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9128955593625887\n",
      "Step - 14897, Loss - 0.7152887606398883, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.098601739692306\n",
      "Step - 14898, Loss - 0.5855372590262127, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.229140401856016\n",
      "Step - 14899, Loss - 0.5744402846409888, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.155559082999528\n",
      "Step - 14900, Loss - 0.7875004317966898, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.480217981182997\n",
      "Step - 14901, Loss - 0.7564463391432585, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7108290376554395\n",
      "Step - 14902, Loss - 0.538843658805691, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7737287507675499\n",
      "Step - 14903, Loss - 0.9710502713983774, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7694900868312289\n",
      "Step - 14904, Loss - 0.8208656351432121, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5821933913895487\n",
      "Step - 14905, Loss - 0.4588885566582558, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.328330041088256\n",
      "Step - 14906, Loss - 0.7356577565566116, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.8387740187467003\n",
      "Step - 14907, Loss - 0.6481955010562187, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.3740596846714224\n",
      "Step - 14908, Loss - 0.7167046213638315, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.197938956900007\n",
      "Step - 14909, Loss - 0.6680181923379743, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5216642310591337\n",
      "Step - 14910, Loss - 0.6649289168044044, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6811147825290544\n",
      "Step - 14911, Loss - 0.8813312115184655, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5730002924972732\n",
      "Step - 14912, Loss - 0.7551454508407403, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5518035005956473\n",
      "Step - 14913, Loss - 0.9416236702939441, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.8087143540581043\n",
      "Step - 14914, Loss - 0.7773204621630068, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6926086027141125\n",
      "Step - 14915, Loss - 0.6239241554795937, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6446881162385752\n",
      "Step - 14916, Loss - 0.6861798831730566, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8929280954684052\n",
      "Step - 14917, Loss - 0.588683498222365, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9615761717587751\n",
      "Step - 14918, Loss - 0.6584996688829678, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.2050192757624276\n",
      "Step - 14919, Loss - 0.9419178306556752, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3865892199933105\n",
      "Step - 14920, Loss - 0.8255103226528672, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5658667642533608\n",
      "Step - 14921, Loss - 0.6693969825953958, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5696395251017704\n",
      "Step - 14922, Loss - 0.7614153427907571, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4472238358433904\n",
      "Step - 14923, Loss - 0.8042139477808325, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0399888629407579\n",
      "Step - 14924, Loss - 0.4932796772478015, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0469820252168467\n",
      "Step - 14925, Loss - 0.6139538403941794, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5609110642566235\n",
      "Step - 14926, Loss - 0.7860369602471873, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8816560751162695\n",
      "Step - 14927, Loss - 0.770639709504734, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1917448061503175\n",
      "Step - 14928, Loss - 0.6093862039249847, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8759694150672793\n",
      "Step - 14929, Loss - 0.7712132151506315, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9830414335042453\n",
      "Step - 14930, Loss - 0.7216107845258979, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.9892880977650997\n",
      "Step - 14931, Loss - 0.6837389980015817, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5638006184925002\n",
      "Step - 14932, Loss - 0.6111400586248006, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7596725735447052\n",
      "Step - 14933, Loss - 0.8409772786312092, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.21326393006422717\n",
      "Step - 14934, Loss - 0.8019818653399017, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8105933110611557\n",
      "Step - 14935, Loss - 0.7211145765643168, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6898069628377912\n",
      "Step - 14936, Loss - 0.6197079233325274, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4576051705663289\n",
      "Step - 14937, Loss - 0.6862533437205341, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0578400892789561\n",
      "Step - 14938, Loss - 0.6770474202120633, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6369971121131106\n",
      "Step - 14939, Loss - 0.6892534776530899, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.917679028640889\n",
      "Step - 14940, Loss - 0.5696184997610474, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.771230913827989\n",
      "Step - 14941, Loss - 0.47934717140088295, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1392649413396405\n",
      "Step - 14942, Loss - 0.8362221297110025, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.605047189405677\n",
      "Step - 14943, Loss - 0.7780005886656526, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3785518056952837\n",
      "Step - 14944, Loss - 0.5620462668824338, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.162061380129734\n",
      "Step - 14945, Loss - 1.0038229686406344, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.304668192201635\n",
      "Step - 14946, Loss - 0.737791059473766, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0382742340920927\n",
      "Step - 14947, Loss - 0.8090900005517302, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.4472632874645064\n",
      "Step - 14948, Loss - 0.7009864243768483, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.51236952983518\n",
      "Step - 14949, Loss - 0.6762491215079242, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.5721981936811575\n",
      "Step - 14950, Loss - 0.6000166764186416, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8516169063962296\n",
      "Step - 14951, Loss - 0.9053838291348669, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2235421893728393\n",
      "Step - 14952, Loss - 0.7227932795910493, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5188216093585614\n",
      "Step - 14953, Loss - 0.7046002076972426, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.102736061732404\n",
      "Step - 14954, Loss - 0.7276307036143896, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.118628109589711\n",
      "Step - 14955, Loss - 0.6465307998733731, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.865588492679664\n",
      "Step - 14956, Loss - 0.6131126527697759, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.781534283961835\n",
      "Step - 14957, Loss - 0.6947661205649669, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6010861596561463\n",
      "Step - 14958, Loss - 0.8299906861783626, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7224906843502048\n",
      "Step - 14959, Loss - 0.5968833826564519, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.272458134569773\n",
      "Step - 14960, Loss - 0.7602358761012791, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.824016836080996\n",
      "Step - 14961, Loss - 0.7396140008281444, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2834530666572987\n",
      "Step - 14962, Loss - 0.7543681822413696, Learning Rate - 3.0517578125e-06, magnitude of gradient - 3.3210469254908648\n",
      "Step - 14963, Loss - 0.6492920037873678, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.55186269818649\n",
      "Step - 14964, Loss - 0.8062498828912364, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6002782199266732\n",
      "Step - 14965, Loss - 0.7010323310446256, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.632242652823993\n",
      "Step - 14966, Loss - 0.754892279148743, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0590950034142503\n",
      "Step - 14967, Loss - 0.7849403806845721, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2343720914890883\n",
      "Step - 14968, Loss - 0.5364695041943656, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.031928590154761\n",
      "Step - 14969, Loss - 0.7560299959502108, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.5322328697295806\n",
      "Step - 14970, Loss - 0.6595364683784887, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.8154692351993336\n",
      "Step - 14971, Loss - 0.5593785345079921, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6027742716890693\n",
      "Step - 14972, Loss - 0.5022242494379368, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6195338218106374\n",
      "Step - 14973, Loss - 0.683867794194633, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.194552612651565\n",
      "Step - 14974, Loss - 0.6764809039856571, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2981704356372832\n",
      "Step - 14975, Loss - 0.5490465407361254, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.249034908387075\n",
      "Step - 14976, Loss - 0.490547198531076, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.2617357674501335\n",
      "Step - 14977, Loss - 0.9433017515456077, Learning Rate - 3.0517578125e-06, magnitude of gradient - 2.311707727881525\n",
      "Step - 14978, Loss - 0.7856945266956367, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.4878670202782597\n",
      "Step - 14979, Loss - 0.748695998142631, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6197198882093251\n",
      "Step - 14980, Loss - 0.5952377476949537, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.021454536758818\n",
      "Step - 14981, Loss - 0.6149650222066614, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.186098675255811\n",
      "Step - 14982, Loss - 0.7640692751634961, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.542427376500612\n",
      "Step - 14983, Loss - 0.6976159466416691, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.88239871138482\n",
      "Step - 14984, Loss - 0.7248197302526195, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8544266668989678\n",
      "Step - 14985, Loss - 0.7215366566508168, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0754917349607926\n",
      "Step - 14986, Loss - 0.588868078285772, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.48379253305911724\n",
      "Step - 14987, Loss - 0.6717854640916889, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.6041116493226075\n",
      "Step - 14988, Loss - 0.7004426356053575, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.7917982154446004\n",
      "Step - 14989, Loss - 0.5907942040646257, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.7040779574701135\n",
      "Step - 14990, Loss - 0.6004392859306594, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.34941551103734697\n",
      "Step - 14991, Loss - 0.9385785499155782, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.1262617573851754\n",
      "Step - 14992, Loss - 0.723441420411217, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8757256208850901\n",
      "Step - 14993, Loss - 0.7433978434632831, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.9283763721704957\n",
      "Step - 14994, Loss - 0.7960752142601617, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.6051378343583963\n",
      "Step - 14995, Loss - 0.6448990147509077, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0967367731500566\n",
      "Step - 14996, Loss - 0.7559517494978735, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.185447667168773\n",
      "Step - 14997, Loss - 0.6474808104834943, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.0649565772128005\n",
      "Step - 14998, Loss - 0.670355858284142, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8217371714818439\n",
      "Step - 14999, Loss - 0.7774289267507728, Learning Rate - 3.0517578125e-06, magnitude of gradient - 1.3563116791369503\n",
      "Step - 15000, Loss - 0.519721181562358, Learning Rate - 3.0517578125e-06, magnitude of gradient - 0.8596213372292617\n",
      "Step - 15001, Loss - 0.783356575098343, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8687742225348201\n",
      "Step - 15002, Loss - 0.7066954128855959, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.045178801608524\n",
      "Step - 15003, Loss - 0.5401891270053417, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.26565122495828\n",
      "Step - 15004, Loss - 0.7540300092478954, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0109383088221184\n",
      "Step - 15005, Loss - 0.7403274130740418, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5001550632892922\n",
      "Step - 15006, Loss - 0.6211982742146274, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7379725656313384\n",
      "Step - 15007, Loss - 0.7892704364002209, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6574771139283194\n",
      "Step - 15008, Loss - 0.6299926775577638, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5144412867637679\n",
      "Step - 15009, Loss - 1.0242298109160421, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1990082568205143\n",
      "Step - 15010, Loss - 0.6123092606181395, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0377853144369846\n",
      "Step - 15011, Loss - 0.7738790455596681, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2306341495623911\n",
      "Step - 15012, Loss - 0.6463515253673692, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7656319762014039\n",
      "Step - 15013, Loss - 0.6234844544633371, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7156750907784762\n",
      "Step - 15014, Loss - 0.6855819611440036, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5440922971433018\n",
      "Step - 15015, Loss - 0.6143281649660762, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4374203166171686\n",
      "Step - 15016, Loss - 0.7821687487965036, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7623515477887506\n",
      "Step - 15017, Loss - 0.7766621397867155, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.01786660262627\n",
      "Step - 15018, Loss - 0.8497969726889625, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2703618208278296\n",
      "Step - 15019, Loss - 0.6773152283504407, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.679415555441921\n",
      "Step - 15020, Loss - 0.8801501739859344, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9626336549651028\n",
      "Step - 15021, Loss - 0.7797798705564439, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0603973177909423\n",
      "Step - 15022, Loss - 0.6363541360077388, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4994067834589908\n",
      "Step - 15023, Loss - 0.7978899904775953, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.124832290139763\n",
      "Step - 15024, Loss - 0.861453262156356, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.498005068031856\n",
      "Step - 15025, Loss - 0.8623289549665368, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0472754396629609\n",
      "Step - 15026, Loss - 0.5687460668015447, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.252786729595335\n",
      "Step - 15027, Loss - 0.8127982217210752, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7562188561092908\n",
      "Step - 15028, Loss - 0.7804627437938474, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7188803617941817\n",
      "Step - 15029, Loss - 0.686601025174578, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4975129796189304\n",
      "Step - 15030, Loss - 0.5641226479382395, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7211621199862508\n",
      "Step - 15031, Loss - 0.7296778182805408, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5687218929566569\n",
      "Step - 15032, Loss - 0.745576196871099, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0350121525992593\n",
      "Step - 15033, Loss - 0.6687936470784984, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.086202206090564\n",
      "Step - 15034, Loss - 0.5651878198919349, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.613391828980852\n",
      "Step - 15035, Loss - 0.7130752254936109, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9186736431083394\n",
      "Step - 15036, Loss - 0.6757723564568654, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7563155134431999\n",
      "Step - 15037, Loss - 0.8556076505864835, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0252452734946127\n",
      "Step - 15038, Loss - 0.5811808204433973, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1172989808137748\n",
      "Step - 15039, Loss - 0.6408914674018089, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7820816302115863\n",
      "Step - 15040, Loss - 0.653973982117134, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.4537108628636041\n",
      "Step - 15041, Loss - 0.6321408939030762, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3305435394764018\n",
      "Step - 15042, Loss - 0.7353746618172252, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4746021769159432\n",
      "Step - 15043, Loss - 0.72306199303324, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2274522095810407\n",
      "Step - 15044, Loss - 0.8095445553809761, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.800980107309457\n",
      "Step - 15045, Loss - 0.8671907299021493, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.012866957639465\n",
      "Step - 15046, Loss - 0.7144874495097214, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3864641830622673\n",
      "Step - 15047, Loss - 0.5744701972479533, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4990608139517096\n",
      "Step - 15048, Loss - 0.6303954359333629, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0941067371231215\n",
      "Step - 15049, Loss - 0.8226575323859316, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2798272242854927\n",
      "Step - 15050, Loss - 0.7346895315979258, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8733471680626791\n",
      "Step - 15051, Loss - 0.6831723913988085, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5117656050576177\n",
      "Step - 15052, Loss - 0.7144424464531152, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9166863035394458\n",
      "Step - 15053, Loss - 0.6622385871814359, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8615751745676117\n",
      "Step - 15054, Loss - 0.6017311711297614, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7848020155231952\n",
      "Step - 15055, Loss - 0.9669926507805269, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4558373455574471\n",
      "Step - 15056, Loss - 0.7565095816486354, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.41171724885286504\n",
      "Step - 15057, Loss - 0.8052225447564418, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4388824806480074\n",
      "Step - 15058, Loss - 0.7667125143282392, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2669587691685353\n",
      "Step - 15059, Loss - 0.5334170997256694, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2798129352496028\n",
      "Step - 15060, Loss - 0.452241895830072, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.872314436646366\n",
      "Step - 15061, Loss - 0.7755989606159477, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5990917051052879\n",
      "Step - 15062, Loss - 0.7825211699746646, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1330087264593593\n",
      "Step - 15063, Loss - 0.699142768947075, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1794231181502066\n",
      "Step - 15064, Loss - 0.724964762103307, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7490249828540292\n",
      "Step - 15065, Loss - 0.587933534915522, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6741768343526726\n",
      "Step - 15066, Loss - 0.6980624565344611, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.305351455435178\n",
      "Step - 15067, Loss - 0.7019168419001716, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0531030930289147\n",
      "Step - 15068, Loss - 0.5874689098937728, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.19246511978911\n",
      "Step - 15069, Loss - 0.7936929742228209, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8958219273430297\n",
      "Step - 15070, Loss - 0.5708901831786263, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.420971409627157\n",
      "Step - 15071, Loss - 0.8735418657316553, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.1915500215155284\n",
      "Step - 15072, Loss - 0.602807540097376, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2977805014468864\n",
      "Step - 15073, Loss - 0.6174363202968484, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5411185246442125\n",
      "Step - 15074, Loss - 0.7705795576683774, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.41138895081870935\n",
      "Step - 15075, Loss - 0.5992065807167228, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6315305846222525\n",
      "Step - 15076, Loss - 0.7326392691435617, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2945175809270202\n",
      "Step - 15077, Loss - 0.47630306415016554, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2682005827454164\n",
      "Step - 15078, Loss - 0.9814334576495913, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.2934201176311464\n",
      "Step - 15079, Loss - 0.7002925835950432, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3942758024841801\n",
      "Step - 15080, Loss - 0.8491306565374097, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.17123943479075\n",
      "Step - 15081, Loss - 0.8315651888220165, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7522776918780679\n",
      "Step - 15082, Loss - 0.5983785284754313, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2469726453849364\n",
      "Step - 15083, Loss - 0.7320397387754851, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9026441770699133\n",
      "Step - 15084, Loss - 0.7520726672578028, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5170248333469556\n",
      "Step - 15085, Loss - 0.7013418626385506, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.607835480928251\n",
      "Step - 15086, Loss - 0.9098162459160936, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.9817528766958388\n",
      "Step - 15087, Loss - 0.6323525186536435, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6230302964288491\n",
      "Step - 15088, Loss - 0.7831642753250916, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4929015338510314\n",
      "Step - 15089, Loss - 0.7728636310014141, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.345071900550141\n",
      "Step - 15090, Loss - 0.6424080949028079, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8651797467947957\n",
      "Step - 15091, Loss - 0.8065387696791265, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9960453532080171\n",
      "Step - 15092, Loss - 0.5536368226713335, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9852882385238526\n",
      "Step - 15093, Loss - 0.6826088047042707, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4964136134984167\n",
      "Step - 15094, Loss - 0.8565152522062601, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4946838681870636\n",
      "Step - 15095, Loss - 0.8536722334321438, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5663131923883158\n",
      "Step - 15096, Loss - 0.773558562290309, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3672878283923002\n",
      "Step - 15097, Loss - 0.8752948904164616, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0078014953248304\n",
      "Step - 15098, Loss - 0.5660922057671698, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3178895012885623\n",
      "Step - 15099, Loss - 0.7109225493424175, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.714569299385746\n",
      "Step - 15100, Loss - 0.6442332344922772, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.712066741622992\n",
      "Step - 15101, Loss - 0.7197637277551618, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8379187599516755\n",
      "Step - 15102, Loss - 0.6539335526014506, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2040134115856027\n",
      "Step - 15103, Loss - 0.8892212169941173, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.5819847140698036\n",
      "Step - 15104, Loss - 0.8558605968602871, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.093503422508414\n",
      "Step - 15105, Loss - 0.7914488406444941, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4521465546214087\n",
      "Step - 15106, Loss - 0.4656825759069994, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.762549078261255\n",
      "Step - 15107, Loss - 0.6632854469873077, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7390787985500011\n",
      "Step - 15108, Loss - 0.850052703185316, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.123094714027056\n",
      "Step - 15109, Loss - 0.7445338404138742, Learning Rate - 1.52587890625e-06, magnitude of gradient - 3.5466592216578103\n",
      "Step - 15110, Loss - 0.6376425696901473, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7295902326251242\n",
      "Step - 15111, Loss - 0.8228763007047273, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.6819468244410594\n",
      "Step - 15112, Loss - 0.661083151168529, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.178677448629363\n",
      "Step - 15113, Loss - 0.5273019970276797, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0385617270091656\n",
      "Step - 15114, Loss - 0.623566242820457, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7117471034838099\n",
      "Step - 15115, Loss - 0.827312030920199, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.387092008534616\n",
      "Step - 15116, Loss - 0.702051649338686, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5638748122346601\n",
      "Step - 15117, Loss - 0.576715412052722, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7612250060888561\n",
      "Step - 15118, Loss - 0.7145394977582669, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7432730451283112\n",
      "Step - 15119, Loss - 0.7335269214029209, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9198532674953789\n",
      "Step - 15120, Loss - 0.8013426957946133, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3320651984575633\n",
      "Step - 15121, Loss - 0.6373644811608987, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7379646444839743\n",
      "Step - 15122, Loss - 0.6440837841926642, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0796396141338884\n",
      "Step - 15123, Loss - 0.7354903789756224, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.979018722521535\n",
      "Step - 15124, Loss - 0.6093825433130656, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.499146769433822\n",
      "Step - 15125, Loss - 0.6882147375810669, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.443396473192864\n",
      "Step - 15126, Loss - 0.7760740064010845, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1272282944791785\n",
      "Step - 15127, Loss - 0.8029089380379286, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.944415963743638\n",
      "Step - 15128, Loss - 0.7310456804079548, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1772715066492168\n",
      "Step - 15129, Loss - 0.5613330074517774, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.225497529940478\n",
      "Step - 15130, Loss - 0.6818440841931328, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8526239610696605\n",
      "Step - 15131, Loss - 0.6702337084557952, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9360571492456344\n",
      "Step - 15132, Loss - 0.48856256669000075, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.31160172646174544\n",
      "Step - 15133, Loss - 0.7613133575103482, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.272839473787085\n",
      "Step - 15134, Loss - 0.815448680590869, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5542657743966993\n",
      "Step - 15135, Loss - 0.663581746134671, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3559432301014425\n",
      "Step - 15136, Loss - 0.6393882362451722, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0787993196098025\n",
      "Step - 15137, Loss - 0.588546269684932, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9464068420664725\n",
      "Step - 15138, Loss - 0.529932528086493, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8500381805153507\n",
      "Step - 15139, Loss - 0.5832025608662064, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8004881572455479\n",
      "Step - 15140, Loss - 0.5687234879700214, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8370247405083753\n",
      "Step - 15141, Loss - 0.8024972745508243, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6515249513879096\n",
      "Step - 15142, Loss - 0.45405938226450016, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5112819196017813\n",
      "Step - 15143, Loss - 0.6922778219073324, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7747708060845502\n",
      "Step - 15144, Loss - 0.6758478744551295, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6887072822060932\n",
      "Step - 15145, Loss - 0.7733660721528577, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9829479161120488\n",
      "Step - 15146, Loss - 0.8053985864787774, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3298483637942908\n",
      "Step - 15147, Loss - 0.5219342167267433, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9008791119156299\n",
      "Step - 15148, Loss - 0.6774848853079886, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7878755485195542\n",
      "Step - 15149, Loss - 0.7797598381352296, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7596385034566001\n",
      "Step - 15150, Loss - 0.77246074185216, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7670391023418468\n",
      "Step - 15151, Loss - 0.6800581815345332, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2333236393088705\n",
      "Step - 15152, Loss - 0.6454979464750217, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7079042846681995\n",
      "Step - 15153, Loss - 0.7161169942738868, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.43375613944104424\n",
      "Step - 15154, Loss - 0.6571105909303352, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6925929036064412\n",
      "Step - 15155, Loss - 0.6002471388503057, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3445593677414345\n",
      "Step - 15156, Loss - 0.6096383513159426, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2585983302966723\n",
      "Step - 15157, Loss - 0.6790706201781985, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.1112697059266416\n",
      "Step - 15158, Loss - 0.7706182544218452, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6787054621609008\n",
      "Step - 15159, Loss - 0.72076430448256, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7479653661025247\n",
      "Step - 15160, Loss - 0.5770574273254718, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6006593030229778\n",
      "Step - 15161, Loss - 0.7591561509773702, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5517656614119085\n",
      "Step - 15162, Loss - 0.6076851097286269, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6205828191644343\n",
      "Step - 15163, Loss - 0.7698746892100992, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9398179028857588\n",
      "Step - 15164, Loss - 0.5870997868255292, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1996122942884895\n",
      "Step - 15165, Loss - 0.6758436866027605, Learning Rate - 1.52587890625e-06, magnitude of gradient - 3.8032553875013213\n",
      "Step - 15166, Loss - 0.6923363414444575, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7101017771811946\n",
      "Step - 15167, Loss - 0.7125257814555184, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.27584847986140826\n",
      "Step - 15168, Loss - 0.6231904402382848, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.1813613550149686\n",
      "Step - 15169, Loss - 0.7872934011328481, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7691592877423644\n",
      "Step - 15170, Loss - 0.521694281563863, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2526361245016189\n",
      "Step - 15171, Loss - 0.601598862122034, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.1029941428805126\n",
      "Step - 15172, Loss - 0.591337624612863, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6611977231362126\n",
      "Step - 15173, Loss - 0.7579137891043158, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.3042386254008449\n",
      "Step - 15174, Loss - 0.7127490712112624, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3249513803822648\n",
      "Step - 15175, Loss - 0.7962518788923949, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.707438203803495\n",
      "Step - 15176, Loss - 0.5630552256123993, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.35695517160825707\n",
      "Step - 15177, Loss - 0.6834707948604996, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9719008012298735\n",
      "Step - 15178, Loss - 0.6807258266440365, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.523239370923489\n",
      "Step - 15179, Loss - 0.5999545579342906, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4149314091421605\n",
      "Step - 15180, Loss - 0.7962390007327704, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1338081325960792\n",
      "Step - 15181, Loss - 0.5337736383902683, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.109802962203999\n",
      "Step - 15182, Loss - 0.8239478079592758, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9166513159478712\n",
      "Step - 15183, Loss - 0.6735480955521076, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8478965637268597\n",
      "Step - 15184, Loss - 0.6642775882828236, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.270184727611818\n",
      "Step - 15185, Loss - 0.7677257327534377, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.877856767952466\n",
      "Step - 15186, Loss - 0.7821195665610429, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5292794224258447\n",
      "Step - 15187, Loss - 0.8162747867346563, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5431499880064541\n",
      "Step - 15188, Loss - 0.6371449408457767, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8392660794053082\n",
      "Step - 15189, Loss - 0.7922614816177963, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9606010642421211\n",
      "Step - 15190, Loss - 0.8175083122700704, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7859773232877492\n",
      "Step - 15191, Loss - 0.7854860711570799, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8636566844619412\n",
      "Step - 15192, Loss - 0.6210464467853212, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6847941902180483\n",
      "Step - 15193, Loss - 0.7405461111635037, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2169984987286149\n",
      "Step - 15194, Loss - 0.6798832132538646, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7662830162140339\n",
      "Step - 15195, Loss - 0.8024140765221552, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0626037647630548\n",
      "Step - 15196, Loss - 0.7404124785614642, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.7406037139247257\n",
      "Step - 15197, Loss - 0.7704700106716663, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7672524263815582\n",
      "Step - 15198, Loss - 0.7972425998070388, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.083995644976185\n",
      "Step - 15199, Loss - 0.7885778675204711, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.122676322400224\n",
      "Step - 15200, Loss - 0.6032772191410516, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9142496971974305\n",
      "Step - 15201, Loss - 0.8510466554681929, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8971758082021736\n",
      "Step - 15202, Loss - 0.4405596666905872, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3435018632112645\n",
      "Step - 15203, Loss - 0.7665018023543522, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7047583833630343\n",
      "Step - 15204, Loss - 0.7774248372658704, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6704039599403542\n",
      "Step - 15205, Loss - 0.49430087150989405, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7063728605019496\n",
      "Step - 15206, Loss - 0.5641042845204172, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3254300326955777\n",
      "Step - 15207, Loss - 0.7045844236256462, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9047102716765676\n",
      "Step - 15208, Loss - 0.6334958519400057, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6866372672360666\n",
      "Step - 15209, Loss - 0.7754154267663422, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.621870735494586\n",
      "Step - 15210, Loss - 0.7989971384847437, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1939281705100908\n",
      "Step - 15211, Loss - 0.6216442807985896, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.367425180764556\n",
      "Step - 15212, Loss - 0.5704158532341209, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5030612839901532\n",
      "Step - 15213, Loss - 0.5418085644305269, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0082213949715555\n",
      "Step - 15214, Loss - 0.6891347086154493, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7611719472538071\n",
      "Step - 15215, Loss - 0.8338032335563897, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.488374343383868\n",
      "Step - 15216, Loss - 0.6025929297701142, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7529352654791882\n",
      "Step - 15217, Loss - 0.7710350652538639, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5968521232970042\n",
      "Step - 15218, Loss - 0.7582060860343003, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.5833339155637693\n",
      "Step - 15219, Loss - 0.5886100659997865, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9187762409061146\n",
      "Step - 15220, Loss - 0.8667819222876678, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.43623129855698733\n",
      "Step - 15221, Loss - 0.8525922438622748, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1188629962637378\n",
      "Step - 15222, Loss - 0.7412022382644172, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.428352234489921\n",
      "Step - 15223, Loss - 0.7561721985831947, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6979454357920941\n",
      "Step - 15224, Loss - 0.7056909729970561, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1120443131649256\n",
      "Step - 15225, Loss - 0.5764411046397017, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5046627625953942\n",
      "Step - 15226, Loss - 0.793101558320547, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5098182221496608\n",
      "Step - 15227, Loss - 0.8497447299474452, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.165101847121327\n",
      "Step - 15228, Loss - 0.7192453846368869, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.004574794464616\n",
      "Step - 15229, Loss - 0.6741569901692064, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.4648979822341249\n",
      "Step - 15230, Loss - 0.7598324823096536, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2102433299555597\n",
      "Step - 15231, Loss - 0.7493587123575964, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.2387102780718346\n",
      "Step - 15232, Loss - 0.6514597494001777, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.33365675638062997\n",
      "Step - 15233, Loss - 0.632253602808826, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8394564052138063\n",
      "Step - 15234, Loss - 0.7163653472736611, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0772578034519893\n",
      "Step - 15235, Loss - 0.6762163629137223, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6846499222681315\n",
      "Step - 15236, Loss - 0.7538429995845842, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5573818453916926\n",
      "Step - 15237, Loss - 0.7083599907568543, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7938355567964837\n",
      "Step - 15238, Loss - 0.814525518714112, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8483075091082791\n",
      "Step - 15239, Loss - 0.544950448738293, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7698732148092396\n",
      "Step - 15240, Loss - 0.8318107598740968, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7780251624742032\n",
      "Step - 15241, Loss - 0.7554126119710822, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.4796760048858216\n",
      "Step - 15242, Loss - 0.7537254100826483, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8424519683772158\n",
      "Step - 15243, Loss - 0.8817944008741991, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.4382400545012306\n",
      "Step - 15244, Loss - 0.7005218362925383, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6137533389154725\n",
      "Step - 15245, Loss - 0.7612752643200558, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6377503513055793\n",
      "Step - 15246, Loss - 0.635495527361038, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6122093448806474\n",
      "Step - 15247, Loss - 0.8109781858073235, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7090825152731725\n",
      "Step - 15248, Loss - 0.7552191632238168, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.179675767632006\n",
      "Step - 15249, Loss - 0.6765115191091914, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3545565023174686\n",
      "Step - 15250, Loss - 0.6220902289127777, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3630850808009876\n",
      "Step - 15251, Loss - 0.5576455795395838, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7929033842975635\n",
      "Step - 15252, Loss - 0.4339115410358801, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3365019421479096\n",
      "Step - 15253, Loss - 0.6637780862610595, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.1776650984931168\n",
      "Step - 15254, Loss - 0.7640266190414627, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1839240325669818\n",
      "Step - 15255, Loss - 0.6708254920507529, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0598476048126528\n",
      "Step - 15256, Loss - 0.6186626977098463, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1827483947273714\n",
      "Step - 15257, Loss - 0.5865987183807684, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5633625626189649\n",
      "Step - 15258, Loss - 0.8895892882314782, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2684931782320683\n",
      "Step - 15259, Loss - 0.8150594622455267, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2560531583022139\n",
      "Step - 15260, Loss - 0.7645073013587739, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4351475825842592\n",
      "Step - 15261, Loss - 0.8873711590490874, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7699245517608795\n",
      "Step - 15262, Loss - 0.7831018626948401, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7818433141721169\n",
      "Step - 15263, Loss - 0.31247778819547534, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7519366371585743\n",
      "Step - 15264, Loss - 0.641770267003334, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2836638944720893\n",
      "Step - 15265, Loss - 0.4727712235282177, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9442595327358851\n",
      "Step - 15266, Loss - 0.5477785860421605, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0062217470687285\n",
      "Step - 15267, Loss - 0.6572819630417595, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6309745155508868\n",
      "Step - 15268, Loss - 0.7723299915211433, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8096067185533317\n",
      "Step - 15269, Loss - 0.9656636162645557, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.4734300365351247\n",
      "Step - 15270, Loss - 0.6325782091475552, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.469469204739121\n",
      "Step - 15271, Loss - 0.9882746372770332, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9639994432476886\n",
      "Step - 15272, Loss - 0.4655025840738919, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5181352301557725\n",
      "Step - 15273, Loss - 0.707540134888476, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.712534155195619\n",
      "Step - 15274, Loss - 0.8014627752801929, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8481725710078146\n",
      "Step - 15275, Loss - 0.5554723098188897, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.274418792099752\n",
      "Step - 15276, Loss - 0.6054389515465601, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7388802335805558\n",
      "Step - 15277, Loss - 0.5722486352603757, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2208720978698977\n",
      "Step - 15278, Loss - 0.6315649231363756, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.2723961844505041\n",
      "Step - 15279, Loss - 0.6161679889462361, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8069033944677447\n",
      "Step - 15280, Loss - 0.6198813369220535, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.729951358812177\n",
      "Step - 15281, Loss - 0.8701945275683083, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9642172349184348\n",
      "Step - 15282, Loss - 0.7033085475338097, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.777393470328978\n",
      "Step - 15283, Loss - 0.9056001480634872, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6436104344912414\n",
      "Step - 15284, Loss - 0.8532520319263825, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1469471817061405\n",
      "Step - 15285, Loss - 0.7651953261074097, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8678344797501715\n",
      "Step - 15286, Loss - 0.5944705910477213, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.060520482219541\n",
      "Step - 15287, Loss - 0.7604077829201548, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6512447869989113\n",
      "Step - 15288, Loss - 0.5188095022697552, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.5697660521465004\n",
      "Step - 15289, Loss - 0.5526176258591459, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8162732518595694\n",
      "Step - 15290, Loss - 0.626231825551979, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1324669542183083\n",
      "Step - 15291, Loss - 0.5209163364807394, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7554384360127528\n",
      "Step - 15292, Loss - 0.6666124259354247, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8250816750990949\n",
      "Step - 15293, Loss - 0.6697414300294837, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5406544372767064\n",
      "Step - 15294, Loss - 0.5991939414824502, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.48970189889247323\n",
      "Step - 15295, Loss - 0.851441959457181, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1826794544388428\n",
      "Step - 15296, Loss - 0.5955674053667842, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6595947796851391\n",
      "Step - 15297, Loss - 0.7059345116774463, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.258409006104088\n",
      "Step - 15298, Loss - 0.7385799008100392, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.378690171872441\n",
      "Step - 15299, Loss - 0.7707845104309068, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1688839893091356\n",
      "Step - 15300, Loss - 0.6824976393377036, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.634345829775131\n",
      "Step - 15301, Loss - 0.6743989098977187, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6711365556511026\n",
      "Step - 15302, Loss - 0.720660330544507, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1984075746649616\n",
      "Step - 15303, Loss - 0.6136942231922045, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.0397768753709142\n",
      "Step - 15304, Loss - 0.613893451453388, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1586479698223622\n",
      "Step - 15305, Loss - 0.6617345715769061, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7102606968039323\n",
      "Step - 15306, Loss - 0.7067902382236338, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3707428962493593\n",
      "Step - 15307, Loss - 0.6673468368294215, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1273375702361002\n",
      "Step - 15308, Loss - 0.5862699554099616, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.74770900734537\n",
      "Step - 15309, Loss - 0.8593803211398665, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3150001330615624\n",
      "Step - 15310, Loss - 0.6882170743074512, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.738952606141553\n",
      "Step - 15311, Loss - 0.7304621655717987, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6259160032974658\n",
      "Step - 15312, Loss - 0.724417213223824, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.230561701188352\n",
      "Step - 15313, Loss - 0.6779922558782769, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.20750485537189345\n",
      "Step - 15314, Loss - 0.5588038606540984, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5074045579098125\n",
      "Step - 15315, Loss - 0.8239493298443891, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9684216249725275\n",
      "Step - 15316, Loss - 0.8556670801202879, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.401158161507101\n",
      "Step - 15317, Loss - 0.6168199810153848, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.080491239464585\n",
      "Step - 15318, Loss - 0.7980038213047769, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6873593501745704\n",
      "Step - 15319, Loss - 0.8101002553530703, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.289506350632108\n",
      "Step - 15320, Loss - 0.7090482658608862, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.5444979810217485\n",
      "Step - 15321, Loss - 0.6802928174426189, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.152127348880173\n",
      "Step - 15322, Loss - 0.47023178060376597, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.46900923378010256\n",
      "Step - 15323, Loss - 0.5848222495120532, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3668548164463459\n",
      "Step - 15324, Loss - 0.6923382877074276, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8003364538744702\n",
      "Step - 15325, Loss - 0.6014764208661254, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.4022341546712084\n",
      "Step - 15326, Loss - 0.5186559019073809, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.31853086010141984\n",
      "Step - 15327, Loss - 0.686447964492332, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5273358736949334\n",
      "Step - 15328, Loss - 0.6956356172248714, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7968982127920132\n",
      "Step - 15329, Loss - 0.8798360595723429, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8268062117089563\n",
      "Step - 15330, Loss - 0.5695323754261943, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9767004407054622\n",
      "Step - 15331, Loss - 0.6856791966947328, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2315871798678797\n",
      "Step - 15332, Loss - 0.8968957594996141, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0394575470976204\n",
      "Step - 15333, Loss - 0.7763084932121647, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4212385946070616\n",
      "Step - 15334, Loss - 0.5482820566373955, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3018588920936087\n",
      "Step - 15335, Loss - 0.4745850760482905, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7133115418935376\n",
      "Step - 15336, Loss - 0.8082741048823293, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5216406283785728\n",
      "Step - 15337, Loss - 0.8445985460113014, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8105067256453267\n",
      "Step - 15338, Loss - 0.5474781230532828, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8855157661916759\n",
      "Step - 15339, Loss - 0.7157826712521027, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.811559300731089\n",
      "Step - 15340, Loss - 0.5515789948444684, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.071622653566074\n",
      "Step - 15341, Loss - 0.7139255578247, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7260260114591851\n",
      "Step - 15342, Loss - 0.7597269091617825, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.624840058009697\n",
      "Step - 15343, Loss - 0.8089800283688762, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6837483281702994\n",
      "Step - 15344, Loss - 0.7688435611967677, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0924895373098047\n",
      "Step - 15345, Loss - 0.683422351789772, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8675499182023734\n",
      "Step - 15346, Loss - 0.7855020359114059, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.994980949364311\n",
      "Step - 15347, Loss - 0.735564781339679, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.6641175439014746\n",
      "Step - 15348, Loss - 0.6347846654874696, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4245924420114777\n",
      "Step - 15349, Loss - 0.681023570013797, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9790961471137849\n",
      "Step - 15350, Loss - 0.4418780683396957, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.2530567762714907\n",
      "Step - 15351, Loss - 0.7361589527580266, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.3398919093634899\n",
      "Step - 15352, Loss - 0.6773390138874658, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8324292346770945\n",
      "Step - 15353, Loss - 0.6623368340604073, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5016273649208014\n",
      "Step - 15354, Loss - 0.7733543750362467, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.4755159534646616\n",
      "Step - 15355, Loss - 0.5498602052299032, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5066318884215594\n",
      "Step - 15356, Loss - 0.5600618624679143, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0159189204596304\n",
      "Step - 15357, Loss - 0.7001834639865783, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.3970172754602323\n",
      "Step - 15358, Loss - 0.6431646399758623, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6289421793435452\n",
      "Step - 15359, Loss - 0.601910794879556, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5042313675039002\n",
      "Step - 15360, Loss - 0.6641311228544384, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0823577859304145\n",
      "Step - 15361, Loss - 0.5756808269680518, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7530773910188523\n",
      "Step - 15362, Loss - 0.7188312566160723, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1003912054327676\n",
      "Step - 15363, Loss - 0.671070511089071, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7369586916774397\n",
      "Step - 15364, Loss - 0.610493999817804, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9601476613360801\n",
      "Step - 15365, Loss - 0.6756499576922704, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3017043027978663\n",
      "Step - 15366, Loss - 0.8407583126214995, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1195562442428986\n",
      "Step - 15367, Loss - 0.793173065320991, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7429132615177672\n",
      "Step - 15368, Loss - 0.6098912365117342, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.1429291771240493\n",
      "Step - 15369, Loss - 0.5461528856497353, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3243942284945294\n",
      "Step - 15370, Loss - 0.6867466750306463, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7369871097906723\n",
      "Step - 15371, Loss - 0.9373268858802661, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.3875620963181987\n",
      "Step - 15372, Loss - 0.7197285800453893, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6398078823892319\n",
      "Step - 15373, Loss - 0.6981289110553959, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7164388681037983\n",
      "Step - 15374, Loss - 0.7839183959165876, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9825122142876977\n",
      "Step - 15375, Loss - 0.6255182235608029, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1270767188820197\n",
      "Step - 15376, Loss - 0.5942277074718415, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.4735849069811201\n",
      "Step - 15377, Loss - 0.9068758481311859, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.002333226319244\n",
      "Step - 15378, Loss - 0.6337312915535902, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5067580740550626\n",
      "Step - 15379, Loss - 0.7464677468501357, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2453400784505049\n",
      "Step - 15380, Loss - 0.9653564690790597, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.447687071650682\n",
      "Step - 15381, Loss - 0.7483000881947873, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.175020566373694\n",
      "Step - 15382, Loss - 0.7411099590188139, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1800884952924633\n",
      "Step - 15383, Loss - 0.6554228994223503, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8417729581811229\n",
      "Step - 15384, Loss - 0.7167733142454986, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1271051766052793\n",
      "Step - 15385, Loss - 0.7353910380459876, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0238140772604496\n",
      "Step - 15386, Loss - 0.6021994088571307, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3560034559017253\n",
      "Step - 15387, Loss - 0.7099698021619663, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8591921439294955\n",
      "Step - 15388, Loss - 0.5872875313073926, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.778829859092996\n",
      "Step - 15389, Loss - 0.7245590927616234, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.334437044641095\n",
      "Step - 15390, Loss - 0.6501897476574504, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4062531135803877\n",
      "Step - 15391, Loss - 0.7378947289460525, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9639508773249843\n",
      "Step - 15392, Loss - 0.6821546723028112, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6097680015160067\n",
      "Step - 15393, Loss - 0.8428744042897713, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8277754223186097\n",
      "Step - 15394, Loss - 0.8110851718758096, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.376177852997636\n",
      "Step - 15395, Loss - 0.5173875658446833, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.370275444415634\n",
      "Step - 15396, Loss - 0.8042186082862472, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7180932692364397\n",
      "Step - 15397, Loss - 0.7818678676144004, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8779783587015436\n",
      "Step - 15398, Loss - 0.6425722800596071, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0125100761244898\n",
      "Step - 15399, Loss - 0.6122843883921013, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.47388224065655776\n",
      "Step - 15400, Loss - 0.6682562334312883, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8040148951935115\n",
      "Step - 15401, Loss - 0.919910742784195, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4589196659422168\n",
      "Step - 15402, Loss - 0.7409795154256189, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2545635468417589\n",
      "Step - 15403, Loss - 0.8187318724726926, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9673202119561641\n",
      "Step - 15404, Loss - 0.9494919589975175, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.326162766987012\n",
      "Step - 15405, Loss - 0.7010271122166792, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.802446858792047\n",
      "Step - 15406, Loss - 0.597059955196267, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5914830038227641\n",
      "Step - 15407, Loss - 0.5972953227906629, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.8041698498550067\n",
      "Step - 15408, Loss - 0.6273518024428423, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.132488418567699\n",
      "Step - 15409, Loss - 0.8478770987893005, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7310207674549583\n",
      "Step - 15410, Loss - 0.6797911821920346, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5475406265321052\n",
      "Step - 15411, Loss - 0.880150220825396, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7926081165192488\n",
      "Step - 15412, Loss - 0.6764138800407361, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.3155426351949852\n",
      "Step - 15413, Loss - 0.5690700323842315, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1258919532597567\n",
      "Step - 15414, Loss - 0.7096658637472539, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8501950373129923\n",
      "Step - 15415, Loss - 0.5711793096577041, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3661116339164874\n",
      "Step - 15416, Loss - 0.8621004480654073, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4600822594654415\n",
      "Step - 15417, Loss - 0.776865671314004, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0956088153760206\n",
      "Step - 15418, Loss - 0.7888909547094958, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1858225971607868\n",
      "Step - 15419, Loss - 0.7501620108420133, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5296081871390463\n",
      "Step - 15420, Loss - 0.7779025037840128, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9005922478077952\n",
      "Step - 15421, Loss - 0.47683085137181686, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.2962057310423685\n",
      "Step - 15422, Loss - 0.7366543121883544, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0789740121187166\n",
      "Step - 15423, Loss - 0.6351831762545534, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7492616323601584\n",
      "Step - 15424, Loss - 0.7799184293129632, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.06232078209983\n",
      "Step - 15425, Loss - 0.5558796774845682, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9227756305648498\n",
      "Step - 15426, Loss - 0.6235701888067744, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5578081552817842\n",
      "Step - 15427, Loss - 0.9124899273168128, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.0854859297899364\n",
      "Step - 15428, Loss - 0.6491278084637047, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6772492500330579\n",
      "Step - 15429, Loss - 0.6086649868261503, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7232782031456152\n",
      "Step - 15430, Loss - 0.618310263983797, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1509694571639943\n",
      "Step - 15431, Loss - 0.7110531699282836, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.47143016905616514\n",
      "Step - 15432, Loss - 0.6092171601235339, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.521793061781034\n",
      "Step - 15433, Loss - 0.7787595052779395, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5385080558674153\n",
      "Step - 15434, Loss - 0.7758280150577934, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5359043130603323\n",
      "Step - 15435, Loss - 0.7166822822875114, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.3295162393864052\n",
      "Step - 15436, Loss - 0.8610509179015267, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.1867820128018343\n",
      "Step - 15437, Loss - 0.730382650282751, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5726946939425666\n",
      "Step - 15438, Loss - 0.6056461640768595, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9289301716202875\n",
      "Step - 15439, Loss - 0.967681452144448, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5204507867282977\n",
      "Step - 15440, Loss - 0.7625458126915703, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8218374263450461\n",
      "Step - 15441, Loss - 0.775962250731551, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.0892888787130905\n",
      "Step - 15442, Loss - 0.6540287070585485, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8016765210702356\n",
      "Step - 15443, Loss - 0.7932396321292445, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.407445699740354\n",
      "Step - 15444, Loss - 0.792719469043105, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2821376476253306\n",
      "Step - 15445, Loss - 0.5733481844127253, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6225072529466935\n",
      "Step - 15446, Loss - 0.5374150780108098, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.187911151963994\n",
      "Step - 15447, Loss - 0.7506667056942333, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6523528227136275\n",
      "Step - 15448, Loss - 0.7915280690924926, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8542060337105586\n",
      "Step - 15449, Loss - 0.6047734868269175, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0045342212544204\n",
      "Step - 15450, Loss - 0.6585735909692146, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4697418150485022\n",
      "Step - 15451, Loss - 0.593710127243088, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5137108739809875\n",
      "Step - 15452, Loss - 0.5161322074288397, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7317189469179842\n",
      "Step - 15453, Loss - 0.6544150098288329, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8322887915586719\n",
      "Step - 15454, Loss - 0.7108467771494258, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.0055365831199947\n",
      "Step - 15455, Loss - 0.7634197266785865, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5659709018593712\n",
      "Step - 15456, Loss - 0.6114741949670559, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9648884760510137\n",
      "Step - 15457, Loss - 0.5986940758709451, Learning Rate - 1.52587890625e-06, magnitude of gradient - 3.2447840804810135\n",
      "Step - 15458, Loss - 0.8885557156412982, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2363829387137573\n",
      "Step - 15459, Loss - 0.7100892266618547, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4034270732556746\n",
      "Step - 15460, Loss - 0.873856398940683, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4252395271040248\n",
      "Step - 15461, Loss - 0.660252790347049, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6693764391209858\n",
      "Step - 15462, Loss - 0.8639920848743388, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6326931649618953\n",
      "Step - 15463, Loss - 0.5118287858865397, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.275014474064829\n",
      "Step - 15464, Loss - 0.721151210042195, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6522668060344121\n",
      "Step - 15465, Loss - 0.596597335689303, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9160086843356008\n",
      "Step - 15466, Loss - 0.7143772992362689, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.989161661782453\n",
      "Step - 15467, Loss - 0.6521704273024349, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.983762218662841\n",
      "Step - 15468, Loss - 0.7360136628392172, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5757946500814062\n",
      "Step - 15469, Loss - 0.6479224793283437, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.909648066124652\n",
      "Step - 15470, Loss - 0.7472839923310624, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.32073287231727415\n",
      "Step - 15471, Loss - 0.6867104031629971, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5885838921795532\n",
      "Step - 15472, Loss - 0.8937060195617655, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4316413965725843\n",
      "Step - 15473, Loss - 0.8510256162309521, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8512509215010986\n",
      "Step - 15474, Loss - 0.6989600165034804, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1262145694364645\n",
      "Step - 15475, Loss - 0.7786600175440499, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.797714590038603\n",
      "Step - 15476, Loss - 0.6661692650399604, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3725008734130622\n",
      "Step - 15477, Loss - 0.5602459867105718, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9402725153276795\n",
      "Step - 15478, Loss - 0.7231995275618113, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6345491246390338\n",
      "Step - 15479, Loss - 0.6334122390305607, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0678818296304566\n",
      "Step - 15480, Loss - 0.7804063433081099, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2490057126458498\n",
      "Step - 15481, Loss - 0.6018921703619472, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5607696062189018\n",
      "Step - 15482, Loss - 0.82378838988679, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9948208757241521\n",
      "Step - 15483, Loss - 0.8596999215299473, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.5959725195092536\n",
      "Step - 15484, Loss - 0.5536096513216112, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.917066137914888\n",
      "Step - 15485, Loss - 0.5120515965667528, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5008520614901946\n",
      "Step - 15486, Loss - 0.5335003400799117, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9053072865052876\n",
      "Step - 15487, Loss - 0.7919457893644867, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9445214485640061\n",
      "Step - 15488, Loss - 0.6131276357241235, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.417158605253449\n",
      "Step - 15489, Loss - 0.7866517060638931, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5997966472232122\n",
      "Step - 15490, Loss - 0.7393583522731887, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7515384058350714\n",
      "Step - 15491, Loss - 0.6550489302219902, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7527878651764752\n",
      "Step - 15492, Loss - 0.6323135060920413, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.469031940295455\n",
      "Step - 15493, Loss - 0.7810359121168794, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8807917365415374\n",
      "Step - 15494, Loss - 0.7744798256840566, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1362284986193951\n",
      "Step - 15495, Loss - 0.8065559303422135, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5286616511441316\n",
      "Step - 15496, Loss - 0.8291664085908919, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.45650389241037476\n",
      "Step - 15497, Loss - 0.5577519054658706, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9640886218141536\n",
      "Step - 15498, Loss - 0.7101357014567996, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.557980566641766\n",
      "Step - 15499, Loss - 0.7905374469519344, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1666326517250072\n",
      "Step - 15500, Loss - 0.7963670083104315, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0084149342089812\n",
      "Step - 15501, Loss - 0.792853437794554, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.337090899663578\n",
      "Step - 15502, Loss - 0.5915656324681845, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3866258122609219\n",
      "Step - 15503, Loss - 0.6449889338413302, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6400877818078443\n",
      "Step - 15504, Loss - 0.5515127663546909, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4170868657998479\n",
      "Step - 15505, Loss - 0.7754507608697436, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6130120828267906\n",
      "Step - 15506, Loss - 0.731448503931517, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.590234624862495\n",
      "Step - 15507, Loss - 0.671923434792415, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7275975761612881\n",
      "Step - 15508, Loss - 0.7243597256241874, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8837575843228918\n",
      "Step - 15509, Loss - 0.7186842590487433, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0058949774920607\n",
      "Step - 15510, Loss - 0.6942509016372396, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8946856495190807\n",
      "Step - 15511, Loss - 0.7097650740922828, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9592980790269923\n",
      "Step - 15512, Loss - 0.7089068090190103, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2644873357101543\n",
      "Step - 15513, Loss - 0.5362952569736856, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8242912002237734\n",
      "Step - 15514, Loss - 0.6830463061920068, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.968954518413938\n",
      "Step - 15515, Loss - 0.5863997112154385, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.4084940657705541\n",
      "Step - 15516, Loss - 0.793876605615873, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6722883949471354\n",
      "Step - 15517, Loss - 0.8387293367122863, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.224856227238345\n",
      "Step - 15518, Loss - 0.5615298126873303, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.2791186606106044\n",
      "Step - 15519, Loss - 0.5153599019153929, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4679029866644733\n",
      "Step - 15520, Loss - 0.5522652312625261, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0508146398815275\n",
      "Step - 15521, Loss - 0.825278318541334, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7535610748750328\n",
      "Step - 15522, Loss - 0.7816697431815413, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5380874383020743\n",
      "Step - 15523, Loss - 0.8065749522070386, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0931293332816605\n",
      "Step - 15524, Loss - 0.6847637208658806, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0104500444053492\n",
      "Step - 15525, Loss - 0.6624505978979425, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6669093811737437\n",
      "Step - 15526, Loss - 0.6881651562390744, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.064660376769099\n",
      "Step - 15527, Loss - 0.8290283484140702, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0704028686736187\n",
      "Step - 15528, Loss - 0.699930013064807, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4136516062655213\n",
      "Step - 15529, Loss - 0.7694086908894767, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.04096269204367\n",
      "Step - 15530, Loss - 0.6736714495677502, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6369329800390884\n",
      "Step - 15531, Loss - 0.7756632414339029, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.332392738344414\n",
      "Step - 15532, Loss - 0.6344542634463486, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.0713027234311756\n",
      "Step - 15533, Loss - 0.7882992342222446, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1627477187240935\n",
      "Step - 15534, Loss - 0.9735608336619127, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8469795148058696\n",
      "Step - 15535, Loss - 0.6420522662907168, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.292931293252955\n",
      "Step - 15536, Loss - 0.6228711323871265, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0851257143713688\n",
      "Step - 15537, Loss - 0.7798965469660688, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2580499320047143\n",
      "Step - 15538, Loss - 0.5963029927910816, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.437975632079213\n",
      "Step - 15539, Loss - 0.6195656086315893, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7887032967928717\n",
      "Step - 15540, Loss - 0.6124566056638159, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7677344772208292\n",
      "Step - 15541, Loss - 0.7024128309377172, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2834389130889188\n",
      "Step - 15542, Loss - 0.7868013944458382, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.0785246871026444\n",
      "Step - 15543, Loss - 0.6735097581019116, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4306744010164094\n",
      "Step - 15544, Loss - 0.7729522584544549, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9605399267996424\n",
      "Step - 15545, Loss - 0.6863165238793416, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8978224695995053\n",
      "Step - 15546, Loss - 0.579434560903868, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2570787470012033\n",
      "Step - 15547, Loss - 0.7093913862827975, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6976315058819575\n",
      "Step - 15548, Loss - 0.9224673829371667, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.391638012737133\n",
      "Step - 15549, Loss - 0.6738904599228137, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.175658572609243\n",
      "Step - 15550, Loss - 0.7847869624012999, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3878997169570537\n",
      "Step - 15551, Loss - 0.8083131207700889, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8973990528626703\n",
      "Step - 15552, Loss - 0.6428287132800741, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0118020241888341\n",
      "Step - 15553, Loss - 0.5393975125276105, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7323951524316668\n",
      "Step - 15554, Loss - 0.6971288013012333, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.31427157661891764\n",
      "Step - 15555, Loss - 0.8104569126349196, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8944413411180088\n",
      "Step - 15556, Loss - 0.8447652509689839, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.039628275639729\n",
      "Step - 15557, Loss - 0.7546536939738572, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.374670810352811\n",
      "Step - 15558, Loss - 0.6151816822192029, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9842333583788231\n",
      "Step - 15559, Loss - 0.7632890627524871, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7782594642364402\n",
      "Step - 15560, Loss - 0.7039622273904053, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4601923592418944\n",
      "Step - 15561, Loss - 0.7287668190462219, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6624932322272161\n",
      "Step - 15562, Loss - 0.6631106087884132, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8507270352510513\n",
      "Step - 15563, Loss - 0.6335119777581812, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6402005299260345\n",
      "Step - 15564, Loss - 0.7450205415045236, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.1131052377019977\n",
      "Step - 15565, Loss - 0.7449834449331766, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4477724804287304\n",
      "Step - 15566, Loss - 0.7748754471695224, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1290619028135096\n",
      "Step - 15567, Loss - 0.6823615218408875, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.6039581266907774\n",
      "Step - 15568, Loss - 0.6054338157320669, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5631145699297246\n",
      "Step - 15569, Loss - 0.6633164938780648, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0788145528943018\n",
      "Step - 15570, Loss - 0.8231990837897513, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4953832260853244\n",
      "Step - 15571, Loss - 0.8258583216249781, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9569109104068074\n",
      "Step - 15572, Loss - 0.6381390468398517, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8190220304711237\n",
      "Step - 15573, Loss - 0.6253791401140446, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7047388039719533\n",
      "Step - 15574, Loss - 0.8274048749095173, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5428773904979552\n",
      "Step - 15575, Loss - 0.5898520336269474, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.390507467428313\n",
      "Step - 15576, Loss - 1.0045708556704425, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.272172223275175\n",
      "Step - 15577, Loss - 0.7715345050762576, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.01392449086462\n",
      "Step - 15578, Loss - 0.6301573614733826, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.231282447416528\n",
      "Step - 15579, Loss - 0.8139994188563827, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5805578769081432\n",
      "Step - 15580, Loss - 0.7403615318310022, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1476049208382504\n",
      "Step - 15581, Loss - 0.9224205803953065, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.7436481133017883\n",
      "Step - 15582, Loss - 0.8252215993657417, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.069544645371384\n",
      "Step - 15583, Loss - 0.662679286011313, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9801144939345046\n",
      "Step - 15584, Loss - 0.6310102008810219, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.31027861621315456\n",
      "Step - 15585, Loss - 0.8704172510040159, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5324020037704404\n",
      "Step - 15586, Loss - 0.7738837908511561, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6471604715198243\n",
      "Step - 15587, Loss - 0.7924361648030669, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5588467916706985\n",
      "Step - 15588, Loss - 0.47534360671177006, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.180243375413889\n",
      "Step - 15589, Loss - 0.7063727660984459, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7109576699656107\n",
      "Step - 15590, Loss - 0.7076200239377214, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.117438316604623\n",
      "Step - 15591, Loss - 0.5629048946675105, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.708358284214645\n",
      "Step - 15592, Loss - 0.8445255557130057, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7838194881022689\n",
      "Step - 15593, Loss - 0.531349433815078, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.43185469845252\n",
      "Step - 15594, Loss - 0.6598477484079569, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.4854983371162733\n",
      "Step - 15595, Loss - 0.7863916896296226, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2946919980501619\n",
      "Step - 15596, Loss - 0.4863179765395658, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6070349103283006\n",
      "Step - 15597, Loss - 0.7658092001491611, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.3554776359227345\n",
      "Step - 15598, Loss - 0.58482837954765, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7765313518609187\n",
      "Step - 15599, Loss - 0.6609510449579278, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8461546029855652\n",
      "Step - 15600, Loss - 0.7114151018049568, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0836722366802638\n",
      "Step - 15601, Loss - 0.6884868602147342, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6693311222404156\n",
      "Step - 15602, Loss - 0.6481264477326358, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6462993246325662\n",
      "Step - 15603, Loss - 0.5516766869366123, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.7908262785639746\n",
      "Step - 15604, Loss - 0.8128847952829761, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1538036337328736\n",
      "Step - 15605, Loss - 0.6759938669947778, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4317239301808244\n",
      "Step - 15606, Loss - 0.6930911002324767, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5539971735363378\n",
      "Step - 15607, Loss - 0.7108375257810904, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.559354732561333\n",
      "Step - 15608, Loss - 0.6387168688252342, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6218275615535983\n",
      "Step - 15609, Loss - 0.6056692621722599, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3170799058417026\n",
      "Step - 15610, Loss - 0.6156536122878472, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7563688508932828\n",
      "Step - 15611, Loss - 0.6952107255414055, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1122087896806154\n",
      "Step - 15612, Loss - 0.7656210146940071, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.39349074769869435\n",
      "Step - 15613, Loss - 0.8314469821893977, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6262986691751534\n",
      "Step - 15614, Loss - 0.6436580109132327, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3888582016010071\n",
      "Step - 15615, Loss - 0.7070170032518138, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1761400107030586\n",
      "Step - 15616, Loss - 0.6748607921548235, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.3223049604522131\n",
      "Step - 15617, Loss - 0.5289276131056406, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6963381457973126\n",
      "Step - 15618, Loss - 0.6841785153238462, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9889435516033401\n",
      "Step - 15619, Loss - 0.7667053305307546, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.585227141403812\n",
      "Step - 15620, Loss - 0.8014264011125101, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.046692172349398\n",
      "Step - 15621, Loss - 0.4882389456542454, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4364817033718476\n",
      "Step - 15622, Loss - 0.5843762354430222, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5045671531712366\n",
      "Step - 15623, Loss - 0.6389709269505259, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.199945330390002\n",
      "Step - 15624, Loss - 0.646531616239735, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9105013065156988\n",
      "Step - 15625, Loss - 0.5288319831138186, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1961292953977836\n",
      "Step - 15626, Loss - 0.6662429395204349, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.0495636449270096\n",
      "Step - 15627, Loss - 0.6845607022483557, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.931148091550709\n",
      "Step - 15628, Loss - 0.7603107422205881, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2306885601660218\n",
      "Step - 15629, Loss - 0.5675662497952852, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7779592171131707\n",
      "Step - 15630, Loss - 0.9030085857345264, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.92440410813001\n",
      "Step - 15631, Loss - 0.7293714957189563, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.49027657359295074\n",
      "Step - 15632, Loss - 0.6921745683950002, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8649289806791323\n",
      "Step - 15633, Loss - 0.8710635962521783, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.512714440238732\n",
      "Step - 15634, Loss - 0.48314709421438634, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.9318512600894744\n",
      "Step - 15635, Loss - 0.7442947532735993, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.3629780007408436\n",
      "Step - 15636, Loss - 0.5918592247030009, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9410819444013946\n",
      "Step - 15637, Loss - 0.6928916832002545, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1918720011074535\n",
      "Step - 15638, Loss - 0.9064582411323356, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5728943798212465\n",
      "Step - 15639, Loss - 0.8509160277777774, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5536197280468398\n",
      "Step - 15640, Loss - 0.7271508838025442, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9272167537039426\n",
      "Step - 15641, Loss - 0.5873626980135641, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2882333908877752\n",
      "Step - 15642, Loss - 0.9559188341552975, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1290412027651267\n",
      "Step - 15643, Loss - 0.6877781263579462, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.855271277862729\n",
      "Step - 15644, Loss - 0.6901241015854367, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8615649986071969\n",
      "Step - 15645, Loss - 0.652096026510545, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1712265502882433\n",
      "Step - 15646, Loss - 0.8622797221436845, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.0217373388159006\n",
      "Step - 15647, Loss - 0.6876337641692867, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.472305220999374\n",
      "Step - 15648, Loss - 0.7520044554252759, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5619845790435556\n",
      "Step - 15649, Loss - 0.7337979942663996, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8700169949856393\n",
      "Step - 15650, Loss - 0.6908100528454735, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.328039372134694\n",
      "Step - 15651, Loss - 0.8097736137350463, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.100416043885133\n",
      "Step - 15652, Loss - 0.6183409009237202, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.285559643736948\n",
      "Step - 15653, Loss - 0.6073424982638953, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6604705949396816\n",
      "Step - 15654, Loss - 0.7106909220394778, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0128929055382678\n",
      "Step - 15655, Loss - 0.8641610446308997, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.421778709136204\n",
      "Step - 15656, Loss - 0.8132428897841617, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0388944379869816\n",
      "Step - 15657, Loss - 0.7157949675054311, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6039947114692124\n",
      "Step - 15658, Loss - 0.7045686824492196, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.060744586014266\n",
      "Step - 15659, Loss - 0.7132020678704665, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9294773961872127\n",
      "Step - 15660, Loss - 0.7906407874291653, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1154536694471242\n",
      "Step - 15661, Loss - 0.7207815990877919, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6262307479918149\n",
      "Step - 15662, Loss - 0.745898571101917, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2188149243001438\n",
      "Step - 15663, Loss - 0.8547170322542659, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3039274275442858\n",
      "Step - 15664, Loss - 0.5760313818254641, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0431360010394544\n",
      "Step - 15665, Loss - 0.8525667056334538, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7984533287439601\n",
      "Step - 15666, Loss - 0.6650901975547688, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5804433677436889\n",
      "Step - 15667, Loss - 0.7881103548704836, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.787732388227182\n",
      "Step - 15668, Loss - 0.670968365012785, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8251585354400576\n",
      "Step - 15669, Loss - 0.7389570100927956, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1678967414010317\n",
      "Step - 15670, Loss - 0.6762781995159248, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.563362870231697\n",
      "Step - 15671, Loss - 0.44269685472786924, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7062137229841885\n",
      "Step - 15672, Loss - 0.5846258390496848, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1144732317913932\n",
      "Step - 15673, Loss - 0.6814732920047066, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8309109834118732\n",
      "Step - 15674, Loss - 0.4978708653539452, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7514875713133307\n",
      "Step - 15675, Loss - 0.9122579960989119, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1257397510893306\n",
      "Step - 15676, Loss - 0.7121197375357653, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.269355890264376\n",
      "Step - 15677, Loss - 0.6062002635107067, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.23177925691101342\n",
      "Step - 15678, Loss - 0.5683884927811269, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9677496255259526\n",
      "Step - 15679, Loss - 0.7572694177827528, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9227668008557548\n",
      "Step - 15680, Loss - 0.6713567055485481, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9603185244451016\n",
      "Step - 15681, Loss - 0.822987035248012, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5546198434923596\n",
      "Step - 15682, Loss - 0.8865373507289982, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2804384354213156\n",
      "Step - 15683, Loss - 0.7794118255868687, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.188909362593424\n",
      "Step - 15684, Loss - 0.5744252947572182, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8998913562838587\n",
      "Step - 15685, Loss - 0.6573681246174339, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.627649618882494\n",
      "Step - 15686, Loss - 0.9076827888771146, Learning Rate - 1.52587890625e-06, magnitude of gradient - 3.058856950739943\n",
      "Step - 15687, Loss - 0.7098970082008838, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0554832753146497\n",
      "Step - 15688, Loss - 0.9237551671060724, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3689599761839375\n",
      "Step - 15689, Loss - 0.8057432048511989, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7556447891104312\n",
      "Step - 15690, Loss - 0.6639707454424354, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0115468760496307\n",
      "Step - 15691, Loss - 0.758356892631986, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.3723618897381777\n",
      "Step - 15692, Loss - 0.6028890148319063, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.125188424344365\n",
      "Step - 15693, Loss - 0.6341251160135566, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.5297586370387917\n",
      "Step - 15694, Loss - 0.4772461314006783, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9924833701722009\n",
      "Step - 15695, Loss - 0.8256099339498145, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7734870124143101\n",
      "Step - 15696, Loss - 0.732218728662651, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.738268967633129\n",
      "Step - 15697, Loss - 0.6533620768647407, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1809691959509407\n",
      "Step - 15698, Loss - 0.6046464346018544, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6313733888534556\n",
      "Step - 15699, Loss - 0.6246687219252016, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6078925393752512\n",
      "Step - 15700, Loss - 0.6066415649998796, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1640906341737332\n",
      "Step - 15701, Loss - 0.7043998792720741, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.243645133440713\n",
      "Step - 15702, Loss - 0.45884747049116226, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2376748423800457\n",
      "Step - 15703, Loss - 0.6036726546299699, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7392562267650006\n",
      "Step - 15704, Loss - 0.6012935431243821, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4411050886651082\n",
      "Step - 15705, Loss - 0.706721137890483, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6071546825245389\n",
      "Step - 15706, Loss - 0.6986944701295653, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3654634241436248\n",
      "Step - 15707, Loss - 0.7337784654511088, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.694001130946808\n",
      "Step - 15708, Loss - 0.5424176679974392, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4359100203421802\n",
      "Step - 15709, Loss - 0.7389572688422366, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1963871592487068\n",
      "Step - 15710, Loss - 0.7748711265652792, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6044110465785758\n",
      "Step - 15711, Loss - 0.5350386967915545, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.965471853129533\n",
      "Step - 15712, Loss - 0.836998030853999, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.508343619666926\n",
      "Step - 15713, Loss - 0.8163469761682215, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3751563166390632\n",
      "Step - 15714, Loss - 0.9614658106129905, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.153275604980401\n",
      "Step - 15715, Loss - 0.5861344627293607, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9056611414131547\n",
      "Step - 15716, Loss - 0.6593946165552134, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6582168919485862\n",
      "Step - 15717, Loss - 0.6065553700894979, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5041511917769472\n",
      "Step - 15718, Loss - 0.7173290158867736, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.1724179427874173\n",
      "Step - 15719, Loss - 0.8524374433477312, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1007283791688325\n",
      "Step - 15720, Loss - 0.6667325773607742, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.23010576664267876\n",
      "Step - 15721, Loss - 0.850551228704772, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.8680786221843353\n",
      "Step - 15722, Loss - 0.666473414441141, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.12433364503425039\n",
      "Step - 15723, Loss - 0.7350374865112203, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5558022189183092\n",
      "Step - 15724, Loss - 0.6762809624038418, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6181929970879233\n",
      "Step - 15725, Loss - 0.6649456551413621, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.0732785864716967\n",
      "Step - 15726, Loss - 0.6009606364495275, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6806392668858667\n",
      "Step - 15727, Loss - 0.6815217614980558, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5136117125063416\n",
      "Step - 15728, Loss - 0.6626858083555265, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9606167757726612\n",
      "Step - 15729, Loss - 0.7523238596315887, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.2347151568485257\n",
      "Step - 15730, Loss - 0.6865876497265544, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3578476509863322\n",
      "Step - 15731, Loss - 0.6856576090166745, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.6758733828334282\n",
      "Step - 15732, Loss - 0.7099954682073251, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3417547234362146\n",
      "Step - 15733, Loss - 0.6266846419540959, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7442496210213647\n",
      "Step - 15734, Loss - 0.571653432699412, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5751959857496374\n",
      "Step - 15735, Loss - 0.5574118791803815, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9558430739419991\n",
      "Step - 15736, Loss - 0.6124497109973694, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.2869573366267993\n",
      "Step - 15737, Loss - 0.6465149668707428, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.0006294448528\n",
      "Step - 15738, Loss - 0.6199790941888371, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7407583522446257\n",
      "Step - 15739, Loss - 0.6061601983044809, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5498925483302074\n",
      "Step - 15740, Loss - 0.523482176016434, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6072504972847417\n",
      "Step - 15741, Loss - 0.5157375166561385, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.709171301591736\n",
      "Step - 15742, Loss - 0.5321586498088121, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.0181570778234974\n",
      "Step - 15743, Loss - 0.6922213019791951, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.102629147451176\n",
      "Step - 15744, Loss - 0.8995137624512259, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.1276448541160633\n",
      "Step - 15745, Loss - 0.6702087075451757, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.611667614152685\n",
      "Step - 15746, Loss - 0.9787453050967677, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9841280601962503\n",
      "Step - 15747, Loss - 0.6054399031880769, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6318313068044732\n",
      "Step - 15748, Loss - 0.6203666393073707, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.323953073495211\n",
      "Step - 15749, Loss - 0.7608454203652746, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3088188526652427\n",
      "Step - 15750, Loss - 0.8208502893750059, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.1012363682492805\n",
      "Step - 15751, Loss - 0.6610150535085786, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.4919143906235114\n",
      "Step - 15752, Loss - 0.5072153663997311, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.552253618418381\n",
      "Step - 15753, Loss - 0.8777955545343771, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0465759348217147\n",
      "Step - 15754, Loss - 0.7204029421438531, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1868611259950033\n",
      "Step - 15755, Loss - 0.5201739441353281, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6361742779872492\n",
      "Step - 15756, Loss - 0.6396158471032025, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1732077319426988\n",
      "Step - 15757, Loss - 0.4877445363995801, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.4141223240206418\n",
      "Step - 15758, Loss - 0.6815913079129736, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1113534982941242\n",
      "Step - 15759, Loss - 0.6774088402501414, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.4645696497323418\n",
      "Step - 15760, Loss - 0.6428954401905893, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.340718196446143\n",
      "Step - 15761, Loss - 0.7273346620748067, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.84156671712616\n",
      "Step - 15762, Loss - 0.6500082877666633, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1063983933523\n",
      "Step - 15763, Loss - 0.7228585536352805, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7516987104871123\n",
      "Step - 15764, Loss - 0.6275449041306032, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.068645620316231\n",
      "Step - 15765, Loss - 0.8588072801016491, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2939409176882426\n",
      "Step - 15766, Loss - 0.820990617414318, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9924486449221532\n",
      "Step - 15767, Loss - 0.7934752155403026, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2978943692374498\n",
      "Step - 15768, Loss - 0.7326495030882607, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.033510552546211\n",
      "Step - 15769, Loss - 0.5385826673380548, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.197359349396355\n",
      "Step - 15770, Loss - 0.7941850404587447, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8115805697408275\n",
      "Step - 15771, Loss - 0.4815914633714587, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6640928603187701\n",
      "Step - 15772, Loss - 0.6603115401795304, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.809946323210869\n",
      "Step - 15773, Loss - 0.5908728445064115, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5813429407637776\n",
      "Step - 15774, Loss - 0.7441483283648346, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2888582691910835\n",
      "Step - 15775, Loss - 0.7942661500791177, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.0682121986496713\n",
      "Step - 15776, Loss - 0.7258941220288735, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3675841857633753\n",
      "Step - 15777, Loss - 0.6340665073983692, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7900133808107792\n",
      "Step - 15778, Loss - 0.7283058425143338, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.281228916082107\n",
      "Step - 15779, Loss - 0.6361644373556614, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.43078626991054186\n",
      "Step - 15780, Loss - 0.7309371953792321, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.406674631916585\n",
      "Step - 15781, Loss - 0.656202736942306, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.7325905817315457\n",
      "Step - 15782, Loss - 0.8716810104080706, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.0466289278171477\n",
      "Step - 15783, Loss - 0.7138074984912607, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.1587143741980053\n",
      "Step - 15784, Loss - 0.7505728570956951, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.812187364046377\n",
      "Step - 15785, Loss - 0.7813115925288325, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3119880866752267\n",
      "Step - 15786, Loss - 0.9622077410701549, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2925661795855923\n",
      "Step - 15787, Loss - 0.6015450371205096, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5361978988114944\n",
      "Step - 15788, Loss - 0.6266385512207678, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.859109722988685\n",
      "Step - 15789, Loss - 0.7944882533066769, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1933425850082244\n",
      "Step - 15790, Loss - 0.7319455176998284, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1499237834891147\n",
      "Step - 15791, Loss - 0.9085592448538276, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8535274907656636\n",
      "Step - 15792, Loss - 0.7926061466940076, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.982197309100817\n",
      "Step - 15793, Loss - 0.6876984176434289, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.183971125596936\n",
      "Step - 15794, Loss - 0.6951596881114274, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3511111944372083\n",
      "Step - 15795, Loss - 0.785905839769809, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9211237881846736\n",
      "Step - 15796, Loss - 0.5824446221233063, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7877646000150347\n",
      "Step - 15797, Loss - 0.7568894014205055, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1803425522884334\n",
      "Step - 15798, Loss - 0.7845932124043578, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7410546742986635\n",
      "Step - 15799, Loss - 0.7497120041220837, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9259418953560294\n",
      "Step - 15800, Loss - 0.6734242966326275, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7485988909317827\n",
      "Step - 15801, Loss - 0.7539707400724794, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7498763426514541\n",
      "Step - 15802, Loss - 0.6998808127281146, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0978313120727072\n",
      "Step - 15803, Loss - 0.4311319096161529, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9540281515210902\n",
      "Step - 15804, Loss - 0.6455061842096068, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9480978761497842\n",
      "Step - 15805, Loss - 0.732571131406035, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5815319988402157\n",
      "Step - 15806, Loss - 0.8241240977993807, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4948114775763088\n",
      "Step - 15807, Loss - 0.633619848963576, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6976454702228877\n",
      "Step - 15808, Loss - 0.7053034922386071, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7991722813074403\n",
      "Step - 15809, Loss - 0.6917633832559394, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1150789226297766\n",
      "Step - 15810, Loss - 0.46849264579428707, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6921826932411421\n",
      "Step - 15811, Loss - 0.675366100800704, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.2165479891870277\n",
      "Step - 15812, Loss - 0.5894008795610959, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7402600313337622\n",
      "Step - 15813, Loss - 0.716168935295559, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4339590408635283\n",
      "Step - 15814, Loss - 0.6617302506204972, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.584028210604222\n",
      "Step - 15815, Loss - 0.6820150952833411, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8299835116675537\n",
      "Step - 15816, Loss - 0.7899298842777162, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.562586072795587\n",
      "Step - 15817, Loss - 0.700473198748339, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5479254513467793\n",
      "Step - 15818, Loss - 0.6377405488250464, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5104787428897817\n",
      "Step - 15819, Loss - 0.7825876604990918, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5602798969990719\n",
      "Step - 15820, Loss - 0.5892707051658339, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1055186139354392\n",
      "Step - 15821, Loss - 0.6572345300744104, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7601285922599904\n",
      "Step - 15822, Loss - 0.6419412091099255, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5746320913315733\n",
      "Step - 15823, Loss - 0.8909192065518158, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.067454147413799\n",
      "Step - 15824, Loss - 0.6940161549725926, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9787947028093098\n",
      "Step - 15825, Loss - 0.6560529000019522, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.3604715794245179\n",
      "Step - 15826, Loss - 0.7314082840519939, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4089649796116397\n",
      "Step - 15827, Loss - 0.5702339625844755, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.245002240502126\n",
      "Step - 15828, Loss - 0.7353339721975076, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1177591170299472\n",
      "Step - 15829, Loss - 0.41810913332246846, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9608173126584483\n",
      "Step - 15830, Loss - 0.8510057568895706, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7721106597931676\n",
      "Step - 15831, Loss - 0.661908009243356, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6352691979332904\n",
      "Step - 15832, Loss - 0.5516426355921735, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1809519236473702\n",
      "Step - 15833, Loss - 0.7756289221831649, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8398594450014268\n",
      "Step - 15834, Loss - 0.688276424496083, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8440512201279926\n",
      "Step - 15835, Loss - 0.8209284020170501, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.117506531932249\n",
      "Step - 15836, Loss - 0.5845800123770641, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6509451589814712\n",
      "Step - 15837, Loss - 0.7658403517934717, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.38624054179575856\n",
      "Step - 15838, Loss - 0.6091875597600894, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9895511682591536\n",
      "Step - 15839, Loss - 0.7396148855288092, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2818587614741\n",
      "Step - 15840, Loss - 0.8343843969064813, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.1196666110465836\n",
      "Step - 15841, Loss - 0.5991534499485424, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.090550482787556\n",
      "Step - 15842, Loss - 0.601837088297913, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9444675667432989\n",
      "Step - 15843, Loss - 0.6930371616735954, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9509826737165796\n",
      "Step - 15844, Loss - 0.6338898239559443, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.0419822602738513\n",
      "Step - 15845, Loss - 0.7909065751482219, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7565693854242941\n",
      "Step - 15846, Loss - 0.7896494731187456, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6101993994419586\n",
      "Step - 15847, Loss - 0.4798191322822023, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.3525670469479354\n",
      "Step - 15848, Loss - 0.6191311364352562, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.747612194915667\n",
      "Step - 15849, Loss - 0.9257702455096299, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9727004146110462\n",
      "Step - 15850, Loss - 0.753770830584862, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4351913381558328\n",
      "Step - 15851, Loss - 0.90500959505875, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.1811329890602225\n",
      "Step - 15852, Loss - 0.6331259749673118, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.475313853980409\n",
      "Step - 15853, Loss - 0.6896810744595663, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9580242244264161\n",
      "Step - 15854, Loss - 0.6133089773022554, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9671223811682736\n",
      "Step - 15855, Loss - 0.6299761358556266, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.4501635315222708\n",
      "Step - 15856, Loss - 0.6521967248635836, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6227861858016868\n",
      "Step - 15857, Loss - 0.9651797953509995, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.36710025427431\n",
      "Step - 15858, Loss - 0.5990163329751108, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8215645618721341\n",
      "Step - 15859, Loss - 0.6564899194548025, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8474527732802207\n",
      "Step - 15860, Loss - 0.8856958314992809, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7578010228995369\n",
      "Step - 15861, Loss - 0.7432888886307831, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7122363444038455\n",
      "Step - 15862, Loss - 0.5598617800158717, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0418046529366038\n",
      "Step - 15863, Loss - 0.4607299323854196, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2000034929038539\n",
      "Step - 15864, Loss - 0.7186789006576279, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5453201535759695\n",
      "Step - 15865, Loss - 0.8770630523440904, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9693824866497718\n",
      "Step - 15866, Loss - 0.7321474880758327, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7927432477132658\n",
      "Step - 15867, Loss - 0.6697712555489188, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8275631549953151\n",
      "Step - 15868, Loss - 0.7312898476992059, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6501503729701408\n",
      "Step - 15869, Loss - 0.7125068997460343, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9878967325016692\n",
      "Step - 15870, Loss - 0.6252178735049632, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.947615763108843\n",
      "Step - 15871, Loss - 0.8063736884970567, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.237266503168971\n",
      "Step - 15872, Loss - 0.5278246368072242, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.039886334112227\n",
      "Step - 15873, Loss - 0.8042948790751873, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2958277716445756\n",
      "Step - 15874, Loss - 0.549034012254612, Learning Rate - 1.52587890625e-06, magnitude of gradient - 3.1419498234269048\n",
      "Step - 15875, Loss - 0.6107789946766363, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3666093728374014\n",
      "Step - 15876, Loss - 0.6787339300435543, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9079698031483259\n",
      "Step - 15877, Loss - 0.5842203351167531, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8070922685720758\n",
      "Step - 15878, Loss - 0.7061772113197414, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.22178863657474\n",
      "Step - 15879, Loss - 0.6274869858529947, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6446047019819372\n",
      "Step - 15880, Loss - 0.7771173637056636, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9302118225167179\n",
      "Step - 15881, Loss - 0.7636582931555957, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2981970095279234\n",
      "Step - 15882, Loss - 0.8583283899127555, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9972163905571874\n",
      "Step - 15883, Loss - 0.7937471583622524, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.322859887167373\n",
      "Step - 15884, Loss - 0.7883559286367596, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.63141054412754\n",
      "Step - 15885, Loss - 0.6813035703973557, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0422018507544022\n",
      "Step - 15886, Loss - 0.6817944990300236, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.394599710750203\n",
      "Step - 15887, Loss - 0.660888441993115, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.589733947839927\n",
      "Step - 15888, Loss - 0.7975775872697185, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2687450494588417\n",
      "Step - 15889, Loss - 0.8846976054706579, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4764806465658906\n",
      "Step - 15890, Loss - 0.6853304612786484, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.01557551327053\n",
      "Step - 15891, Loss - 0.6453974402638666, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9829677492083279\n",
      "Step - 15892, Loss - 0.7478903486880806, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6419402466484232\n",
      "Step - 15893, Loss - 0.712888023572518, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2876751285028398\n",
      "Step - 15894, Loss - 0.750522410760672, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.108022231281643\n",
      "Step - 15895, Loss - 0.7127575057707491, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.7236106394082231\n",
      "Step - 15896, Loss - 0.8181620445896552, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0479335122342428\n",
      "Step - 15897, Loss - 0.9669384598856179, Learning Rate - 1.52587890625e-06, magnitude of gradient - 3.0000589622180143\n",
      "Step - 15898, Loss - 0.6171147553847, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.9001772675961\n",
      "Step - 15899, Loss - 0.5292518599953115, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4282584984357123\n",
      "Step - 15900, Loss - 0.7209242818886283, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0667475540655542\n",
      "Step - 15901, Loss - 0.6397742780829158, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1397991888256394\n",
      "Step - 15902, Loss - 0.6594795695828078, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.660565016857595\n",
      "Step - 15903, Loss - 0.8637401771360597, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5239224787664338\n",
      "Step - 15904, Loss - 0.39334349203957225, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.2369415485783324\n",
      "Step - 15905, Loss - 0.8251244414808085, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6065746475491518\n",
      "Step - 15906, Loss - 0.6386863434195096, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9341898851609384\n",
      "Step - 15907, Loss - 0.669929787369315, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6349899894565524\n",
      "Step - 15908, Loss - 0.7308616021320923, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7788491298490788\n",
      "Step - 15909, Loss - 0.6627193723261932, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.2592839904527493\n",
      "Step - 15910, Loss - 0.6609303157683292, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9589842178099971\n",
      "Step - 15911, Loss - 0.6907070399839029, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4877020976554598\n",
      "Step - 15912, Loss - 0.6552948328592801, Learning Rate - 1.52587890625e-06, magnitude of gradient - 3.222333149130805\n",
      "Step - 15913, Loss - 0.6766302112200671, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.3599549958228473\n",
      "Step - 15914, Loss - 0.745104282105362, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1650858358717964\n",
      "Step - 15915, Loss - 0.9389781674692349, Learning Rate - 1.52587890625e-06, magnitude of gradient - 3.0728095517148026\n",
      "Step - 15916, Loss - 0.8520772719757275, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8122753574306958\n",
      "Step - 15917, Loss - 0.7211099071002209, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.2877826231024309\n",
      "Step - 15918, Loss - 0.9259743504330953, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5776936453206165\n",
      "Step - 15919, Loss - 0.8701887755254135, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4572863597745478\n",
      "Step - 15920, Loss - 0.5888878881077815, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.32479917959628746\n",
      "Step - 15921, Loss - 0.691941340835693, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.4571689749376664\n",
      "Step - 15922, Loss - 0.8937182808345251, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8698257083378567\n",
      "Step - 15923, Loss - 0.5795497256591176, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6496492543540546\n",
      "Step - 15924, Loss - 0.7780404765956571, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7355891200547007\n",
      "Step - 15925, Loss - 0.7369953154643205, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.230277228340344\n",
      "Step - 15926, Loss - 0.6320750194339173, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.306520985591765\n",
      "Step - 15927, Loss - 0.8123301574273134, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2331462985597985\n",
      "Step - 15928, Loss - 0.7067535847893768, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8871671856656558\n",
      "Step - 15929, Loss - 0.8322863574843661, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5642541339935415\n",
      "Step - 15930, Loss - 0.6669959857138801, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.290056278599403\n",
      "Step - 15931, Loss - 0.7121056070384253, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.4578590737587061\n",
      "Step - 15932, Loss - 0.578414348019139, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.181515014884719\n",
      "Step - 15933, Loss - 0.762144801508327, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5699366307170537\n",
      "Step - 15934, Loss - 0.7612220197303109, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6648216932699198\n",
      "Step - 15935, Loss - 0.651996271146273, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7519821303458354\n",
      "Step - 15936, Loss - 0.719943378951345, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7807964306530003\n",
      "Step - 15937, Loss - 0.6557989140644847, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.311906024374217\n",
      "Step - 15938, Loss - 0.7290083720256312, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3709158210701018\n",
      "Step - 15939, Loss - 0.7104841672323204, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.557521635142794\n",
      "Step - 15940, Loss - 0.6490649925942422, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.4674282059568612\n",
      "Step - 15941, Loss - 0.6431057717037056, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.3350580803666891\n",
      "Step - 15942, Loss - 0.617365795847483, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0107704210947186\n",
      "Step - 15943, Loss - 0.7123090137433841, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.4028580570263503\n",
      "Step - 15944, Loss - 0.7628370201798078, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5745179441881236\n",
      "Step - 15945, Loss - 0.6913608398520159, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.5074862705539345\n",
      "Step - 15946, Loss - 0.6104967067686622, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.013636133456542\n",
      "Step - 15947, Loss - 0.5242097867264094, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.375459658976314\n",
      "Step - 15948, Loss - 0.4889407779584715, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0603479573980739\n",
      "Step - 15949, Loss - 0.7547902528032114, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7334015385443052\n",
      "Step - 15950, Loss - 0.8216000244508279, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4865650336945209\n",
      "Step - 15951, Loss - 0.5172702119739949, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.869574354077881\n",
      "Step - 15952, Loss - 0.7575597571631297, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5374265281988715\n",
      "Step - 15953, Loss - 0.8709049542978182, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6268785846546074\n",
      "Step - 15954, Loss - 0.8593403143459213, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4733148657500874\n",
      "Step - 15955, Loss - 0.5368262492502696, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.290732185931633\n",
      "Step - 15956, Loss - 0.7631106395849911, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.1350027216854006\n",
      "Step - 15957, Loss - 0.7641596038603422, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9515796871360958\n",
      "Step - 15958, Loss - 0.7218623939872463, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4568958456902683\n",
      "Step - 15959, Loss - 0.7079988552081244, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5823800875114805\n",
      "Step - 15960, Loss - 0.7916281737950656, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.935582209222273\n",
      "Step - 15961, Loss - 0.8799477717046087, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.6014059182262883\n",
      "Step - 15962, Loss - 0.7986311445716083, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9867938453811657\n",
      "Step - 15963, Loss - 0.7708573921882627, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0677107526913463\n",
      "Step - 15964, Loss - 0.6833434651984859, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4075615387675955\n",
      "Step - 15965, Loss - 0.6313703480362185, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.9663816330965418\n",
      "Step - 15966, Loss - 0.6344050830907964, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3456856850981935\n",
      "Step - 15967, Loss - 0.5891519798697264, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.5064618787442065\n",
      "Step - 15968, Loss - 0.7556274103048994, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.2453403649346912\n",
      "Step - 15969, Loss - 0.7913828300284806, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.102954455117678\n",
      "Step - 15970, Loss - 0.5983375621412448, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.499760020692728\n",
      "Step - 15971, Loss - 0.698231398172289, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7537296203834616\n",
      "Step - 15972, Loss - 0.5333260562036336, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.08212750572456\n",
      "Step - 15973, Loss - 0.676844917058186, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.421457074121524\n",
      "Step - 15974, Loss - 0.7611743358078775, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6712065072392045\n",
      "Step - 15975, Loss - 0.6783380149540006, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.149948144478711\n",
      "Step - 15976, Loss - 0.6647431519559814, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7016597727101518\n",
      "Step - 15977, Loss - 0.6146703092179937, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.956343255879843\n",
      "Step - 15978, Loss - 0.7671210501808663, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0822455410851277\n",
      "Step - 15979, Loss - 0.5591244624425039, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.7222605490252934\n",
      "Step - 15980, Loss - 0.7145083285172111, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.8019320450170393\n",
      "Step - 15981, Loss - 0.9388986763079614, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.4235508965188346\n",
      "Step - 15982, Loss - 0.7609404710953397, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8880712994289287\n",
      "Step - 15983, Loss - 0.8338364740146377, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.38725187344613626\n",
      "Step - 15984, Loss - 0.8566016043505188, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.352775785913959\n",
      "Step - 15985, Loss - 0.7531748710183802, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.6662612467396629\n",
      "Step - 15986, Loss - 0.7270769465662795, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9490038819686734\n",
      "Step - 15987, Loss - 0.4307337049307813, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0884588032580924\n",
      "Step - 15988, Loss - 0.6375384693966428, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.1647213242955536\n",
      "Step - 15989, Loss - 0.7827260571486298, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.0355876542872786\n",
      "Step - 15990, Loss - 0.7786306227100448, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3627293203586093\n",
      "Step - 15991, Loss - 0.6600026106301287, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.3868438067596762\n",
      "Step - 15992, Loss - 0.6546310582694509, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.5215638770778845\n",
      "Step - 15993, Loss - 0.750990735607309, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8556235857133948\n",
      "Step - 15994, Loss - 0.5475498900934034, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9651693488985658\n",
      "Step - 15995, Loss - 0.9707232195777308, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.9461957612853736\n",
      "Step - 15996, Loss - 0.6978099843968482, Learning Rate - 1.52587890625e-06, magnitude of gradient - 1.0220940172259028\n",
      "Step - 15997, Loss - 0.6721420772526336, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8444637779465194\n",
      "Step - 15998, Loss - 0.7464903447130132, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8918248822389097\n",
      "Step - 15999, Loss - 0.6637027562046482, Learning Rate - 1.52587890625e-06, magnitude of gradient - 2.76345587508727\n",
      "Step - 16000, Loss - 0.5528949740666894, Learning Rate - 1.52587890625e-06, magnitude of gradient - 0.8977868876013203\n",
      "Step - 16001, Loss - 0.6765148861438648, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.799927064377038\n",
      "Step - 16002, Loss - 0.7430242788468612, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1830107778447376\n",
      "Step - 16003, Loss - 0.7978604427078151, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.13528517959005446\n",
      "Step - 16004, Loss - 0.6578800968508602, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6372051364620387\n",
      "Step - 16005, Loss - 0.7984418526952153, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6848568398701924\n",
      "Step - 16006, Loss - 0.5956205870850064, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6536111585423385\n",
      "Step - 16007, Loss - 0.7213435753945094, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.034154710523983\n",
      "Step - 16008, Loss - 0.6791531988019617, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4595972017007759\n",
      "Step - 16009, Loss - 0.8057699909036251, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6290216199914216\n",
      "Step - 16010, Loss - 0.6557725022442186, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8658350698770004\n",
      "Step - 16011, Loss - 0.8255045498183671, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8198039002622426\n",
      "Step - 16012, Loss - 0.5398455292089687, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.720837293641253\n",
      "Step - 16013, Loss - 0.6365987891443332, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9141503484845117\n",
      "Step - 16014, Loss - 0.6946596411925682, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.2774422271732986\n",
      "Step - 16015, Loss - 0.9125823560450803, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6886090160440812\n",
      "Step - 16016, Loss - 0.7172162130304104, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8730423380132155\n",
      "Step - 16017, Loss - 0.6901877092611566, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6862811404404342\n",
      "Step - 16018, Loss - 0.4726376353247632, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.702580385462769\n",
      "Step - 16019, Loss - 0.8072506054184792, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5316486531312521\n",
      "Step - 16020, Loss - 0.7257089307905403, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7581389955805129\n",
      "Step - 16021, Loss - 0.7654781816515984, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3128715579302637\n",
      "Step - 16022, Loss - 0.7276944784112952, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3939602656997196\n",
      "Step - 16023, Loss - 0.7967217146829109, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.24131793057863835\n",
      "Step - 16024, Loss - 0.7161810477815551, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.196883395018877\n",
      "Step - 16025, Loss - 0.6503538883010063, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1912982549760232\n",
      "Step - 16026, Loss - 0.7220682731844362, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.024606784988969\n",
      "Step - 16027, Loss - 0.9303439151977458, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9022374888749007\n",
      "Step - 16028, Loss - 0.7641226858204998, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5222078448373964\n",
      "Step - 16029, Loss - 0.7346925723524558, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2276332622943953\n",
      "Step - 16030, Loss - 0.7910609741426169, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.94867717648908\n",
      "Step - 16031, Loss - 0.862964503586633, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.637096071148477\n",
      "Step - 16032, Loss - 0.6004183101627438, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.966554128620621\n",
      "Step - 16033, Loss - 0.6999808728434839, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.799611176158559\n",
      "Step - 16034, Loss - 0.7614868258405164, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8662644847438385\n",
      "Step - 16035, Loss - 0.8103610121952645, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5827640199682842\n",
      "Step - 16036, Loss - 0.6795807660826432, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.3409632522545759\n",
      "Step - 16037, Loss - 0.6132168042084509, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6784108545184384\n",
      "Step - 16038, Loss - 0.7033826926041773, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7491263091389831\n",
      "Step - 16039, Loss - 0.600463915334843, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.9905007725917305\n",
      "Step - 16040, Loss - 0.5429024819396723, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6720240490091411\n",
      "Step - 16041, Loss - 0.4826050950075081, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.534644942092873\n",
      "Step - 16042, Loss - 0.7147667271614645, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2922425548338263\n",
      "Step - 16043, Loss - 0.6443616736658062, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5993011318935649\n",
      "Step - 16044, Loss - 0.7096413185089747, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1016454356759906\n",
      "Step - 16045, Loss - 0.6773395080451001, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8678000342432787\n",
      "Step - 16046, Loss - 0.5983601760092314, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6862072567984566\n",
      "Step - 16047, Loss - 0.7621266829278454, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5166858931608465\n",
      "Step - 16048, Loss - 0.6194834814826617, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.11624024154479\n",
      "Step - 16049, Loss - 0.7892604418065042, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9387401057843231\n",
      "Step - 16050, Loss - 0.8690657665296647, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.2863390753266395\n",
      "Step - 16051, Loss - 0.6251710828074292, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7085393655863312\n",
      "Step - 16052, Loss - 0.7132114227466908, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.741721921663816\n",
      "Step - 16053, Loss - 0.6617685759804119, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8665372566540982\n",
      "Step - 16054, Loss - 0.727870233263294, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6711996438761195\n",
      "Step - 16055, Loss - 0.5825853406948289, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3827224467327435\n",
      "Step - 16056, Loss - 0.7482348116539068, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5347154661799525\n",
      "Step - 16057, Loss - 0.821090752737372, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8134963436723397\n",
      "Step - 16058, Loss - 0.6491790161116756, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5562882349242506\n",
      "Step - 16059, Loss - 0.5265366599888398, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9840703368481981\n",
      "Step - 16060, Loss - 0.7356880815320186, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5181893152397168\n",
      "Step - 16061, Loss - 0.7715093009574193, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.49173858236170137\n",
      "Step - 16062, Loss - 0.7419287223205999, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3508013363714042\n",
      "Step - 16063, Loss - 0.6580187280051764, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6096724648289036\n",
      "Step - 16064, Loss - 0.7153555859896735, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3293113093898952\n",
      "Step - 16065, Loss - 0.5332613148795138, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2072084062270978\n",
      "Step - 16066, Loss - 0.5191226907917227, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.949727386112878\n",
      "Step - 16067, Loss - 0.620179001094687, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8805360580971533\n",
      "Step - 16068, Loss - 0.7718770537044235, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8363924735043318\n",
      "Step - 16069, Loss - 0.6121910206787389, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9770355158698323\n",
      "Step - 16070, Loss - 0.7934522484360137, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0210737778557228\n",
      "Step - 16071, Loss - 0.7005541858133584, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.2987242781645336\n",
      "Step - 16072, Loss - 0.6894346644034818, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7590511028429393\n",
      "Step - 16073, Loss - 0.8131885673657163, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3216295853833768\n",
      "Step - 16074, Loss - 0.6081170021144722, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.1775929871715007\n",
      "Step - 16075, Loss - 0.6432551430198061, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.799367733138369\n",
      "Step - 16076, Loss - 0.753568482930699, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3537242829455145\n",
      "Step - 16077, Loss - 0.49997177135613363, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.31662769613223646\n",
      "Step - 16078, Loss - 0.7811303484830632, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5339425364025843\n",
      "Step - 16079, Loss - 0.6929345618765047, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6066692069354662\n",
      "Step - 16080, Loss - 0.6364104815582358, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6321725714234957\n",
      "Step - 16081, Loss - 0.7626201711083638, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0953838889279954\n",
      "Step - 16082, Loss - 0.8263893125201334, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.012452319550157\n",
      "Step - 16083, Loss - 0.6186849908303216, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.882869540062296\n",
      "Step - 16084, Loss - 0.528595106918856, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8650130905271302\n",
      "Step - 16085, Loss - 0.6805131791659419, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5412890745068344\n",
      "Step - 16086, Loss - 0.7457569292273829, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7370206641882326\n",
      "Step - 16087, Loss - 0.7706339209491763, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5558194751545038\n",
      "Step - 16088, Loss - 0.6314490418930986, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.525221400981815\n",
      "Step - 16089, Loss - 0.7525284178703039, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4176146175034658\n",
      "Step - 16090, Loss - 0.6662889423555184, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7062339347566122\n",
      "Step - 16091, Loss - 0.7679124998723834, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6411695697304745\n",
      "Step - 16092, Loss - 0.7350061086901194, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.40108268195624996\n",
      "Step - 16093, Loss - 0.5423298646176429, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2585473899988424\n",
      "Step - 16094, Loss - 0.7359067204244103, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.4042879926489857\n",
      "Step - 16095, Loss - 0.6643745697972969, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1013176163823857\n",
      "Step - 16096, Loss - 0.857900642627981, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7167183335915261\n",
      "Step - 16097, Loss - 0.6194530865784925, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.599105903650816\n",
      "Step - 16098, Loss - 0.5876446442206772, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.0994642559997976\n",
      "Step - 16099, Loss - 0.5515694904529423, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.44559479911987704\n",
      "Step - 16100, Loss - 0.6930206003874478, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.212560144559587\n",
      "Step - 16101, Loss - 0.6490456571013865, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1341972614710625\n",
      "Step - 16102, Loss - 0.6069435737176763, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1711836529403088\n",
      "Step - 16103, Loss - 0.5887569005078402, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3413321030981151\n",
      "Step - 16104, Loss - 0.9238159530505273, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.3095680159082037\n",
      "Step - 16105, Loss - 0.6305591814930291, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9319313386765986\n",
      "Step - 16106, Loss - 0.5646269821896421, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6925266579698017\n",
      "Step - 16107, Loss - 0.8528898088394223, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6951019684871753\n",
      "Step - 16108, Loss - 0.6954409984385633, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9155035512477999\n",
      "Step - 16109, Loss - 0.46717753763360803, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.872068404097605\n",
      "Step - 16110, Loss - 0.7146604880410067, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.1078837293331105\n",
      "Step - 16111, Loss - 0.613987295608603, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1795963953832986\n",
      "Step - 16112, Loss - 0.8254575007215167, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9085500053417935\n",
      "Step - 16113, Loss - 0.5312072930261452, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.33841878597579345\n",
      "Step - 16114, Loss - 0.5568767249724508, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7263041966688898\n",
      "Step - 16115, Loss - 0.7653759815082346, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.883241542652824\n",
      "Step - 16116, Loss - 0.5145797394651954, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3377160226625076\n",
      "Step - 16117, Loss - 0.7239459706660927, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1433260882270186\n",
      "Step - 16118, Loss - 0.8245147481245059, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6900790349929484\n",
      "Step - 16119, Loss - 0.5822184856757453, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9047071780721715\n",
      "Step - 16120, Loss - 0.8131664249382264, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5914602238679567\n",
      "Step - 16121, Loss - 0.7304488802205847, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3832295336298126\n",
      "Step - 16122, Loss - 0.5997886994322918, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8912087758665999\n",
      "Step - 16123, Loss - 0.5692986203500335, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.24870275556517754\n",
      "Step - 16124, Loss - 0.5822331415547827, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9556118569494995\n",
      "Step - 16125, Loss - 0.7534127237178259, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8561681166192691\n",
      "Step - 16126, Loss - 0.6187324510797344, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9467302861670501\n",
      "Step - 16127, Loss - 0.6588372869277352, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8188037972979627\n",
      "Step - 16128, Loss - 0.5354716905116792, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8748879250759395\n",
      "Step - 16129, Loss - 0.7510623795218421, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4331876448631469\n",
      "Step - 16130, Loss - 0.8796644241890128, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4151321160586023\n",
      "Step - 16131, Loss - 0.7162741954834502, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.994293229903058\n",
      "Step - 16132, Loss - 0.6656825721808312, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6839366078306024\n",
      "Step - 16133, Loss - 0.531536925089678, Learning Rate - 7.62939453125e-07, magnitude of gradient - 3.2995558114825823\n",
      "Step - 16134, Loss - 0.8847052106620973, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8018454328106224\n",
      "Step - 16135, Loss - 0.682895111390335, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.9209612644483807\n",
      "Step - 16136, Loss - 0.9023890679974281, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.865276223429505\n",
      "Step - 16137, Loss - 0.7931479529877237, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.1567846555565042\n",
      "Step - 16138, Loss - 0.7765527880565678, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4730855221679224\n",
      "Step - 16139, Loss - 0.7149316352144266, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3411507878672368\n",
      "Step - 16140, Loss - 0.5438465042025766, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7131719777661261\n",
      "Step - 16141, Loss - 0.7145598088164029, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7774709373426416\n",
      "Step - 16142, Loss - 0.6754377518821499, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9799518426701669\n",
      "Step - 16143, Loss - 0.8249504538853095, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7928456107438087\n",
      "Step - 16144, Loss - 0.7842514403685554, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.44589795843379126\n",
      "Step - 16145, Loss - 0.8027917101464429, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9810707605527783\n",
      "Step - 16146, Loss - 0.6523426269187119, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.1389687705235345\n",
      "Step - 16147, Loss - 0.6938573952732708, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.268766266131175\n",
      "Step - 16148, Loss - 0.771894964998642, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5181616892886187\n",
      "Step - 16149, Loss - 0.7909436331388189, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9326858641642031\n",
      "Step - 16150, Loss - 0.5511074334650413, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.913174036211424\n",
      "Step - 16151, Loss - 0.5762292018199358, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7485499186106703\n",
      "Step - 16152, Loss - 0.6222352536550133, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4467892828728872\n",
      "Step - 16153, Loss - 0.6226163166150462, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8731183635119171\n",
      "Step - 16154, Loss - 0.7169522510778947, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0966659334710056\n",
      "Step - 16155, Loss - 0.6577601622283427, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5906658733337096\n",
      "Step - 16156, Loss - 0.6834518764253154, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8858183200206782\n",
      "Step - 16157, Loss - 0.5224537056112409, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8882504154469921\n",
      "Step - 16158, Loss - 0.6361617747337343, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8413409083862646\n",
      "Step - 16159, Loss - 0.4531483179668328, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2009755576169152\n",
      "Step - 16160, Loss - 0.8258560351122728, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.396206170171763\n",
      "Step - 16161, Loss - 0.591892804890694, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.45322144924993707\n",
      "Step - 16162, Loss - 0.666119124786771, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.18329630995743368\n",
      "Step - 16163, Loss - 0.6844527416227371, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4551741958871824\n",
      "Step - 16164, Loss - 0.7123023641281643, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6100479309208425\n",
      "Step - 16165, Loss - 0.8177433147972833, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6665672986184061\n",
      "Step - 16166, Loss - 1.0604267890396089, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4419011475538515\n",
      "Step - 16167, Loss - 0.6701767579131026, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8820664084636199\n",
      "Step - 16168, Loss - 0.9143017024740575, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.37970332850690364\n",
      "Step - 16169, Loss - 0.5837645557762192, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6692942582116936\n",
      "Step - 16170, Loss - 0.7222560797111071, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7865229797971808\n",
      "Step - 16171, Loss - 0.7250463049458882, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8972297710621767\n",
      "Step - 16172, Loss - 0.5236895269669233, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6410645406915518\n",
      "Step - 16173, Loss - 0.5469873267693798, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7112226638974268\n",
      "Step - 16174, Loss - 0.507189821691907, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6452787842434504\n",
      "Step - 16175, Loss - 0.6918881499055046, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.0645010514584827\n",
      "Step - 16176, Loss - 0.7242904148661622, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9953846838335474\n",
      "Step - 16177, Loss - 0.47999598533308513, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.4780342345659814\n",
      "Step - 16178, Loss - 0.7019528546182296, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.197901769996864\n",
      "Step - 16179, Loss - 0.711362534612721, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1950244167994757\n",
      "Step - 16180, Loss - 0.6395777677999125, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5200690802173282\n",
      "Step - 16181, Loss - 0.6266440426736232, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.369716550445895\n",
      "Step - 16182, Loss - 0.7570409044478238, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1717051090276156\n",
      "Step - 16183, Loss - 0.7463172426442951, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6921868258999855\n",
      "Step - 16184, Loss - 0.6716554543945435, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4923529847612196\n",
      "Step - 16185, Loss - 0.6504947147493266, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8708403294222198\n",
      "Step - 16186, Loss - 0.8657705365628838, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1294875077333573\n",
      "Step - 16187, Loss - 0.6571546717616881, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4320764254545666\n",
      "Step - 16188, Loss - 0.665409198572688, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6187378171705342\n",
      "Step - 16189, Loss - 0.7097110971158477, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8820071097632933\n",
      "Step - 16190, Loss - 0.6349693860639994, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9304674905493951\n",
      "Step - 16191, Loss - 0.7888855288429177, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.310304476139197\n",
      "Step - 16192, Loss - 0.597443136903538, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6122179504569588\n",
      "Step - 16193, Loss - 0.5631432338923439, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7593797900919548\n",
      "Step - 16194, Loss - 0.6590019296668558, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.3015856554825964\n",
      "Step - 16195, Loss - 0.6655875403025201, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.144999516865293\n",
      "Step - 16196, Loss - 0.6901728494538533, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4889574676845803\n",
      "Step - 16197, Loss - 0.5853953110818445, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0533075534542988\n",
      "Step - 16198, Loss - 0.5247183255277175, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3155053776807168\n",
      "Step - 16199, Loss - 0.739304988293756, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.944828700859681\n",
      "Step - 16200, Loss - 0.8773552832388378, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1635345730578035\n",
      "Step - 16201, Loss - 0.9609780093290559, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9923947388617153\n",
      "Step - 16202, Loss - 0.5860341295217151, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7416173599017692\n",
      "Step - 16203, Loss - 0.7510119027455224, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3506574111378356\n",
      "Step - 16204, Loss - 0.7028060108741322, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1667283796569123\n",
      "Step - 16205, Loss - 0.6876278286946393, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.2939503269568381\n",
      "Step - 16206, Loss - 0.5611803355249525, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.541831886787139\n",
      "Step - 16207, Loss - 0.552643631957954, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3011418494807916\n",
      "Step - 16208, Loss - 0.7165784427892942, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6771895417627457\n",
      "Step - 16209, Loss - 0.8324833341952088, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.484700863027352\n",
      "Step - 16210, Loss - 0.7600318496990595, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4555780673481193\n",
      "Step - 16211, Loss - 0.5369304808748696, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8369910846811062\n",
      "Step - 16212, Loss - 0.7442828165381488, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3987750751911825\n",
      "Step - 16213, Loss - 0.6678324943832399, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7563297128636601\n",
      "Step - 16214, Loss - 0.6964887592082353, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6447039715424145\n",
      "Step - 16215, Loss - 0.6615783906231723, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9618110188248923\n",
      "Step - 16216, Loss - 0.8134071923185762, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6135976434507415\n",
      "Step - 16217, Loss - 0.5840242564812584, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6634017271077359\n",
      "Step - 16218, Loss - 0.7455839342663153, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5970250484496602\n",
      "Step - 16219, Loss - 0.7968025779498887, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3393733238177574\n",
      "Step - 16220, Loss - 0.7287956506253407, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0662160318138434\n",
      "Step - 16221, Loss - 0.7360369016736972, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0030118579169933\n",
      "Step - 16222, Loss - 0.4896312777023239, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.613785062754355\n",
      "Step - 16223, Loss - 0.8283315511226879, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5416629740329888\n",
      "Step - 16224, Loss - 0.9464437250830902, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.250703423365443\n",
      "Step - 16225, Loss - 0.6101447729934946, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8315680159448098\n",
      "Step - 16226, Loss - 0.588855130087458, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5377894767812592\n",
      "Step - 16227, Loss - 0.5049629498255123, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5065169362266766\n",
      "Step - 16228, Loss - 0.6580126911908681, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2782407166582987\n",
      "Step - 16229, Loss - 0.6855480335008887, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4286439482548874\n",
      "Step - 16230, Loss - 0.703041858498439, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2968188449154858\n",
      "Step - 16231, Loss - 0.7551590183226528, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5413405619087303\n",
      "Step - 16232, Loss - 0.8525499541475084, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.756594539292254\n",
      "Step - 16233, Loss - 0.5478250238588742, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1186491409179056\n",
      "Step - 16234, Loss - 0.6219566051118983, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0531418840389792\n",
      "Step - 16235, Loss - 0.5393134076832304, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6760994018645602\n",
      "Step - 16236, Loss - 0.7190544658335847, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0114928620302024\n",
      "Step - 16237, Loss - 0.5882747772634028, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.447323948981014\n",
      "Step - 16238, Loss - 0.846580171574955, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1399352856161726\n",
      "Step - 16239, Loss - 0.5144266138404702, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9976686930345486\n",
      "Step - 16240, Loss - 0.6130893508536002, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2847423870773969\n",
      "Step - 16241, Loss - 0.6302359149124296, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6246461321891656\n",
      "Step - 16242, Loss - 0.7810100425689251, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1630958706535073\n",
      "Step - 16243, Loss - 0.65971355405209, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3561332888114284\n",
      "Step - 16244, Loss - 0.6120383353278508, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7656994894285436\n",
      "Step - 16245, Loss - 0.5918428279236982, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3010304589622819\n",
      "Step - 16246, Loss - 0.7825132991832427, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1741154938299798\n",
      "Step - 16247, Loss - 0.5541169066031237, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.208070140881096\n",
      "Step - 16248, Loss - 0.6372424302943442, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.198812881595007\n",
      "Step - 16249, Loss - 0.6115443971074198, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9903927964684277\n",
      "Step - 16250, Loss - 0.6653035984978897, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9128320547960593\n",
      "Step - 16251, Loss - 0.6665939969121715, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2697973590388794\n",
      "Step - 16252, Loss - 0.7396104676520243, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.41063615736866876\n",
      "Step - 16253, Loss - 0.6631038360821025, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.700729403276803\n",
      "Step - 16254, Loss - 0.6368974062781132, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6609522546520854\n",
      "Step - 16255, Loss - 0.5871712092667362, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.397559621858733\n",
      "Step - 16256, Loss - 0.37859222354352834, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6757343394182083\n",
      "Step - 16257, Loss - 0.6282464438895129, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9615210503832252\n",
      "Step - 16258, Loss - 0.8020232668854849, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.2784733525028047\n",
      "Step - 16259, Loss - 0.6465383067942088, Learning Rate - 7.62939453125e-07, magnitude of gradient - 3.183790463438401\n",
      "Step - 16260, Loss - 0.7417318428604758, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9288699803773177\n",
      "Step - 16261, Loss - 0.5447946297055266, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.41731369333142443\n",
      "Step - 16262, Loss - 0.6928662672889663, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2077102194578737\n",
      "Step - 16263, Loss - 0.9457043869695014, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.32967651224429706\n",
      "Step - 16264, Loss - 0.5525531745100751, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8472382777685012\n",
      "Step - 16265, Loss - 0.579616074902839, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1613353569280407\n",
      "Step - 16266, Loss - 0.9265310551188966, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4607978523709225\n",
      "Step - 16267, Loss - 0.5831411173072452, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9486125075308514\n",
      "Step - 16268, Loss - 0.8832045597110153, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3312833457045294\n",
      "Step - 16269, Loss - 0.8767143786073068, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5465086141247005\n",
      "Step - 16270, Loss - 0.7190252822256089, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0638570454347909\n",
      "Step - 16271, Loss - 0.7606276074215359, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4198112312084983\n",
      "Step - 16272, Loss - 0.8218058630992628, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4620135855229884\n",
      "Step - 16273, Loss - 0.5902557829969969, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.35846119244071467\n",
      "Step - 16274, Loss - 0.7118080613160388, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4064432771765078\n",
      "Step - 16275, Loss - 0.7552128831849633, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7617374319872836\n",
      "Step - 16276, Loss - 0.6671896842512649, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.3406086810894433\n",
      "Step - 16277, Loss - 0.5784287040100567, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0898705559073067\n",
      "Step - 16278, Loss - 0.614100743627875, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7934925244154576\n",
      "Step - 16279, Loss - 0.7310702297418349, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.049214358358661\n",
      "Step - 16280, Loss - 0.6959314398979257, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0726006639066477\n",
      "Step - 16281, Loss - 0.7023810442634546, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1070387953623058\n",
      "Step - 16282, Loss - 0.586261000154265, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3578148950879048\n",
      "Step - 16283, Loss - 0.6742936510436863, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5817602588443416\n",
      "Step - 16284, Loss - 0.7113682104404071, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3297480284200793\n",
      "Step - 16285, Loss - 0.6982266688284956, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.38015109261410895\n",
      "Step - 16286, Loss - 0.7829488015587769, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6007750176818287\n",
      "Step - 16287, Loss - 0.666873611413835, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.435184048241388\n",
      "Step - 16288, Loss - 0.8645029235807887, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.0290286624764327\n",
      "Step - 16289, Loss - 0.7018416487113047, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.2960102946830018\n",
      "Step - 16290, Loss - 0.7467390658670487, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2688917736508385\n",
      "Step - 16291, Loss - 0.7384979756022046, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1271964511130241\n",
      "Step - 16292, Loss - 0.6783550413741664, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8420361345913645\n",
      "Step - 16293, Loss - 0.8312035964376401, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3456945971347434\n",
      "Step - 16294, Loss - 0.5897347956591161, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8659588344869009\n",
      "Step - 16295, Loss - 0.8350508164676214, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5932188948193494\n",
      "Step - 16296, Loss - 0.7217603020995962, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.74391052505061\n",
      "Step - 16297, Loss - 0.6195782637818921, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.94250645057758\n",
      "Step - 16298, Loss - 0.6972003879010639, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0760492739209566\n",
      "Step - 16299, Loss - 0.8500431398104298, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7694132307392655\n",
      "Step - 16300, Loss - 0.6669989191329351, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6667547828131142\n",
      "Step - 16301, Loss - 0.7079856905854193, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8703501619281315\n",
      "Step - 16302, Loss - 0.6300366546378511, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.306978027292066\n",
      "Step - 16303, Loss - 0.7499905376066169, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6672977484231108\n",
      "Step - 16304, Loss - 0.6332815378872523, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8896641349571145\n",
      "Step - 16305, Loss - 0.647812845499188, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7483884802057915\n",
      "Step - 16306, Loss - 0.6664980625907589, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.774088600181995\n",
      "Step - 16307, Loss - 0.9057877511526298, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.5902872235329495\n",
      "Step - 16308, Loss - 0.6995477278481348, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3384649393024932\n",
      "Step - 16309, Loss - 0.9231680771753424, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.573541686031769\n",
      "Step - 16310, Loss - 0.7635091228594912, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8375468984978269\n",
      "Step - 16311, Loss - 0.7411092792076462, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9398081813243833\n",
      "Step - 16312, Loss - 0.6114572276101705, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0995793099862556\n",
      "Step - 16313, Loss - 0.5823559343079245, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2491143874069413\n",
      "Step - 16314, Loss - 0.7696717165954359, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2216658905233748\n",
      "Step - 16315, Loss - 0.543177508687225, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8452437744877186\n",
      "Step - 16316, Loss - 0.5988054683635156, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7082238051039886\n",
      "Step - 16317, Loss - 0.5434620074089206, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7925359881005674\n",
      "Step - 16318, Loss - 0.8118378644032402, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8692804925082964\n",
      "Step - 16319, Loss - 0.8830693927456195, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.8624658330903094\n",
      "Step - 16320, Loss - 0.5748733021450998, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2367659972580487\n",
      "Step - 16321, Loss - 0.8349996932957209, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7131815176933972\n",
      "Step - 16322, Loss - 0.8937775216965553, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.752019775756137\n",
      "Step - 16323, Loss - 0.6773602747254736, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4195627856757689\n",
      "Step - 16324, Loss - 0.5633998894646153, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.016315772505557\n",
      "Step - 16325, Loss - 0.7357276608610933, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6383847530924631\n",
      "Step - 16326, Loss - 0.6047709177759328, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.4285019111762143\n",
      "Step - 16327, Loss - 0.5297661054279452, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4323548480280193\n",
      "Step - 16328, Loss - 0.7641091391106913, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8209728643842639\n",
      "Step - 16329, Loss - 0.5386338250096868, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5203682972158556\n",
      "Step - 16330, Loss - 0.6027850463168076, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4822224724787885\n",
      "Step - 16331, Loss - 0.7295932444676793, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1332604512325213\n",
      "Step - 16332, Loss - 0.8126500622248651, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7200775718741579\n",
      "Step - 16333, Loss - 0.8866720104931046, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9516558083981899\n",
      "Step - 16334, Loss - 0.7660886888001422, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9425339565265861\n",
      "Step - 16335, Loss - 0.7198294698826561, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1723199650012814\n",
      "Step - 16336, Loss - 0.5282913471910263, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8983729162120182\n",
      "Step - 16337, Loss - 0.7113893530956676, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2591954703023285\n",
      "Step - 16338, Loss - 0.646793825746699, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.38808210608855914\n",
      "Step - 16339, Loss - 0.601374560055538, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.5208311786707966\n",
      "Step - 16340, Loss - 0.7090745336818237, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.883627070033407\n",
      "Step - 16341, Loss - 0.5252273610459066, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5033960006781374\n",
      "Step - 16342, Loss - 0.8220677640242706, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1537608323883166\n",
      "Step - 16343, Loss - 0.6782970413654601, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6920296839857712\n",
      "Step - 16344, Loss - 0.7259665771018461, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8947317926842568\n",
      "Step - 16345, Loss - 0.467618664908017, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0853382699858423\n",
      "Step - 16346, Loss - 0.5837412300181761, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.7561923287026495\n",
      "Step - 16347, Loss - 0.8156852052650333, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4988674297421405\n",
      "Step - 16348, Loss - 0.7517326941659017, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3046398230605505\n",
      "Step - 16349, Loss - 0.5738839514513567, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8103356889157908\n",
      "Step - 16350, Loss - 0.9495876113183332, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4541795902245516\n",
      "Step - 16351, Loss - 0.664799168575908, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6622019676472245\n",
      "Step - 16352, Loss - 0.6824797636679186, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1877095373101274\n",
      "Step - 16353, Loss - 0.7033639938261931, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0169106233509424\n",
      "Step - 16354, Loss - 0.5975079424018782, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2754780532270544\n",
      "Step - 16355, Loss - 0.8153780042556007, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6222402952918231\n",
      "Step - 16356, Loss - 0.5709785341615281, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8184530754179185\n",
      "Step - 16357, Loss - 0.7987723872777192, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7448951157744899\n",
      "Step - 16358, Loss - 0.7042164037727658, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.888329815451011\n",
      "Step - 16359, Loss - 0.8336014837321153, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0857496659803831\n",
      "Step - 16360, Loss - 0.5848776700816305, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6770494168598897\n",
      "Step - 16361, Loss - 0.6776088088012535, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3547444865648608\n",
      "Step - 16362, Loss - 0.666966514785039, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8586515945606775\n",
      "Step - 16363, Loss - 0.6612099237470368, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2110655752349295\n",
      "Step - 16364, Loss - 0.6233833272238651, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7618433956763628\n",
      "Step - 16365, Loss - 0.6116215968938299, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8525802492034704\n",
      "Step - 16366, Loss - 0.6787641512881037, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8371888058031786\n",
      "Step - 16367, Loss - 0.7224955614204606, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.327778154545431\n",
      "Step - 16368, Loss - 0.7739637100163353, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0581454632146878\n",
      "Step - 16369, Loss - 0.7927401420312535, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.431229240764915\n",
      "Step - 16370, Loss - 0.6906170866826155, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8040828574080576\n",
      "Step - 16371, Loss - 0.751870378001952, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7580149114561566\n",
      "Step - 16372, Loss - 0.6712554368297601, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3233308088124802\n",
      "Step - 16373, Loss - 0.8369492098817771, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.881664302811352\n",
      "Step - 16374, Loss - 0.8824123709653221, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8037675073421382\n",
      "Step - 16375, Loss - 0.8819531297178425, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0123623197224045\n",
      "Step - 16376, Loss - 0.6899087473514807, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6414554787491547\n",
      "Step - 16377, Loss - 0.7110078107837655, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7890034916181415\n",
      "Step - 16378, Loss - 0.7420363144094004, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5148613130021686\n",
      "Step - 16379, Loss - 0.5958140116479393, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4696368190765183\n",
      "Step - 16380, Loss - 0.5931157401599341, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9574270403368884\n",
      "Step - 16381, Loss - 0.7461523378216159, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9013268465597135\n",
      "Step - 16382, Loss - 0.8367974354033749, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.153100938357217\n",
      "Step - 16383, Loss - 0.8235547533210343, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1383114919651247\n",
      "Step - 16384, Loss - 0.6970903128834245, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.969962678105191\n",
      "Step - 16385, Loss - 0.641345630293414, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.099786669139423\n",
      "Step - 16386, Loss - 0.6598142666587297, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2694209743625544\n",
      "Step - 16387, Loss - 0.6890673566978424, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5294839664258055\n",
      "Step - 16388, Loss - 0.7944680710986888, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.0709951931406216\n",
      "Step - 16389, Loss - 0.5700136391588516, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.280475744041667\n",
      "Step - 16390, Loss - 0.7949272738723883, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9510754962133374\n",
      "Step - 16391, Loss - 0.6286099074832661, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.6296947274448317\n",
      "Step - 16392, Loss - 0.7702534344384306, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.0601222517454154\n",
      "Step - 16393, Loss - 0.8543235583952369, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6171625310583873\n",
      "Step - 16394, Loss - 0.9080966398157806, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3784931463949746\n",
      "Step - 16395, Loss - 0.8108157385056554, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9058833369713732\n",
      "Step - 16396, Loss - 0.6687759437744245, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6522037461843337\n",
      "Step - 16397, Loss - 0.6436450066864497, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0178148733841177\n",
      "Step - 16398, Loss - 1.0097039520177309, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7740511768333236\n",
      "Step - 16399, Loss - 0.6790275908486497, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.139592968261051\n",
      "Step - 16400, Loss - 0.6610066599954247, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8709797542879917\n",
      "Step - 16401, Loss - 0.5640306926318794, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0553117776336796\n",
      "Step - 16402, Loss - 0.6982782096463422, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7173569854890286\n",
      "Step - 16403, Loss - 0.5411801420090971, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9325434006818563\n",
      "Step - 16404, Loss - 1.0501725414187906, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0217284462028562\n",
      "Step - 16405, Loss - 0.45940758111847785, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.46497838327964436\n",
      "Step - 16406, Loss - 0.7551251572514387, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8121419240773028\n",
      "Step - 16407, Loss - 0.5428540458472919, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.422742898240846\n",
      "Step - 16408, Loss - 0.760411918252127, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9885254172518763\n",
      "Step - 16409, Loss - 0.7821658273336838, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7725994566377552\n",
      "Step - 16410, Loss - 0.7780603303407049, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9242414019335725\n",
      "Step - 16411, Loss - 0.7923381883127952, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.078401660049699\n",
      "Step - 16412, Loss - 0.8416536607637464, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.504671655591318\n",
      "Step - 16413, Loss - 0.6093064867959228, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8270592530902146\n",
      "Step - 16414, Loss - 0.552510529857602, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8542025244092888\n",
      "Step - 16415, Loss - 0.8849085624047356, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8879226767903993\n",
      "Step - 16416, Loss - 0.825836266620149, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9439269992309005\n",
      "Step - 16417, Loss - 0.632546700821034, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9249197827796998\n",
      "Step - 16418, Loss - 0.7422253414681365, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0451496438503929\n",
      "Step - 16419, Loss - 0.60723694051972, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.058785421987691\n",
      "Step - 16420, Loss - 0.8494891121632187, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3597766805915743\n",
      "Step - 16421, Loss - 0.6735289889006325, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4586710649667638\n",
      "Step - 16422, Loss - 0.5993187128842099, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0661124984524364\n",
      "Step - 16423, Loss - 0.6199129250784127, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7167555854111277\n",
      "Step - 16424, Loss - 0.41127655857142437, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2221156864318705\n",
      "Step - 16425, Loss - 0.8781187078090575, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3882092846818264\n",
      "Step - 16426, Loss - 0.48368340135205223, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6087927015780992\n",
      "Step - 16427, Loss - 0.7024406344893952, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6912600382047926\n",
      "Step - 16428, Loss - 0.6863496334813024, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.4810924233728984\n",
      "Step - 16429, Loss - 0.5706939151735654, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.134596573887109\n",
      "Step - 16430, Loss - 0.547279890604411, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7343768275419166\n",
      "Step - 16431, Loss - 0.6660287693800881, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5943816984051535\n",
      "Step - 16432, Loss - 0.7343666721163387, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.472277393938883\n",
      "Step - 16433, Loss - 0.43602061398777825, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.221642101244346\n",
      "Step - 16434, Loss - 0.5771242966333848, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7261607875914309\n",
      "Step - 16435, Loss - 1.0113308067372486, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9551207228013394\n",
      "Step - 16436, Loss - 0.711527605949134, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9685835884276028\n",
      "Step - 16437, Loss - 0.6225117998702923, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3449923550328922\n",
      "Step - 16438, Loss - 0.9206411560140505, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5163815081854224\n",
      "Step - 16439, Loss - 0.7244250150205982, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6370714036650775\n",
      "Step - 16440, Loss - 0.7561769732293089, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5741522990147752\n",
      "Step - 16441, Loss - 0.6148287129478891, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8335300063688933\n",
      "Step - 16442, Loss - 0.7631170378945071, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.42408165982149426\n",
      "Step - 16443, Loss - 0.8229615415079925, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.11835809770417\n",
      "Step - 16444, Loss - 0.6548068401769322, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.900057058602295\n",
      "Step - 16445, Loss - 0.596815343012565, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.751364886267019\n",
      "Step - 16446, Loss - 0.6290993757017941, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7744818880961621\n",
      "Step - 16447, Loss - 0.7565807433692893, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5512785131068334\n",
      "Step - 16448, Loss - 0.5925972806874316, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0368334868206919\n",
      "Step - 16449, Loss - 0.6846444470496619, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4135803048725635\n",
      "Step - 16450, Loss - 0.7214100673783961, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.705709910795992\n",
      "Step - 16451, Loss - 0.7343213495684051, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8235002042658581\n",
      "Step - 16452, Loss - 0.6169052143248324, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8978025846353831\n",
      "Step - 16453, Loss - 0.9181978209061216, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4503246121683473\n",
      "Step - 16454, Loss - 0.7819553358052906, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.198278784790749\n",
      "Step - 16455, Loss - 0.585844396553463, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8770826212111031\n",
      "Step - 16456, Loss - 0.6300290552204257, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7096664333064944\n",
      "Step - 16457, Loss - 0.7243119291924196, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0383964876134413\n",
      "Step - 16458, Loss - 0.594047642642539, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0410140563147967\n",
      "Step - 16459, Loss - 0.7564085238599109, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3287132739849803\n",
      "Step - 16460, Loss - 0.919473065694445, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6707120905323802\n",
      "Step - 16461, Loss - 0.7819501332274956, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.088289758527839\n",
      "Step - 16462, Loss - 0.657304463890524, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9577654591131906\n",
      "Step - 16463, Loss - 0.44580994453660044, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.803485197780731\n",
      "Step - 16464, Loss - 0.5174267178631367, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9921893853286998\n",
      "Step - 16465, Loss - 0.6527858883292315, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5729434783200231\n",
      "Step - 16466, Loss - 0.652801645336266, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8501604243132828\n",
      "Step - 16467, Loss - 0.5953740917924542, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9258440037149387\n",
      "Step - 16468, Loss - 0.6508021118116483, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6876681135969249\n",
      "Step - 16469, Loss - 0.6509927125104984, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9343269005736485\n",
      "Step - 16470, Loss - 0.648180895170947, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2560344274306554\n",
      "Step - 16471, Loss - 0.6689578006753066, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.912258523636484\n",
      "Step - 16472, Loss - 0.582069049563045, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9076810587437721\n",
      "Step - 16473, Loss - 0.7749317620834657, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3928596050395035\n",
      "Step - 16474, Loss - 0.5915702611357823, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6499106338036005\n",
      "Step - 16475, Loss - 0.7606860307142317, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5633476635459379\n",
      "Step - 16476, Loss - 0.8723744648891778, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5459818379432917\n",
      "Step - 16477, Loss - 0.8844974058717008, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.2397998575878417\n",
      "Step - 16478, Loss - 0.8211086090048239, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.873274975988103\n",
      "Step - 16479, Loss - 0.6666515183503328, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.391645672059527\n",
      "Step - 16480, Loss - 0.6028704931093248, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7959240262009777\n",
      "Step - 16481, Loss - 0.72148900788474, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.3187994207603192\n",
      "Step - 16482, Loss - 0.6550786144071887, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.73568815587783\n",
      "Step - 16483, Loss - 0.7147500719475322, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8961062788554517\n",
      "Step - 16484, Loss - 0.7747314388266414, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6313492163650324\n",
      "Step - 16485, Loss - 0.8181423619751521, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3090852269771147\n",
      "Step - 16486, Loss - 0.45549926314246947, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7679417342139283\n",
      "Step - 16487, Loss - 0.7199944769771038, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5883811984503562\n",
      "Step - 16488, Loss - 0.6811677859561094, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.7322389358543355\n",
      "Step - 16489, Loss - 0.7776175542368075, Learning Rate - 7.62939453125e-07, magnitude of gradient - 3.190502602745304\n",
      "Step - 16490, Loss - 0.7366778906728225, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9796114304208065\n",
      "Step - 16491, Loss - 0.6077421012471185, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4996881138468945\n",
      "Step - 16492, Loss - 0.6960768118381391, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1512863097714723\n",
      "Step - 16493, Loss - 0.7177360027367694, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7854212629164833\n",
      "Step - 16494, Loss - 0.6159188305529086, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5106954896668958\n",
      "Step - 16495, Loss - 0.6806212423503032, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7672713903107955\n",
      "Step - 16496, Loss - 0.622816847297418, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1170304803072588\n",
      "Step - 16497, Loss - 0.773106619366901, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.186034518244749\n",
      "Step - 16498, Loss - 0.9691428299863938, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2528614057373149\n",
      "Step - 16499, Loss - 0.8397489737477555, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.938765445435203\n",
      "Step - 16500, Loss - 0.8024167788634686, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0992374794554263\n",
      "Step - 16501, Loss - 0.6256080275824608, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.2707243308986476\n",
      "Step - 16502, Loss - 0.7310069879211558, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.1574161620896537\n",
      "Step - 16503, Loss - 0.801382326914098, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8668041695608224\n",
      "Step - 16504, Loss - 0.9192635176192352, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5693905897044609\n",
      "Step - 16505, Loss - 0.6872084469114479, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3304757180160167\n",
      "Step - 16506, Loss - 0.600668338172291, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5244392496026616\n",
      "Step - 16507, Loss - 0.5639482556128822, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7211507074960515\n",
      "Step - 16508, Loss - 0.7307994811759839, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6973778355457152\n",
      "Step - 16509, Loss - 0.7715529452565597, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1114429138685715\n",
      "Step - 16510, Loss - 0.6793987459195933, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4253480542300097\n",
      "Step - 16511, Loss - 0.7653282557850315, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.086344827263774\n",
      "Step - 16512, Loss - 0.5152426350173186, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9212999955293057\n",
      "Step - 16513, Loss - 0.8031066439519412, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5314705633361655\n",
      "Step - 16514, Loss - 0.677506410128501, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7421041779905682\n",
      "Step - 16515, Loss - 0.7843667551396911, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8133346561053925\n",
      "Step - 16516, Loss - 0.6284806605369031, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1610867116233692\n",
      "Step - 16517, Loss - 0.6596824539151047, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6445505835647843\n",
      "Step - 16518, Loss - 0.7076800508987626, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.854185413789742\n",
      "Step - 16519, Loss - 0.7180692011263898, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0480610130363355\n",
      "Step - 16520, Loss - 0.7149698562593997, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8947098976570222\n",
      "Step - 16521, Loss - 0.649348258342991, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7961348979984525\n",
      "Step - 16522, Loss - 0.6697840318270492, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9221252604006991\n",
      "Step - 16523, Loss - 0.7763030245227415, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6466121153773781\n",
      "Step - 16524, Loss - 0.6246682355458856, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5864135685073744\n",
      "Step - 16525, Loss - 0.6319779060362987, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.31752918180619055\n",
      "Step - 16526, Loss - 0.6232490808452154, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.857848940984888\n",
      "Step - 16527, Loss - 0.6642707179538765, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5609389142471951\n",
      "Step - 16528, Loss - 0.7811435017396071, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6760952816877017\n",
      "Step - 16529, Loss - 0.7650567467622794, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1564867054889467\n",
      "Step - 16530, Loss - 0.8947333400993872, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.055188443210085\n",
      "Step - 16531, Loss - 0.746768685241678, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5372889060341353\n",
      "Step - 16532, Loss - 0.6927055382248432, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.711035218397551\n",
      "Step - 16533, Loss - 0.7343570205864258, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9516433439552159\n",
      "Step - 16534, Loss - 0.7059410114510425, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.087698489270255\n",
      "Step - 16535, Loss - 0.4771489971866583, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6419636603133411\n",
      "Step - 16536, Loss - 0.631840562482145, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0781647498696993\n",
      "Step - 16537, Loss - 0.8645005344024, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.014941923582895\n",
      "Step - 16538, Loss - 0.8005641545810604, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8298069590444167\n",
      "Step - 16539, Loss - 0.5939130453643825, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0636233675560982\n",
      "Step - 16540, Loss - 0.7275584055559535, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0509520427959815\n",
      "Step - 16541, Loss - 0.6345641630297543, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2193263872875815\n",
      "Step - 16542, Loss - 0.7035502895588758, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2582127130701055\n",
      "Step - 16543, Loss - 0.7143795250698866, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5464788524747028\n",
      "Step - 16544, Loss - 0.5076383967425028, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5572604971654596\n",
      "Step - 16545, Loss - 0.8523190718097511, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7485618931548972\n",
      "Step - 16546, Loss - 0.6239746545091328, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4888481629289925\n",
      "Step - 16547, Loss - 0.8067944282835889, Learning Rate - 7.62939453125e-07, magnitude of gradient - 3.0189223856835743\n",
      "Step - 16548, Loss - 0.7881876329462754, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6503285649441971\n",
      "Step - 16549, Loss - 0.7839986774404504, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5058448781792784\n",
      "Step - 16550, Loss - 0.893937516580899, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3449516233551708\n",
      "Step - 16551, Loss - 0.653250451393061, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.1397001872105212\n",
      "Step - 16552, Loss - 0.7162096735145683, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3691037134364121\n",
      "Step - 16553, Loss - 0.6350976740744815, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5170828010544204\n",
      "Step - 16554, Loss - 0.677047393097324, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5349656963978908\n",
      "Step - 16555, Loss - 0.4858978185614459, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.0277976137347165\n",
      "Step - 16556, Loss - 0.7026580882546252, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1498942000605918\n",
      "Step - 16557, Loss - 0.796823696975969, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.047976566375655\n",
      "Step - 16558, Loss - 0.7327181685095834, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.2733620824560025\n",
      "Step - 16559, Loss - 0.7084048574420703, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.1008492800928926\n",
      "Step - 16560, Loss - 0.9987513757459519, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.12307613240745\n",
      "Step - 16561, Loss - 0.7554023237761448, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.962513837697074\n",
      "Step - 16562, Loss - 0.8578836084846585, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.2781835634166069\n",
      "Step - 16563, Loss - 0.8091663634734121, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.29214205888834055\n",
      "Step - 16564, Loss - 0.709524546276926, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.48282168298221884\n",
      "Step - 16565, Loss - 0.7278774492638711, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6369873093223873\n",
      "Step - 16566, Loss - 0.7505025834890873, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7333178506147661\n",
      "Step - 16567, Loss - 0.6215920473683175, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.134070754081983\n",
      "Step - 16568, Loss - 0.7460048480898158, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0468196017851579\n",
      "Step - 16569, Loss - 0.5799247998988972, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.43010681094774533\n",
      "Step - 16570, Loss - 0.5956070073683427, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4470454448894522\n",
      "Step - 16571, Loss - 0.5144653507516439, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3449573119671767\n",
      "Step - 16572, Loss - 0.6893400336418156, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0510918828658062\n",
      "Step - 16573, Loss - 0.7652905045522633, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6782905958339136\n",
      "Step - 16574, Loss - 0.7890457074650902, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8946819297581332\n",
      "Step - 16575, Loss - 0.7716704861137194, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4638184690556593\n",
      "Step - 16576, Loss - 0.6506462351471729, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.891153763107478\n",
      "Step - 16577, Loss - 0.7847756233316048, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4215416814363973\n",
      "Step - 16578, Loss - 0.6190142051803292, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.0030271217781324\n",
      "Step - 16579, Loss - 0.7189771237364049, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.817367552452292\n",
      "Step - 16580, Loss - 0.8942664244241262, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3986642243297998\n",
      "Step - 16581, Loss - 0.6743316433922973, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4525888794366238\n",
      "Step - 16582, Loss - 0.717544145887841, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.996745068372108\n",
      "Step - 16583, Loss - 0.604757218854981, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1600548798581956\n",
      "Step - 16584, Loss - 0.7884392096622773, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9730639054920635\n",
      "Step - 16585, Loss - 0.7112221850867853, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0286257295808054\n",
      "Step - 16586, Loss - 0.6494119599038444, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1571754621301358\n",
      "Step - 16587, Loss - 0.7372142233522992, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2777214892112239\n",
      "Step - 16588, Loss - 0.47133137441270684, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7892386581704155\n",
      "Step - 16589, Loss - 0.7371211770853324, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.8220667320744086\n",
      "Step - 16590, Loss - 0.9547977227058677, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0674009757085168\n",
      "Step - 16591, Loss - 0.6088955064679886, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3070043045653237\n",
      "Step - 16592, Loss - 0.709345902048699, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9549020669236299\n",
      "Step - 16593, Loss - 0.423059272960954, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1240183640494494\n",
      "Step - 16594, Loss - 0.7578906374291684, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.971465181878584\n",
      "Step - 16595, Loss - 0.7835023517834863, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8251669746690268\n",
      "Step - 16596, Loss - 0.6392957211621009, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4992628826366567\n",
      "Step - 16597, Loss - 0.6088315804423784, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1564465965302266\n",
      "Step - 16598, Loss - 0.5753197244304745, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9199585780397543\n",
      "Step - 16599, Loss - 0.6425013572012057, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5994509645236084\n",
      "Step - 16600, Loss - 0.5799968087670668, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6079641252016141\n",
      "Step - 16601, Loss - 0.711760893484597, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3941078484579454\n",
      "Step - 16602, Loss - 0.8013339618230275, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.440428265584803\n",
      "Step - 16603, Loss - 0.8808379324537587, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6051978214551241\n",
      "Step - 16604, Loss - 0.6501208201091204, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2959201407299419\n",
      "Step - 16605, Loss - 0.6679508852422769, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.593889758025607\n",
      "Step - 16606, Loss - 0.6195940454693541, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4116464936039985\n",
      "Step - 16607, Loss - 0.7778143140127656, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5696456508242063\n",
      "Step - 16608, Loss - 0.9296640550628108, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8208267608798336\n",
      "Step - 16609, Loss - 0.8274303282330123, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2455281421417814\n",
      "Step - 16610, Loss - 0.6303492514308019, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1535293703261416\n",
      "Step - 16611, Loss - 0.5840605766410011, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.3287706348923629\n",
      "Step - 16612, Loss - 0.5812746670213585, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8174433806357462\n",
      "Step - 16613, Loss - 0.7270677527157695, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2588643972035782\n",
      "Step - 16614, Loss - 0.7629365029750903, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0099376251557275\n",
      "Step - 16615, Loss - 0.7987485624855317, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1497662550825025\n",
      "Step - 16616, Loss - 0.5098472407077046, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.0798971709533647\n",
      "Step - 16617, Loss - 0.7995423015493961, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.112923586758445\n",
      "Step - 16618, Loss - 0.7669326832846823, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8141991835387343\n",
      "Step - 16619, Loss - 0.6099358405896135, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.944580815898493\n",
      "Step - 16620, Loss - 0.5688341843056068, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.349743275574084\n",
      "Step - 16621, Loss - 0.656775480340573, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.1450618170870714\n",
      "Step - 16622, Loss - 0.7246036482020064, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.959788541852573\n",
      "Step - 16623, Loss - 0.7308565044523794, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4474550063105758\n",
      "Step - 16624, Loss - 0.652772909723845, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.46173465483826814\n",
      "Step - 16625, Loss - 0.82873146181414, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6360590148971152\n",
      "Step - 16626, Loss - 0.7014898570701233, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.40086879439566525\n",
      "Step - 16627, Loss - 0.6744249265645025, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5840417264023434\n",
      "Step - 16628, Loss - 0.7291515671719538, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8072937063550101\n",
      "Step - 16629, Loss - 0.6374774033015227, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6655015715155315\n",
      "Step - 16630, Loss - 0.5349364494778165, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.3553270158973562\n",
      "Step - 16631, Loss - 0.78737463669522, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.244633587998756\n",
      "Step - 16632, Loss - 0.6237167914419102, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7863754235606711\n",
      "Step - 16633, Loss - 0.6587211554450081, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.01804665940149\n",
      "Step - 16634, Loss - 0.6166444187892259, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8244040044861897\n",
      "Step - 16635, Loss - 0.6188484520927576, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8796832395840377\n",
      "Step - 16636, Loss - 0.5908610838868769, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.5894453335638534\n",
      "Step - 16637, Loss - 0.7260929651442981, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8276529588496199\n",
      "Step - 16638, Loss - 0.7764832119517259, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.4027289640168097\n",
      "Step - 16639, Loss - 0.6049431464767537, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1859956358909907\n",
      "Step - 16640, Loss - 0.6102552525669107, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.120881122224807\n",
      "Step - 16641, Loss - 0.5090225780709551, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6742908016504976\n",
      "Step - 16642, Loss - 0.6512650603888095, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.296318980759853\n",
      "Step - 16643, Loss - 0.5518124081352738, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.320214124797589\n",
      "Step - 16644, Loss - 0.48044099406244917, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8285325904536753\n",
      "Step - 16645, Loss - 0.675740184098582, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.035474605090525\n",
      "Step - 16646, Loss - 0.7602539311798633, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.689162762130108\n",
      "Step - 16647, Loss - 0.8269968601956509, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9906694841458802\n",
      "Step - 16648, Loss - 0.6884908202724064, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6565206827909519\n",
      "Step - 16649, Loss - 0.697302761359498, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0093764191693826\n",
      "Step - 16650, Loss - 0.4713143903441894, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1068657398823398\n",
      "Step - 16651, Loss - 0.8658755703667724, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.997635101333582\n",
      "Step - 16652, Loss - 0.7496267087887416, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0142193070458172\n",
      "Step - 16653, Loss - 0.58423365059821, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8464990992055395\n",
      "Step - 16654, Loss - 0.6503183522457391, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8979647413317616\n",
      "Step - 16655, Loss - 0.617856176461326, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.426530025666766\n",
      "Step - 16656, Loss - 0.7737630552206187, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.002169495690381\n",
      "Step - 16657, Loss - 0.8103051338860134, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9147182878353248\n",
      "Step - 16658, Loss - 0.7943179045763815, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0241645725167665\n",
      "Step - 16659, Loss - 0.7654609525384852, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.275073562735849\n",
      "Step - 16660, Loss - 0.6721464157857907, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4252934746409186\n",
      "Step - 16661, Loss - 0.6001624159316645, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8235824384146642\n",
      "Step - 16662, Loss - 0.7597499398048845, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8195711304711628\n",
      "Step - 16663, Loss - 0.6490321355967561, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0399903821917598\n",
      "Step - 16664, Loss - 0.8742175000647266, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3224451782731041\n",
      "Step - 16665, Loss - 0.692994244087226, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0710855679564326\n",
      "Step - 16666, Loss - 0.8651056358812568, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7850593540855129\n",
      "Step - 16667, Loss - 0.709278223022992, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8186871674906665\n",
      "Step - 16668, Loss - 0.8379957151996669, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.302428448525049\n",
      "Step - 16669, Loss - 0.823284594837365, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5440163984054642\n",
      "Step - 16670, Loss - 0.5711450909122241, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9466769716618348\n",
      "Step - 16671, Loss - 0.56852228696606, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2904604301949103\n",
      "Step - 16672, Loss - 0.6738602100011752, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0090828692112346\n",
      "Step - 16673, Loss - 0.7813679631505994, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7902855847972973\n",
      "Step - 16674, Loss - 0.7141686902639749, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.1517965255676477\n",
      "Step - 16675, Loss - 0.7559565662025695, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0382254461328297\n",
      "Step - 16676, Loss - 0.7031406100867187, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6677423105767064\n",
      "Step - 16677, Loss - 0.6666412625275754, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7127877227925989\n",
      "Step - 16678, Loss - 0.630245917253827, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9974778547873162\n",
      "Step - 16679, Loss - 0.7659113483613997, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.1278817532972294\n",
      "Step - 16680, Loss - 0.6908075435739465, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2063439133829859\n",
      "Step - 16681, Loss - 0.7465786937282836, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8334166133539515\n",
      "Step - 16682, Loss - 0.784268890918125, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5605089280779522\n",
      "Step - 16683, Loss - 0.8442906479772115, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6628383546846404\n",
      "Step - 16684, Loss - 0.8885388052687642, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.313913128100129\n",
      "Step - 16685, Loss - 0.6252931744330913, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3151096060109608\n",
      "Step - 16686, Loss - 0.8238173327351371, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8594496619872989\n",
      "Step - 16687, Loss - 0.5375109129356087, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8879679403175458\n",
      "Step - 16688, Loss - 0.6349811559917797, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3744986425838586\n",
      "Step - 16689, Loss - 0.5283059235232594, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6398513224956532\n",
      "Step - 16690, Loss - 0.611789319656509, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6431266485971695\n",
      "Step - 16691, Loss - 0.7022999194728513, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9023185527709128\n",
      "Step - 16692, Loss - 0.6799395888468838, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.807933004747566\n",
      "Step - 16693, Loss - 0.7337065356255872, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9388267561263423\n",
      "Step - 16694, Loss - 0.8045924267289953, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.79728480861942\n",
      "Step - 16695, Loss - 0.7362985447105342, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4658702032577118\n",
      "Step - 16696, Loss - 0.8523800850896815, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8079511674505439\n",
      "Step - 16697, Loss - 0.6342469378665745, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9593718764226669\n",
      "Step - 16698, Loss - 0.8027394538773932, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.145907825215891\n",
      "Step - 16699, Loss - 0.593938671986445, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0790788290677493\n",
      "Step - 16700, Loss - 0.7503503430108269, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5422982251990822\n",
      "Step - 16701, Loss - 0.7174869681094735, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5748417541561297\n",
      "Step - 16702, Loss - 0.6976729016813453, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4605158301144345\n",
      "Step - 16703, Loss - 0.6245856841268345, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.287118129073129\n",
      "Step - 16704, Loss - 0.7976076108314656, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3897847818305624\n",
      "Step - 16705, Loss - 0.673087297614613, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6567910852475176\n",
      "Step - 16706, Loss - 0.5858112793454907, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.845774882414273\n",
      "Step - 16707, Loss - 0.6845503880404264, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.264230407322182\n",
      "Step - 16708, Loss - 0.6579633814502999, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5615941864863041\n",
      "Step - 16709, Loss - 0.7377563857952991, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0223234927972125\n",
      "Step - 16710, Loss - 0.8039086213277453, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2262106568037074\n",
      "Step - 16711, Loss - 0.6499501517586528, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0995276251434467\n",
      "Step - 16712, Loss - 0.7062106294865672, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4125835354711747\n",
      "Step - 16713, Loss - 0.7438995185257661, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4661122739120405\n",
      "Step - 16714, Loss - 0.6916860818747748, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6224004758177577\n",
      "Step - 16715, Loss - 0.7338278566660597, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6164711343862246\n",
      "Step - 16716, Loss - 0.7329249102357207, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1427920358587695\n",
      "Step - 16717, Loss - 0.8090564188743714, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.1642225231919774\n",
      "Step - 16718, Loss - 0.7569490813533566, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.015608707272434\n",
      "Step - 16719, Loss - 0.7650508230534654, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6435979475674848\n",
      "Step - 16720, Loss - 0.6133374978019377, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.935292038177506\n",
      "Step - 16721, Loss - 0.7307698902667764, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7697603755740801\n",
      "Step - 16722, Loss - 0.45852363548386327, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.0890400752909097\n",
      "Step - 16723, Loss - 0.8203001622923309, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6943184108314865\n",
      "Step - 16724, Loss - 0.8383084631068332, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8231843599690183\n",
      "Step - 16725, Loss - 0.8027752886257928, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1803352869661503\n",
      "Step - 16726, Loss - 0.7189264641504158, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.649282685355646\n",
      "Step - 16727, Loss - 0.7302971814183012, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.309822473125524\n",
      "Step - 16728, Loss - 0.7229474851756075, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.694644636387977\n",
      "Step - 16729, Loss - 0.6167509887347308, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7373528998850305\n",
      "Step - 16730, Loss - 0.6370148805476511, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5332504021989668\n",
      "Step - 16731, Loss - 0.8270812887472982, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8361667161068269\n",
      "Step - 16732, Loss - 0.8044726997419331, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.790720970484693\n",
      "Step - 16733, Loss - 0.664235175487359, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3441623492508517\n",
      "Step - 16734, Loss - 0.8132752781617083, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1166839276029852\n",
      "Step - 16735, Loss - 0.5471130179805692, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6953567881213869\n",
      "Step - 16736, Loss - 0.7775983435511868, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2121013100162412\n",
      "Step - 16737, Loss - 0.7272632748833457, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4699444255746916\n",
      "Step - 16738, Loss - 0.58181665372872, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5309275197797658\n",
      "Step - 16739, Loss - 0.6340821775954374, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7719355255192378\n",
      "Step - 16740, Loss - 0.6882893014566724, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7497660903246273\n",
      "Step - 16741, Loss - 0.5991226412729844, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.624551669012192\n",
      "Step - 16742, Loss - 0.5758301597939434, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.708752661172359\n",
      "Step - 16743, Loss - 0.7405078746397357, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6065194309825273\n",
      "Step - 16744, Loss - 0.693227205091401, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1582703543009196\n",
      "Step - 16745, Loss - 0.6741796320901833, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.139065933523491\n",
      "Step - 16746, Loss - 0.7237388047572941, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9208370878204895\n",
      "Step - 16747, Loss - 0.6847828176301421, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.38112651649350654\n",
      "Step - 16748, Loss - 0.5730451986281712, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5055876764181115\n",
      "Step - 16749, Loss - 0.7861227691672693, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.022439865484249\n",
      "Step - 16750, Loss - 0.6774966393501298, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.457628620430651\n",
      "Step - 16751, Loss - 0.5393113327318679, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8530229438058121\n",
      "Step - 16752, Loss - 0.6130013437769609, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8028881141637687\n",
      "Step - 16753, Loss - 0.7565231602639078, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4666015859911652\n",
      "Step - 16754, Loss - 0.8118661518955431, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4793176635257934\n",
      "Step - 16755, Loss - 0.8133087426247312, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0141908190397215\n",
      "Step - 16756, Loss - 0.8026421508374705, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7587005030698311\n",
      "Step - 16757, Loss - 0.612592857631665, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.096774761340109\n",
      "Step - 16758, Loss - 0.6499714605759978, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9353358643772737\n",
      "Step - 16759, Loss - 0.7654221487881474, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.34818145304990455\n",
      "Step - 16760, Loss - 0.5696258674693678, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3755083112671966\n",
      "Step - 16761, Loss - 0.5416811372816224, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.767963938228839\n",
      "Step - 16762, Loss - 0.8439367857333295, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7679268933749155\n",
      "Step - 16763, Loss - 0.7698595919046985, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9667070310903183\n",
      "Step - 16764, Loss - 0.6769978218985695, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6783773531207322\n",
      "Step - 16765, Loss - 0.5961228460150342, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7476387758680274\n",
      "Step - 16766, Loss - 0.8303624459729019, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.125367578356547\n",
      "Step - 16767, Loss - 0.8764843980138723, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4354392275647883\n",
      "Step - 16768, Loss - 0.674501429182484, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5141225202636851\n",
      "Step - 16769, Loss - 0.5528376822397235, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.235166526462288\n",
      "Step - 16770, Loss - 0.6635394307967273, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9192698191583273\n",
      "Step - 16771, Loss - 0.6757929278754319, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7610330977880909\n",
      "Step - 16772, Loss - 0.7762573046320504, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0086408602530499\n",
      "Step - 16773, Loss - 0.5565999079442737, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.4231444286326322\n",
      "Step - 16774, Loss - 0.7726226367124722, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2068334184797684\n",
      "Step - 16775, Loss - 0.6286268388697949, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.711487654382104\n",
      "Step - 16776, Loss - 0.7826112354668672, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.540721579860132\n",
      "Step - 16777, Loss - 0.8795347490021656, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.441956169159249\n",
      "Step - 16778, Loss - 0.7000539369805949, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8267945246859286\n",
      "Step - 16779, Loss - 0.8937982290523954, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4034199894611117\n",
      "Step - 16780, Loss - 0.7412051592570545, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8457887375954836\n",
      "Step - 16781, Loss - 0.7502361982591876, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2358510841843635\n",
      "Step - 16782, Loss - 0.6726591983712003, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5187368840399414\n",
      "Step - 16783, Loss - 0.7874016488179907, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6480134998858389\n",
      "Step - 16784, Loss - 0.7241922698853833, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5660968114841893\n",
      "Step - 16785, Loss - 0.5299818454224644, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7880402217553079\n",
      "Step - 16786, Loss - 0.7818411713305258, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.061009751483292\n",
      "Step - 16787, Loss - 0.9467205892817679, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.7685816529106266\n",
      "Step - 16788, Loss - 0.6945601183028728, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8141574867344565\n",
      "Step - 16789, Loss - 0.7023170537187976, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.401071759394481\n",
      "Step - 16790, Loss - 0.6446769785044132, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1759826759219845\n",
      "Step - 16791, Loss - 0.5870017266978113, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1717664338042455\n",
      "Step - 16792, Loss - 0.6877645333242577, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6130642837959598\n",
      "Step - 16793, Loss - 0.8509950098747548, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.1660344543955636\n",
      "Step - 16794, Loss - 0.6416764796914662, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.465495031715573\n",
      "Step - 16795, Loss - 0.8821454208388495, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0792585054636556\n",
      "Step - 16796, Loss - 0.7526160754159642, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.443501257770624\n",
      "Step - 16797, Loss - 0.7516225092126857, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.5588503971607954\n",
      "Step - 16798, Loss - 0.6008817994223016, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4246768929827646\n",
      "Step - 16799, Loss - 0.8047879136029535, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8018921464094936\n",
      "Step - 16800, Loss - 0.6889874540134358, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.911883877591987\n",
      "Step - 16801, Loss - 0.6022272255848058, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4831022129018296\n",
      "Step - 16802, Loss - 0.6326766331805408, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.893042213513474\n",
      "Step - 16803, Loss - 0.7407707211704546, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8573706153637781\n",
      "Step - 16804, Loss - 0.7938913407075983, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7692429964181382\n",
      "Step - 16805, Loss - 0.8667750683675993, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0610844383185503\n",
      "Step - 16806, Loss - 0.7083283313668255, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.776721294631881\n",
      "Step - 16807, Loss - 0.7811298068521659, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2804190844855863\n",
      "Step - 16808, Loss - 0.669328372460719, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7041321099893636\n",
      "Step - 16809, Loss - 0.7106076214560529, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3857768133492179\n",
      "Step - 16810, Loss - 0.6812256108006254, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5741051165433142\n",
      "Step - 16811, Loss - 0.7694831306070832, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.854588742296979\n",
      "Step - 16812, Loss - 0.6859569609100888, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.955323538047131\n",
      "Step - 16813, Loss - 0.4739014209826837, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.25064552944487\n",
      "Step - 16814, Loss - 0.7667500911069798, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6121640064192133\n",
      "Step - 16815, Loss - 0.8120061059382628, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0071631318105767\n",
      "Step - 16816, Loss - 0.751393205276667, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9689494555284336\n",
      "Step - 16817, Loss - 0.6803720527306849, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7462971427568983\n",
      "Step - 16818, Loss - 0.8621690682424666, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9403029045257465\n",
      "Step - 16819, Loss - 0.8467655553457119, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4110588859581927\n",
      "Step - 16820, Loss - 0.6322380726772199, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.1177876694493505\n",
      "Step - 16821, Loss - 0.6014027330686228, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3599444935717446\n",
      "Step - 16822, Loss - 0.6582630624647937, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.957324077799117\n",
      "Step - 16823, Loss - 0.9016713145806884, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8855211527469689\n",
      "Step - 16824, Loss - 0.6888327349310657, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7491015505863617\n",
      "Step - 16825, Loss - 0.8218873912999769, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9735193122176524\n",
      "Step - 16826, Loss - 0.7630141225796616, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7272949000536019\n",
      "Step - 16827, Loss - 0.9705503438055296, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8036768284881222\n",
      "Step - 16828, Loss - 0.5713735532237645, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7696750563019522\n",
      "Step - 16829, Loss - 0.6396229870676694, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5576117381411269\n",
      "Step - 16830, Loss - 0.6888862773675304, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1171394757908575\n",
      "Step - 16831, Loss - 0.6789373825564362, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.48639969914243025\n",
      "Step - 16832, Loss - 0.4766056008151166, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.427037871586542\n",
      "Step - 16833, Loss - 0.7781910174581383, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5234022225222408\n",
      "Step - 16834, Loss - 0.542185490680426, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7497419873257142\n",
      "Step - 16835, Loss - 0.6378609637634309, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5385748210788509\n",
      "Step - 16836, Loss - 0.6799329643158843, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7728316544012324\n",
      "Step - 16837, Loss - 0.5615229940155853, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.804909075052718\n",
      "Step - 16838, Loss - 0.7084706240442215, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6149724173110038\n",
      "Step - 16839, Loss - 0.6340094504441735, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9840539959580024\n",
      "Step - 16840, Loss - 0.7993636482738725, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8848386892904819\n",
      "Step - 16841, Loss - 0.6167610511080392, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4092716385991761\n",
      "Step - 16842, Loss - 0.6981065328507291, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5199332610198286\n",
      "Step - 16843, Loss - 0.5765743363749528, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.893482373368736\n",
      "Step - 16844, Loss - 0.7452600899067701, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4781487256998161\n",
      "Step - 16845, Loss - 0.7278640580733137, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8351627291989637\n",
      "Step - 16846, Loss - 0.6435806536592509, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3722368144017891\n",
      "Step - 16847, Loss - 0.9011327236272295, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0535154377434035\n",
      "Step - 16848, Loss - 0.7952316890509677, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1434156584984752\n",
      "Step - 16849, Loss - 0.8848338872674796, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6185950154637994\n",
      "Step - 16850, Loss - 0.6540068851371401, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7184391479575801\n",
      "Step - 16851, Loss - 0.5471890383212262, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0890126182127298\n",
      "Step - 16852, Loss - 0.8438120169580065, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.289850454557316\n",
      "Step - 16853, Loss - 0.7003193936790271, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6254626441502777\n",
      "Step - 16854, Loss - 0.8358189673704092, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.2289909693680498\n",
      "Step - 16855, Loss - 0.5981569980956929, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7341760571318627\n",
      "Step - 16856, Loss - 0.6734324343636927, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7467725570716707\n",
      "Step - 16857, Loss - 0.6303023332403013, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.827942832073192\n",
      "Step - 16858, Loss - 0.8797899590640812, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4964364708179239\n",
      "Step - 16859, Loss - 0.6630426215968558, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9896862146508726\n",
      "Step - 16860, Loss - 0.8193691759839236, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0029334494813686\n",
      "Step - 16861, Loss - 0.5294285573990355, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.47297969756324\n",
      "Step - 16862, Loss - 0.7026415172827253, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8075481542389789\n",
      "Step - 16863, Loss - 0.7141625933934258, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6633972742908394\n",
      "Step - 16864, Loss - 0.5727720606636417, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4302200751725926\n",
      "Step - 16865, Loss - 0.6030502392634247, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8574876675046934\n",
      "Step - 16866, Loss - 0.7916953937348599, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6074220783145706\n",
      "Step - 16867, Loss - 0.822617082106591, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.026180288466982\n",
      "Step - 16868, Loss - 0.6495691363007874, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7583209962176336\n",
      "Step - 16869, Loss - 0.7152955722310849, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8432332510009559\n",
      "Step - 16870, Loss - 0.8687207321236796, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.229235199143947\n",
      "Step - 16871, Loss - 0.7001914184771413, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0318010168439484\n",
      "Step - 16872, Loss - 0.6873219467626808, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.3387213467877267\n",
      "Step - 16873, Loss - 0.7106407347811095, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8920951637366663\n",
      "Step - 16874, Loss - 0.7997458041913482, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1671983998426367\n",
      "Step - 16875, Loss - 0.7056240046485149, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8054359172892018\n",
      "Step - 16876, Loss - 0.6495907720244043, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3535012860473392\n",
      "Step - 16877, Loss - 0.6657710752691668, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6660811815011929\n",
      "Step - 16878, Loss - 0.8137881502222337, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9252576165174871\n",
      "Step - 16879, Loss - 0.6648562027368825, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.4147437663775\n",
      "Step - 16880, Loss - 0.6883908182149737, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7342288792096779\n",
      "Step - 16881, Loss - 0.7880856085905739, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8042852731741134\n",
      "Step - 16882, Loss - 0.508056497159372, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.65974538385274\n",
      "Step - 16883, Loss - 0.6492077387889444, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.093810447724914\n",
      "Step - 16884, Loss - 0.7839832940453254, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2113199812542257\n",
      "Step - 16885, Loss - 0.6172338891245275, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9197927765531236\n",
      "Step - 16886, Loss - 0.871097091695299, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1208591335583054\n",
      "Step - 16887, Loss - 0.8338963088819494, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6521741660446335\n",
      "Step - 16888, Loss - 0.5845526031223136, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2317793943553579\n",
      "Step - 16889, Loss - 0.7053646744800274, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9191002166524559\n",
      "Step - 16890, Loss - 0.6257830744266326, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3601258255962112\n",
      "Step - 16891, Loss - 0.9594727389510149, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1125465435242656\n",
      "Step - 16892, Loss - 0.7187210271360004, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8626576073855392\n",
      "Step - 16893, Loss - 0.6016982007305216, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.544590079802308\n",
      "Step - 16894, Loss - 0.7085619935551126, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.819983515396554\n",
      "Step - 16895, Loss - 0.942439638319834, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0433298464257386\n",
      "Step - 16896, Loss - 0.7118841565238693, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7975184664324451\n",
      "Step - 16897, Loss - 0.5112768069894345, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.316338722084828\n",
      "Step - 16898, Loss - 0.7630208506213604, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.418891247401829\n",
      "Step - 16899, Loss - 0.6904559882729497, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8665833770613711\n",
      "Step - 16900, Loss - 0.8493597529518409, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.024236782255141\n",
      "Step - 16901, Loss - 0.7648619187618163, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9652843172920916\n",
      "Step - 16902, Loss - 0.5653161610496634, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0363910987249085\n",
      "Step - 16903, Loss - 0.8096451826195848, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7194217828683226\n",
      "Step - 16904, Loss - 0.6623290114938086, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6070974311693643\n",
      "Step - 16905, Loss - 0.8780941354580367, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4568892654592809\n",
      "Step - 16906, Loss - 0.8090258481370376, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9278595283959715\n",
      "Step - 16907, Loss - 0.6866107144191824, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8020900075359682\n",
      "Step - 16908, Loss - 0.5305841941588683, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9339081174106552\n",
      "Step - 16909, Loss - 0.6914602310687868, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.416883974008577\n",
      "Step - 16910, Loss - 0.8912056387977592, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.720630410031306\n",
      "Step - 16911, Loss - 0.5844908275130939, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.424278295579723\n",
      "Step - 16912, Loss - 0.6330482098129491, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8715534251397793\n",
      "Step - 16913, Loss - 0.6588908771326311, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.546878727712512\n",
      "Step - 16914, Loss - 0.8943812849918651, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.649278872610844\n",
      "Step - 16915, Loss - 0.969069136679834, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.208658352637846\n",
      "Step - 16916, Loss - 0.6004818750210013, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5430311412739348\n",
      "Step - 16917, Loss - 0.5846100255220872, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3493857017247086\n",
      "Step - 16918, Loss - 0.6723567776401266, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7131906485087001\n",
      "Step - 16919, Loss - 0.5624093225635339, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.130739570539373\n",
      "Step - 16920, Loss - 0.8384641047181931, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1504814978796514\n",
      "Step - 16921, Loss - 0.5444329423561615, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8194988914566042\n",
      "Step - 16922, Loss - 0.5242982188028451, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4340048749178005\n",
      "Step - 16923, Loss - 0.7683714688937073, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.3665817510350378\n",
      "Step - 16924, Loss - 0.6783260838902468, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.27200398975581963\n",
      "Step - 16925, Loss - 0.7373312732220942, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7980770018631407\n",
      "Step - 16926, Loss - 0.8253318899636098, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9963632280742782\n",
      "Step - 16927, Loss - 0.6458825281036225, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4180302627670298\n",
      "Step - 16928, Loss - 0.7636582599346822, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.45710410691985\n",
      "Step - 16929, Loss - 1.0522149161572982, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0134118829057404\n",
      "Step - 16930, Loss - 0.7658868949612094, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5275966335453922\n",
      "Step - 16931, Loss - 0.7535591956288047, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.3704714305337919\n",
      "Step - 16932, Loss - 0.7461347739623108, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8606092918380632\n",
      "Step - 16933, Loss - 0.8131930563521275, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2596125001777676\n",
      "Step - 16934, Loss - 0.7775150100901371, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.729473628853302\n",
      "Step - 16935, Loss - 0.7636646870576125, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9238458463759228\n",
      "Step - 16936, Loss - 0.7671839952244353, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7501606906510614\n",
      "Step - 16937, Loss - 0.9740098051839827, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.122311944685358\n",
      "Step - 16938, Loss - 0.612487654942022, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.28552906639224573\n",
      "Step - 16939, Loss - 0.4873816234692041, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6051374823259439\n",
      "Step - 16940, Loss - 0.5689117158841146, Learning Rate - 7.62939453125e-07, magnitude of gradient - 3.8165154552108547\n",
      "Step - 16941, Loss - 0.6232314759274321, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.118758972181082\n",
      "Step - 16942, Loss - 0.8354219106284635, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4810754406152964\n",
      "Step - 16943, Loss - 0.7625828438221529, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6658399645818553\n",
      "Step - 16944, Loss - 0.7592765910132527, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8834230887464086\n",
      "Step - 16945, Loss - 0.7700719973529924, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3555317580859494\n",
      "Step - 16946, Loss - 0.6083106805121868, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9914538405436051\n",
      "Step - 16947, Loss - 0.6128640025940901, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8368609641559543\n",
      "Step - 16948, Loss - 0.877549761361444, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.670628145615934\n",
      "Step - 16949, Loss - 0.6755845874268582, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5733997188017477\n",
      "Step - 16950, Loss - 0.8736388959421345, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.212031518629923\n",
      "Step - 16951, Loss - 0.6049474165032771, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5115389433359546\n",
      "Step - 16952, Loss - 0.7761165626360738, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0209185526685063\n",
      "Step - 16953, Loss - 0.72151252402393, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9546883738456627\n",
      "Step - 16954, Loss - 0.7377128288366399, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.275049621195497\n",
      "Step - 16955, Loss - 0.5329230149682722, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.9222985903773246\n",
      "Step - 16956, Loss - 0.6525687269865987, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6376064938859407\n",
      "Step - 16957, Loss - 0.6932049910882908, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.881173102545759\n",
      "Step - 16958, Loss - 0.8214313174210076, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.75646567520651\n",
      "Step - 16959, Loss - 0.991330814665904, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.303519086383836\n",
      "Step - 16960, Loss - 0.620346947938794, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7521834692069637\n",
      "Step - 16961, Loss - 0.7256738526461508, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6871623124472792\n",
      "Step - 16962, Loss - 0.6542748409894892, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6966609860478556\n",
      "Step - 16963, Loss - 0.714989928383379, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8472471752940456\n",
      "Step - 16964, Loss - 0.6265526355381132, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.000513792052724\n",
      "Step - 16965, Loss - 0.803546158922995, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9348947201827287\n",
      "Step - 16966, Loss - 0.6694406250898849, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1761470910015193\n",
      "Step - 16967, Loss - 0.6259898311517866, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1310515949700655\n",
      "Step - 16968, Loss - 0.6509612981522179, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.6842317895849208\n",
      "Step - 16969, Loss - 0.7286223852158251, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.15789405518911206\n",
      "Step - 16970, Loss - 0.6779590598288076, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.5836755116593376\n",
      "Step - 16971, Loss - 0.6950002426885371, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9891085527597567\n",
      "Step - 16972, Loss - 0.8003528394608224, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9555357928120811\n",
      "Step - 16973, Loss - 0.5571324602944867, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.2449544390562097\n",
      "Step - 16974, Loss - 0.6809985700822936, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.7880171578110472\n",
      "Step - 16975, Loss - 0.5897183515556474, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6426909920980378\n",
      "Step - 16976, Loss - 0.7331812159620905, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1876085033270993\n",
      "Step - 16977, Loss - 0.6491166973277239, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3889830502721525\n",
      "Step - 16978, Loss - 0.530056644082789, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.7105933353443725\n",
      "Step - 16979, Loss - 0.5798486844541744, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3630423243832377\n",
      "Step - 16980, Loss - 0.48588032424217537, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8764429262872605\n",
      "Step - 16981, Loss - 0.5174728821746607, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.4056297966138157\n",
      "Step - 16982, Loss - 0.6399770740476977, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.8566124904280072\n",
      "Step - 16983, Loss - 0.7031112584117174, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6107057163301214\n",
      "Step - 16984, Loss - 0.6101685014550938, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0475210704249092\n",
      "Step - 16985, Loss - 0.6886054926055981, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.6628929219474613\n",
      "Step - 16986, Loss - 0.7207850725542763, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1495447945355122\n",
      "Step - 16987, Loss - 0.45309928074981104, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.417614709006317\n",
      "Step - 16988, Loss - 0.7282510947134299, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.46212992000696257\n",
      "Step - 16989, Loss - 0.6424738310142946, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.853961063461916\n",
      "Step - 16990, Loss - 0.6340221805830474, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.9069417628874037\n",
      "Step - 16991, Loss - 0.7790719950992069, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.6169432849838145\n",
      "Step - 16992, Loss - 0.6265360296093607, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.3210971508295108\n",
      "Step - 16993, Loss - 0.6720770242717327, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1051484250859591\n",
      "Step - 16994, Loss - 0.6336307582695522, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.979946526192455\n",
      "Step - 16995, Loss - 0.7115442305629055, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.4088978988431666\n",
      "Step - 16996, Loss - 0.7748054156358779, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.5303313173142064\n",
      "Step - 16997, Loss - 0.6937577965356451, Learning Rate - 7.62939453125e-07, magnitude of gradient - 2.034098906002107\n",
      "Step - 16998, Loss - 0.6651380837448276, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.1317267510865086\n",
      "Step - 16999, Loss - 0.7853397377963004, Learning Rate - 7.62939453125e-07, magnitude of gradient - 0.8293702432184646\n",
      "Step - 17000, Loss - 0.6170054084568958, Learning Rate - 7.62939453125e-07, magnitude of gradient - 1.0216584847801462\n",
      "Step - 17001, Loss - 0.441620799310757, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3941178966084193\n",
      "Step - 17002, Loss - 0.5770189650822959, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2510125378011347\n",
      "Step - 17003, Loss - 0.8184163561526415, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7316127828795872\n",
      "Step - 17004, Loss - 0.7519028727746226, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2014969299424634\n",
      "Step - 17005, Loss - 0.6280718408202214, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2060857171885049\n",
      "Step - 17006, Loss - 0.6337533246048491, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.040460488421776\n",
      "Step - 17007, Loss - 0.7595992788232677, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5169056358817032\n",
      "Step - 17008, Loss - 0.6543018600452257, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9143804778010994\n",
      "Step - 17009, Loss - 0.8193074274487994, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.202891567295498\n",
      "Step - 17010, Loss - 0.6334805615929511, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.640251312943081\n",
      "Step - 17011, Loss - 0.6715055980642424, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6852554940873141\n",
      "Step - 17012, Loss - 0.786035265968047, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4517095499780086\n",
      "Step - 17013, Loss - 0.5318192480375163, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.452166792391862\n",
      "Step - 17014, Loss - 0.4942746991285446, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1272432828303862\n",
      "Step - 17015, Loss - 0.6179106498398322, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.338935863179256\n",
      "Step - 17016, Loss - 0.6283292560730827, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6258632621037316\n",
      "Step - 17017, Loss - 0.5551214098401644, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6663430591592431\n",
      "Step - 17018, Loss - 0.8790667266145228, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.9429247271056127\n",
      "Step - 17019, Loss - 0.5637289614856702, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0003452076925319\n",
      "Step - 17020, Loss - 0.6364444600314536, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.013777636792284\n",
      "Step - 17021, Loss - 0.8177307967384551, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7246227905346869\n",
      "Step - 17022, Loss - 0.745407348411389, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7797256114703226\n",
      "Step - 17023, Loss - 0.8284991325762225, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.689273405784289\n",
      "Step - 17024, Loss - 0.6417575609815536, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5798451451823579\n",
      "Step - 17025, Loss - 0.7924571053123028, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.2234554756134997\n",
      "Step - 17026, Loss - 0.7345263337511766, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7784356649451727\n",
      "Step - 17027, Loss - 0.6325333190007222, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.151609756368166\n",
      "Step - 17028, Loss - 0.6577493056542767, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6575405090311466\n",
      "Step - 17029, Loss - 0.788525678028478, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6296581292552875\n",
      "Step - 17030, Loss - 0.8056590505457888, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.2024932024277923\n",
      "Step - 17031, Loss - 0.643543859978857, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4269479716997024\n",
      "Step - 17032, Loss - 0.6686648794751062, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.3761596780617054\n",
      "Step - 17033, Loss - 0.6741661352760954, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3082337825388293\n",
      "Step - 17034, Loss - 0.9683531202893082, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5363314993833757\n",
      "Step - 17035, Loss - 0.5451758718423452, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.006804228548715\n",
      "Step - 17036, Loss - 0.671927134246859, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3108362904001467\n",
      "Step - 17037, Loss - 0.6669888896018098, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2666632203959023\n",
      "Step - 17038, Loss - 0.6868645425744903, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.2084569236701274\n",
      "Step - 17039, Loss - 0.903978665204925, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2022416056643903\n",
      "Step - 17040, Loss - 0.4900810533795846, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.75244812671741\n",
      "Step - 17041, Loss - 0.7542518732848404, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7710448981534914\n",
      "Step - 17042, Loss - 0.6742802438653391, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4608424570074288\n",
      "Step - 17043, Loss - 0.4755820516568296, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6245220135468681\n",
      "Step - 17044, Loss - 0.7210309891937781, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.291146564189513\n",
      "Step - 17045, Loss - 0.804284458241175, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6505972520339016\n",
      "Step - 17046, Loss - 0.8333989889878569, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8445746107955271\n",
      "Step - 17047, Loss - 0.6125493987442019, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5667510760927911\n",
      "Step - 17048, Loss - 0.6827379313720959, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5298870041722763\n",
      "Step - 17049, Loss - 0.7485936112698549, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7419703580858595\n",
      "Step - 17050, Loss - 0.9109915454012806, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.79253363312314\n",
      "Step - 17051, Loss - 0.7359757367612068, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4298901340951442\n",
      "Step - 17052, Loss - 0.574239707868506, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9665826695968861\n",
      "Step - 17053, Loss - 0.7370541524677704, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.111869895499104\n",
      "Step - 17054, Loss - 0.6319305835152478, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7864723720683642\n",
      "Step - 17055, Loss - 0.46332154155466754, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.648776280503303\n",
      "Step - 17056, Loss - 0.8372190709209766, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4737349115857288\n",
      "Step - 17057, Loss - 0.6478626138134329, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1280785764595962\n",
      "Step - 17058, Loss - 0.7334899822036401, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5017302644629804\n",
      "Step - 17059, Loss - 0.8469107923353495, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5914132215386556\n",
      "Step - 17060, Loss - 0.7551759299598644, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6162652424629765\n",
      "Step - 17061, Loss - 0.7878794457963987, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6288478877265395\n",
      "Step - 17062, Loss - 0.6927478984299067, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9563318855308913\n",
      "Step - 17063, Loss - 0.7135199519522526, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5585589902541909\n",
      "Step - 17064, Loss - 0.609070631073291, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7610199499064476\n",
      "Step - 17065, Loss - 0.8180355005033962, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.1632178206559053\n",
      "Step - 17066, Loss - 0.7826125753946795, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8658624554400691\n",
      "Step - 17067, Loss - 0.42617864597925514, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5676210714999425\n",
      "Step - 17068, Loss - 0.5873140091607483, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3313646457453792\n",
      "Step - 17069, Loss - 0.6652588334343019, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0406922278168176\n",
      "Step - 17070, Loss - 0.8496273510643089, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.162346353266385\n",
      "Step - 17071, Loss - 0.583047584763928, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.595919439219425\n",
      "Step - 17072, Loss - 0.5677278900356169, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.013211139844107\n",
      "Step - 17073, Loss - 0.8414379413550722, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5854830821564417\n",
      "Step - 17074, Loss - 0.6975042908941809, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1436924104303154\n",
      "Step - 17075, Loss - 0.655583415782, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.108472532931132\n",
      "Step - 17076, Loss - 0.5912561441306138, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6872050647743869\n",
      "Step - 17077, Loss - 0.5760755335893304, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.766466008087285\n",
      "Step - 17078, Loss - 0.7128941068700606, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8446361498377066\n",
      "Step - 17079, Loss - 0.8226151358366197, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7974313850783145\n",
      "Step - 17080, Loss - 0.7055242437088135, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.37115794456225\n",
      "Step - 17081, Loss - 1.009188072056571, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6309438402137517\n",
      "Step - 17082, Loss - 0.7260744552636632, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5633050947580855\n",
      "Step - 17083, Loss - 0.5599001881921755, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7802534553105499\n",
      "Step - 17084, Loss - 0.5515881927550913, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7823369889139243\n",
      "Step - 17085, Loss - 0.6220808286511201, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7528609914061292\n",
      "Step - 17086, Loss - 0.7404580695861439, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7763392768192906\n",
      "Step - 17087, Loss - 0.6386043036865747, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.3823585321609509\n",
      "Step - 17088, Loss - 0.49263766603780257, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8493117952302227\n",
      "Step - 17089, Loss - 0.9219431560042252, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9001013275949825\n",
      "Step - 17090, Loss - 0.7104485406855514, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2067544535106143\n",
      "Step - 17091, Loss - 0.7636407122094012, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.509326042484956\n",
      "Step - 17092, Loss - 0.7557910625529848, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.233410919725264\n",
      "Step - 17093, Loss - 0.7007763278731799, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8129698410415382\n",
      "Step - 17094, Loss - 0.6562770248258123, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8681359393982762\n",
      "Step - 17095, Loss - 0.8311891044642061, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.049469793630414\n",
      "Step - 17096, Loss - 0.6562744273710462, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4516232491108003\n",
      "Step - 17097, Loss - 0.7232820528203127, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.10330326241425389\n",
      "Step - 17098, Loss - 0.6623797386489958, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8401691357145173\n",
      "Step - 17099, Loss - 0.5894878701570653, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5823260401586994\n",
      "Step - 17100, Loss - 0.8346482746252838, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.074872351671062\n",
      "Step - 17101, Loss - 0.8108580789204508, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.329839904164728\n",
      "Step - 17102, Loss - 1.0771461304728323, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7270857307799058\n",
      "Step - 17103, Loss - 0.644578210632245, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7946844303097303\n",
      "Step - 17104, Loss - 0.54402449991795, Learning Rate - 3.814697265625e-07, magnitude of gradient - 3.37840872789412\n",
      "Step - 17105, Loss - 0.6323972672305049, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2713962700230952\n",
      "Step - 17106, Loss - 0.7557001871661566, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.04404873759891\n",
      "Step - 17107, Loss - 0.6163129309692409, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7080777601129452\n",
      "Step - 17108, Loss - 0.8378041503420472, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5747115251893509\n",
      "Step - 17109, Loss - 0.7888924206115826, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.753137345157687\n",
      "Step - 17110, Loss - 0.770652903730609, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5139356972008984\n",
      "Step - 17111, Loss - 0.7318943909143405, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.3711666926945343\n",
      "Step - 17112, Loss - 0.7228509958154994, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.899317098772673\n",
      "Step - 17113, Loss - 0.6534802445377342, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2741916011024847\n",
      "Step - 17114, Loss - 0.7461573587028022, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4350754162588886\n",
      "Step - 17115, Loss - 0.7328190318621058, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8534060477931664\n",
      "Step - 17116, Loss - 0.598451226601818, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.550300981712394\n",
      "Step - 17117, Loss - 0.645102065531849, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1809039472825016\n",
      "Step - 17118, Loss - 0.5314568983117903, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.22860858372479642\n",
      "Step - 17119, Loss - 0.7353792204101892, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9743696327691956\n",
      "Step - 17120, Loss - 0.5768248660338169, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7084770253058751\n",
      "Step - 17121, Loss - 0.69530611314011, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4478695661143548\n",
      "Step - 17122, Loss - 0.7952055350384992, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8607926293813439\n",
      "Step - 17123, Loss - 0.6127949309035717, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.375452039752534\n",
      "Step - 17124, Loss - 0.7920681964614266, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6178493285824607\n",
      "Step - 17125, Loss - 0.7608579858748004, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.865269309355288\n",
      "Step - 17126, Loss - 0.8414659855756488, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.0687892981603073\n",
      "Step - 17127, Loss - 0.7895174772492345, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7013124697998393\n",
      "Step - 17128, Loss - 0.7109689267440501, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3333746156619721\n",
      "Step - 17129, Loss - 0.8064069523614114, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.66341215850814\n",
      "Step - 17130, Loss - 0.7935379414752192, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2375465721628989\n",
      "Step - 17131, Loss - 0.5501845190074222, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9633203461664157\n",
      "Step - 17132, Loss - 0.7194037046743554, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7888849879963478\n",
      "Step - 17133, Loss - 0.49819090633961893, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.3973077653521529\n",
      "Step - 17134, Loss - 0.789028073759566, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0010570740240567\n",
      "Step - 17135, Loss - 0.6413667393378364, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.515110722752637\n",
      "Step - 17136, Loss - 0.7864375061783194, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.035799245862707\n",
      "Step - 17137, Loss - 0.6805974852308244, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0857695740130395\n",
      "Step - 17138, Loss - 0.7100984261974607, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.083329381804807\n",
      "Step - 17139, Loss - 0.6167638860617913, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1414431314888864\n",
      "Step - 17140, Loss - 0.511760743562959, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6859565816569015\n",
      "Step - 17141, Loss - 0.7827459453766705, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7274572022325558\n",
      "Step - 17142, Loss - 0.6200959514908602, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9352070400560606\n",
      "Step - 17143, Loss - 0.7221380445173442, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8500485494184269\n",
      "Step - 17144, Loss - 0.6096524598848578, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8529653719403963\n",
      "Step - 17145, Loss - 0.6575171974615014, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.9121801198995894\n",
      "Step - 17146, Loss - 0.9038251043911848, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5720216424239049\n",
      "Step - 17147, Loss - 0.7914457837027259, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2708925576477932\n",
      "Step - 17148, Loss - 0.8911132204854108, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4245480441509255\n",
      "Step - 17149, Loss - 0.7501374103243206, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4989975411584328\n",
      "Step - 17150, Loss - 0.7362965910441025, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1233372233900392\n",
      "Step - 17151, Loss - 0.7742888963035723, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2954694279749628\n",
      "Step - 17152, Loss - 0.46485860166935683, Learning Rate - 3.814697265625e-07, magnitude of gradient - 3.433334823074283\n",
      "Step - 17153, Loss - 0.7083950742549805, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6957971941001262\n",
      "Step - 17154, Loss - 0.6781887049786952, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8984503113404166\n",
      "Step - 17155, Loss - 0.6601268339329888, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.183740168790616\n",
      "Step - 17156, Loss - 0.8014821640529688, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7656146809922377\n",
      "Step - 17157, Loss - 0.9601190446799739, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.9582461814007837\n",
      "Step - 17158, Loss - 0.7347659643377982, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.31046933807240584\n",
      "Step - 17159, Loss - 0.8147949484224877, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3048426665334723\n",
      "Step - 17160, Loss - 0.7100638658980016, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9517968696716904\n",
      "Step - 17161, Loss - 0.7413806682357441, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1856104757591122\n",
      "Step - 17162, Loss - 0.8311520497658021, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.440993166857084\n",
      "Step - 17163, Loss - 0.7495999125931232, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.1367455407404914\n",
      "Step - 17164, Loss - 0.7540233792120941, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1661108202307962\n",
      "Step - 17165, Loss - 0.8807569514985792, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7019717578902696\n",
      "Step - 17166, Loss - 0.6351156842355459, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8164593285920734\n",
      "Step - 17167, Loss - 0.6182182801999697, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3146400690670543\n",
      "Step - 17168, Loss - 0.6499619101385117, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6621427553798591\n",
      "Step - 17169, Loss - 0.7417626283018729, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.3639896003084729\n",
      "Step - 17170, Loss - 0.6545908367635298, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.234669239026423\n",
      "Step - 17171, Loss - 0.8727030226822048, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6908940307406168\n",
      "Step - 17172, Loss - 0.8047007943519539, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5958020100145993\n",
      "Step - 17173, Loss - 0.6908325749864491, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.9975266156625693\n",
      "Step - 17174, Loss - 0.7071868131924586, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3184721988390848\n",
      "Step - 17175, Loss - 0.4644362746805673, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4168375049201638\n",
      "Step - 17176, Loss - 0.8383012989618575, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.9267614382891145\n",
      "Step - 17177, Loss - 0.6455950280731501, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5160073689700604\n",
      "Step - 17178, Loss - 0.6969828254918974, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0315840856757312\n",
      "Step - 17179, Loss - 0.8865695731840314, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7246937516281734\n",
      "Step - 17180, Loss - 0.6641310835057874, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2740194451213234\n",
      "Step - 17181, Loss - 0.6448119898660323, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.224835475984148\n",
      "Step - 17182, Loss - 0.6818245825058538, Learning Rate - 3.814697265625e-07, magnitude of gradient - 3.0860723714674694\n",
      "Step - 17183, Loss - 0.7121906010654002, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0975814848641008\n",
      "Step - 17184, Loss - 0.5659434929894416, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4033254452601507\n",
      "Step - 17185, Loss - 0.7858024540893499, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7994810982352546\n",
      "Step - 17186, Loss - 0.5946529969761631, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5936313723483052\n",
      "Step - 17187, Loss - 0.7112350823409244, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9507979381608676\n",
      "Step - 17188, Loss - 0.5503327921439698, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2981338358600123\n",
      "Step - 17189, Loss - 0.832841072429281, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.310988746272753\n",
      "Step - 17190, Loss - 0.8140192391603517, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0480520934686999\n",
      "Step - 17191, Loss - 0.7726559129034573, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.675459385375629\n",
      "Step - 17192, Loss - 0.7537980359736205, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4537486912985562\n",
      "Step - 17193, Loss - 0.7789280160434399, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1242641288706674\n",
      "Step - 17194, Loss - 0.7133454108630515, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.62992274393353\n",
      "Step - 17195, Loss - 0.5693234172904966, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4533550614867756\n",
      "Step - 17196, Loss - 0.7362461968819688, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3003392161983411\n",
      "Step - 17197, Loss - 0.7098077913707328, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4688408154055528\n",
      "Step - 17198, Loss - 0.681903155205898, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5347832691843795\n",
      "Step - 17199, Loss - 0.6943898840284504, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.804873947816968\n",
      "Step - 17200, Loss - 0.6528935197788224, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1224729393499828\n",
      "Step - 17201, Loss - 0.7282141625156002, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5916221553136065\n",
      "Step - 17202, Loss - 0.6198795765595293, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.746224613498784\n",
      "Step - 17203, Loss - 0.9072861345599021, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6415320124768316\n",
      "Step - 17204, Loss - 0.7978930840699806, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0696698387752406\n",
      "Step - 17205, Loss - 0.8093456845591399, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2581498805617397\n",
      "Step - 17206, Loss - 0.797003095897711, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8217620102687655\n",
      "Step - 17207, Loss - 0.6223051170415557, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6717383504534861\n",
      "Step - 17208, Loss - 0.8148038942669082, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5450109858207994\n",
      "Step - 17209, Loss - 0.6862809669913996, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.2800339166244341\n",
      "Step - 17210, Loss - 0.6053434010339843, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2491324427183172\n",
      "Step - 17211, Loss - 0.7036437423658488, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1255651882801816\n",
      "Step - 17212, Loss - 0.7074318106073547, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4002401992962614\n",
      "Step - 17213, Loss - 0.815263170387066, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7398766678986581\n",
      "Step - 17214, Loss - 1.0576824968428467, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2581373713429067\n",
      "Step - 17215, Loss - 0.7101140677077249, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7210691217608235\n",
      "Step - 17216, Loss - 0.5756178761895062, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.1121122700777875\n",
      "Step - 17217, Loss - 0.6427698737314435, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5130866518284357\n",
      "Step - 17218, Loss - 0.690066392849245, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7419285388528181\n",
      "Step - 17219, Loss - 0.9505095914375056, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5702132062368985\n",
      "Step - 17220, Loss - 0.7695928279401082, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1071963136793728\n",
      "Step - 17221, Loss - 0.609239656353304, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2855674939829371\n",
      "Step - 17222, Loss - 0.8169360234223177, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1245977931334787\n",
      "Step - 17223, Loss - 0.5918581818988641, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0610475457875586\n",
      "Step - 17224, Loss - 0.785586552666762, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.846711859236868\n",
      "Step - 17225, Loss - 0.6920832787033597, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2836011118453763\n",
      "Step - 17226, Loss - 0.7095248590825372, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5020346883478201\n",
      "Step - 17227, Loss - 0.6013364446547272, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8472181178487178\n",
      "Step - 17228, Loss - 0.710659513934227, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0632640106048115\n",
      "Step - 17229, Loss - 0.7984259290425293, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.3279866412662821\n",
      "Step - 17230, Loss - 0.5604053069218297, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.9394505012921752\n",
      "Step - 17231, Loss - 0.48979040460051176, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8301605333117256\n",
      "Step - 17232, Loss - 0.617511341103334, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.3681080023256504\n",
      "Step - 17233, Loss - 0.5476918367120643, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9060166446877272\n",
      "Step - 17234, Loss - 0.8236649264957157, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1608011720026437\n",
      "Step - 17235, Loss - 1.0526091607155508, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.297185086903799\n",
      "Step - 17236, Loss - 0.6842739553361079, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7797794979512156\n",
      "Step - 17237, Loss - 0.6669795851019872, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1770772623154673\n",
      "Step - 17238, Loss - 0.6456218330485952, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1130805445146175\n",
      "Step - 17239, Loss - 0.666938071296948, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.4013711687614543\n",
      "Step - 17240, Loss - 0.6428780374529103, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.975443118239911\n",
      "Step - 17241, Loss - 0.6540471933152949, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6372902391269825\n",
      "Step - 17242, Loss - 0.7330713973811399, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.7486800402203797\n",
      "Step - 17243, Loss - 0.6523773082010996, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9780165358203574\n",
      "Step - 17244, Loss - 0.8803442919210989, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8361967319637882\n",
      "Step - 17245, Loss - 0.6505819275486701, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7944897941371647\n",
      "Step - 17246, Loss - 0.4901780897777316, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2418482928497059\n",
      "Step - 17247, Loss - 0.6803633676213349, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0010370351630429\n",
      "Step - 17248, Loss - 0.8400370908879882, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.074803829237093\n",
      "Step - 17249, Loss - 0.7062963332660346, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0303031427495932\n",
      "Step - 17250, Loss - 0.7934012473197618, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1509959207323364\n",
      "Step - 17251, Loss - 0.6838738593405486, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5434351844179016\n",
      "Step - 17252, Loss - 0.3267502601812725, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9365930261346386\n",
      "Step - 17253, Loss - 0.6839904600219956, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7070366699930977\n",
      "Step - 17254, Loss - 0.7566108198430654, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8980670169240362\n",
      "Step - 17255, Loss - 0.6603182056929804, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3299259175033635\n",
      "Step - 17256, Loss - 0.6589236411061127, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.679407176335181\n",
      "Step - 17257, Loss - 0.8618462639996091, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7830280442447624\n",
      "Step - 17258, Loss - 0.7987389820716941, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8477369578363974\n",
      "Step - 17259, Loss - 0.734256772857582, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5831679639777607\n",
      "Step - 17260, Loss - 0.7048194437914808, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9484049026327307\n",
      "Step - 17261, Loss - 0.7572288849298328, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.9864461013578076\n",
      "Step - 17262, Loss - 0.8117037144946333, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5900078675532455\n",
      "Step - 17263, Loss - 0.7341417961654676, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9761931001812245\n",
      "Step - 17264, Loss - 0.8394991221390207, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7886462886800003\n",
      "Step - 17265, Loss - 0.728096532847343, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9732075579039874\n",
      "Step - 17266, Loss - 0.6450141787302018, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.35207784276867293\n",
      "Step - 17267, Loss - 0.6695535211451643, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9157970508599034\n",
      "Step - 17268, Loss - 0.7071865186353596, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.340278599456414\n",
      "Step - 17269, Loss - 0.5896137195103426, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1969710943686327\n",
      "Step - 17270, Loss - 0.6772168357329025, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0689589914435398\n",
      "Step - 17271, Loss - 0.6187363625377876, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8730967735221545\n",
      "Step - 17272, Loss - 0.831511419407468, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.540290391309304\n",
      "Step - 17273, Loss - 0.7001343832424867, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7582534306245121\n",
      "Step - 17274, Loss - 0.6623677201851503, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.3639770091133611\n",
      "Step - 17275, Loss - 0.48878083315538184, Learning Rate - 3.814697265625e-07, magnitude of gradient - 3.3387200157254835\n",
      "Step - 17276, Loss - 0.6745795666279826, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8074358012515666\n",
      "Step - 17277, Loss - 0.488697512105272, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3763577148253638\n",
      "Step - 17278, Loss - 0.6655812455602114, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0512290376553501\n",
      "Step - 17279, Loss - 0.7522644636568612, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.24057010263591197\n",
      "Step - 17280, Loss - 0.7138243412273176, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2264932632837058\n",
      "Step - 17281, Loss - 0.5176438369099843, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3401905804812293\n",
      "Step - 17282, Loss - 0.5082956726229111, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1282520455326204\n",
      "Step - 17283, Loss - 0.6723179654196485, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.846170550854622\n",
      "Step - 17284, Loss - 0.6743797580923967, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9131048852318302\n",
      "Step - 17285, Loss - 0.6028815092242209, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6046209809912756\n",
      "Step - 17286, Loss - 0.8732561632694644, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6400225370651718\n",
      "Step - 17287, Loss - 0.6552035461280772, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3165304218700558\n",
      "Step - 17288, Loss - 0.7109621289322577, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9933209317400439\n",
      "Step - 17289, Loss - 0.835331265236172, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.0057450300703814\n",
      "Step - 17290, Loss - 0.6655020445448476, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3160882487938659\n",
      "Step - 17291, Loss - 0.808669137043499, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7601187286826427\n",
      "Step - 17292, Loss - 0.694784410720121, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5158688764368089\n",
      "Step - 17293, Loss - 0.5364337685979772, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9359520819288543\n",
      "Step - 17294, Loss - 0.6122744655248549, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.942653897504375\n",
      "Step - 17295, Loss - 0.6714639654671141, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8451506721042533\n",
      "Step - 17296, Loss - 0.7465599556149483, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.766852712980695\n",
      "Step - 17297, Loss - 0.6488052290341305, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.9805808850694628\n",
      "Step - 17298, Loss - 0.5628868817290189, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6768203879394689\n",
      "Step - 17299, Loss - 0.6355319121751835, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.41909359370268745\n",
      "Step - 17300, Loss - 0.7670470285838813, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.691062018499251\n",
      "Step - 17301, Loss - 0.6433716832293136, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6341887542873732\n",
      "Step - 17302, Loss - 0.825726675248413, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4454199148651408\n",
      "Step - 17303, Loss - 0.7826397356026116, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7578495457354711\n",
      "Step - 17304, Loss - 0.7966074970354334, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8939013910133975\n",
      "Step - 17305, Loss - 0.6692528320132428, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6845918849288588\n",
      "Step - 17306, Loss - 0.5932543739650371, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7957439337924133\n",
      "Step - 17307, Loss - 0.6456042964050062, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4043269699149544\n",
      "Step - 17308, Loss - 0.9718254937720859, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8675480418401262\n",
      "Step - 17309, Loss - 0.4255398287694025, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.882466904404047\n",
      "Step - 17310, Loss - 0.7835964589776317, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.3681480727788515\n",
      "Step - 17311, Loss - 0.6053100766101174, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.165012088824362\n",
      "Step - 17312, Loss - 0.6537657565244567, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.155297781614191\n",
      "Step - 17313, Loss - 0.6699350545157492, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.739717502424223\n",
      "Step - 17314, Loss - 0.8684763337485852, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3477584054502036\n",
      "Step - 17315, Loss - 0.6164488514535469, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5898773678240734\n",
      "Step - 17316, Loss - 0.8066861409376115, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1357037104078778\n",
      "Step - 17317, Loss - 0.5142659734409231, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6122395449639848\n",
      "Step - 17318, Loss - 0.8015986649123532, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4170870028783595\n",
      "Step - 17319, Loss - 0.7169556151891893, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7555530243073513\n",
      "Step - 17320, Loss - 0.7661015304232208, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7489930148829231\n",
      "Step - 17321, Loss - 0.5055953163561306, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.322515134380394\n",
      "Step - 17322, Loss - 0.6681581760030638, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6818937050985711\n",
      "Step - 17323, Loss - 0.6239358654932601, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.500805530380019\n",
      "Step - 17324, Loss - 0.6634627995174709, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9655191346521486\n",
      "Step - 17325, Loss - 0.7973881118167457, Learning Rate - 3.814697265625e-07, magnitude of gradient - 3.140775982327459\n",
      "Step - 17326, Loss - 0.6907630797754942, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.44007877553421515\n",
      "Step - 17327, Loss - 0.601808236243179, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.403226255204851\n",
      "Step - 17328, Loss - 0.6550798123485907, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2718004429404532\n",
      "Step - 17329, Loss - 0.725285845191088, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.121823940157129\n",
      "Step - 17330, Loss - 0.7650141581209662, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.48426805460797756\n",
      "Step - 17331, Loss - 0.7389530868415287, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3508699227137386\n",
      "Step - 17332, Loss - 0.7067330414917344, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4741931214773039\n",
      "Step - 17333, Loss - 0.5392694612127404, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0708564811548607\n",
      "Step - 17334, Loss - 0.6304486868488243, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7752240026669553\n",
      "Step - 17335, Loss - 0.6279081790241832, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4097920189223982\n",
      "Step - 17336, Loss - 0.8132984078999355, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.87265224223819\n",
      "Step - 17337, Loss - 0.5806047899230813, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4561650411708673\n",
      "Step - 17338, Loss - 0.9857951633252423, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.3932193588560016\n",
      "Step - 17339, Loss - 0.7181157820356902, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0725614291296997\n",
      "Step - 17340, Loss - 0.6386165679153493, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4771742839543638\n",
      "Step - 17341, Loss - 0.6577098050616503, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3215161582890553\n",
      "Step - 17342, Loss - 0.6182920349736949, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1713837886235556\n",
      "Step - 17343, Loss - 0.6975368123211019, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3772874317960302\n",
      "Step - 17344, Loss - 0.5897234084357392, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.08997830402721\n",
      "Step - 17345, Loss - 0.7652430877052077, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7309827159624904\n",
      "Step - 17346, Loss - 0.7970747713383299, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0367665282004415\n",
      "Step - 17347, Loss - 0.6354401930694888, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3515786335628102\n",
      "Step - 17348, Loss - 0.7468616354578084, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.365611059152868\n",
      "Step - 17349, Loss - 0.7793290844698899, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1570902949884656\n",
      "Step - 17350, Loss - 0.5497220546617283, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.009397836648719\n",
      "Step - 17351, Loss - 0.7700054990461505, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4261782044138235\n",
      "Step - 17352, Loss - 0.7337727982549582, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7293647807181554\n",
      "Step - 17353, Loss - 0.7037102929828967, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.881883930923116\n",
      "Step - 17354, Loss - 0.7888049266123501, Learning Rate - 3.814697265625e-07, magnitude of gradient - 3.3287872673629737\n",
      "Step - 17355, Loss - 0.8402936662095328, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.3969306801843757\n",
      "Step - 17356, Loss - 0.5635487077646983, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7231780456571653\n",
      "Step - 17357, Loss - 0.6406957406192852, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.157037214169229\n",
      "Step - 17358, Loss - 0.600519804632409, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.0145337935861747\n",
      "Step - 17359, Loss - 0.6347242596055105, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9990919121258988\n",
      "Step - 17360, Loss - 0.7987705643313233, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.544614410960457\n",
      "Step - 17361, Loss - 0.7526322649663836, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3592779107389819\n",
      "Step - 17362, Loss - 0.467473473090591, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1962664205836857\n",
      "Step - 17363, Loss - 0.5290475573866527, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8700418594079179\n",
      "Step - 17364, Loss - 0.4864509936102924, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7294478717119454\n",
      "Step - 17365, Loss - 0.6201306518348763, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.835945570426334\n",
      "Step - 17366, Loss - 0.5679160334625414, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4679241591918555\n",
      "Step - 17367, Loss - 0.5915115976849097, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9604669300254345\n",
      "Step - 17368, Loss - 0.5719980600700397, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0681162940387106\n",
      "Step - 17369, Loss - 0.677013262599438, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7004893624342861\n",
      "Step - 17370, Loss - 0.6189114775074519, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8095459735635244\n",
      "Step - 17371, Loss - 0.8619580011693828, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.3000420654613327\n",
      "Step - 17372, Loss - 0.7378501396304946, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1415099420219996\n",
      "Step - 17373, Loss - 0.6140095174589429, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9975786627425646\n",
      "Step - 17374, Loss - 0.5617089228952111, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5936159271890978\n",
      "Step - 17375, Loss - 0.7423629526184097, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6584116366516093\n",
      "Step - 17376, Loss - 0.8214717351734857, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7655455261157551\n",
      "Step - 17377, Loss - 0.8530006902378394, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3778632115804839\n",
      "Step - 17378, Loss - 0.7725569011402235, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7162316606852528\n",
      "Step - 17379, Loss - 0.7735739963422428, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.872443101889827\n",
      "Step - 17380, Loss - 0.6959460588185377, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.235962320142227\n",
      "Step - 17381, Loss - 0.6476767378067978, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6204699407794829\n",
      "Step - 17382, Loss - 0.7386356493242479, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9648392755903282\n",
      "Step - 17383, Loss - 0.5603042861974762, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7081266473289933\n",
      "Step - 17384, Loss - 0.9238734245345022, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9194059062008683\n",
      "Step - 17385, Loss - 0.6895750601456233, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9048943697624556\n",
      "Step - 17386, Loss - 0.5840541378385751, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8092931285469599\n",
      "Step - 17387, Loss - 0.6511595454323434, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1624262970912256\n",
      "Step - 17388, Loss - 0.6753781443963967, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6746688604960231\n",
      "Step - 17389, Loss - 0.8451800487486087, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.897753599233724\n",
      "Step - 17390, Loss - 0.6446600952718903, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9576046465490322\n",
      "Step - 17391, Loss - 0.7147622175515069, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6216909672624573\n",
      "Step - 17392, Loss - 0.5898359736844229, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8302412842059989\n",
      "Step - 17393, Loss - 0.5781108902551976, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.0016630699240276\n",
      "Step - 17394, Loss - 0.7410719588419439, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.023817618686369\n",
      "Step - 17395, Loss - 0.6483855908665463, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.258874376955264\n",
      "Step - 17396, Loss - 0.6888893169683474, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.07664075017333\n",
      "Step - 17397, Loss - 0.4567265400377109, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.17945940576724168\n",
      "Step - 17398, Loss - 0.5905053979406455, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.571758131502454\n",
      "Step - 17399, Loss - 0.7630145849645469, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3138053401866832\n",
      "Step - 17400, Loss - 0.7107387909430415, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.882734809932659\n",
      "Step - 17401, Loss - 0.5953106267040955, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4942979659555233\n",
      "Step - 17402, Loss - 0.8347739567536205, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5451180946237202\n",
      "Step - 17403, Loss - 0.6647313036005429, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8620841111979883\n",
      "Step - 17404, Loss - 0.6225782340962154, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6670957967325555\n",
      "Step - 17405, Loss - 0.7665983450155138, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8283100546165073\n",
      "Step - 17406, Loss - 0.7689361248612601, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6217359814699192\n",
      "Step - 17407, Loss - 0.6544383989119511, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7109107611274774\n",
      "Step - 17408, Loss - 0.7400695340777501, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.2318453114019805\n",
      "Step - 17409, Loss - 0.855545968508161, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1707045423796731\n",
      "Step - 17410, Loss - 0.47407895745863043, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1283431278984548\n",
      "Step - 17411, Loss - 0.7986861974956094, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.069771498058118\n",
      "Step - 17412, Loss - 0.7955147677178865, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.1473456412955367\n",
      "Step - 17413, Loss - 0.5453002334310922, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9410416627367552\n",
      "Step - 17414, Loss - 0.6801883756363648, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9019728008088107\n",
      "Step - 17415, Loss - 0.7476295022442525, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5665743241681126\n",
      "Step - 17416, Loss - 0.6574280980654638, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.215678182009971\n",
      "Step - 17417, Loss - 0.8255222454589716, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7476561399988564\n",
      "Step - 17418, Loss - 0.7745271192960038, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.352934469916129\n",
      "Step - 17419, Loss - 0.5981404298868253, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6343423649823046\n",
      "Step - 17420, Loss - 0.731579320465132, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5474261898361407\n",
      "Step - 17421, Loss - 0.636069906212948, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.4018542618108607\n",
      "Step - 17422, Loss - 0.7880998476825268, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.33652447394710133\n",
      "Step - 17423, Loss - 0.750747681118289, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7661575743181186\n",
      "Step - 17424, Loss - 0.6051996403681104, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2412076275985546\n",
      "Step - 17425, Loss - 0.689138528511737, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3854271036518386\n",
      "Step - 17426, Loss - 0.6963738138291335, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.3640409642778072\n",
      "Step - 17427, Loss - 0.763039321520681, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8734412898356507\n",
      "Step - 17428, Loss - 0.6542590313007904, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0506997120699373\n",
      "Step - 17429, Loss - 0.39991778577077736, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9795435396088303\n",
      "Step - 17430, Loss - 0.8411881300932565, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8815802330789537\n",
      "Step - 17431, Loss - 0.6706446614792211, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2194573431464821\n",
      "Step - 17432, Loss - 0.6232728596221928, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8359714611712771\n",
      "Step - 17433, Loss - 0.7623763761096882, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4705040851030613\n",
      "Step - 17434, Loss - 0.8041844394522465, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5989844816639351\n",
      "Step - 17435, Loss - 0.8549659879646268, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1621238125698885\n",
      "Step - 17436, Loss - 0.8254570570864876, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6888825322370867\n",
      "Step - 17437, Loss - 0.6836739234801076, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6941988668599844\n",
      "Step - 17438, Loss - 0.636588847967084, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7692314150154924\n",
      "Step - 17439, Loss - 0.8632730467391043, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.201169222466895\n",
      "Step - 17440, Loss - 0.8102413962023247, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1245979633504817\n",
      "Step - 17441, Loss - 0.565305976996249, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.48927663325411\n",
      "Step - 17442, Loss - 0.6696875746866287, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.794153684774541\n",
      "Step - 17443, Loss - 0.6192684122402257, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.813475988006681\n",
      "Step - 17444, Loss - 0.4530087098974118, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.433000806901211\n",
      "Step - 17445, Loss - 0.7112591498758117, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4939470404250214\n",
      "Step - 17446, Loss - 0.6071343291146444, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.39625807899875765\n",
      "Step - 17447, Loss - 0.482802548323654, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.322391731911095\n",
      "Step - 17448, Loss - 0.6822853726663134, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.727748382941116\n",
      "Step - 17449, Loss - 0.7509886443002043, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3376949098284903\n",
      "Step - 17450, Loss - 0.7229718848320822, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5896519776108995\n",
      "Step - 17451, Loss - 0.6784935413588469, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.350291100093369\n",
      "Step - 17452, Loss - 0.6358889458991291, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1287107820103879\n",
      "Step - 17453, Loss - 0.6888976235044129, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.9959159248176157\n",
      "Step - 17454, Loss - 0.6074090676030955, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.48660052207514026\n",
      "Step - 17455, Loss - 0.8300759092617035, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7012558894398007\n",
      "Step - 17456, Loss - 0.7572956500157916, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.34862890587133805\n",
      "Step - 17457, Loss - 0.6397552181384442, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0158542162186404\n",
      "Step - 17458, Loss - 0.85454922486376, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7340011117442352\n",
      "Step - 17459, Loss - 0.7002318955839884, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9816701732294486\n",
      "Step - 17460, Loss - 0.8277961404694664, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8823759577889795\n",
      "Step - 17461, Loss - 0.8629863487473657, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8446283158856331\n",
      "Step - 17462, Loss - 0.9130571558658318, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.892647323838115\n",
      "Step - 17463, Loss - 0.7386556471373237, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.833222393691783\n",
      "Step - 17464, Loss - 0.7510232555596533, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8794694583166591\n",
      "Step - 17465, Loss - 0.795645197358638, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6721804420825086\n",
      "Step - 17466, Loss - 0.8008098761606477, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8186723902857305\n",
      "Step - 17467, Loss - 0.7713596060383727, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.054183238948437\n",
      "Step - 17468, Loss - 0.7388916374080448, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.4839721906098924\n",
      "Step - 17469, Loss - 0.8147120280240209, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.945178507533027\n",
      "Step - 17470, Loss - 0.6205933647031677, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5307183412025122\n",
      "Step - 17471, Loss - 0.7989917964731419, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.700057446769189\n",
      "Step - 17472, Loss - 0.7612603269648888, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9699697815992037\n",
      "Step - 17473, Loss - 0.6966279299798583, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.27476963687385036\n",
      "Step - 17474, Loss - 0.9431762011442408, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.051093124243131\n",
      "Step - 17475, Loss - 0.7996743108934976, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0892186433810416\n",
      "Step - 17476, Loss - 0.6029792743424981, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.5084962961296946\n",
      "Step - 17477, Loss - 0.7180373146966128, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5098836367780808\n",
      "Step - 17478, Loss - 0.5339224175803475, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.489550073035208\n",
      "Step - 17479, Loss - 0.7266478102470662, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9736515780388821\n",
      "Step - 17480, Loss - 0.6600008328706954, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5800780848369687\n",
      "Step - 17481, Loss - 0.7095842257978641, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.819767887753044\n",
      "Step - 17482, Loss - 0.7837463526610543, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7132191027603842\n",
      "Step - 17483, Loss - 0.7347778419688713, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0592146002739886\n",
      "Step - 17484, Loss - 0.7255030855078536, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5212088652350737\n",
      "Step - 17485, Loss - 0.9093983356732352, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.9044491278327123\n",
      "Step - 17486, Loss - 0.7669505688715573, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.571290381759914\n",
      "Step - 17487, Loss - 0.6826333765103634, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5417082690269606\n",
      "Step - 17488, Loss - 0.8007255302418124, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4776426162558246\n",
      "Step - 17489, Loss - 0.5476232488783414, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3839855398934995\n",
      "Step - 17490, Loss - 0.7331798376126453, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.315034776271231\n",
      "Step - 17491, Loss - 0.5404406746413963, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3473864546141667\n",
      "Step - 17492, Loss - 0.9819341608380089, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8860477364482369\n",
      "Step - 17493, Loss - 0.6057281311052103, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.063004678726707\n",
      "Step - 17494, Loss - 0.5696225549029788, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2776750755998796\n",
      "Step - 17495, Loss - 0.7928844532762643, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6115585002894959\n",
      "Step - 17496, Loss - 0.8102128805648914, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2936246198763692\n",
      "Step - 17497, Loss - 0.8781144005513406, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4780617465452435\n",
      "Step - 17498, Loss - 0.7885350593781528, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.239195373723341\n",
      "Step - 17499, Loss - 0.5967914330174177, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6696849489666415\n",
      "Step - 17500, Loss - 0.5482251214241081, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6591968221183248\n",
      "Step - 17501, Loss - 0.9016915745696824, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.885021506110527\n",
      "Step - 17502, Loss - 0.6575761159771677, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9598320541269691\n",
      "Step - 17503, Loss - 0.9424130207871713, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6734294767206217\n",
      "Step - 17504, Loss - 0.8344011542065496, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8162888624158742\n",
      "Step - 17505, Loss - 0.9187204684550005, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.550244003468954\n",
      "Step - 17506, Loss - 0.7632990208670364, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5788690587776711\n",
      "Step - 17507, Loss - 0.7555307730075893, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4238152275887293\n",
      "Step - 17508, Loss - 0.7763422741504826, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5813825377348576\n",
      "Step - 17509, Loss - 0.6333558327057265, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.3474104556872755\n",
      "Step - 17510, Loss - 0.6744672070098997, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2359183521569583\n",
      "Step - 17511, Loss - 0.8160637480383387, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8332939289981547\n",
      "Step - 17512, Loss - 0.6804422991849979, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.424777128677624\n",
      "Step - 17513, Loss - 0.5233254974754967, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6652051710294251\n",
      "Step - 17514, Loss - 0.7803322023086057, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8853993491269518\n",
      "Step - 17515, Loss - 0.5628124478088825, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.49254705872625626\n",
      "Step - 17516, Loss - 0.7030476260289358, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.756836707834561\n",
      "Step - 17517, Loss - 0.7285842117113144, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.668693847080705\n",
      "Step - 17518, Loss - 0.6931208718281535, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.763296355308618\n",
      "Step - 17519, Loss - 0.6771270924684647, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.2637826076184459\n",
      "Step - 17520, Loss - 0.8917757136207722, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5199855738158963\n",
      "Step - 17521, Loss - 0.4979111242997811, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.157155102992832\n",
      "Step - 17522, Loss - 0.7577523891646746, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7521048229423637\n",
      "Step - 17523, Loss - 0.7438311923673475, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.898744989712672\n",
      "Step - 17524, Loss - 0.6873817683978648, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8207998804082829\n",
      "Step - 17525, Loss - 0.7061708354348022, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0765878094211958\n",
      "Step - 17526, Loss - 0.5797618537742686, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2001362986022808\n",
      "Step - 17527, Loss - 1.0346671341015492, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8122635328558157\n",
      "Step - 17528, Loss - 0.8364160300800705, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5613851426073388\n",
      "Step - 17529, Loss - 0.5836545832595539, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.1299468372625516\n",
      "Step - 17530, Loss - 0.7209221228323629, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.987652345913874\n",
      "Step - 17531, Loss - 0.9892729313432377, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.676900489900404\n",
      "Step - 17532, Loss - 0.5781623713091966, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4983971393743616\n",
      "Step - 17533, Loss - 0.6826146507796905, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3115213593343469\n",
      "Step - 17534, Loss - 0.5599040682462829, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6818404208742311\n",
      "Step - 17535, Loss - 0.7447861684838801, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.203573690103189\n",
      "Step - 17536, Loss - 0.6206110511499252, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.333280174840581\n",
      "Step - 17537, Loss - 0.6740420878894039, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3530799695137843\n",
      "Step - 17538, Loss - 0.8104207376105705, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.58310528665733\n",
      "Step - 17539, Loss - 0.654282890163725, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3080085476448073\n",
      "Step - 17540, Loss - 0.7296302507949957, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.2665125805194046\n",
      "Step - 17541, Loss - 0.8161140855545836, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.9652791610000824\n",
      "Step - 17542, Loss - 0.7105660992074262, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.47342874006268854\n",
      "Step - 17543, Loss - 0.6589616237350094, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4223296701176786\n",
      "Step - 17544, Loss - 0.5544786400038196, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8430414611432683\n",
      "Step - 17545, Loss - 0.8158580380566114, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7617417664407877\n",
      "Step - 17546, Loss - 0.9717006163567167, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0351227033660062\n",
      "Step - 17547, Loss - 0.5366686146822038, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2711159592255588\n",
      "Step - 17548, Loss - 0.743211584440074, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8175936859521588\n",
      "Step - 17549, Loss - 0.7526697991862809, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0451499028727607\n",
      "Step - 17550, Loss - 0.6583586668804113, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4771600983528717\n",
      "Step - 17551, Loss - 0.6629202214631733, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6372285236329933\n",
      "Step - 17552, Loss - 0.6886486094851232, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4072949397619907\n",
      "Step - 17553, Loss - 0.5331121108261969, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3141838218812594\n",
      "Step - 17554, Loss - 0.6387424552824043, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.159644846175099\n",
      "Step - 17555, Loss - 0.8570286337238328, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6156203126103502\n",
      "Step - 17556, Loss - 0.8662855420214404, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7059706370996668\n",
      "Step - 17557, Loss - 0.8391582843983245, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7647529702127718\n",
      "Step - 17558, Loss - 0.515023006976308, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8898188884091525\n",
      "Step - 17559, Loss - 0.6432307248893707, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.426908963749903\n",
      "Step - 17560, Loss - 0.8762792368188174, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.260530637477777\n",
      "Step - 17561, Loss - 0.8204342929526424, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6349315071569112\n",
      "Step - 17562, Loss - 0.6345859677656778, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6116345313608974\n",
      "Step - 17563, Loss - 0.6937563676738522, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0769574990959518\n",
      "Step - 17564, Loss - 0.5629783209030623, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.2508002386797532\n",
      "Step - 17565, Loss - 0.6643303652990643, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6108007680571955\n",
      "Step - 17566, Loss - 0.7803849289227441, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.41902404692900563\n",
      "Step - 17567, Loss - 0.747359183191909, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2025441778304242\n",
      "Step - 17568, Loss - 0.8845608533019909, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2896384740935398\n",
      "Step - 17569, Loss - 0.9344165518096935, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3421424576729928\n",
      "Step - 17570, Loss - 0.7503152069009096, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.253046072021896\n",
      "Step - 17571, Loss - 0.7511223537149552, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.435263154161266\n",
      "Step - 17572, Loss - 0.5847543609541592, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8499323743679329\n",
      "Step - 17573, Loss - 0.6906067965707318, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0965550998104612\n",
      "Step - 17574, Loss - 0.807802566876107, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0564714699760658\n",
      "Step - 17575, Loss - 0.7736607283064107, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6402846134820019\n",
      "Step - 17576, Loss - 0.5691797536658318, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8301094406581585\n",
      "Step - 17577, Loss - 0.6014736141735504, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.3437944812228541\n",
      "Step - 17578, Loss - 0.7468859846303346, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7523073909912383\n",
      "Step - 17579, Loss - 0.711098145013836, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8319000602010866\n",
      "Step - 17580, Loss - 0.7916782312701791, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8065487464167735\n",
      "Step - 17581, Loss - 0.7078573965439834, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.35096495869977906\n",
      "Step - 17582, Loss - 0.6656829930114467, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6869246817951287\n",
      "Step - 17583, Loss - 0.8301374006742853, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.3700843452971098\n",
      "Step - 17584, Loss - 0.9555436602842426, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9513337013562739\n",
      "Step - 17585, Loss - 0.7339367590309108, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7773842027144429\n",
      "Step - 17586, Loss - 0.7455287002923, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4033554595317632\n",
      "Step - 17587, Loss - 0.8304307443628187, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3289401365429778\n",
      "Step - 17588, Loss - 0.5203098187079652, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.306399242165381\n",
      "Step - 17589, Loss - 0.5845558224009233, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5835219011397691\n",
      "Step - 17590, Loss - 0.819608725148132, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4991202641822305\n",
      "Step - 17591, Loss - 0.4870335441049189, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4456403228617809\n",
      "Step - 17592, Loss - 0.6923210247042731, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7118358946615595\n",
      "Step - 17593, Loss - 0.637135249056715, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4720725943497124\n",
      "Step - 17594, Loss - 0.6971538013059759, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.297242887945773\n",
      "Step - 17595, Loss - 0.5411914484090012, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8573143891878575\n",
      "Step - 17596, Loss - 0.759209627183986, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.159928882892518\n",
      "Step - 17597, Loss - 0.7366276128203861, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2748650690314183\n",
      "Step - 17598, Loss - 0.7202714778526613, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5840772049149934\n",
      "Step - 17599, Loss - 0.5500016077183567, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8765721566697282\n",
      "Step - 17600, Loss - 0.5099124111313211, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8046968760563429\n",
      "Step - 17601, Loss - 0.7069953152433107, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3803543576190715\n",
      "Step - 17602, Loss - 0.6636614834186877, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.047616921404636\n",
      "Step - 17603, Loss - 0.8143228632341625, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0063457174568784\n",
      "Step - 17604, Loss - 0.6676862485689531, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5092185156965064\n",
      "Step - 17605, Loss - 0.6634169094897255, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6483725273068012\n",
      "Step - 17606, Loss - 0.7040254047687986, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.3531994774166515\n",
      "Step - 17607, Loss - 0.5551650718843606, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9836346997648847\n",
      "Step - 17608, Loss - 0.8279881475020109, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5341704303145518\n",
      "Step - 17609, Loss - 0.7393825873628496, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.75564826839863\n",
      "Step - 17610, Loss - 0.641022806344115, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2474031577025395\n",
      "Step - 17611, Loss - 0.6507301493682565, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.420113819083682\n",
      "Step - 17612, Loss - 0.7362298284181245, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4569744569705496\n",
      "Step - 17613, Loss - 0.6717929267282483, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0899604635623796\n",
      "Step - 17614, Loss - 0.6797767964329817, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8978551623041282\n",
      "Step - 17615, Loss - 0.6937514639680733, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5621732153013858\n",
      "Step - 17616, Loss - 0.8106127580626381, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.81990858100154\n",
      "Step - 17617, Loss - 0.6873398917723543, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8699730591906599\n",
      "Step - 17618, Loss - 0.767940089363413, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8543196601072183\n",
      "Step - 17619, Loss - 0.5288484408031081, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3968223717039807\n",
      "Step - 17620, Loss - 0.6354778804454245, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.3157139493462533\n",
      "Step - 17621, Loss - 0.6265653482803075, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4765055518943093\n",
      "Step - 17622, Loss - 0.8886823991128309, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6235687030260586\n",
      "Step - 17623, Loss - 0.491987618365372, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5323759838491618\n",
      "Step - 17624, Loss - 0.7678984944154477, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6381850103005187\n",
      "Step - 17625, Loss - 0.849150894065921, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9261520719294424\n",
      "Step - 17626, Loss - 0.6053542964130223, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6143071335627005\n",
      "Step - 17627, Loss - 0.6091098163941572, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.041713414743142\n",
      "Step - 17628, Loss - 0.8457864090218401, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8465365560997062\n",
      "Step - 17629, Loss - 0.6927265109334971, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6003639930395945\n",
      "Step - 17630, Loss - 0.9684486031373969, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5860370586913708\n",
      "Step - 17631, Loss - 0.7578119957613505, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.02532525941686\n",
      "Step - 17632, Loss - 0.6428321969644764, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7978779431611861\n",
      "Step - 17633, Loss - 0.7285133440946308, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.918217383398327\n",
      "Step - 17634, Loss - 0.7746315822524531, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4990177706033604\n",
      "Step - 17635, Loss - 0.6772650578951371, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.46079302200395234\n",
      "Step - 17636, Loss - 0.618424382740041, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1895552446958968\n",
      "Step - 17637, Loss - 0.7539108622525232, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4715351093295523\n",
      "Step - 17638, Loss - 0.7003607980885915, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6192796436613748\n",
      "Step - 17639, Loss - 0.6987327953828917, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4974896094736347\n",
      "Step - 17640, Loss - 0.554783419762842, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6450052454523791\n",
      "Step - 17641, Loss - 0.6375579611991072, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.354063553016577\n",
      "Step - 17642, Loss - 0.5215409076805864, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7644051238174685\n",
      "Step - 17643, Loss - 0.8695760469876817, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5638712296731515\n",
      "Step - 17644, Loss - 0.6556158015385539, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6187773235308281\n",
      "Step - 17645, Loss - 0.7792899353367903, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2608394988582747\n",
      "Step - 17646, Loss - 0.650129043677276, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1138351115476208\n",
      "Step - 17647, Loss - 0.7477479802484751, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7377052306076457\n",
      "Step - 17648, Loss - 0.5791321035363565, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5138007594444347\n",
      "Step - 17649, Loss - 0.712481435507289, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8022502270427601\n",
      "Step - 17650, Loss - 0.8534023012568734, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8258856651637252\n",
      "Step - 17651, Loss - 0.613890732593642, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1663140459669503\n",
      "Step - 17652, Loss - 0.5586240989737251, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6903943138624153\n",
      "Step - 17653, Loss - 0.6896716343363457, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.44690307547111013\n",
      "Step - 17654, Loss - 0.5991847644377831, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9518198647607758\n",
      "Step - 17655, Loss - 0.6745832224163253, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.291012265379532\n",
      "Step - 17656, Loss - 0.738629734551502, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9102822894649256\n",
      "Step - 17657, Loss - 0.8311668002632777, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7130497765440078\n",
      "Step - 17658, Loss - 0.7204619926615579, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.728813166141875\n",
      "Step - 17659, Loss - 0.7505243988559114, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7594111633398829\n",
      "Step - 17660, Loss - 0.6587400662815639, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9656105739410193\n",
      "Step - 17661, Loss - 0.743171538991949, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.49914938320364804\n",
      "Step - 17662, Loss - 0.8844327421242616, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.2879069257690261\n",
      "Step - 17663, Loss - 0.5947792167136219, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.6318588373481893\n",
      "Step - 17664, Loss - 0.5534755797657018, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8994350984747335\n",
      "Step - 17665, Loss - 0.8641096068480918, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4877401594774409\n",
      "Step - 17666, Loss - 0.5913316158278757, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0917605816960174\n",
      "Step - 17667, Loss - 0.6086469505839363, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.3718693426278285\n",
      "Step - 17668, Loss - 0.7078923653708373, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0162165689954368\n",
      "Step - 17669, Loss - 0.7104257291203906, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.3976267067024326\n",
      "Step - 17670, Loss - 0.8665380545861893, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8859138358872821\n",
      "Step - 17671, Loss - 0.623122466140605, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.14800589594433852\n",
      "Step - 17672, Loss - 0.4893865514295341, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.434039108971778\n",
      "Step - 17673, Loss - 0.5867105667964169, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0101776398879758\n",
      "Step - 17674, Loss - 0.7366180744432607, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.3766939183483002\n",
      "Step - 17675, Loss - 0.5765566987014661, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.48192602016210956\n",
      "Step - 17676, Loss - 0.7270070267998197, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7303318540946873\n",
      "Step - 17677, Loss - 0.5465918009747698, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8835479224444198\n",
      "Step - 17678, Loss - 0.5736636265935441, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.3513185003927015\n",
      "Step - 17679, Loss - 0.6848623269406415, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3452298038476391\n",
      "Step - 17680, Loss - 0.8521976953802299, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.958949614344692\n",
      "Step - 17681, Loss - 0.7618751857389383, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7891718436236257\n",
      "Step - 17682, Loss - 0.8811838309599469, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.274394253391639\n",
      "Step - 17683, Loss - 0.6743002228705941, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0370778001214405\n",
      "Step - 17684, Loss - 0.6855750174419887, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9717860420368557\n",
      "Step - 17685, Loss - 0.4939340593397728, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9449068591224854\n",
      "Step - 17686, Loss - 0.7646740649311998, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6479653239217036\n",
      "Step - 17687, Loss - 0.718782082742431, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9418799611332515\n",
      "Step - 17688, Loss - 0.7108682440897204, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.531674937400736\n",
      "Step - 17689, Loss - 0.7267234275268276, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.44828252738033464\n",
      "Step - 17690, Loss - 0.6436063520802122, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5193584254554079\n",
      "Step - 17691, Loss - 0.6635276336799867, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0771803901455044\n",
      "Step - 17692, Loss - 0.6604489682989357, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6373691067740501\n",
      "Step - 17693, Loss - 0.783965635662985, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6343647753004714\n",
      "Step - 17694, Loss - 0.7136110979981547, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6398080872806773\n",
      "Step - 17695, Loss - 0.44579989885518445, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8052495933435044\n",
      "Step - 17696, Loss - 0.5677371721992982, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5023140412488999\n",
      "Step - 17697, Loss - 0.8401053901411217, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3097136981261435\n",
      "Step - 17698, Loss - 0.83708248418372, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.485905715484869\n",
      "Step - 17699, Loss - 0.6993235621530035, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3500123104712514\n",
      "Step - 17700, Loss - 0.6270187186351086, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2330977687123417\n",
      "Step - 17701, Loss - 0.6853237219283028, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.47458391786590576\n",
      "Step - 17702, Loss - 0.6641966995295066, Learning Rate - 3.814697265625e-07, magnitude of gradient - 3.1117345654860022\n",
      "Step - 17703, Loss - 0.8981596612892699, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.9229609350978842\n",
      "Step - 17704, Loss - 0.5317914927441196, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.140518700507767\n",
      "Step - 17705, Loss - 0.6255225787569736, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5049470889430236\n",
      "Step - 17706, Loss - 0.6843403212850169, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8687566137486529\n",
      "Step - 17707, Loss - 0.7912508527533832, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.27545810348275784\n",
      "Step - 17708, Loss - 0.7311012764819745, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1730446936014494\n",
      "Step - 17709, Loss - 0.6882684706712224, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7453286103520558\n",
      "Step - 17710, Loss - 0.8466613631330315, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9411986364131036\n",
      "Step - 17711, Loss - 0.7464615284630387, Learning Rate - 3.814697265625e-07, magnitude of gradient - 3.2993083617261947\n",
      "Step - 17712, Loss - 0.7408987009323006, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9374302407658446\n",
      "Step - 17713, Loss - 0.7441636149733455, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5807509308693698\n",
      "Step - 17714, Loss - 0.7106706711111509, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1536145397116744\n",
      "Step - 17715, Loss - 0.8126724443972828, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1741489271476604\n",
      "Step - 17716, Loss - 0.7510339012846121, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8225802623272362\n",
      "Step - 17717, Loss - 0.709022036445394, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2994134300439117\n",
      "Step - 17718, Loss - 0.6861591420545317, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4222790262328375\n",
      "Step - 17719, Loss - 0.8108478060673758, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5551869224443793\n",
      "Step - 17720, Loss - 0.6847866704669711, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.38221692650412853\n",
      "Step - 17721, Loss - 0.7269879263782775, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.726150317453225\n",
      "Step - 17722, Loss - 0.7177354356590099, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7793915161650615\n",
      "Step - 17723, Loss - 0.4986959907978302, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4588460209976992\n",
      "Step - 17724, Loss - 0.850383895666232, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3630483675412892\n",
      "Step - 17725, Loss - 0.9775412316019407, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.70072376180967\n",
      "Step - 17726, Loss - 0.577217200910542, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5689681864013072\n",
      "Step - 17727, Loss - 0.6854891913850053, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5803825308797157\n",
      "Step - 17728, Loss - 0.7768352495618731, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1505211552410126\n",
      "Step - 17729, Loss - 0.7280144879606589, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4028395851302463\n",
      "Step - 17730, Loss - 0.7528892074918974, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5658627249097\n",
      "Step - 17731, Loss - 0.6562021698051173, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1567643989002734\n",
      "Step - 17732, Loss - 0.9639111245612619, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9025233454246012\n",
      "Step - 17733, Loss - 0.8000028594007926, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4147024388330358\n",
      "Step - 17734, Loss - 0.6019546803178363, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7179652644746837\n",
      "Step - 17735, Loss - 0.7066779833716056, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6094939754971507\n",
      "Step - 17736, Loss - 0.5794560517284544, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.190157212630971\n",
      "Step - 17737, Loss - 0.8044885892419003, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1411777475887959\n",
      "Step - 17738, Loss - 0.6711612181620558, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4381393409384773\n",
      "Step - 17739, Loss - 0.6551185478133603, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.2625027226973845\n",
      "Step - 17740, Loss - 0.5892053043160715, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4149052165678624\n",
      "Step - 17741, Loss - 0.7329128172189743, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0523539327076248\n",
      "Step - 17742, Loss - 0.6843454870936311, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.24601909421810092\n",
      "Step - 17743, Loss - 0.6490905026689198, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8521715173295534\n",
      "Step - 17744, Loss - 0.4881263245772885, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.739689950911108\n",
      "Step - 17745, Loss - 0.741055429822828, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6115084781060434\n",
      "Step - 17746, Loss - 0.6353710730043323, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5823533992130608\n",
      "Step - 17747, Loss - 0.7308772677863061, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7788554010078534\n",
      "Step - 17748, Loss - 0.9272508148989502, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.026372151841584\n",
      "Step - 17749, Loss - 0.7208735028683944, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.315596375479427\n",
      "Step - 17750, Loss - 0.7515335361078376, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1064646049756972\n",
      "Step - 17751, Loss - 0.7192793395593099, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1818602504916609\n",
      "Step - 17752, Loss - 0.789913706409398, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0594808975358283\n",
      "Step - 17753, Loss - 0.6189733612653232, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6359011230740664\n",
      "Step - 17754, Loss - 0.6231462169186585, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8808845929300786\n",
      "Step - 17755, Loss - 0.5949739755730135, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.628831739707993\n",
      "Step - 17756, Loss - 0.7114393715215374, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7439746146831417\n",
      "Step - 17757, Loss - 0.7886707267195607, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5174490301618141\n",
      "Step - 17758, Loss - 0.6841019082931922, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.040560530038934\n",
      "Step - 17759, Loss - 0.6742233137972055, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.945964532591048\n",
      "Step - 17760, Loss - 0.7746024660624187, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.533985667142292\n",
      "Step - 17761, Loss - 0.6391394126396144, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.408330356328264\n",
      "Step - 17762, Loss - 0.6226734939734906, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1863431030583111\n",
      "Step - 17763, Loss - 0.7231043059786252, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4322976622197203\n",
      "Step - 17764, Loss - 0.6623127566271478, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4448868911891636\n",
      "Step - 17765, Loss - 0.7171657144921499, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.1429996614792044\n",
      "Step - 17766, Loss - 0.7417625199367975, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4348742132058483\n",
      "Step - 17767, Loss - 0.8463698153678776, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1811992358872894\n",
      "Step - 17768, Loss - 0.5648848265029397, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.1765670991909083\n",
      "Step - 17769, Loss - 0.6536964041250525, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1754310839053936\n",
      "Step - 17770, Loss - 0.7767330286379693, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.044462019019565\n",
      "Step - 17771, Loss - 0.6378515907659823, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.378044323547629\n",
      "Step - 17772, Loss - 0.5327413912422083, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9261084377081936\n",
      "Step - 17773, Loss - 0.5959531785643388, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.48124261280425884\n",
      "Step - 17774, Loss - 0.6841573143660875, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7691716943557838\n",
      "Step - 17775, Loss - 0.974230284672317, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.7239912707017258\n",
      "Step - 17776, Loss - 0.6409262624036529, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5305408557630092\n",
      "Step - 17777, Loss - 0.446514879226129, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.310836370603171\n",
      "Step - 17778, Loss - 0.659626721826974, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8536058884071064\n",
      "Step - 17779, Loss - 0.7234580294537496, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.44112501910089397\n",
      "Step - 17780, Loss - 0.5911589565860358, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1300198278879496\n",
      "Step - 17781, Loss - 1.0141063900279503, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.0367235205904413\n",
      "Step - 17782, Loss - 0.6788319177427917, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5903357651947432\n",
      "Step - 17783, Loss - 0.7369016069359402, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7835477304683165\n",
      "Step - 17784, Loss - 0.7564912892317339, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.811001553534022\n",
      "Step - 17785, Loss - 0.46432873432474586, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8815993167698338\n",
      "Step - 17786, Loss - 0.7796233749285498, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8220714784500137\n",
      "Step - 17787, Loss - 0.7627096387015875, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1903640095220651\n",
      "Step - 17788, Loss - 0.6265183283575219, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.729520162755564\n",
      "Step - 17789, Loss - 0.6534429562417093, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9180157814754186\n",
      "Step - 17790, Loss - 0.9299820370565581, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.5743776943622807\n",
      "Step - 17791, Loss - 0.6142331869684675, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8266370666213333\n",
      "Step - 17792, Loss - 0.8869837626452889, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9536443855070599\n",
      "Step - 17793, Loss - 0.5749098901594394, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7302560112140999\n",
      "Step - 17794, Loss - 0.7866716041155318, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2510348934357751\n",
      "Step - 17795, Loss - 0.4320738289665496, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.9654306926307015\n",
      "Step - 17796, Loss - 0.7137224703426448, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1464565554443245\n",
      "Step - 17797, Loss - 0.5077651747629098, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2238769132931617\n",
      "Step - 17798, Loss - 0.5381735130874357, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9353656388490493\n",
      "Step - 17799, Loss - 0.7229551349674106, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4100136778145549\n",
      "Step - 17800, Loss - 0.6220838320413888, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0762171571993167\n",
      "Step - 17801, Loss - 0.7845694000852222, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0542374505573198\n",
      "Step - 17802, Loss - 0.6581415758478378, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5184368205260677\n",
      "Step - 17803, Loss - 0.5815263883488246, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9049155294524003\n",
      "Step - 17804, Loss - 0.6415224641449295, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.249142371892213\n",
      "Step - 17805, Loss - 0.7177815502196354, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.657511768957297\n",
      "Step - 17806, Loss - 0.6472360727985614, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9602356410147439\n",
      "Step - 17807, Loss - 0.7397903306473035, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.751793447096597\n",
      "Step - 17808, Loss - 0.6432394001494421, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7258456987654406\n",
      "Step - 17809, Loss - 0.787042849263131, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6092017282932403\n",
      "Step - 17810, Loss - 0.7759147476165373, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4016279746117124\n",
      "Step - 17811, Loss - 0.900929434803044, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2685160983645596\n",
      "Step - 17812, Loss - 0.4211607694022517, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.007998149188262\n",
      "Step - 17813, Loss - 0.6384204574082621, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8470909302796052\n",
      "Step - 17814, Loss - 0.8102937354658688, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5400412282321618\n",
      "Step - 17815, Loss - 0.6411587569302106, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6574881187763784\n",
      "Step - 17816, Loss - 0.638240241321443, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6448523926309355\n",
      "Step - 17817, Loss - 0.7751341031707972, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.459544201589625\n",
      "Step - 17818, Loss - 0.6574284742156974, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7054776734304263\n",
      "Step - 17819, Loss - 0.701472188018188, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1936614288433411\n",
      "Step - 17820, Loss - 0.6983380826103605, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8742083886415116\n",
      "Step - 17821, Loss - 0.6807717181545674, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.9084311556626645\n",
      "Step - 17822, Loss - 0.9007753335485376, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.749547373982295\n",
      "Step - 17823, Loss - 0.6718315689367992, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.327386398308345\n",
      "Step - 17824, Loss - 0.8409923816385746, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7412433072025518\n",
      "Step - 17825, Loss - 0.6820735354715016, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6343707125225466\n",
      "Step - 17826, Loss - 0.6205194448753932, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.3305786364733989\n",
      "Step - 17827, Loss - 0.7510750211933075, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4449374947680411\n",
      "Step - 17828, Loss - 0.7266937842937342, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4758397885520265\n",
      "Step - 17829, Loss - 0.6188112995748188, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7229697057439655\n",
      "Step - 17830, Loss - 0.612157685588807, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.695887965455461\n",
      "Step - 17831, Loss - 0.5680031982962714, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.262300777934092\n",
      "Step - 17832, Loss - 0.7572884804067318, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.47910510294697\n",
      "Step - 17833, Loss - 0.7028590177429664, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1045208186574402\n",
      "Step - 17834, Loss - 0.6526619329830856, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1849055777496325\n",
      "Step - 17835, Loss - 0.6424612008462015, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6616692497995085\n",
      "Step - 17836, Loss - 0.5707057717489474, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.9639355424975715\n",
      "Step - 17837, Loss - 0.77089429154716, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0967882787081713\n",
      "Step - 17838, Loss - 0.572170127248474, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7872002276596665\n",
      "Step - 17839, Loss - 0.8748651144298802, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3291998095164816\n",
      "Step - 17840, Loss - 0.6518751861263746, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.5148915039561404\n",
      "Step - 17841, Loss - 0.7444473874313107, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.3412610249292176\n",
      "Step - 17842, Loss - 0.6771071124312588, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.7567556183827304\n",
      "Step - 17843, Loss - 0.7047846725682694, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4792404968991409\n",
      "Step - 17844, Loss - 0.7570494911706339, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7995579610265746\n",
      "Step - 17845, Loss - 0.6990561971355129, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6159593442447713\n",
      "Step - 17846, Loss - 0.5753848168181237, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.39607462103117047\n",
      "Step - 17847, Loss - 0.9135575616513884, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9910154021043891\n",
      "Step - 17848, Loss - 0.8940977099657408, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1688896440711642\n",
      "Step - 17849, Loss - 0.534019302305389, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4429032328734377\n",
      "Step - 17850, Loss - 0.6757739365688316, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.048781495589716\n",
      "Step - 17851, Loss - 0.662382158749818, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.566194507983866\n",
      "Step - 17852, Loss - 0.8486645132690788, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5512474532685951\n",
      "Step - 17853, Loss - 0.8393197473165767, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.344749562506681\n",
      "Step - 17854, Loss - 0.6223388255000943, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7666663479558838\n",
      "Step - 17855, Loss - 0.7007964048682767, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.238481341620657\n",
      "Step - 17856, Loss - 0.6643083952503892, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1499511802625457\n",
      "Step - 17857, Loss - 0.785943051935028, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1411087757673761\n",
      "Step - 17858, Loss - 0.8384288902322342, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6472922058425106\n",
      "Step - 17859, Loss - 0.9855042849321889, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5910416021083549\n",
      "Step - 17860, Loss - 0.6457940520117844, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.44566500655024865\n",
      "Step - 17861, Loss - 0.36306128525107517, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9499111002592641\n",
      "Step - 17862, Loss - 0.6011133465894285, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1751212990686557\n",
      "Step - 17863, Loss - 0.696958949222584, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5462835363555179\n",
      "Step - 17864, Loss - 0.5909496935671019, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6395572401553635\n",
      "Step - 17865, Loss - 0.6045682003189938, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6359037522789583\n",
      "Step - 17866, Loss - 0.5817771351231481, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7335234705985543\n",
      "Step - 17867, Loss - 0.5533257814718537, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.10289442334331\n",
      "Step - 17868, Loss - 0.7431323043720321, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2473905231180595\n",
      "Step - 17869, Loss - 0.6636381643748277, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3548857298148216\n",
      "Step - 17870, Loss - 0.7623753610607362, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5882786107088571\n",
      "Step - 17871, Loss - 0.6093274792970031, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.73744014041016\n",
      "Step - 17872, Loss - 0.7458404631318285, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1824070676701979\n",
      "Step - 17873, Loss - 0.5815082341300172, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.486968454106992\n",
      "Step - 17874, Loss - 0.9473969278581993, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5091185320698615\n",
      "Step - 17875, Loss - 0.8046039870483414, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9033313641474023\n",
      "Step - 17876, Loss - 0.7970159839107429, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4455988538047158\n",
      "Step - 17877, Loss - 0.9912502314920056, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4933865258422878\n",
      "Step - 17878, Loss - 0.6599046657341934, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7447170812961883\n",
      "Step - 17879, Loss - 0.6375156270980437, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.564249938931723\n",
      "Step - 17880, Loss - 0.6971517065385578, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.254833347389128\n",
      "Step - 17881, Loss - 0.7716273876706183, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4975396623063013\n",
      "Step - 17882, Loss - 0.5542247588257962, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1746808820422954\n",
      "Step - 17883, Loss - 0.6139251450273525, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5987557593275032\n",
      "Step - 17884, Loss - 0.7424708327981455, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.4798523383926425\n",
      "Step - 17885, Loss - 0.5236634192396584, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4190748165841968\n",
      "Step - 17886, Loss - 0.8161677719362302, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1392740101330843\n",
      "Step - 17887, Loss - 0.6441589940854806, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4716536999230186\n",
      "Step - 17888, Loss - 0.9775621547797527, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.8765089298479722\n",
      "Step - 17889, Loss - 0.7619364103193512, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2429638336442979\n",
      "Step - 17890, Loss - 0.8512318662007461, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.285875591567457\n",
      "Step - 17891, Loss - 0.6924148832989674, Learning Rate - 3.814697265625e-07, magnitude of gradient - 3.6640335497999175\n",
      "Step - 17892, Loss - 0.7001145556372477, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.116113842864133\n",
      "Step - 17893, Loss - 0.7877171058540636, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9077153619875932\n",
      "Step - 17894, Loss - 0.5424834726770904, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.14137826150704524\n",
      "Step - 17895, Loss - 0.6849493721105312, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.667180374894122\n",
      "Step - 17896, Loss - 0.8129727945741124, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.839712272459462\n",
      "Step - 17897, Loss - 0.5492659640659331, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.512420437182193\n",
      "Step - 17898, Loss - 0.5719552240216188, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7113494411084571\n",
      "Step - 17899, Loss - 0.764977528000989, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5561677318120908\n",
      "Step - 17900, Loss - 0.8048610026899646, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.042378241007559\n",
      "Step - 17901, Loss - 0.7357850502230856, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6360845988345807\n",
      "Step - 17902, Loss - 0.7191447810947016, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.9585089775242748\n",
      "Step - 17903, Loss - 0.8135152558407832, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6745766270246176\n",
      "Step - 17904, Loss - 0.6916515997412074, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.9634421504123556\n",
      "Step - 17905, Loss - 0.6064333980464746, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.6259984954723246\n",
      "Step - 17906, Loss - 0.9313735873899828, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6274822820236146\n",
      "Step - 17907, Loss - 0.7991899381912028, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.865630693033118\n",
      "Step - 17908, Loss - 0.7081145253374451, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6843057105656102\n",
      "Step - 17909, Loss - 0.5441212863249345, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2567394531724312\n",
      "Step - 17910, Loss - 0.7256675000071604, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5599790679976913\n",
      "Step - 17911, Loss - 0.8528534462068746, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8895655397091393\n",
      "Step - 17912, Loss - 0.6474395547191333, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9155179443437825\n",
      "Step - 17913, Loss - 0.8501513519870727, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.070490407149335\n",
      "Step - 17914, Loss - 0.7229526771566325, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8519713129095496\n",
      "Step - 17915, Loss - 0.7257473910385591, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.079251760634847\n",
      "Step - 17916, Loss - 0.575726284936493, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.923178053379297\n",
      "Step - 17917, Loss - 0.7049464163699257, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0429589022311714\n",
      "Step - 17918, Loss - 0.6628009636426184, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1660782664092704\n",
      "Step - 17919, Loss - 0.6396656297881611, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7673623768924025\n",
      "Step - 17920, Loss - 0.5972626272397541, Learning Rate - 3.814697265625e-07, magnitude of gradient - 3.0935967744134887\n",
      "Step - 17921, Loss - 0.8225282001433001, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.374549741927015\n",
      "Step - 17922, Loss - 0.8396359398846756, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6493719686067889\n",
      "Step - 17923, Loss - 0.8659866855600328, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.02614885694622\n",
      "Step - 17924, Loss - 0.5686589516570258, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.465767390004296\n",
      "Step - 17925, Loss - 0.5429663981078154, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4587218073772475\n",
      "Step - 17926, Loss - 0.7142946374639286, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3602833455797478\n",
      "Step - 17927, Loss - 0.836286860424577, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9075600495423448\n",
      "Step - 17928, Loss - 0.773476864435382, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2384816918616157\n",
      "Step - 17929, Loss - 0.5870279299198312, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4639435541645733\n",
      "Step - 17930, Loss - 0.5043893425033547, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1919043007943895\n",
      "Step - 17931, Loss - 0.7831254312948409, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0903598392669562\n",
      "Step - 17932, Loss - 0.6705190429784602, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3370608983440135\n",
      "Step - 17933, Loss - 0.5964698887851471, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1927878112491543\n",
      "Step - 17934, Loss - 0.7361168276913078, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.37691288733405054\n",
      "Step - 17935, Loss - 0.7098524058479166, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2156304931933066\n",
      "Step - 17936, Loss - 0.6578957548648054, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7810912076061719\n",
      "Step - 17937, Loss - 0.6468317579639035, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6141301335361798\n",
      "Step - 17938, Loss - 0.7288446846061445, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1564843043422621\n",
      "Step - 17939, Loss - 0.9661944443600968, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.099526626659446\n",
      "Step - 17940, Loss - 0.7040513500134027, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2052443689703287\n",
      "Step - 17941, Loss - 0.6728252400207112, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7235511499514197\n",
      "Step - 17942, Loss - 0.6527882152917298, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9890711961107299\n",
      "Step - 17943, Loss - 0.6500225881211621, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.273357351191416\n",
      "Step - 17944, Loss - 0.5506317568728245, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7699408766280025\n",
      "Step - 17945, Loss - 0.7936859930420783, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7567598579300913\n",
      "Step - 17946, Loss - 0.5978799770726941, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.22842749776295937\n",
      "Step - 17947, Loss - 0.6594923973794613, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6393757253408219\n",
      "Step - 17948, Loss - 0.5983723920875014, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9549277770462984\n",
      "Step - 17949, Loss - 0.8327299000790249, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2044111111282225\n",
      "Step - 17950, Loss - 0.7467273828138292, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9268794893832566\n",
      "Step - 17951, Loss - 0.5569477910669511, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4316700780520089\n",
      "Step - 17952, Loss - 0.7223459717987472, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5800967638639454\n",
      "Step - 17953, Loss - 0.6524178047436764, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4556508296641462\n",
      "Step - 17954, Loss - 0.6813641547384538, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8524273281386535\n",
      "Step - 17955, Loss - 0.7705280325923384, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7165457860189706\n",
      "Step - 17956, Loss - 0.6905121325934281, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4960167695085979\n",
      "Step - 17957, Loss - 0.8715162684288023, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.2437485842493907\n",
      "Step - 17958, Loss - 0.8540300048494583, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1084378190237116\n",
      "Step - 17959, Loss - 0.6744817990862848, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6271662954828344\n",
      "Step - 17960, Loss - 0.6415143483053853, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7330131993951825\n",
      "Step - 17961, Loss - 0.7276359189381383, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.363787446343865\n",
      "Step - 17962, Loss - 0.558073391107709, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3464750873010336\n",
      "Step - 17963, Loss - 0.8175964701253683, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7083916248240831\n",
      "Step - 17964, Loss - 0.9424360947632886, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.9148484082515296\n",
      "Step - 17965, Loss - 0.5421148718115295, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.1610898782568357\n",
      "Step - 17966, Loss - 0.4661958383855243, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6527063108986507\n",
      "Step - 17967, Loss - 0.8719955816918596, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3935584593602002\n",
      "Step - 17968, Loss - 0.6480993578410053, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.1759363150445103\n",
      "Step - 17969, Loss - 0.8144131449213322, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7324528195949385\n",
      "Step - 17970, Loss - 0.7654365457329338, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.291683868095658\n",
      "Step - 17971, Loss - 0.6500828877248264, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8260768814518666\n",
      "Step - 17972, Loss - 0.8131234525461956, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3029619748490164\n",
      "Step - 17973, Loss - 0.6905039062034727, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.865956533270765\n",
      "Step - 17974, Loss - 0.7381430250164769, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.1871496837704223\n",
      "Step - 17975, Loss - 0.6325550325383805, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.6327385123096039\n",
      "Step - 17976, Loss - 0.7259854849605769, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0997921923549085\n",
      "Step - 17977, Loss - 0.8151355008250379, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.0047380702487043\n",
      "Step - 17978, Loss - 0.996193949118923, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6896503920356118\n",
      "Step - 17979, Loss - 0.684929462204551, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9695514105403863\n",
      "Step - 17980, Loss - 0.7934828794306492, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9648583800129429\n",
      "Step - 17981, Loss - 0.8825035079851079, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4642575074114779\n",
      "Step - 17982, Loss - 0.6938810192815046, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.5302610802502586\n",
      "Step - 17983, Loss - 0.8951319239006471, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8145662859544581\n",
      "Step - 17984, Loss - 0.578407224832926, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7086518749502893\n",
      "Step - 17985, Loss - 0.6778832317319836, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.6939619497715566\n",
      "Step - 17986, Loss - 0.6622786873875894, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.7078109801770055\n",
      "Step - 17987, Loss - 0.5832495501122756, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.084814765088213\n",
      "Step - 17988, Loss - 0.49363295345319086, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.4740042586229223\n",
      "Step - 17989, Loss - 0.7722556876879841, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.0170061303802553\n",
      "Step - 17990, Loss - 0.7140633767382335, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4755970030675401\n",
      "Step - 17991, Loss - 0.6785650627746491, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.424905499317124\n",
      "Step - 17992, Loss - 0.7249545765962694, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.8877275497748772\n",
      "Step - 17993, Loss - 0.7809085584165208, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.4741395719691577\n",
      "Step - 17994, Loss - 0.6888210765220303, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.5277493199692938\n",
      "Step - 17995, Loss - 0.6215695240471373, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.295900915939928\n",
      "Step - 17996, Loss - 0.7151904761265648, Learning Rate - 3.814697265625e-07, magnitude of gradient - 0.9437848364458316\n",
      "Step - 17997, Loss - 0.8592119387771318, Learning Rate - 3.814697265625e-07, magnitude of gradient - 2.2839360882203534\n",
      "Step - 17998, Loss - 0.7873595357365714, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.3128764123078587\n",
      "Step - 17999, Loss - 0.6213905737921124, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.2175709565073323\n",
      "Step - 18000, Loss - 0.7078517658759234, Learning Rate - 3.814697265625e-07, magnitude of gradient - 1.7621389430690082\n",
      "Step - 18001, Loss - 0.6236840113592136, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7991604887450914\n",
      "Step - 18002, Loss - 0.6962065093274398, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3923152436676183\n",
      "Step - 18003, Loss - 0.6952063033525266, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4029572534939367\n",
      "Step - 18004, Loss - 0.6939365942733859, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.645594855299879\n",
      "Step - 18005, Loss - 0.6968334457161208, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2719874049564142\n",
      "Step - 18006, Loss - 0.6179928885332867, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.427079079478579\n",
      "Step - 18007, Loss - 0.5700288635010452, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6738128242765086\n",
      "Step - 18008, Loss - 0.7214410911339503, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6209068774002546\n",
      "Step - 18009, Loss - 0.6650695772938748, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.2271130746976895\n",
      "Step - 18010, Loss - 0.7387782173304269, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0633164061848437\n",
      "Step - 18011, Loss - 0.6735430424416917, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5271004567745232\n",
      "Step - 18012, Loss - 0.7655340941589996, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9295875969961531\n",
      "Step - 18013, Loss - 0.605308258652981, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1943769679656557\n",
      "Step - 18014, Loss - 0.7752815215863118, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9143748586569364\n",
      "Step - 18015, Loss - 0.5378674452817612, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.025474198682506\n",
      "Step - 18016, Loss - 0.6131078130512594, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5301325936311113\n",
      "Step - 18017, Loss - 0.9948022072801691, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7327921265153345\n",
      "Step - 18018, Loss - 0.6367620412958165, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9744183749144846\n",
      "Step - 18019, Loss - 0.675709188179251, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4298084752705479\n",
      "Step - 18020, Loss - 0.7960577221733979, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.5173047316956025\n",
      "Step - 18021, Loss - 0.6321889075349761, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7585416881702959\n",
      "Step - 18022, Loss - 0.7042685665496788, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0445025088338995\n",
      "Step - 18023, Loss - 0.6300120997535265, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1727573494644385\n",
      "Step - 18024, Loss - 1.0657186907151202, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9627807311023513\n",
      "Step - 18025, Loss - 0.787911272641523, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1047057855624633\n",
      "Step - 18026, Loss - 0.6558575653182821, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.08196200127646\n",
      "Step - 18027, Loss - 0.6580268454970621, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4167504342180972\n",
      "Step - 18028, Loss - 0.7063245877996203, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9671366599053204\n",
      "Step - 18029, Loss - 0.71331910142244, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9928164894524684\n",
      "Step - 18030, Loss - 0.6250584642748152, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.40364996455128\n",
      "Step - 18031, Loss - 0.5811702750410136, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.016465896675676\n",
      "Step - 18032, Loss - 0.6801702774312626, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0828065015601822\n",
      "Step - 18033, Loss - 0.6101078481303327, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5683132506333067\n",
      "Step - 18034, Loss - 0.7296693402411638, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8600431188638982\n",
      "Step - 18035, Loss - 0.7912515855883998, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.561202971740416\n",
      "Step - 18036, Loss - 0.6010878149305027, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2902616840740162\n",
      "Step - 18037, Loss - 0.5541493073164903, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6884181001820338\n",
      "Step - 18038, Loss - 0.7384309229451204, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3192727784764218\n",
      "Step - 18039, Loss - 0.6773643489733715, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1998474721732855\n",
      "Step - 18040, Loss - 0.6140025972991159, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9600872986874835\n",
      "Step - 18041, Loss - 0.6713924033456248, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.3540203803177069\n",
      "Step - 18042, Loss - 0.7027710635869593, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3668887687793645\n",
      "Step - 18043, Loss - 0.7018977932862558, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.632659056248633\n",
      "Step - 18044, Loss - 0.7853706096556963, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3972355611685205\n",
      "Step - 18045, Loss - 0.6329986215739177, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8473813509445047\n",
      "Step - 18046, Loss - 0.8748736305524034, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0807346368604303\n",
      "Step - 18047, Loss - 0.5795836407394099, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.639922934368036\n",
      "Step - 18048, Loss - 0.8826701341334846, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4660193318564616\n",
      "Step - 18049, Loss - 0.7868019161025968, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6387026186410492\n",
      "Step - 18050, Loss - 0.7806905016501882, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.400061838737985\n",
      "Step - 18051, Loss - 0.878411649484538, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0789819336430508\n",
      "Step - 18052, Loss - 0.6181027608029682, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0974036273343792\n",
      "Step - 18053, Loss - 0.8627342155749085, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9967613360737348\n",
      "Step - 18054, Loss - 0.6087650285219568, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0374564335404381\n",
      "Step - 18055, Loss - 0.6726304209353872, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4261887955656753\n",
      "Step - 18056, Loss - 0.6899607150826896, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5958170357658155\n",
      "Step - 18057, Loss - 0.6343058561388198, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0978684435788768\n",
      "Step - 18058, Loss - 0.6388225774157505, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5315232424376735\n",
      "Step - 18059, Loss - 0.7080873580301668, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4077727138992404\n",
      "Step - 18060, Loss - 0.5569321695282464, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.337686038692248\n",
      "Step - 18061, Loss - 0.6547762353078136, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.3172348641912883\n",
      "Step - 18062, Loss - 0.8585713950043582, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1553756051362483\n",
      "Step - 18063, Loss - 0.6138737728074903, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8966928959696482\n",
      "Step - 18064, Loss - 0.7305442243757847, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6842914965869257\n",
      "Step - 18065, Loss - 0.8538960545500726, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7592173261576274\n",
      "Step - 18066, Loss - 0.6557816039737017, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0579489098797095\n",
      "Step - 18067, Loss - 0.888161655155407, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9272457359860895\n",
      "Step - 18068, Loss - 0.6426833574232468, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.205429789121711\n",
      "Step - 18069, Loss - 0.8049662657276887, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.296356358979174\n",
      "Step - 18070, Loss - 0.6420759007419286, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.323093085710946\n",
      "Step - 18071, Loss - 0.6800022668510859, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0732586086200953\n",
      "Step - 18072, Loss - 0.7659984158442337, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1154274916492608\n",
      "Step - 18073, Loss - 0.6754098756405645, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5430592227270391\n",
      "Step - 18074, Loss - 0.6516443755610005, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.467178954235223\n",
      "Step - 18075, Loss - 0.6420341515385878, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4906544486696405\n",
      "Step - 18076, Loss - 0.7281270345159593, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.811962425181284\n",
      "Step - 18077, Loss - 0.9090666387036912, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.0249062380947884\n",
      "Step - 18078, Loss - 0.589862876804169, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6381217762271993\n",
      "Step - 18079, Loss - 0.781316701626191, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9317776796606325\n",
      "Step - 18080, Loss - 0.6225860986887444, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7396462076208616\n",
      "Step - 18081, Loss - 0.7027289574036615, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7136145262990816\n",
      "Step - 18082, Loss - 0.7129255302031066, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6180692009640942\n",
      "Step - 18083, Loss - 0.5566588561085873, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2203967430248674\n",
      "Step - 18084, Loss - 0.8036205487748913, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8947541767514673\n",
      "Step - 18085, Loss - 0.6819420640936227, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9866369352407671\n",
      "Step - 18086, Loss - 0.734767801820369, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6920137981067402\n",
      "Step - 18087, Loss - 0.6438308588598621, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.306493400111681\n",
      "Step - 18088, Loss - 0.5507769449650709, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6793293354039652\n",
      "Step - 18089, Loss - 0.7028673072449187, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.545375166669735\n",
      "Step - 18090, Loss - 0.6788136910146546, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2995176253699179\n",
      "Step - 18091, Loss - 0.8981159647924088, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.2673561688816126\n",
      "Step - 18092, Loss - 0.6592655281137174, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.4749700803431995\n",
      "Step - 18093, Loss - 0.7850225608032945, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.36981698745296\n",
      "Step - 18094, Loss - 0.5362775318545177, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.455745936733849\n",
      "Step - 18095, Loss - 0.842863949561691, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.75468732054741\n",
      "Step - 18096, Loss - 0.7951986615843714, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9260616042651647\n",
      "Step - 18097, Loss - 0.6843731024281885, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.21297916973260078\n",
      "Step - 18098, Loss - 0.7871516884464484, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.012680894600046\n",
      "Step - 18099, Loss - 0.7392610325461467, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8576264006722554\n",
      "Step - 18100, Loss - 0.6690278811055548, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9492510266477954\n",
      "Step - 18101, Loss - 0.6359685429227191, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.398390149225908\n",
      "Step - 18102, Loss - 0.7773402795067945, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2670563679279474\n",
      "Step - 18103, Loss - 0.5533780697126025, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.173439136356646\n",
      "Step - 18104, Loss - 0.6215509816541372, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6325566900894843\n",
      "Step - 18105, Loss - 0.8046027320401667, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0933145337706136\n",
      "Step - 18106, Loss - 0.6788203670948619, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5687800067738789\n",
      "Step - 18107, Loss - 0.8988243757307796, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9060513730904323\n",
      "Step - 18108, Loss - 0.7197421148554319, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2294886530327038\n",
      "Step - 18109, Loss - 0.5946977292612289, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2635064000204712\n",
      "Step - 18110, Loss - 0.6926370484301809, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.195803890817756\n",
      "Step - 18111, Loss - 0.676224329917227, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4400814539030098\n",
      "Step - 18112, Loss - 0.7184569093760721, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2533994951400613\n",
      "Step - 18113, Loss - 0.8113332771577179, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.567569066258141\n",
      "Step - 18114, Loss - 0.840875499975806, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9207298888809478\n",
      "Step - 18115, Loss - 0.8652106775118923, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4632096267427255\n",
      "Step - 18116, Loss - 0.7195263812026705, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5735199461856713\n",
      "Step - 18117, Loss - 0.5455576737335115, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 3.1006376860783282\n",
      "Step - 18118, Loss - 0.47985778908572313, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3586112323807205\n",
      "Step - 18119, Loss - 0.7008709407646088, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6420756890576815\n",
      "Step - 18120, Loss - 0.653276112724464, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1899826210153224\n",
      "Step - 18121, Loss - 0.5361974338399714, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3601013552727328\n",
      "Step - 18122, Loss - 0.7503856840049867, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7827295727315563\n",
      "Step - 18123, Loss - 0.6223960146391314, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0224710021327157\n",
      "Step - 18124, Loss - 0.7624957590322218, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4048467025456541\n",
      "Step - 18125, Loss - 0.782106589068954, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.767337201236166\n",
      "Step - 18126, Loss - 0.6459897783074635, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9674321469978941\n",
      "Step - 18127, Loss - 0.7127823167910039, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.43748940476527637\n",
      "Step - 18128, Loss - 0.6356291455265737, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9807740392554143\n",
      "Step - 18129, Loss - 0.7159947259252633, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4300845401407741\n",
      "Step - 18130, Loss - 0.9005300008819019, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3231812055586505\n",
      "Step - 18131, Loss - 0.6560853847441953, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.3169708072617672\n",
      "Step - 18132, Loss - 0.6019141466274112, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.362726447453834\n",
      "Step - 18133, Loss - 0.7322461063889147, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3993592590396415\n",
      "Step - 18134, Loss - 0.5686350787622656, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1868775350198355\n",
      "Step - 18135, Loss - 0.6593255453518387, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0611995312859988\n",
      "Step - 18136, Loss - 0.6607379669725008, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.322892326593694\n",
      "Step - 18137, Loss - 0.9001634945247426, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.350873456849658\n",
      "Step - 18138, Loss - 0.7634092164168428, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9757623828058493\n",
      "Step - 18139, Loss - 0.7895656028413001, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9127909539327992\n",
      "Step - 18140, Loss - 0.7951852383286664, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5511640262966858\n",
      "Step - 18141, Loss - 0.6389474535626507, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.3548867315258697\n",
      "Step - 18142, Loss - 0.5622495925966192, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2683799213124118\n",
      "Step - 18143, Loss - 0.6609366513637157, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7067683128733092\n",
      "Step - 18144, Loss - 0.7116236576300148, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8800832883565721\n",
      "Step - 18145, Loss - 0.6132166569282355, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8871968447196599\n",
      "Step - 18146, Loss - 0.6569346012203015, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.747380560930015\n",
      "Step - 18147, Loss - 0.6538524337385353, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6723501719070397\n",
      "Step - 18148, Loss - 0.6216400786788795, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.392736450987933\n",
      "Step - 18149, Loss - 0.6088481212187973, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6029305587034989\n",
      "Step - 18150, Loss - 0.6688057915007544, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7958193968266832\n",
      "Step - 18151, Loss - 0.8048508816177526, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4525557846473559\n",
      "Step - 18152, Loss - 0.5998985097903566, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5313531557480592\n",
      "Step - 18153, Loss - 0.6976346267330193, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8381589160974365\n",
      "Step - 18154, Loss - 0.6269457414042374, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8931877444553662\n",
      "Step - 18155, Loss - 0.6923088720427722, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.680972100128097\n",
      "Step - 18156, Loss - 0.601719468170549, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6031722935764234\n",
      "Step - 18157, Loss - 0.7197133052145344, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7480248121324504\n",
      "Step - 18158, Loss - 0.6055423215235598, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.37333375398057883\n",
      "Step - 18159, Loss - 0.7417692531609191, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0661205895586716\n",
      "Step - 18160, Loss - 0.7760587887479632, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0983521549747015\n",
      "Step - 18161, Loss - 0.6016396391956421, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5159581041353003\n",
      "Step - 18162, Loss - 0.655716233141656, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4507776897728863\n",
      "Step - 18163, Loss - 0.6803424847070518, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8154026778846526\n",
      "Step - 18164, Loss - 0.5010687922529565, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1054294479182607\n",
      "Step - 18165, Loss - 0.8430981903519839, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.091570244373256\n",
      "Step - 18166, Loss - 0.6576829769742889, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1564686387345193\n",
      "Step - 18167, Loss - 0.7980610742931883, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7839638984981852\n",
      "Step - 18168, Loss - 0.5693048939052179, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3194533863990023\n",
      "Step - 18169, Loss - 0.791518528738883, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8562067873030039\n",
      "Step - 18170, Loss - 0.6250744952182449, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1237725774861258\n",
      "Step - 18171, Loss - 0.5527607486175742, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0094923170163626\n",
      "Step - 18172, Loss - 0.7182414015803005, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.567112474620028\n",
      "Step - 18173, Loss - 0.7319263577166056, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.088605247931002\n",
      "Step - 18174, Loss - 0.8015730409367733, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6965714535929982\n",
      "Step - 18175, Loss - 0.7067439965685345, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.86713806702062\n",
      "Step - 18176, Loss - 0.710179309782482, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6324509027149259\n",
      "Step - 18177, Loss - 0.5635720651520737, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9655198830322729\n",
      "Step - 18178, Loss - 0.8575399319575201, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1530427610236855\n",
      "Step - 18179, Loss - 0.6114605362622219, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7037196579003782\n",
      "Step - 18180, Loss - 0.719252880835995, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2179818622260365\n",
      "Step - 18181, Loss - 0.6820642842104401, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.560141733908607\n",
      "Step - 18182, Loss - 0.6517414462946547, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6875888868771927\n",
      "Step - 18183, Loss - 0.5911203307057886, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7456873654237448\n",
      "Step - 18184, Loss - 0.6873187334507389, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.0394475830575325\n",
      "Step - 18185, Loss - 0.8037459166452626, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.026908049143067\n",
      "Step - 18186, Loss - 0.7240366792544936, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.401249916169691\n",
      "Step - 18187, Loss - 0.7205715377190477, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2458554636659687\n",
      "Step - 18188, Loss - 0.8095104130435808, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0351118361933962\n",
      "Step - 18189, Loss - 0.8602884112361822, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.1562535998849577\n",
      "Step - 18190, Loss - 0.7762836695061104, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3943045213584564\n",
      "Step - 18191, Loss - 0.8270219367301003, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6731573479641234\n",
      "Step - 18192, Loss - 0.8586941986754273, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4261261907708764\n",
      "Step - 18193, Loss - 0.774546423496964, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6075480984153786\n",
      "Step - 18194, Loss - 0.6532004511810034, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3874577331002325\n",
      "Step - 18195, Loss - 0.7578080589328953, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.1439789015825212\n",
      "Step - 18196, Loss - 0.6725997373373309, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8639526010232406\n",
      "Step - 18197, Loss - 0.6026305286053155, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0619057672973473\n",
      "Step - 18198, Loss - 0.6789456966764611, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7196151809169702\n",
      "Step - 18199, Loss - 0.7966071121224844, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.011273011193201\n",
      "Step - 18200, Loss - 0.8673895293384347, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8755628870147272\n",
      "Step - 18201, Loss - 0.9019025353882677, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.476469843814309\n",
      "Step - 18202, Loss - 0.5842362583243434, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9921075060747068\n",
      "Step - 18203, Loss - 0.6362281814406334, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1169304581707746\n",
      "Step - 18204, Loss - 0.5474875866595798, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4936123530095814\n",
      "Step - 18205, Loss - 0.5356178550822817, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.943630405167113\n",
      "Step - 18206, Loss - 0.6700528858488676, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5639230693796\n",
      "Step - 18207, Loss - 0.6353269125411085, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.34770395845134394\n",
      "Step - 18208, Loss - 0.6254442958548727, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1930093919528886\n",
      "Step - 18209, Loss - 0.5650378667125209, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9004618912337973\n",
      "Step - 18210, Loss - 0.6858806814768184, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6656118451181041\n",
      "Step - 18211, Loss - 0.6829459130361717, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8839054453074985\n",
      "Step - 18212, Loss - 0.678891873635995, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5470180456760776\n",
      "Step - 18213, Loss - 0.7534133768139278, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3554799554335797\n",
      "Step - 18214, Loss - 0.7278625458948009, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.0053295095851262\n",
      "Step - 18215, Loss - 0.5892500753841338, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1054753599048777\n",
      "Step - 18216, Loss - 0.6685658153866748, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7813803734735274\n",
      "Step - 18217, Loss - 0.6870138935426662, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0652937222900174\n",
      "Step - 18218, Loss - 0.6285207999208755, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4699439278008681\n",
      "Step - 18219, Loss - 0.5825769675208171, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.516143380503855\n",
      "Step - 18220, Loss - 0.8641422042910389, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7956917894612712\n",
      "Step - 18221, Loss - 0.6934354697058462, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9631150075464694\n",
      "Step - 18222, Loss - 0.5455248924264867, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.47839553090577974\n",
      "Step - 18223, Loss - 0.6863184440061053, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5968942918584006\n",
      "Step - 18224, Loss - 0.8571831667375841, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5657760729425252\n",
      "Step - 18225, Loss - 0.6332132214410853, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.1683897101466796\n",
      "Step - 18226, Loss - 0.674835182906186, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5181473301023795\n",
      "Step - 18227, Loss - 0.6114912250668055, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5512463374612253\n",
      "Step - 18228, Loss - 0.8054660469469425, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6970681363328294\n",
      "Step - 18229, Loss - 0.6466646439306328, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.538689867700719\n",
      "Step - 18230, Loss - 0.8965516640675645, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6747077491696014\n",
      "Step - 18231, Loss - 0.6607374404911481, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7907628409796605\n",
      "Step - 18232, Loss - 0.6598368960678351, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.266633798549786\n",
      "Step - 18233, Loss - 0.5485626374874404, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6441834570762088\n",
      "Step - 18234, Loss - 0.7246961509832472, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6063446131803955\n",
      "Step - 18235, Loss - 0.6368042231939341, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9507647578493328\n",
      "Step - 18236, Loss - 0.6424433431877858, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4195914830101637\n",
      "Step - 18237, Loss - 0.6461171845524627, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.249863336683252\n",
      "Step - 18238, Loss - 0.7286066157815662, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7903017309895781\n",
      "Step - 18239, Loss - 0.664758717576291, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.165520054376437\n",
      "Step - 18240, Loss - 0.5225201032336216, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9001468134087837\n",
      "Step - 18241, Loss - 0.5686281206968008, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5753275800152862\n",
      "Step - 18242, Loss - 0.7429997735524267, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.6260340991191766\n",
      "Step - 18243, Loss - 0.7788056866670452, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.46416885859836865\n",
      "Step - 18244, Loss - 0.599692441175611, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1216039728870604\n",
      "Step - 18245, Loss - 0.7345995941226666, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0217168621811565\n",
      "Step - 18246, Loss - 0.7501339437003379, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.291603079484876\n",
      "Step - 18247, Loss - 0.6661052906315923, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0481338572525651\n",
      "Step - 18248, Loss - 0.7263664362552764, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8338036544978986\n",
      "Step - 18249, Loss - 0.7186257169973187, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5425837952624633\n",
      "Step - 18250, Loss - 0.7709986964050015, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5944894879188236\n",
      "Step - 18251, Loss - 0.8784787020379621, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.593914856018423\n",
      "Step - 18252, Loss - 0.7764508360307925, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2079915591483716\n",
      "Step - 18253, Loss - 0.911186481129738, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4263378284384918\n",
      "Step - 18254, Loss - 0.924042029403384, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.940313105559008\n",
      "Step - 18255, Loss - 0.4931408152603529, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.632648191872638\n",
      "Step - 18256, Loss - 0.6275478703188424, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0958400571770466\n",
      "Step - 18257, Loss - 0.6402896998403972, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8600051132481308\n",
      "Step - 18258, Loss - 0.7136780361896965, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8763573605355062\n",
      "Step - 18259, Loss - 0.6669943988417673, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3595915006973707\n",
      "Step - 18260, Loss - 0.803100552641382, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.072892436397562\n",
      "Step - 18261, Loss - 0.6407336510825014, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.3817189810266665\n",
      "Step - 18262, Loss - 0.6411017584268683, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1233245024956546\n",
      "Step - 18263, Loss - 0.6233454182420719, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9098996996048518\n",
      "Step - 18264, Loss - 0.7223165178992041, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5218472605283565\n",
      "Step - 18265, Loss - 0.8024618329634888, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9526799205140679\n",
      "Step - 18266, Loss - 1.0114546713738841, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.650778691185552\n",
      "Step - 18267, Loss - 0.7101588893915198, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.250770361545134\n",
      "Step - 18268, Loss - 0.6773053344388301, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.23582297684451572\n",
      "Step - 18269, Loss - 0.6034000071673832, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.092265214917716\n",
      "Step - 18270, Loss - 0.6642228550558609, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7756187998917667\n",
      "Step - 18271, Loss - 0.5048827257916111, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9614163253580463\n",
      "Step - 18272, Loss - 0.7735554924940198, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.701657052470705\n",
      "Step - 18273, Loss - 0.8410491070843436, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9446868404270609\n",
      "Step - 18274, Loss - 0.5726923359688351, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0282055604451215\n",
      "Step - 18275, Loss - 0.8373813630197603, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1129356164029092\n",
      "Step - 18276, Loss - 0.8569821617525452, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3017031927802245\n",
      "Step - 18277, Loss - 0.6785052448805435, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6792156914811563\n",
      "Step - 18278, Loss - 0.7275487986056139, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.541016148787207\n",
      "Step - 18279, Loss - 0.6341217070109569, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0378056958637092\n",
      "Step - 18280, Loss - 0.7448685333933971, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7149377371748777\n",
      "Step - 18281, Loss - 0.8461852066112373, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3282126539419565\n",
      "Step - 18282, Loss - 0.5497664962588388, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7636124819495619\n",
      "Step - 18283, Loss - 0.7303844748102523, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0407832201200986\n",
      "Step - 18284, Loss - 0.6805972468002023, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.47784932984832623\n",
      "Step - 18285, Loss - 0.46778810700873785, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8003482303751843\n",
      "Step - 18286, Loss - 0.7274820049940836, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6522794863284874\n",
      "Step - 18287, Loss - 0.7615096506548658, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5792486769472731\n",
      "Step - 18288, Loss - 0.6936846924513526, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.2145454104808876\n",
      "Step - 18289, Loss - 0.5573061085163684, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.2338149025064769\n",
      "Step - 18290, Loss - 0.7541835416328849, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.042971914846103\n",
      "Step - 18291, Loss - 0.6865613505959801, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0033644304539469\n",
      "Step - 18292, Loss - 0.721573286838225, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.7675971846847958\n",
      "Step - 18293, Loss - 0.8078507510657422, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.505583077155235\n",
      "Step - 18294, Loss - 0.6786414869037992, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1182176888876674\n",
      "Step - 18295, Loss - 0.7079879563830692, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8479204463639395\n",
      "Step - 18296, Loss - 0.7924853894147991, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4990687991234606\n",
      "Step - 18297, Loss - 0.8046586266924464, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4712866848023087\n",
      "Step - 18298, Loss - 0.6928154545565575, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6300078847056922\n",
      "Step - 18299, Loss - 0.6786286611792482, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0110086134486345\n",
      "Step - 18300, Loss - 0.7706704148663734, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7601536071651703\n",
      "Step - 18301, Loss - 0.8250555374444712, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8120658141077034\n",
      "Step - 18302, Loss - 0.7037302359637515, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3674307006458346\n",
      "Step - 18303, Loss - 0.646353046782763, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.771825856083254\n",
      "Step - 18304, Loss - 0.5725686610016942, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5718751343829694\n",
      "Step - 18305, Loss - 0.4262076761423532, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9403481000200382\n",
      "Step - 18306, Loss - 0.831081845179103, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2235335428141432\n",
      "Step - 18307, Loss - 0.7387945011089248, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.9852386351826907\n",
      "Step - 18308, Loss - 0.7312907833909381, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4193029102971289\n",
      "Step - 18309, Loss - 0.6243206590349784, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0786021529040715\n",
      "Step - 18310, Loss - 0.7365537789071573, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.515693664717384\n",
      "Step - 18311, Loss - 0.545528269321091, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4147695061622148\n",
      "Step - 18312, Loss - 0.8090886665645717, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2619524983185646\n",
      "Step - 18313, Loss - 0.5301043471246623, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.000410494365942\n",
      "Step - 18314, Loss - 0.8190148309026601, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.0306071330755033\n",
      "Step - 18315, Loss - 0.8137131908739159, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.179439725692322\n",
      "Step - 18316, Loss - 0.7511111566801418, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7447129257618201\n",
      "Step - 18317, Loss - 0.9554987469004423, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6543416732709537\n",
      "Step - 18318, Loss - 0.9710677248010648, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.8033805714133355\n",
      "Step - 18319, Loss - 0.770117364714776, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6806865643208895\n",
      "Step - 18320, Loss - 0.41203677842579356, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7416936534418537\n",
      "Step - 18321, Loss - 0.8647817313061533, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.1736716922048367\n",
      "Step - 18322, Loss - 0.5104623758707325, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.3257411011372233\n",
      "Step - 18323, Loss - 0.6998098305341838, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4872401389395096\n",
      "Step - 18324, Loss - 0.6828310675685714, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8147740222263948\n",
      "Step - 18325, Loss - 0.7242074388938399, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8367182493330132\n",
      "Step - 18326, Loss - 0.6864520691633986, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9440958022504484\n",
      "Step - 18327, Loss - 0.8330487032902718, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3632427462845251\n",
      "Step - 18328, Loss - 0.5734062822867242, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.535375289622044\n",
      "Step - 18329, Loss - 0.7996364136872739, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1499622025439882\n",
      "Step - 18330, Loss - 0.6771456050796876, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5396567193857954\n",
      "Step - 18331, Loss - 0.7077731516693775, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8562708234243892\n",
      "Step - 18332, Loss - 0.5665122658685753, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0877294222205705\n",
      "Step - 18333, Loss - 0.7622207758646263, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.195306193527372\n",
      "Step - 18334, Loss - 0.7395617543699237, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.298692068492781\n",
      "Step - 18335, Loss - 0.6873575667649904, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6767300876889981\n",
      "Step - 18336, Loss - 0.6111043638882269, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.374602250248391\n",
      "Step - 18337, Loss - 0.6563848214475839, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3160788679513176\n",
      "Step - 18338, Loss - 0.6402869268277014, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7991705121186092\n",
      "Step - 18339, Loss - 0.7325731663680588, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8090801503186772\n",
      "Step - 18340, Loss - 0.6163182926645463, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1130698055958925\n",
      "Step - 18341, Loss - 0.6495170858108544, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.34501553102004223\n",
      "Step - 18342, Loss - 0.6862302268110014, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1170700518174517\n",
      "Step - 18343, Loss - 0.7504708801138891, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.952338738366126\n",
      "Step - 18344, Loss - 0.54988601238878, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5155320282500162\n",
      "Step - 18345, Loss - 0.4815775918074349, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.445061981417053\n",
      "Step - 18346, Loss - 0.9247089759171482, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.0839512866001337\n",
      "Step - 18347, Loss - 0.7253545456540825, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9879420672557537\n",
      "Step - 18348, Loss - 0.7803779059125162, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7919223463199423\n",
      "Step - 18349, Loss - 0.7470351064487906, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.2860167317147203\n",
      "Step - 18350, Loss - 0.8179088307945229, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5124623557407324\n",
      "Step - 18351, Loss - 0.7291487499614723, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2351469040350331\n",
      "Step - 18352, Loss - 0.7709635453492782, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.125636233877694\n",
      "Step - 18353, Loss - 0.6767795461313579, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9422138749212218\n",
      "Step - 18354, Loss - 0.537010048742462, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9221984700171377\n",
      "Step - 18355, Loss - 0.8689527154560802, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.060105465345043\n",
      "Step - 18356, Loss - 0.7960667230197742, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7019272872092288\n",
      "Step - 18357, Loss - 0.6369286855266626, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1813262134686449\n",
      "Step - 18358, Loss - 0.8893760654095597, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9589601541036145\n",
      "Step - 18359, Loss - 0.8275442279240618, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9976799821958582\n",
      "Step - 18360, Loss - 0.7109868850718902, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9767494916381271\n",
      "Step - 18361, Loss - 0.6845800856415454, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2358744058964966\n",
      "Step - 18362, Loss - 0.5256456268997857, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5025192264057694\n",
      "Step - 18363, Loss - 0.7556823990109927, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9947661232517461\n",
      "Step - 18364, Loss - 0.5542086673647016, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9392091897055763\n",
      "Step - 18365, Loss - 0.6479617046529835, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0705845036752413\n",
      "Step - 18366, Loss - 0.740346416503924, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.2678716672470394\n",
      "Step - 18367, Loss - 0.9121499263351713, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8675068896346754\n",
      "Step - 18368, Loss - 0.7630769316099281, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6466355955557502\n",
      "Step - 18369, Loss - 0.6687714106187548, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9708968537881382\n",
      "Step - 18370, Loss - 0.8078167123261837, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4933824518775052\n",
      "Step - 18371, Loss - 0.7502361330176933, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9431390085829878\n",
      "Step - 18372, Loss - 0.6683968126145117, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.72467249329307\n",
      "Step - 18373, Loss - 0.5554045648421267, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9756636085440685\n",
      "Step - 18374, Loss - 0.7839608465516109, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.397151081572529\n",
      "Step - 18375, Loss - 0.7724562490029475, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9066120452499763\n",
      "Step - 18376, Loss - 0.5528425564990527, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0353284135692364\n",
      "Step - 18377, Loss - 0.6006349456192078, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3457639898113694\n",
      "Step - 18378, Loss - 0.7029822009103658, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5982739639276258\n",
      "Step - 18379, Loss - 0.7519016696474227, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7393171586553281\n",
      "Step - 18380, Loss - 0.7376219047385164, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7299708636117737\n",
      "Step - 18381, Loss - 0.6448430683571343, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.361585910959156\n",
      "Step - 18382, Loss - 0.6554959879915796, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7798706779241393\n",
      "Step - 18383, Loss - 0.6824272891287754, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.35846193090459655\n",
      "Step - 18384, Loss - 0.7679182383601515, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6435004405513911\n",
      "Step - 18385, Loss - 0.7003464438100587, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.42641702904631\n",
      "Step - 18386, Loss - 0.9625039736025183, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.440815564367653\n",
      "Step - 18387, Loss - 0.6400232056353892, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9866167875215366\n",
      "Step - 18388, Loss - 0.6294933926098043, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0610346361514837\n",
      "Step - 18389, Loss - 0.6091569831927298, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1374895804054808\n",
      "Step - 18390, Loss - 0.6386341786837785, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8280744512848541\n",
      "Step - 18391, Loss - 0.9215405657474838, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4331613531775407\n",
      "Step - 18392, Loss - 0.6514651399270112, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4159607444413878\n",
      "Step - 18393, Loss - 0.706548580507588, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2087188634376782\n",
      "Step - 18394, Loss - 0.6458925044910102, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.716144778078371\n",
      "Step - 18395, Loss - 0.7009072848852207, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9435091888519452\n",
      "Step - 18396, Loss - 0.7292483489697094, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0593713359831287\n",
      "Step - 18397, Loss - 0.596779979784503, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.2121324248825003\n",
      "Step - 18398, Loss - 0.6267779295419189, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8206639821474372\n",
      "Step - 18399, Loss - 0.8936400384170253, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0077817927919033\n",
      "Step - 18400, Loss - 0.6547442120860723, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8544472450037489\n",
      "Step - 18401, Loss - 0.6090424796867692, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2022614717750897\n",
      "Step - 18402, Loss - 0.7356682599738279, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5838751152764132\n",
      "Step - 18403, Loss - 0.5945400027232353, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1397583891446492\n",
      "Step - 18404, Loss - 0.6512407010244999, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.226692169924496\n",
      "Step - 18405, Loss - 1.0427588458235628, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 3.54232455620274\n",
      "Step - 18406, Loss - 0.8978961347412825, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.4490604395014293\n",
      "Step - 18407, Loss - 0.8159479811428453, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.3397440884882337\n",
      "Step - 18408, Loss - 0.6649079499908308, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9218601842118618\n",
      "Step - 18409, Loss - 0.7362088551869522, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.43385465596187855\n",
      "Step - 18410, Loss - 0.7844026219925591, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8121347504157292\n",
      "Step - 18411, Loss - 0.8141641887465357, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8259751381891228\n",
      "Step - 18412, Loss - 0.6234998782104586, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9520868782998209\n",
      "Step - 18413, Loss - 0.6432236464434989, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2144368950172006\n",
      "Step - 18414, Loss - 0.9645320503125767, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.413712102195423\n",
      "Step - 18415, Loss - 0.5369622825233598, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.048851825420679\n",
      "Step - 18416, Loss - 0.7515213248410773, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8034770478999467\n",
      "Step - 18417, Loss - 0.7453595837463115, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.3610329238011249\n",
      "Step - 18418, Loss - 0.6499584320428478, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.32675786537569007\n",
      "Step - 18419, Loss - 0.5980108758407606, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.566902017222538\n",
      "Step - 18420, Loss - 0.5996518139904565, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8514144688344347\n",
      "Step - 18421, Loss - 0.6291615642376549, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8117425567619122\n",
      "Step - 18422, Loss - 0.654701561964605, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2489832209363956\n",
      "Step - 18423, Loss - 0.8060469432296807, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8433774493434644\n",
      "Step - 18424, Loss - 0.4527714802410035, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.0074770993972257\n",
      "Step - 18425, Loss - 0.8189746251916812, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.632242686316834\n",
      "Step - 18426, Loss - 0.7747805764413711, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6008357292912119\n",
      "Step - 18427, Loss - 0.8624983671269637, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2328105052460987\n",
      "Step - 18428, Loss - 0.656657859411347, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6615599482455347\n",
      "Step - 18429, Loss - 0.5479564434974198, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.2061444043190614\n",
      "Step - 18430, Loss - 0.42866580562377743, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5165624303778613\n",
      "Step - 18431, Loss - 0.610391830341944, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5897424092918427\n",
      "Step - 18432, Loss - 0.7599188819987917, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5510604662971328\n",
      "Step - 18433, Loss - 0.7507936403904392, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.2370122728428044\n",
      "Step - 18434, Loss - 0.6323990053604247, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1314223918021074\n",
      "Step - 18435, Loss - 0.6950035952406931, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4796140693078428\n",
      "Step - 18436, Loss - 0.7356592250233892, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9236137991836089\n",
      "Step - 18437, Loss - 0.6332060291473994, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0819602235991022\n",
      "Step - 18438, Loss - 0.5789099380346067, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.852499361465965\n",
      "Step - 18439, Loss - 0.7751310059823349, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.978755217027884\n",
      "Step - 18440, Loss - 0.8340569416706911, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9494300913410446\n",
      "Step - 18441, Loss - 0.7757471427939799, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9390665407275749\n",
      "Step - 18442, Loss - 0.5777258566152523, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.590100633233349\n",
      "Step - 18443, Loss - 0.7052381749267742, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1514761069104629\n",
      "Step - 18444, Loss - 0.6188185507683457, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6052036490000612\n",
      "Step - 18445, Loss - 0.42153030154263915, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3472911063528124\n",
      "Step - 18446, Loss - 0.8702526439595464, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0539490460873506\n",
      "Step - 18447, Loss - 0.6866054954853162, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1154252934132838\n",
      "Step - 18448, Loss - 0.5851856949012573, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7750048246781428\n",
      "Step - 18449, Loss - 0.615834269932042, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8312558836764008\n",
      "Step - 18450, Loss - 0.5956875182188828, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7821735187293565\n",
      "Step - 18451, Loss - 0.5631485645840866, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8791580793728122\n",
      "Step - 18452, Loss - 0.7126250629526005, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3526428249688829\n",
      "Step - 18453, Loss - 0.7540453795237001, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8298054573171091\n",
      "Step - 18454, Loss - 0.6873994172865843, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5853723503792757\n",
      "Step - 18455, Loss - 0.8214478548138077, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6353037333530993\n",
      "Step - 18456, Loss - 0.7936049103897964, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0768095408445464\n",
      "Step - 18457, Loss - 0.8578903698141798, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.941000697806576\n",
      "Step - 18458, Loss - 0.5539410506449193, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3957837472746\n",
      "Step - 18459, Loss - 0.6495405844316261, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.339090351787568\n",
      "Step - 18460, Loss - 0.5782226133595779, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.2789995265394754\n",
      "Step - 18461, Loss - 0.6444410484838605, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6152699861602844\n",
      "Step - 18462, Loss - 0.914231782379064, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8181677990020655\n",
      "Step - 18463, Loss - 0.6965429645345075, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0203852219145844\n",
      "Step - 18464, Loss - 0.921313451224193, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7428826421528324\n",
      "Step - 18465, Loss - 0.6969505011390541, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.501018246729952\n",
      "Step - 18466, Loss - 0.6911543576626535, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.120274295564246\n",
      "Step - 18467, Loss - 0.6840948254945769, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1300466288056696\n",
      "Step - 18468, Loss - 0.8680452243704618, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3946116553708614\n",
      "Step - 18469, Loss - 0.5296844763053888, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3039008651944664\n",
      "Step - 18470, Loss - 0.7595680561956255, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9258453371653944\n",
      "Step - 18471, Loss - 0.5190355853097397, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4480521704709757\n",
      "Step - 18472, Loss - 0.5771985707167098, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8554952149139772\n",
      "Step - 18473, Loss - 0.772642230290554, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 3.217506260550199\n",
      "Step - 18474, Loss - 0.5784144264172874, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.38454573045613266\n",
      "Step - 18475, Loss - 0.9215013531831271, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7120925634295412\n",
      "Step - 18476, Loss - 0.6458588390896628, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6837285561268165\n",
      "Step - 18477, Loss - 0.64906636923282, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.684192265913627\n",
      "Step - 18478, Loss - 0.5479113660530663, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.230788151523304\n",
      "Step - 18479, Loss - 0.6825143948955095, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5339309484221076\n",
      "Step - 18480, Loss - 0.6353729190137873, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6105719718295548\n",
      "Step - 18481, Loss - 0.7097865168379702, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6522400660741545\n",
      "Step - 18482, Loss - 0.4893961250258211, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.2718519756403706\n",
      "Step - 18483, Loss - 0.8791964498171005, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.433267507486361\n",
      "Step - 18484, Loss - 0.7452363110008311, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.48125692387900676\n",
      "Step - 18485, Loss - 0.6132301873719004, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.560572616204658\n",
      "Step - 18486, Loss - 0.63887634758433, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5533594306278425\n",
      "Step - 18487, Loss - 0.634004557324246, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6716430680012979\n",
      "Step - 18488, Loss - 0.5474274115775449, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5575257177456105\n",
      "Step - 18489, Loss - 0.6000445184834545, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4416562854443737\n",
      "Step - 18490, Loss - 0.7766196602650793, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8145768816799944\n",
      "Step - 18491, Loss - 0.727580404869931, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4737384533222483\n",
      "Step - 18492, Loss - 0.4732261801205221, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2991549010992005\n",
      "Step - 18493, Loss - 0.6972460861249263, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.854865843367844\n",
      "Step - 18494, Loss - 0.7472014901657584, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.972695673174848\n",
      "Step - 18495, Loss - 0.8517214540896567, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8926025466511616\n",
      "Step - 18496, Loss - 0.7366803863785795, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4310246330689628\n",
      "Step - 18497, Loss - 0.7772685651010636, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9295376246789316\n",
      "Step - 18498, Loss - 0.7549080880307106, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0324479657710322\n",
      "Step - 18499, Loss - 0.8898329796212784, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9598219371919402\n",
      "Step - 18500, Loss - 0.6316890166819712, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.2628643420195163\n",
      "Step - 18501, Loss - 0.6098107622482811, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.716755180889815\n",
      "Step - 18502, Loss - 0.6728363495275415, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.917528391846348\n",
      "Step - 18503, Loss - 0.7288550765216821, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8204065894987622\n",
      "Step - 18504, Loss - 0.7208815474461769, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8130741926138852\n",
      "Step - 18505, Loss - 0.607406571580469, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2384895634478044\n",
      "Step - 18506, Loss - 0.6775684260262724, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7111070857973492\n",
      "Step - 18507, Loss - 0.7318043038066709, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.004270578575162\n",
      "Step - 18508, Loss - 0.7411797715598862, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5446105029614247\n",
      "Step - 18509, Loss - 0.5942141732910289, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.2479367366657035\n",
      "Step - 18510, Loss - 0.61959821964187, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.25900479980317\n",
      "Step - 18511, Loss - 0.6319758328389453, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6835826095964722\n",
      "Step - 18512, Loss - 0.7122779506394427, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4472632235202945\n",
      "Step - 18513, Loss - 0.6181413271070515, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8153639563139885\n",
      "Step - 18514, Loss - 0.6866113390893624, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6317530223230802\n",
      "Step - 18515, Loss - 0.843174566833375, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9429251676992754\n",
      "Step - 18516, Loss - 0.7148939198666797, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0914691800883476\n",
      "Step - 18517, Loss - 0.5758298668327727, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.3971598806777186\n",
      "Step - 18518, Loss - 0.4980005270405582, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5154848371689602\n",
      "Step - 18519, Loss - 0.5387075020043782, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9329241746255671\n",
      "Step - 18520, Loss - 0.6660295179463223, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7422111242150892\n",
      "Step - 18521, Loss - 0.7677439185230577, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5174988849132147\n",
      "Step - 18522, Loss - 0.6872664535233625, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2003505769709606\n",
      "Step - 18523, Loss - 0.566882189095993, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8216006615417512\n",
      "Step - 18524, Loss - 0.6423552090100617, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6455833746660629\n",
      "Step - 18525, Loss - 0.8684087884663919, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.52566203458219\n",
      "Step - 18526, Loss - 0.6982858054569099, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.439818754575569\n",
      "Step - 18527, Loss - 0.9305115621077967, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9409167425183828\n",
      "Step - 18528, Loss - 1.1393854787196813, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.3639091870282694\n",
      "Step - 18529, Loss - 0.7953788248873032, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.461096670910074\n",
      "Step - 18530, Loss - 0.6886453277354642, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1132723493380283\n",
      "Step - 18531, Loss - 0.6492931277823965, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2865456001947053\n",
      "Step - 18532, Loss - 0.6109772042017585, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1066717263466617\n",
      "Step - 18533, Loss - 0.7564056847653167, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.79044775432178\n",
      "Step - 18534, Loss - 0.662394760150966, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4174057579230603\n",
      "Step - 18535, Loss - 0.6742130031142302, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.900872290834659\n",
      "Step - 18536, Loss - 0.72557910333442, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.602989784910244\n",
      "Step - 18537, Loss - 0.5242981734184088, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1198565418414785\n",
      "Step - 18538, Loss - 0.8015386656259103, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5754398872854127\n",
      "Step - 18539, Loss - 0.6309372556444555, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8136173296330813\n",
      "Step - 18540, Loss - 0.6582039561454838, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.1882074815734125\n",
      "Step - 18541, Loss - 0.6109659733701747, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0904296926020438\n",
      "Step - 18542, Loss - 0.6617438651266953, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.959971303573494\n",
      "Step - 18543, Loss - 0.5491321684863205, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5792620121274186\n",
      "Step - 18544, Loss - 0.7213435462646518, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2677000011733943\n",
      "Step - 18545, Loss - 0.7464229992322517, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0082771303028264\n",
      "Step - 18546, Loss - 0.6701461879444094, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5216352448169418\n",
      "Step - 18547, Loss - 0.5252975291283859, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.3895407436757389\n",
      "Step - 18548, Loss - 0.8468487980922672, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0076027628768904\n",
      "Step - 18549, Loss - 0.9222717608777046, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6356787948999076\n",
      "Step - 18550, Loss - 0.6685949343535585, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8385713826395462\n",
      "Step - 18551, Loss - 0.7508199396661355, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4447928793136138\n",
      "Step - 18552, Loss - 0.673579600299456, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.093342296778475\n",
      "Step - 18553, Loss - 0.922627386850168, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.472276606727168\n",
      "Step - 18554, Loss - 0.6677989402145073, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.3769072455308758\n",
      "Step - 18555, Loss - 0.6591560490840445, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0765183343338451\n",
      "Step - 18556, Loss - 0.8827951188181913, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.138040219933246\n",
      "Step - 18557, Loss - 0.7158754358386341, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.844463690612724\n",
      "Step - 18558, Loss - 0.753578196897054, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5190358245349662\n",
      "Step - 18559, Loss - 0.6557521089547437, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8784386660188835\n",
      "Step - 18560, Loss - 0.704824897278295, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1229378651737385\n",
      "Step - 18561, Loss - 0.8113778806670271, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2448846984434343\n",
      "Step - 18562, Loss - 0.9264139066188264, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.1697742174022454\n",
      "Step - 18563, Loss - 0.7441954673406577, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.1061329936961353\n",
      "Step - 18564, Loss - 0.4896157671417853, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.139591976390961\n",
      "Step - 18565, Loss - 0.5687239754427693, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.567905415149187\n",
      "Step - 18566, Loss - 0.8135599560775271, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0298983977150147\n",
      "Step - 18567, Loss - 0.6815345875954079, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9077567003118616\n",
      "Step - 18568, Loss - 0.7667567201889718, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8197400729686747\n",
      "Step - 18569, Loss - 0.5866643417787907, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6349437216322631\n",
      "Step - 18570, Loss - 0.6933713413045772, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8051841185971972\n",
      "Step - 18571, Loss - 0.5702189307859253, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.3556500917255575\n",
      "Step - 18572, Loss - 0.5607783013071398, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5698215583277442\n",
      "Step - 18573, Loss - 0.5182656641648191, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.82930562825128\n",
      "Step - 18574, Loss - 0.7754728788924412, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7895374978106242\n",
      "Step - 18575, Loss - 0.6934139991077044, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1670390375947048\n",
      "Step - 18576, Loss - 0.5641298292717611, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0185033919838202\n",
      "Step - 18577, Loss - 0.6333265330258355, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4842542574889813\n",
      "Step - 18578, Loss - 0.6828234299073503, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.067129883346569\n",
      "Step - 18579, Loss - 0.4251352825771568, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3761135260475474\n",
      "Step - 18580, Loss - 0.4883901182983673, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1775217174673047\n",
      "Step - 18581, Loss - 0.847441026923788, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8919484719340707\n",
      "Step - 18582, Loss - 0.6130350756178242, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7350339557953419\n",
      "Step - 18583, Loss - 0.7245437575453141, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9384752924878967\n",
      "Step - 18584, Loss - 0.6914415424067634, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5566473227735871\n",
      "Step - 18585, Loss - 0.7214527341056926, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.36537529803331076\n",
      "Step - 18586, Loss - 0.739518424238669, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8793609038139795\n",
      "Step - 18587, Loss - 0.7518652220460318, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3527258789368763\n",
      "Step - 18588, Loss - 0.7834284941495194, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.229723484725728\n",
      "Step - 18589, Loss - 0.7200173258659157, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6112064262187407\n",
      "Step - 18590, Loss - 0.6787270351846896, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9631144417142891\n",
      "Step - 18591, Loss - 0.7297689645479721, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7511924857252534\n",
      "Step - 18592, Loss - 0.5768041729939548, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8033697483665074\n",
      "Step - 18593, Loss - 0.6767550228932894, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6275297210339005\n",
      "Step - 18594, Loss - 0.6099171339130818, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.23880020803734278\n",
      "Step - 18595, Loss - 0.7548534777073519, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2736995556733302\n",
      "Step - 18596, Loss - 0.76483543387368, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5717579782111364\n",
      "Step - 18597, Loss - 0.7279868870951629, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.5121750334359394\n",
      "Step - 18598, Loss - 0.7296627989159783, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6802927981394981\n",
      "Step - 18599, Loss - 0.7310513700228174, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6239469307941494\n",
      "Step - 18600, Loss - 0.5740333278675083, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2759270307401926\n",
      "Step - 18601, Loss - 0.7624377212928588, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1112230790466422\n",
      "Step - 18602, Loss - 0.5746761604019289, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1700754951749077\n",
      "Step - 18603, Loss - 0.923456424219277, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.0515200038799533\n",
      "Step - 18604, Loss - 0.6832777383765104, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1160979799006343\n",
      "Step - 18605, Loss - 0.6865963303003151, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5879988592955008\n",
      "Step - 18606, Loss - 0.9259088284195203, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.544401937219886\n",
      "Step - 18607, Loss - 0.6829358733888983, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0371824748714256\n",
      "Step - 18608, Loss - 0.7230423793384251, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.988238952027525\n",
      "Step - 18609, Loss - 0.8099093496763328, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0136575513226105\n",
      "Step - 18610, Loss - 0.763884575499205, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.83372571981216\n",
      "Step - 18611, Loss - 0.6393561656222275, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.902010661302261\n",
      "Step - 18612, Loss - 0.5950852287311643, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.399463997754475\n",
      "Step - 18613, Loss - 0.624165460039616, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.437812530708296\n",
      "Step - 18614, Loss - 0.6540540618357614, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.997480100820115\n",
      "Step - 18615, Loss - 0.7602963970182804, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.2179907428353562\n",
      "Step - 18616, Loss - 0.5869377629780077, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.518177724783784\n",
      "Step - 18617, Loss - 0.5853642088449302, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5336234760294525\n",
      "Step - 18618, Loss - 0.6280195760684488, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7068226797179793\n",
      "Step - 18619, Loss - 0.6202816283487284, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3511769822542605\n",
      "Step - 18620, Loss - 0.7431614845700705, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.029221318177128\n",
      "Step - 18621, Loss - 0.8082920081689244, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4301330106713583\n",
      "Step - 18622, Loss - 0.6409807269036759, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.545518262931369\n",
      "Step - 18623, Loss - 0.6487485406310981, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5529500175563462\n",
      "Step - 18624, Loss - 0.599294069422168, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.256616111918641\n",
      "Step - 18625, Loss - 0.5983463626414497, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5728034213143834\n",
      "Step - 18626, Loss - 0.6526752537519097, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.21316488596896493\n",
      "Step - 18627, Loss - 0.858796794974805, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9046190767985952\n",
      "Step - 18628, Loss - 0.6588310030140097, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9703497035786575\n",
      "Step - 18629, Loss - 0.7369183770507702, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.287078193450772\n",
      "Step - 18630, Loss - 0.6698184079078476, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8117473634261334\n",
      "Step - 18631, Loss - 0.7491364253133184, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0062268356992923\n",
      "Step - 18632, Loss - 0.6464288511027708, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2553679558703472\n",
      "Step - 18633, Loss - 0.7451845009073024, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.21944832033529\n",
      "Step - 18634, Loss - 0.5676616015479752, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7401633483295142\n",
      "Step - 18635, Loss - 0.4888513124798647, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2002959864524505\n",
      "Step - 18636, Loss - 0.709244712639669, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6754296061962948\n",
      "Step - 18637, Loss - 0.8138908154665102, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1291505854131558\n",
      "Step - 18638, Loss - 0.8405312270561931, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6392414167304596\n",
      "Step - 18639, Loss - 0.7613308986947988, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.078430129792542\n",
      "Step - 18640, Loss - 0.7015413377803739, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9165344307224302\n",
      "Step - 18641, Loss - 0.741471092584814, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9715944170674266\n",
      "Step - 18642, Loss - 0.8447860252768739, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1018834605692096\n",
      "Step - 18643, Loss - 0.7715027145082738, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0410051323805805\n",
      "Step - 18644, Loss - 0.5961111377643795, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1148975501170022\n",
      "Step - 18645, Loss - 0.6604794759059279, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5316870398779505\n",
      "Step - 18646, Loss - 0.6025313849352523, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9716246843773632\n",
      "Step - 18647, Loss - 0.8211488558252915, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7323744126604876\n",
      "Step - 18648, Loss - 0.7327311606596253, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.734694643565791\n",
      "Step - 18649, Loss - 0.5535474559454285, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.055955631582766\n",
      "Step - 18650, Loss - 0.7470250340232409, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6955748569371056\n",
      "Step - 18651, Loss - 0.853933608241056, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.417096819626269\n",
      "Step - 18652, Loss - 0.5276274261638062, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2418115671693428\n",
      "Step - 18653, Loss - 0.8531058684520079, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1262287608007444\n",
      "Step - 18654, Loss - 0.7598755363264937, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.342146150073421\n",
      "Step - 18655, Loss - 0.6379316842922557, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.84468878165219\n",
      "Step - 18656, Loss - 0.6102208388162247, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.078455374471808\n",
      "Step - 18657, Loss - 0.5924333304333068, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.324675398195089\n",
      "Step - 18658, Loss - 0.929200157427938, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8807942734422172\n",
      "Step - 18659, Loss - 0.4831860271651593, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1402385120916438\n",
      "Step - 18660, Loss - 0.6032222614575473, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.316067368495384\n",
      "Step - 18661, Loss - 0.7441641306307529, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4852233042512733\n",
      "Step - 18662, Loss - 0.7090924388914427, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9562662874350039\n",
      "Step - 18663, Loss - 0.5704567479571394, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0394953724286469\n",
      "Step - 18664, Loss - 0.8345762986864333, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8643284528822557\n",
      "Step - 18665, Loss - 0.7404629805955834, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.384177911716887\n",
      "Step - 18666, Loss - 0.6992262070962375, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.033901513043454\n",
      "Step - 18667, Loss - 0.7739971504190459, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.368844773658516\n",
      "Step - 18668, Loss - 0.47412871228913495, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3237769815758755\n",
      "Step - 18669, Loss - 0.619385211092366, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3744458361426406\n",
      "Step - 18670, Loss - 0.7897054464399788, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8521552687370855\n",
      "Step - 18671, Loss - 0.7017679664427737, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4062971562341875\n",
      "Step - 18672, Loss - 0.5535815931164005, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.2957210009972636\n",
      "Step - 18673, Loss - 0.5478127695523525, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6355299168009632\n",
      "Step - 18674, Loss - 0.6478803869171601, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.576783957103698\n",
      "Step - 18675, Loss - 0.6589258133592902, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.665612571015966\n",
      "Step - 18676, Loss - 0.9407423283141154, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7092161904593217\n",
      "Step - 18677, Loss - 0.584686424144101, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4630678195259461\n",
      "Step - 18678, Loss - 0.7937481954912038, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7248199780362461\n",
      "Step - 18679, Loss - 0.6370226138312794, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7943989315202556\n",
      "Step - 18680, Loss - 0.6934312116416664, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.384158937777988\n",
      "Step - 18681, Loss - 0.8429866634601095, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.274887670437527\n",
      "Step - 18682, Loss - 0.9250608454053454, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.5436414952021997\n",
      "Step - 18683, Loss - 0.6953175124891151, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4644290302258323\n",
      "Step - 18684, Loss - 0.7160336285101946, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4654224260037116\n",
      "Step - 18685, Loss - 0.6281977562856478, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4413909764174169\n",
      "Step - 18686, Loss - 0.7057641400979509, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3743336017587884\n",
      "Step - 18687, Loss - 0.9083200754003399, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1703922715812325\n",
      "Step - 18688, Loss - 0.8230971579444067, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.945764980066504\n",
      "Step - 18689, Loss - 0.5874109310289882, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2940124153745678\n",
      "Step - 18690, Loss - 0.8592161064306565, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1751866210852366\n",
      "Step - 18691, Loss - 0.6151366847329105, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0100117084591105\n",
      "Step - 18692, Loss - 0.8162966057988142, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2607099587434636\n",
      "Step - 18693, Loss - 0.6987112531799535, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5964124760385774\n",
      "Step - 18694, Loss - 0.7057600818589143, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5727231772269334\n",
      "Step - 18695, Loss - 0.9383487139992002, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0363427781543426\n",
      "Step - 18696, Loss - 0.5804680401217867, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9023180334232813\n",
      "Step - 18697, Loss - 0.6498423707053789, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0755351475024437\n",
      "Step - 18698, Loss - 0.858950390824864, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8716235064071356\n",
      "Step - 18699, Loss - 0.6241379426699408, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1770209257653046\n",
      "Step - 18700, Loss - 0.7205002225508511, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4379573967573824\n",
      "Step - 18701, Loss - 0.597876445783495, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0699878293130944\n",
      "Step - 18702, Loss - 0.7219316407414129, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5534185020004212\n",
      "Step - 18703, Loss - 0.6737128181593024, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4136346733662477\n",
      "Step - 18704, Loss - 0.7166248391577776, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.167871159756376\n",
      "Step - 18705, Loss - 0.560560918248942, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2160985517540281\n",
      "Step - 18706, Loss - 0.6516317674410065, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1350482312000374\n",
      "Step - 18707, Loss - 0.7179220069962672, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 3.2566734497601892\n",
      "Step - 18708, Loss - 0.7913865202579269, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8438059949384942\n",
      "Step - 18709, Loss - 0.6538633432771033, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7558084617971881\n",
      "Step - 18710, Loss - 0.6141425379711984, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2421482407046458\n",
      "Step - 18711, Loss - 0.9504357619569826, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5493046878236014\n",
      "Step - 18712, Loss - 0.4031510090650331, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.576574685825793\n",
      "Step - 18713, Loss - 0.6566040588459614, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.37223965644245516\n",
      "Step - 18714, Loss - 0.8802225941366355, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.624162423115976\n",
      "Step - 18715, Loss - 0.666626759040172, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2330547076576035\n",
      "Step - 18716, Loss - 0.697039309033739, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0418060937653977\n",
      "Step - 18717, Loss - 0.5931910026077112, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3771774782800523\n",
      "Step - 18718, Loss - 0.6441272873032794, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.2833361253370426\n",
      "Step - 18719, Loss - 0.6804211548586768, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9790099415845719\n",
      "Step - 18720, Loss - 0.800532442775701, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0481826805444614\n",
      "Step - 18721, Loss - 0.7542124654867393, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.501376495319493\n",
      "Step - 18722, Loss - 0.6492493014258858, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9442311636454003\n",
      "Step - 18723, Loss - 0.7683639446749048, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8591857364653815\n",
      "Step - 18724, Loss - 0.7333183325921327, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0334163793780977\n",
      "Step - 18725, Loss - 0.901488617440674, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.5808149531242477\n",
      "Step - 18726, Loss - 0.6296133611335742, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6603727816008406\n",
      "Step - 18727, Loss - 0.7167340782259216, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8854967448507463\n",
      "Step - 18728, Loss - 0.5594312579240837, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4926603670525593\n",
      "Step - 18729, Loss - 0.8182077654416035, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9555098619960108\n",
      "Step - 18730, Loss - 0.7116090450094521, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3056986562878627\n",
      "Step - 18731, Loss - 0.6229433858536795, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1183526036319513\n",
      "Step - 18732, Loss - 0.6692626561695392, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7911255199422498\n",
      "Step - 18733, Loss - 0.6501671794882787, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0271985328456654\n",
      "Step - 18734, Loss - 0.6770571802423117, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6094423950283127\n",
      "Step - 18735, Loss - 0.6368640137432743, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3023006712407172\n",
      "Step - 18736, Loss - 0.662891240817879, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7042025983233672\n",
      "Step - 18737, Loss - 0.8921645377981307, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7438882586572042\n",
      "Step - 18738, Loss - 0.7319730357070577, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3852715953210488\n",
      "Step - 18739, Loss - 0.8477700575475067, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2196963112159134\n",
      "Step - 18740, Loss - 0.8519968647123065, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6467020042858467\n",
      "Step - 18741, Loss - 0.920859557709657, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.171354840868742\n",
      "Step - 18742, Loss - 0.5691983779878613, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.45881501027047633\n",
      "Step - 18743, Loss - 0.6171813716693713, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7707764603077294\n",
      "Step - 18744, Loss - 0.7157687071452036, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6734193992772909\n",
      "Step - 18745, Loss - 0.7011138908854752, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8632978450957737\n",
      "Step - 18746, Loss - 0.6361636284738272, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.011449224315163\n",
      "Step - 18747, Loss - 0.7605790454426833, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4581506837933091\n",
      "Step - 18748, Loss - 0.7137278805167202, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.48183719623565235\n",
      "Step - 18749, Loss - 0.47413519767276235, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3186565047288341\n",
      "Step - 18750, Loss - 0.5080777106665186, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7132218792116223\n",
      "Step - 18751, Loss - 0.7856730642617894, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0148369946860232\n",
      "Step - 18752, Loss - 0.7343851317784241, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4291439163111401\n",
      "Step - 18753, Loss - 0.5202112256914934, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0463323099481523\n",
      "Step - 18754, Loss - 0.6993429503332128, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7203798250957959\n",
      "Step - 18755, Loss - 0.5645896066147527, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.39146687527669877\n",
      "Step - 18756, Loss - 0.6210359652724425, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7746241533741761\n",
      "Step - 18757, Loss - 0.6407243438612282, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.31303463729430453\n",
      "Step - 18758, Loss - 0.6933048348380432, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.153366972770873\n",
      "Step - 18759, Loss - 0.7506286346719488, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3498146781261908\n",
      "Step - 18760, Loss - 0.7051692325053097, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0249326864952049\n",
      "Step - 18761, Loss - 0.4832732038670763, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.383778047823673\n",
      "Step - 18762, Loss - 0.6389532125502054, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2497698212059323\n",
      "Step - 18763, Loss - 0.7744373869280956, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.381536039534955\n",
      "Step - 18764, Loss - 0.4342013556556353, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.114834189147037\n",
      "Step - 18765, Loss - 0.8695746795076282, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5422753470520256\n",
      "Step - 18766, Loss - 0.7666723592478589, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.321395154673019\n",
      "Step - 18767, Loss - 0.5528024415306491, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.032901244681822\n",
      "Step - 18768, Loss - 0.5387080657631962, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8976235861954547\n",
      "Step - 18769, Loss - 0.7060819396891487, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3000811280592393\n",
      "Step - 18770, Loss - 0.6384551346521564, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9625930612286615\n",
      "Step - 18771, Loss - 0.6365603108527691, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.26758939553739686\n",
      "Step - 18772, Loss - 0.6615604084720581, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5838832308315252\n",
      "Step - 18773, Loss - 0.6215419731938815, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2134166501719275\n",
      "Step - 18774, Loss - 0.7614251989814598, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.524912225461804\n",
      "Step - 18775, Loss - 0.7643106818537094, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.653148788171468\n",
      "Step - 18776, Loss - 0.6596140870295059, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8178136762310501\n",
      "Step - 18777, Loss - 0.7388165719034316, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.3901880316301217\n",
      "Step - 18778, Loss - 0.638418650818495, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0592449650992275\n",
      "Step - 18779, Loss - 0.8950933764147326, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3939173262439404\n",
      "Step - 18780, Loss - 0.7440387669648861, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8752313832751416\n",
      "Step - 18781, Loss - 0.668495759509411, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7719934760429944\n",
      "Step - 18782, Loss - 0.7708355609474117, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.960038815791256\n",
      "Step - 18783, Loss - 0.7058431165487172, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2722887016857494\n",
      "Step - 18784, Loss - 0.7449572040140474, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.897100203128868\n",
      "Step - 18785, Loss - 0.8560465094266757, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.21791971828505227\n",
      "Step - 18786, Loss - 0.7939077758088333, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5091906412844185\n",
      "Step - 18787, Loss - 0.799070380409872, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9253792948896223\n",
      "Step - 18788, Loss - 0.6507662535151313, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.3152968044609883\n",
      "Step - 18789, Loss - 0.7370047278324284, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7775697920311996\n",
      "Step - 18790, Loss - 0.6992172311921957, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8842683085861928\n",
      "Step - 18791, Loss - 0.532003982793585, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.7838901623910988\n",
      "Step - 18792, Loss - 0.6846906333667073, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1495279466669996\n",
      "Step - 18793, Loss - 0.8426749012062874, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.398714390900908\n",
      "Step - 18794, Loss - 0.6136104324469699, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.968796728635319\n",
      "Step - 18795, Loss - 0.5040758223117434, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0070750673842228\n",
      "Step - 18796, Loss - 0.6203908441372069, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.51182613763502\n",
      "Step - 18797, Loss - 0.8231756828700179, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4768431837234866\n",
      "Step - 18798, Loss - 0.8057113712025757, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3010182765721694\n",
      "Step - 18799, Loss - 0.8471099146907216, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7614980562101303\n",
      "Step - 18800, Loss - 0.57995222707508, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.3837719318629741\n",
      "Step - 18801, Loss - 0.5917485923825239, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6870021881955864\n",
      "Step - 18802, Loss - 0.8048717375686462, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.124622054890024\n",
      "Step - 18803, Loss - 0.6376699102130141, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9739494487941396\n",
      "Step - 18804, Loss - 0.7111691585550769, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.862648790125028\n",
      "Step - 18805, Loss - 0.7412241608977894, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.0364734943390173\n",
      "Step - 18806, Loss - 0.6981706279385405, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1809911323830025\n",
      "Step - 18807, Loss - 0.6415057112234243, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.622083501309803\n",
      "Step - 18808, Loss - 0.6981291918310745, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0448625150430113\n",
      "Step - 18809, Loss - 0.8372299563564503, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3042557323354085\n",
      "Step - 18810, Loss - 0.6537777671220695, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6888632294648782\n",
      "Step - 18811, Loss - 0.6890885236053358, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.019756169411724\n",
      "Step - 18812, Loss - 0.7349671509020801, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.9861970011218233\n",
      "Step - 18813, Loss - 0.4697383892282811, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2952488160931748\n",
      "Step - 18814, Loss - 0.7500979042154899, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9581051578427053\n",
      "Step - 18815, Loss - 0.7277943009768657, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7094827432649112\n",
      "Step - 18816, Loss - 0.6238869364910721, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0630280067384628\n",
      "Step - 18817, Loss - 0.7145663697222259, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0711509526182528\n",
      "Step - 18818, Loss - 0.6747448205640165, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8292716265233793\n",
      "Step - 18819, Loss - 0.7914279794409527, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.5596128642405827\n",
      "Step - 18820, Loss - 0.564309469816664, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9019056762552291\n",
      "Step - 18821, Loss - 0.6438912363313245, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1438826575190417\n",
      "Step - 18822, Loss - 0.6723710682523267, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5369303207436055\n",
      "Step - 18823, Loss - 0.7564772029206198, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7903557653298748\n",
      "Step - 18824, Loss - 0.6749945187474425, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2457005795125276\n",
      "Step - 18825, Loss - 0.8315758946989898, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.394624795949312\n",
      "Step - 18826, Loss - 0.8043743634063298, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.1888599877372092\n",
      "Step - 18827, Loss - 0.6721994071664762, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0714997728704958\n",
      "Step - 18828, Loss - 0.5791964128962055, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2100792956680646\n",
      "Step - 18829, Loss - 0.5370552991027441, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.29049031374857\n",
      "Step - 18830, Loss - 0.7187769758602176, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5811149795713814\n",
      "Step - 18831, Loss - 0.843542820260806, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8699293910197032\n",
      "Step - 18832, Loss - 0.5940230476808697, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3110517967492956\n",
      "Step - 18833, Loss - 0.7566280367052937, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7514819969736781\n",
      "Step - 18834, Loss - 0.5596550545478282, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8944082984437804\n",
      "Step - 18835, Loss - 0.5273905838367763, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2258795265053748\n",
      "Step - 18836, Loss - 0.9131080203347309, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5628857763628601\n",
      "Step - 18837, Loss - 0.8322524261448141, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6494638039277975\n",
      "Step - 18838, Loss - 0.5634179538651749, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1738989934903845\n",
      "Step - 18839, Loss - 0.575077457418338, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1968788258564953\n",
      "Step - 18840, Loss - 0.6430551125502774, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9540636309162873\n",
      "Step - 18841, Loss - 0.6318497082015246, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0052103598786808\n",
      "Step - 18842, Loss - 0.6821544785087668, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1592235177102554\n",
      "Step - 18843, Loss - 0.755535538336922, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.3569320869567706\n",
      "Step - 18844, Loss - 0.8766865947836395, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0342506445242827\n",
      "Step - 18845, Loss - 0.79990436717592, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.44244383432537077\n",
      "Step - 18846, Loss - 0.6547654104440594, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.868943710638611\n",
      "Step - 18847, Loss - 0.8182908554762031, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8710916303143632\n",
      "Step - 18848, Loss - 0.6224440446607331, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0259314887616313\n",
      "Step - 18849, Loss - 0.9591006651205524, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.083151286285326\n",
      "Step - 18850, Loss - 0.7459156019640276, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.090604103309402\n",
      "Step - 18851, Loss - 0.7693394615665702, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8860676011551514\n",
      "Step - 18852, Loss - 0.7534013077211307, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8341814313808751\n",
      "Step - 18853, Loss - 0.6983939594225826, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.836067378976382\n",
      "Step - 18854, Loss - 0.743929931770022, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9564539455207451\n",
      "Step - 18855, Loss - 0.8643876115993308, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.3792067345580085\n",
      "Step - 18856, Loss - 0.5302807950945974, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5640609326683869\n",
      "Step - 18857, Loss - 0.8347935059725801, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6521393267834539\n",
      "Step - 18858, Loss - 0.7022973828531642, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5739763410680201\n",
      "Step - 18859, Loss - 0.9063234460344116, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.3595787021616205\n",
      "Step - 18860, Loss - 0.6027433023898259, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.533777470621145\n",
      "Step - 18861, Loss - 0.6477620283486911, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6733571166185065\n",
      "Step - 18862, Loss - 0.7924324365520172, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8073546190519343\n",
      "Step - 18863, Loss - 0.7243030597231797, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.155993018803356\n",
      "Step - 18864, Loss - 0.6587143252693597, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6751028532112024\n",
      "Step - 18865, Loss - 0.6411646325563822, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7189513965494112\n",
      "Step - 18866, Loss - 0.8257016519795408, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6834179755699572\n",
      "Step - 18867, Loss - 0.6403171589736352, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.6188311935914634\n",
      "Step - 18868, Loss - 0.8650081365666552, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6773187764658715\n",
      "Step - 18869, Loss - 0.74849946640776, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3742635730891821\n",
      "Step - 18870, Loss - 0.9560181318114966, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5388589532977844\n",
      "Step - 18871, Loss - 0.5522606965739951, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 3.1289807505589713\n",
      "Step - 18872, Loss - 0.6058885056765984, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2479377535927094\n",
      "Step - 18873, Loss - 0.6095656074026892, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7005119181596935\n",
      "Step - 18874, Loss - 0.6548103876901302, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.079608018982662\n",
      "Step - 18875, Loss - 0.695692429357272, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3428085839166732\n",
      "Step - 18876, Loss - 0.5907050765344374, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.88269334238726\n",
      "Step - 18877, Loss - 0.5687626562596954, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7612279347851536\n",
      "Step - 18878, Loss - 0.7084769877265522, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0897825029139878\n",
      "Step - 18879, Loss - 0.6819298170721613, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3222862403411066\n",
      "Step - 18880, Loss - 0.9210362947190338, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9545318503323663\n",
      "Step - 18881, Loss - 0.8924436886440116, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0222223494149698\n",
      "Step - 18882, Loss - 0.8737427584505515, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.029568599090368\n",
      "Step - 18883, Loss - 0.8024451721756781, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.0366922285455127\n",
      "Step - 18884, Loss - 0.6228583607797932, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0863441994983756\n",
      "Step - 18885, Loss - 0.5870586572932271, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5887245058909414\n",
      "Step - 18886, Loss - 0.8521230689925139, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.791877506530332\n",
      "Step - 18887, Loss - 0.8161449745358708, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1570123333502795\n",
      "Step - 18888, Loss - 0.6322320616290695, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.31069242083768595\n",
      "Step - 18889, Loss - 0.6245478875392594, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6231813974367095\n",
      "Step - 18890, Loss - 0.48538365261814975, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2947198205010473\n",
      "Step - 18891, Loss - 0.5220304575793826, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6941488004912574\n",
      "Step - 18892, Loss - 0.7626188273979142, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8039342942578448\n",
      "Step - 18893, Loss - 0.7053100341658171, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.26478973779339215\n",
      "Step - 18894, Loss - 0.6706009112190037, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8442563326326633\n",
      "Step - 18895, Loss - 0.8246657041560425, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2418375232451035\n",
      "Step - 18896, Loss - 0.5243131361857167, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8580146340848233\n",
      "Step - 18897, Loss - 0.6866537262766825, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.5032021817662558\n",
      "Step - 18898, Loss - 0.8185125113949602, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.480528267634763\n",
      "Step - 18899, Loss - 0.6680296460375406, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7445037725898621\n",
      "Step - 18900, Loss - 0.7009773260191458, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7980885836922871\n",
      "Step - 18901, Loss - 0.6745864493436242, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2038923225525435\n",
      "Step - 18902, Loss - 0.6452275461415599, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.424310960622542\n",
      "Step - 18903, Loss - 0.7584658638629316, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5715082824348083\n",
      "Step - 18904, Loss - 0.8884744997950792, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3914912917519604\n",
      "Step - 18905, Loss - 0.6741927954460548, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6769169951901695\n",
      "Step - 18906, Loss - 0.7206171473528772, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.2173758337058382\n",
      "Step - 18907, Loss - 0.8863424903025436, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.64705649432213\n",
      "Step - 18908, Loss - 0.6362238552257102, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4717132438150928\n",
      "Step - 18909, Loss - 0.8719132625075146, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.5911431601370762\n",
      "Step - 18910, Loss - 0.7075874426260009, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0251873992396021\n",
      "Step - 18911, Loss - 0.8261650473817629, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4773888106238043\n",
      "Step - 18912, Loss - 0.7581423519018721, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.888999967056367\n",
      "Step - 18913, Loss - 0.6933311648765359, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.356118704429896\n",
      "Step - 18914, Loss - 0.7956207035263705, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7810909595628064\n",
      "Step - 18915, Loss - 0.6171897935317244, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4589563933771743\n",
      "Step - 18916, Loss - 0.5987622028171001, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.49336882528437\n",
      "Step - 18917, Loss - 0.6800780452951883, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5612559074288356\n",
      "Step - 18918, Loss - 0.532333911751505, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1103408724961954\n",
      "Step - 18919, Loss - 0.8091467479110342, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7358736319195724\n",
      "Step - 18920, Loss - 0.8037121303279915, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.1629600685905572\n",
      "Step - 18921, Loss - 0.8036255171724815, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.278368577977069\n",
      "Step - 18922, Loss - 0.7008221782398703, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.077244987294783\n",
      "Step - 18923, Loss - 0.7478596085601743, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5842572959416552\n",
      "Step - 18924, Loss - 0.6063026750652045, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9163722573037199\n",
      "Step - 18925, Loss - 0.7779719894997623, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.3295752066423778\n",
      "Step - 18926, Loss - 0.6251941018324739, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.5191033526851054\n",
      "Step - 18927, Loss - 0.9266538850346153, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2518809572113996\n",
      "Step - 18928, Loss - 0.7246665870460268, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9680008408962177\n",
      "Step - 18929, Loss - 0.7292662392334114, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5559625167824864\n",
      "Step - 18930, Loss - 0.6465145801728658, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7952826491708742\n",
      "Step - 18931, Loss - 0.6154927318109175, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.4986038501695427\n",
      "Step - 18932, Loss - 0.6123095786543807, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2476829138589332\n",
      "Step - 18933, Loss - 0.7352375885183281, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6194752679185627\n",
      "Step - 18934, Loss - 0.6258830854075954, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6249057223944944\n",
      "Step - 18935, Loss - 0.7574673153796011, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.42420045220923\n",
      "Step - 18936, Loss - 0.5771221795506947, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6873803620569202\n",
      "Step - 18937, Loss - 0.6873618116299103, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.49034880552297855\n",
      "Step - 18938, Loss - 0.9109303605966492, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.290698174182313\n",
      "Step - 18939, Loss - 0.7300542920714428, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7487909507064899\n",
      "Step - 18940, Loss - 0.871202667174914, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5518884608871283\n",
      "Step - 18941, Loss - 0.8422170895245411, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3971078616347135\n",
      "Step - 18942, Loss - 0.6647502185934324, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5319724007295716\n",
      "Step - 18943, Loss - 0.69657942765744, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.8002738629755994\n",
      "Step - 18944, Loss - 0.5687438865733838, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6910056253635124\n",
      "Step - 18945, Loss - 0.7384942248795807, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7390276938438252\n",
      "Step - 18946, Loss - 0.6462322015498594, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.337973928763555\n",
      "Step - 18947, Loss - 0.8160989505159972, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.42005521427205733\n",
      "Step - 18948, Loss - 0.8202887614345977, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8657321359791805\n",
      "Step - 18949, Loss - 0.5889328035458097, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.562778959101153\n",
      "Step - 18950, Loss - 0.7668972085940622, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3074925394211283\n",
      "Step - 18951, Loss - 0.8059780365086289, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9303529934921677\n",
      "Step - 18952, Loss - 0.5594115690018625, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.28067528354333\n",
      "Step - 18953, Loss - 1.0150646600094277, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4392873311742496\n",
      "Step - 18954, Loss - 0.9219975919475195, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.5598597747451197\n",
      "Step - 18955, Loss - 0.8820530396440243, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6533416887790602\n",
      "Step - 18956, Loss - 0.6780033861352207, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6494135778043548\n",
      "Step - 18957, Loss - 0.750989941858643, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8998293666993613\n",
      "Step - 18958, Loss - 0.5899697845075059, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2824236214251399\n",
      "Step - 18959, Loss - 0.9521048523165802, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6242025928648502\n",
      "Step - 18960, Loss - 0.7802648377477247, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0122361552864565\n",
      "Step - 18961, Loss - 0.5717913071415014, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.42105103453992\n",
      "Step - 18962, Loss - 0.6201245776053537, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.798236245034656\n",
      "Step - 18963, Loss - 0.6857109881908866, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.786323068752892\n",
      "Step - 18964, Loss - 0.7442308376459768, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4448153768618421\n",
      "Step - 18965, Loss - 0.6976352841169315, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6817392143443511\n",
      "Step - 18966, Loss - 0.6650498562393038, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5173541140938288\n",
      "Step - 18967, Loss - 0.6950871500298705, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7458956469972654\n",
      "Step - 18968, Loss - 0.7042661634961803, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.561104591457653\n",
      "Step - 18969, Loss - 0.8105552330581937, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2362871163924243\n",
      "Step - 18970, Loss - 0.6289207033679295, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.3786304966099094\n",
      "Step - 18971, Loss - 0.6498575867444799, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9081130573366067\n",
      "Step - 18972, Loss - 0.494825517942077, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3571561527122193\n",
      "Step - 18973, Loss - 0.651263637877354, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.2515479679677328\n",
      "Step - 18974, Loss - 0.8012135130239452, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8997968802008305\n",
      "Step - 18975, Loss - 0.6131961645503339, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.9376558002489728\n",
      "Step - 18976, Loss - 0.9997162016698722, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.3425506326960637\n",
      "Step - 18977, Loss - 0.6977373169963604, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6470209290468595\n",
      "Step - 18978, Loss - 0.6652627432826779, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.238263888681689\n",
      "Step - 18979, Loss - 0.7837681622447725, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.8342749821746781\n",
      "Step - 18980, Loss - 0.7494028739984628, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.31905068130239017\n",
      "Step - 18981, Loss - 0.7473158249402525, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0069059468490267\n",
      "Step - 18982, Loss - 0.5329257842008299, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.5844682654491321\n",
      "Step - 18983, Loss - 0.6312064645716167, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6019357664143519\n",
      "Step - 18984, Loss - 0.6130835699046646, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6624094076481657\n",
      "Step - 18985, Loss - 0.800596730675352, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.3774191020420734\n",
      "Step - 18986, Loss - 0.8145063946431189, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.3591163949715634\n",
      "Step - 18987, Loss - 0.7753068550365737, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6305470127910153\n",
      "Step - 18988, Loss - 0.7017223398164757, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 2.948900289597005\n",
      "Step - 18989, Loss - 0.918247487275506, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.6860659823400321\n",
      "Step - 18990, Loss - 0.39097967108965154, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.7020572019953429\n",
      "Step - 18991, Loss - 0.7800877992285568, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.30656107698836893\n",
      "Step - 18992, Loss - 0.4786346856016812, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.4908280515375762\n",
      "Step - 18993, Loss - 0.8292910434976769, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.9833512291063677\n",
      "Step - 18994, Loss - 0.7510197895308175, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6243746260663652\n",
      "Step - 18995, Loss - 0.7030377043839817, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0519999151543373\n",
      "Step - 18996, Loss - 0.5563184204362437, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.7763639927214228\n",
      "Step - 18997, Loss - 0.6537252145355452, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0031071460298302\n",
      "Step - 18998, Loss - 0.7667107267563087, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 1.0221747201632394\n",
      "Step - 18999, Loss - 0.6521775019619453, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 0.6186644562323811\n",
      "Step - 19000, Loss - 0.7611056320695619, Learning Rate - 1.9073486328125e-07, magnitude of gradient - 3.742997511648819\n",
      "Step - 19001, Loss - 0.6694229120122774, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7706004237920392\n",
      "Step - 19002, Loss - 0.590315643095066, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7137998563402155\n",
      "Step - 19003, Loss - 0.5594492201261518, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.113453819580891\n",
      "Step - 19004, Loss - 0.7959471938544951, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6171035028444776\n",
      "Step - 19005, Loss - 0.6529307295649303, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7027304879348769\n",
      "Step - 19006, Loss - 0.8403309619116398, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2190443656998071\n",
      "Step - 19007, Loss - 0.5806985355826542, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8800641642112872\n",
      "Step - 19008, Loss - 0.6124648456343076, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1089426105358517\n",
      "Step - 19009, Loss - 0.6616123195816597, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.257199436842649\n",
      "Step - 19010, Loss - 0.6702540074236085, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3274403646437953\n",
      "Step - 19011, Loss - 0.6646477264350352, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8645241296435634\n",
      "Step - 19012, Loss - 0.8260735342424672, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1177109994769914\n",
      "Step - 19013, Loss - 0.6343843336270951, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3811871282103743\n",
      "Step - 19014, Loss - 0.8323521301325731, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.055225498433062\n",
      "Step - 19015, Loss - 0.5900535214230844, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3172596246238562\n",
      "Step - 19016, Loss - 0.7423493687304603, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4839450024263805\n",
      "Step - 19017, Loss - 0.7325951763217414, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8656050665269425\n",
      "Step - 19018, Loss - 0.7541427876340667, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.328427577970798\n",
      "Step - 19019, Loss - 0.6921573075342229, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.095360091340665\n",
      "Step - 19020, Loss - 0.6044091220872512, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7680152431485159\n",
      "Step - 19021, Loss - 0.8979793876442128, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.0464587120191573\n",
      "Step - 19022, Loss - 0.6032842755708816, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9775996439459517\n",
      "Step - 19023, Loss - 0.747280244426036, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.778866896995255\n",
      "Step - 19024, Loss - 0.6782834781544317, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.836361177287285\n",
      "Step - 19025, Loss - 0.6848249491105656, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9334768945034706\n",
      "Step - 19026, Loss - 0.5073595107905784, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.9852979044706585\n",
      "Step - 19027, Loss - 0.7727520474175162, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0275583244662738\n",
      "Step - 19028, Loss - 0.6344888015368404, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.628945999748752\n",
      "Step - 19029, Loss - 0.5907222345631624, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7996947847739915\n",
      "Step - 19030, Loss - 0.7425022002021443, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2283568366970001\n",
      "Step - 19031, Loss - 0.4259389775298722, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5102519896700486\n",
      "Step - 19032, Loss - 0.7403547323176168, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6425586323488954\n",
      "Step - 19033, Loss - 0.8608475578302205, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.473980385057671\n",
      "Step - 19034, Loss - 0.8053006230740116, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2651504515983976\n",
      "Step - 19035, Loss - 0.5249385321145239, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4462267731035523\n",
      "Step - 19036, Loss - 0.7120525575625268, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.377555608244238\n",
      "Step - 19037, Loss - 0.6894148335756962, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6053519187949388\n",
      "Step - 19038, Loss - 0.7173792771321, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8441283505128949\n",
      "Step - 19039, Loss - 0.738157254388157, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7547225578613002\n",
      "Step - 19040, Loss - 0.7448006093218321, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0907298560342331\n",
      "Step - 19041, Loss - 0.811888893004309, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5112030027837666\n",
      "Step - 19042, Loss - 0.88022752217055, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4475260812536779\n",
      "Step - 19043, Loss - 0.5679511747865691, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6159226476553414\n",
      "Step - 19044, Loss - 0.613770317818366, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4610083368327995\n",
      "Step - 19045, Loss - 0.7816921560841472, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5894274020263298\n",
      "Step - 19046, Loss - 0.6769067435666869, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7414533974621207\n",
      "Step - 19047, Loss - 0.639695595192705, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.206817038034828\n",
      "Step - 19048, Loss - 0.9220693803479708, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.527022608067292\n",
      "Step - 19049, Loss - 0.6785253350609728, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2562852774124298\n",
      "Step - 19050, Loss - 0.6351648828658695, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2899111306549909\n",
      "Step - 19051, Loss - 0.7604716926822812, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9859852646010414\n",
      "Step - 19052, Loss - 0.7018585837293073, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2173655850186633\n",
      "Step - 19053, Loss - 0.7814113248481667, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5350853163771685\n",
      "Step - 19054, Loss - 0.6357946307852362, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2032267603899662\n",
      "Step - 19055, Loss - 0.6281030444371599, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0443366790012578\n",
      "Step - 19056, Loss - 0.8264607139770216, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.3673941368762615\n",
      "Step - 19057, Loss - 0.8459443900267402, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.297966064589965\n",
      "Step - 19058, Loss - 0.6086423292934517, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0194185692266169\n",
      "Step - 19059, Loss - 0.751564310308928, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3200156476720417\n",
      "Step - 19060, Loss - 0.8250060118454692, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7069133409206523\n",
      "Step - 19061, Loss - 0.6957098620128968, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7293417608032522\n",
      "Step - 19062, Loss - 0.6465343193912743, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9420320422427096\n",
      "Step - 19063, Loss - 0.7434970169939376, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1377555939805806\n",
      "Step - 19064, Loss - 0.6219003739240481, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4920653517213606\n",
      "Step - 19065, Loss - 0.6407152013207544, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.632327618420817\n",
      "Step - 19066, Loss - 0.7767307788488584, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3987243868594335\n",
      "Step - 19067, Loss - 0.5849233509941622, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6802886108882256\n",
      "Step - 19068, Loss - 0.5451519761743989, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.4928995058175625\n",
      "Step - 19069, Loss - 0.7532768074373165, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3996616268204232\n",
      "Step - 19070, Loss - 0.6658884420851937, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6679884046192113\n",
      "Step - 19071, Loss - 0.9034561951074632, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5393428001630045\n",
      "Step - 19072, Loss - 0.6972149203459519, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.1667890388364084\n",
      "Step - 19073, Loss - 0.5202437086689919, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3897313662078543\n",
      "Step - 19074, Loss - 0.5935918792133061, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2306967911819657\n",
      "Step - 19075, Loss - 0.6110321303974879, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.49737499454670303\n",
      "Step - 19076, Loss - 0.6879643163543637, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7328651309538952\n",
      "Step - 19077, Loss - 0.6793035147977663, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5109751628614976\n",
      "Step - 19078, Loss - 0.5574989842851305, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1310983651886037\n",
      "Step - 19079, Loss - 0.5476086331219148, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.260815167061858\n",
      "Step - 19080, Loss - 0.73830649061211, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7814969535342758\n",
      "Step - 19081, Loss - 0.6816701029233077, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6876624192105931\n",
      "Step - 19082, Loss - 0.987852068664373, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7797335513822712\n",
      "Step - 19083, Loss - 0.6361058882660411, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.46746699305045497\n",
      "Step - 19084, Loss - 0.7282036168663354, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4510161678715424\n",
      "Step - 19085, Loss - 0.7162110382574255, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3669233337140534\n",
      "Step - 19086, Loss - 0.7211289833867515, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3306559165338372\n",
      "Step - 19087, Loss - 0.6959703637156035, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.981912145658245\n",
      "Step - 19088, Loss - 0.7332555426634022, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6288466953382403\n",
      "Step - 19089, Loss - 0.7797595956466618, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.4775847514077425\n",
      "Step - 19090, Loss - 0.6104902459242277, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4667196197321215\n",
      "Step - 19091, Loss - 0.8271679449972019, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9996116103763274\n",
      "Step - 19092, Loss - 0.6953355208603902, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1167268010355362\n",
      "Step - 19093, Loss - 0.6828218600681855, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.150378549594573\n",
      "Step - 19094, Loss - 0.7084732525165646, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0831247215078272\n",
      "Step - 19095, Loss - 0.6609946766999287, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4661147341355603\n",
      "Step - 19096, Loss - 0.8260791161853711, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9803237559843784\n",
      "Step - 19097, Loss - 0.6251592617281116, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7300321949729671\n",
      "Step - 19098, Loss - 0.8024982943175011, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.627836382511202\n",
      "Step - 19099, Loss - 0.6474541208342385, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.653303978046477\n",
      "Step - 19100, Loss - 0.677618809526956, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.362706036495528\n",
      "Step - 19101, Loss - 0.7366625902934769, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.163458200387003\n",
      "Step - 19102, Loss - 0.6911968107495561, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.613680298167167\n",
      "Step - 19103, Loss - 1.0283466033534125, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3421400837845978\n",
      "Step - 19104, Loss - 0.6960667839598617, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3971439201698572\n",
      "Step - 19105, Loss - 0.7168765145135485, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5470088380265303\n",
      "Step - 19106, Loss - 0.9153553975831971, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4864450763071324\n",
      "Step - 19107, Loss - 0.7086492461612215, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8464694003019165\n",
      "Step - 19108, Loss - 0.6193559398672653, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.182090657694078\n",
      "Step - 19109, Loss - 0.6284731083848532, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.2023876497448667\n",
      "Step - 19110, Loss - 0.8215507205489875, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2942117407149867\n",
      "Step - 19111, Loss - 0.6544344563521582, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2642837364537949\n",
      "Step - 19112, Loss - 0.5442216205294875, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.0132875188748063\n",
      "Step - 19113, Loss - 0.5408674142504674, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1437611532023915\n",
      "Step - 19114, Loss - 0.7453105149863396, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.3379933940549713\n",
      "Step - 19115, Loss - 0.8588693748118603, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8118233264005172\n",
      "Step - 19116, Loss - 0.6012189480967576, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1109511757883201\n",
      "Step - 19117, Loss - 0.6135727400612305, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4002164243774977\n",
      "Step - 19118, Loss - 0.6305175330976067, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.85671086323082\n",
      "Step - 19119, Loss - 0.6668658139738942, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7373590732517966\n",
      "Step - 19120, Loss - 0.8504315787823831, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.110563892873589\n",
      "Step - 19121, Loss - 0.7951533031532954, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1251251842665384\n",
      "Step - 19122, Loss - 0.5358589364208135, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.42694633285740075\n",
      "Step - 19123, Loss - 0.7734563691700429, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8752286088650506\n",
      "Step - 19124, Loss - 0.7972510554298471, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.44529286041591926\n",
      "Step - 19125, Loss - 0.7620942472297972, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1379572345851696\n",
      "Step - 19126, Loss - 0.990164969093975, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.918948741100689\n",
      "Step - 19127, Loss - 0.5844898733757116, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9875860530457674\n",
      "Step - 19128, Loss - 0.6868972215137922, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7272376603999053\n",
      "Step - 19129, Loss - 0.804396879065568, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.517980891265451\n",
      "Step - 19130, Loss - 0.7711925973303315, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3783009093854681\n",
      "Step - 19131, Loss - 0.5884919952679494, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4944480491836998\n",
      "Step - 19132, Loss - 0.86647991067798, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6244313544098923\n",
      "Step - 19133, Loss - 0.6919344930438283, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.82260656326837\n",
      "Step - 19134, Loss - 0.8510387200736491, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9430459877759484\n",
      "Step - 19135, Loss - 0.6613028381801652, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4353677114299843\n",
      "Step - 19136, Loss - 0.5516367801867491, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4997360156794473\n",
      "Step - 19137, Loss - 0.7094875660530602, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8531075437500304\n",
      "Step - 19138, Loss - 0.8555374009794503, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4041829813545306\n",
      "Step - 19139, Loss - 0.45008986441316823, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.503206071020959\n",
      "Step - 19140, Loss - 0.6413991082064685, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6058782265985523\n",
      "Step - 19141, Loss - 0.765543990043824, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8862809922050681\n",
      "Step - 19142, Loss - 0.612160603386803, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6152390467274976\n",
      "Step - 19143, Loss - 0.7459722244864891, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.116948189677352\n",
      "Step - 19144, Loss - 0.6861605334839506, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7534500685809797\n",
      "Step - 19145, Loss - 0.9498640034831438, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6408667885986257\n",
      "Step - 19146, Loss - 0.8894832615739642, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.413659956230203\n",
      "Step - 19147, Loss - 0.6272982288971275, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.950865259092982\n",
      "Step - 19148, Loss - 0.4602459750649271, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.575076098163973\n",
      "Step - 19149, Loss - 0.8061594996747288, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2927400634680237\n",
      "Step - 19150, Loss - 0.86752946449808, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2707943583203256\n",
      "Step - 19151, Loss - 0.763946481478445, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8460865475278607\n",
      "Step - 19152, Loss - 0.6848115291165287, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6391382572905902\n",
      "Step - 19153, Loss - 0.7415558143836452, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5881985902801331\n",
      "Step - 19154, Loss - 0.902529817903974, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.4947632348303435\n",
      "Step - 19155, Loss - 0.6997147986479783, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1734911277314315\n",
      "Step - 19156, Loss - 0.5163925490128259, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.862327229581643\n",
      "Step - 19157, Loss - 0.6380102938489065, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.0402379973020373\n",
      "Step - 19158, Loss - 0.787813982009191, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9748564114434061\n",
      "Step - 19159, Loss - 0.6128872715237174, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5079721526447307\n",
      "Step - 19160, Loss - 0.6401079843537454, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.518331194167617\n",
      "Step - 19161, Loss - 0.7359846866696412, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8567299501059875\n",
      "Step - 19162, Loss - 0.5325932841101768, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.301679675366635\n",
      "Step - 19163, Loss - 0.6492238362469824, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.38956745691712275\n",
      "Step - 19164, Loss - 0.4101657892011412, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9585009180146488\n",
      "Step - 19165, Loss - 0.6796016945252389, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2287498039522935\n",
      "Step - 19166, Loss - 0.7464626950056324, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2950166227954114\n",
      "Step - 19167, Loss - 0.7117981420506716, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.3796060862264956\n",
      "Step - 19168, Loss - 0.6662494200733522, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9067742945913431\n",
      "Step - 19169, Loss - 0.7980739268954133, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2578930309809722\n",
      "Step - 19170, Loss - 0.7589929877553053, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9230487455287184\n",
      "Step - 19171, Loss - 0.8835660517585027, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6173762607134582\n",
      "Step - 19172, Loss - 0.8536174379662388, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9065893674750154\n",
      "Step - 19173, Loss - 0.734493963318017, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5615133402058748\n",
      "Step - 19174, Loss - 0.8754430953823001, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.6537057559288746\n",
      "Step - 19175, Loss - 0.4420798408584762, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.1233944547001413\n",
      "Step - 19176, Loss - 0.8473795395465427, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.028963411054119\n",
      "Step - 19177, Loss - 0.7750634938234677, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.051028218535272\n",
      "Step - 19178, Loss - 0.4821924252135869, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6696424948764079\n",
      "Step - 19179, Loss - 0.6191885933528245, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8166955847707678\n",
      "Step - 19180, Loss - 0.5180029237898711, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4239734686314696\n",
      "Step - 19181, Loss - 0.6797403835603163, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3907109469114125\n",
      "Step - 19182, Loss - 0.6325382540589815, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 3.171772459209938\n",
      "Step - 19183, Loss - 0.6642330061522341, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7746390143346411\n",
      "Step - 19184, Loss - 0.6316406312692895, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9200862328871261\n",
      "Step - 19185, Loss - 0.9070639264774975, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.135852480587176\n",
      "Step - 19186, Loss - 0.5945658882766882, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7806377393911814\n",
      "Step - 19187, Loss - 0.6634941223506851, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8943472744251783\n",
      "Step - 19188, Loss - 0.6555627879456172, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.348036932877856\n",
      "Step - 19189, Loss - 0.8245964886774579, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.3763002636134454\n",
      "Step - 19190, Loss - 0.8176034820304603, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6922943363453442\n",
      "Step - 19191, Loss - 0.6296045068858248, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.030132441737683\n",
      "Step - 19192, Loss - 0.7036786402661149, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6991692305758188\n",
      "Step - 19193, Loss - 0.7500184296143749, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2498558085038438\n",
      "Step - 19194, Loss - 0.605572695462014, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3781465343064658\n",
      "Step - 19195, Loss - 0.7313046892001615, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5678508673942477\n",
      "Step - 19196, Loss - 0.6756276022696887, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8988500759775122\n",
      "Step - 19197, Loss - 0.7517469456142669, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8245240151265483\n",
      "Step - 19198, Loss - 0.5583773343283274, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.275619981273273\n",
      "Step - 19199, Loss - 0.5789630600268999, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9138669988453794\n",
      "Step - 19200, Loss - 0.6056469252756603, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.2724059991166436\n",
      "Step - 19201, Loss - 0.651733634283683, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.1133667591998426\n",
      "Step - 19202, Loss - 0.6421228832506423, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.710000314186363\n",
      "Step - 19203, Loss - 0.516943292050843, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.586246735364722\n",
      "Step - 19204, Loss - 0.8013949788361464, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8231056962626124\n",
      "Step - 19205, Loss - 0.7255838584895122, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7208030486756094\n",
      "Step - 19206, Loss - 0.6207156151280672, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8684525072549146\n",
      "Step - 19207, Loss - 0.762432929427862, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4235294624335915\n",
      "Step - 19208, Loss - 0.6393005542806071, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6694880939801278\n",
      "Step - 19209, Loss - 0.8106044823288459, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2390067688897224\n",
      "Step - 19210, Loss - 0.6276013144944468, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5740264185903727\n",
      "Step - 19211, Loss - 0.7879612065579843, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3606260747562025\n",
      "Step - 19212, Loss - 0.681258538639032, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.46788311258082776\n",
      "Step - 19213, Loss - 0.7635877563543354, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.29842157718458195\n",
      "Step - 19214, Loss - 0.5327409124397162, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7517022858975317\n",
      "Step - 19215, Loss - 0.7794311315499993, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2913772947958548\n",
      "Step - 19216, Loss - 0.637476973040819, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9736417222759015\n",
      "Step - 19217, Loss - 0.8333587533082405, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5144542136063137\n",
      "Step - 19218, Loss - 0.7585563683642544, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6282558825375444\n",
      "Step - 19219, Loss - 0.49134702491258014, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0323594977791777\n",
      "Step - 19220, Loss - 0.6760551568231925, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9097227347879558\n",
      "Step - 19221, Loss - 0.7094654654805829, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0159302412116138\n",
      "Step - 19222, Loss - 0.6493832262345037, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8665190427843565\n",
      "Step - 19223, Loss - 0.48794873141201817, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5044206381046561\n",
      "Step - 19224, Loss - 0.6509026177452752, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9112412530249708\n",
      "Step - 19225, Loss - 0.6827929108647053, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4191378043118492\n",
      "Step - 19226, Loss - 0.5574589168092133, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0559852848696296\n",
      "Step - 19227, Loss - 0.7238226210496648, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.60010516171411\n",
      "Step - 19228, Loss - 0.663167948534494, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.007586249844687\n",
      "Step - 19229, Loss - 0.6085571238635612, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9949476149895224\n",
      "Step - 19230, Loss - 0.6332234625397821, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.38847481789761534\n",
      "Step - 19231, Loss - 0.846364461225986, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5557214848969199\n",
      "Step - 19232, Loss - 0.822995615328523, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4257975187424972\n",
      "Step - 19233, Loss - 0.7486404027932958, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.0715749693118073\n",
      "Step - 19234, Loss - 0.8339028699716811, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2471051362406773\n",
      "Step - 19235, Loss - 0.9879080044471792, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.327546589036304\n",
      "Step - 19236, Loss - 0.8291426847408361, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4374264690237062\n",
      "Step - 19237, Loss - 0.6830377094734288, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7505498544856489\n",
      "Step - 19238, Loss - 0.6669080682978337, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2814281026970273\n",
      "Step - 19239, Loss - 0.798098365250159, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.837385885693919\n",
      "Step - 19240, Loss - 0.8493147252376979, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6017241886242892\n",
      "Step - 19241, Loss - 0.6739896666222978, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0127829029687228\n",
      "Step - 19242, Loss - 0.49522462098705344, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.055245476048983\n",
      "Step - 19243, Loss - 0.7689090829803286, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4692351226708088\n",
      "Step - 19244, Loss - 0.46231818920311335, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.805172087106454\n",
      "Step - 19245, Loss - 0.6654040551941184, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7508802402114378\n",
      "Step - 19246, Loss - 0.8116907819227653, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9267401613297874\n",
      "Step - 19247, Loss - 0.6853076468506658, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0986732066765101\n",
      "Step - 19248, Loss - 0.8448774157880797, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.130207186109072\n",
      "Step - 19249, Loss - 0.8176793720436303, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1605591325153901\n",
      "Step - 19250, Loss - 0.8989904875617406, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5412476756374065\n",
      "Step - 19251, Loss - 0.6420316813627647, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5576138095368941\n",
      "Step - 19252, Loss - 0.7205833703420434, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.46588430124389435\n",
      "Step - 19253, Loss - 0.886419409807381, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1770923689088921\n",
      "Step - 19254, Loss - 0.6795716145440572, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1657617874568331\n",
      "Step - 19255, Loss - 0.7856434095021666, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1968636542532742\n",
      "Step - 19256, Loss - 0.689856159658292, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1275411220160378\n",
      "Step - 19257, Loss - 0.8495796527800732, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0663505226523138\n",
      "Step - 19258, Loss - 0.5425154326479708, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.8594519973751322\n",
      "Step - 19259, Loss - 0.6680027488784697, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8302587988133399\n",
      "Step - 19260, Loss - 0.4758215524351662, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.291137833180461\n",
      "Step - 19261, Loss - 0.6671125648449036, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9513062695696543\n",
      "Step - 19262, Loss - 0.6607450394283036, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9532404726224261\n",
      "Step - 19263, Loss - 0.6369578639045439, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4978286114621084\n",
      "Step - 19264, Loss - 0.7824167504719193, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9962899399034008\n",
      "Step - 19265, Loss - 0.6164996297911876, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6849476848704004\n",
      "Step - 19266, Loss - 0.6705112357111283, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3839256970110474\n",
      "Step - 19267, Loss - 0.7784550603908, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.389457138272082\n",
      "Step - 19268, Loss - 0.6144666471251014, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9608795037948682\n",
      "Step - 19269, Loss - 0.6074036434816549, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.647061218256339\n",
      "Step - 19270, Loss - 0.8964661956710429, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6575863414351278\n",
      "Step - 19271, Loss - 0.6715889419390453, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.3471261950406985\n",
      "Step - 19272, Loss - 0.7596975192100758, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6401809547094157\n",
      "Step - 19273, Loss - 0.7728346713458081, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.3096685844069041\n",
      "Step - 19274, Loss - 1.0054774386943617, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.735286218544861\n",
      "Step - 19275, Loss - 0.7809693628995036, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5496819131259327\n",
      "Step - 19276, Loss - 0.6474031295338831, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0611362603013674\n",
      "Step - 19277, Loss - 0.6689906318595923, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4659168921710843\n",
      "Step - 19278, Loss - 0.6610432420373042, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1513153206348983\n",
      "Step - 19279, Loss - 0.8761297942047641, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8166179126537096\n",
      "Step - 19280, Loss - 0.7918272182849272, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5723863434055515\n",
      "Step - 19281, Loss - 0.6859449177932392, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5917430505508854\n",
      "Step - 19282, Loss - 0.5499536183069651, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3188146026505505\n",
      "Step - 19283, Loss - 0.7150038839329408, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.41799751945381447\n",
      "Step - 19284, Loss - 0.7844689377353478, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5968567328523647\n",
      "Step - 19285, Loss - 0.9213950126417073, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6629909250234347\n",
      "Step - 19286, Loss - 0.5532797707420247, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1239711225672375\n",
      "Step - 19287, Loss - 0.6482372736768129, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.629996075865817\n",
      "Step - 19288, Loss - 0.6104025163071275, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8177059959857513\n",
      "Step - 19289, Loss - 0.7382855920346554, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1719086752494625\n",
      "Step - 19290, Loss - 0.797714666125052, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7753465182907279\n",
      "Step - 19291, Loss - 0.7173353338197164, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.078788904563435\n",
      "Step - 19292, Loss - 0.8574122776824395, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1449915675363096\n",
      "Step - 19293, Loss - 0.837565838480975, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4125902291543997\n",
      "Step - 19294, Loss - 0.5726282670121486, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3506228366131416\n",
      "Step - 19295, Loss - 0.591799741144127, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9543741479757542\n",
      "Step - 19296, Loss - 0.6509131834840641, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.408140277552462\n",
      "Step - 19297, Loss - 0.9327164801607755, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.515568694837985\n",
      "Step - 19298, Loss - 0.7721371037546192, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7388354351545642\n",
      "Step - 19299, Loss - 0.8977678903764083, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.3152250882288214\n",
      "Step - 19300, Loss - 0.6388158011548665, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6862605072455599\n",
      "Step - 19301, Loss - 0.8173516566790308, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6358821539110814\n",
      "Step - 19302, Loss - 0.6695846353081922, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4923696909350292\n",
      "Step - 19303, Loss - 0.48866423870405246, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6569387957471748\n",
      "Step - 19304, Loss - 0.763068553133589, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.481776386395552\n",
      "Step - 19305, Loss - 0.5559607191334047, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 3.0998128362545065\n",
      "Step - 19306, Loss - 0.7430172931338012, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1679963891948346\n",
      "Step - 19307, Loss - 0.7426969880404448, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.311906820729378\n",
      "Step - 19308, Loss - 0.6830690067790853, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6371061741860807\n",
      "Step - 19309, Loss - 0.7302731826042366, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5683872211747525\n",
      "Step - 19310, Loss - 0.4738663983679488, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.298464936419622\n",
      "Step - 19311, Loss - 0.7190086569268076, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9079736105163551\n",
      "Step - 19312, Loss - 0.7145432950463328, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.48373889059211\n",
      "Step - 19313, Loss - 0.6881120232961154, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3245233782666088\n",
      "Step - 19314, Loss - 0.8989245374920615, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8270982927217607\n",
      "Step - 19315, Loss - 0.5584484763410925, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0417606041637224\n",
      "Step - 19316, Loss - 0.8028215295587863, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6327706500752397\n",
      "Step - 19317, Loss - 0.5926163693851252, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8530048157460054\n",
      "Step - 19318, Loss - 0.6671873658984798, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7274530796509313\n",
      "Step - 19319, Loss - 0.5123662229535478, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.354674730551165\n",
      "Step - 19320, Loss - 0.6484476466169624, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5226446855213015\n",
      "Step - 19321, Loss - 0.8311323180230513, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.984259434821117\n",
      "Step - 19322, Loss - 0.6659067432417093, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9795056544706474\n",
      "Step - 19323, Loss - 0.8043568402725745, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9211792158282117\n",
      "Step - 19324, Loss - 0.6727876209368658, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1308619599183944\n",
      "Step - 19325, Loss - 0.7265041166231897, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7906580382072226\n",
      "Step - 19326, Loss - 0.727235498868875, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.169031681414884\n",
      "Step - 19327, Loss - 0.8936102921622017, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3750491226450299\n",
      "Step - 19328, Loss - 0.7982870347253292, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9254763974964296\n",
      "Step - 19329, Loss - 0.7334942220195475, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1034828404419141\n",
      "Step - 19330, Loss - 0.8470449867568938, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9534079882539495\n",
      "Step - 19331, Loss - 0.8383559664497156, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5665159470686683\n",
      "Step - 19332, Loss - 0.6253203017507674, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5426690776691667\n",
      "Step - 19333, Loss - 1.082190632705842, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.45887163909323364\n",
      "Step - 19334, Loss - 0.6873364187041455, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3138380117261101\n",
      "Step - 19335, Loss - 0.4726899495039563, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.674624901343974\n",
      "Step - 19336, Loss - 0.6817628073912824, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.2481186777516964\n",
      "Step - 19337, Loss - 0.815008303039651, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8966691352256022\n",
      "Step - 19338, Loss - 0.8249701706772001, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2782364673867916\n",
      "Step - 19339, Loss - 0.8243829357284468, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.274894049401611\n",
      "Step - 19340, Loss - 0.8882432334033759, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.1639568024517555\n",
      "Step - 19341, Loss - 0.67288891355809, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 3.055881873014889\n",
      "Step - 19342, Loss - 0.5815987904511332, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4129489340117471\n",
      "Step - 19343, Loss - 0.7439401824529341, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4805575234590014\n",
      "Step - 19344, Loss - 0.6159787881349325, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.096554414779744\n",
      "Step - 19345, Loss - 0.6150720414007832, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8991967214796355\n",
      "Step - 19346, Loss - 0.7041590241885956, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3300839956414554\n",
      "Step - 19347, Loss - 0.7299745074871296, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1634343161392857\n",
      "Step - 19348, Loss - 0.8367445392385913, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7531311055427063\n",
      "Step - 19349, Loss - 0.7949294489991052, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5780360603457224\n",
      "Step - 19350, Loss - 0.5745983487146216, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8672913344115342\n",
      "Step - 19351, Loss - 0.7010640216680645, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7080840176775739\n",
      "Step - 19352, Loss - 0.5852765769346203, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2668786500880522\n",
      "Step - 19353, Loss - 0.5890890640486846, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5550057577489065\n",
      "Step - 19354, Loss - 0.595734273234844, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8271308761268323\n",
      "Step - 19355, Loss - 0.7255216657553171, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7919656924802365\n",
      "Step - 19356, Loss - 0.5095734825149465, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3990374539404458\n",
      "Step - 19357, Loss - 0.5751109447518844, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4712456707143016\n",
      "Step - 19358, Loss - 0.7454007906251385, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3795254258523586\n",
      "Step - 19359, Loss - 0.708090692416772, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8205860110403872\n",
      "Step - 19360, Loss - 0.929156526438319, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9028281314527822\n",
      "Step - 19361, Loss - 0.7129121978442744, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9250039247643684\n",
      "Step - 19362, Loss - 0.612640969305027, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0944512005472096\n",
      "Step - 19363, Loss - 0.7472333050234689, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9526232518462283\n",
      "Step - 19364, Loss - 0.780216763125802, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.252198884090025\n",
      "Step - 19365, Loss - 0.5526148259605425, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5676088681124172\n",
      "Step - 19366, Loss - 0.7133169789424044, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9952397304530097\n",
      "Step - 19367, Loss - 0.7568375804750602, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4076583686940327\n",
      "Step - 19368, Loss - 0.6369250980517526, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1138726249651845\n",
      "Step - 19369, Loss - 0.8153007788604784, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.011427535107353\n",
      "Step - 19370, Loss - 0.5794136234074665, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1019530621146976\n",
      "Step - 19371, Loss - 0.7272522874329392, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9123580847611581\n",
      "Step - 19372, Loss - 0.6647798456236044, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5856433249184341\n",
      "Step - 19373, Loss - 0.6989039903661898, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3619836311212419\n",
      "Step - 19374, Loss - 0.6355987280814513, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5849139505400935\n",
      "Step - 19375, Loss - 0.7200972929721663, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6050598175519177\n",
      "Step - 19376, Loss - 0.7703523902874241, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.675785141574493\n",
      "Step - 19377, Loss - 0.7545705476521566, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1619917409553724\n",
      "Step - 19378, Loss - 0.6669625086205426, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5478707062047041\n",
      "Step - 19379, Loss - 0.6056180220086421, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0307298312392177\n",
      "Step - 19380, Loss - 0.5233551416382856, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.2650972387651948\n",
      "Step - 19381, Loss - 0.7468665221305582, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.34160998161820155\n",
      "Step - 19382, Loss - 0.5427617706589172, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5930721009519482\n",
      "Step - 19383, Loss - 0.6267797128082621, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7468941048850462\n",
      "Step - 19384, Loss - 0.8058621990107447, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0251796257029056\n",
      "Step - 19385, Loss - 0.5835855684408184, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.1733028108530386\n",
      "Step - 19386, Loss - 0.9252335448766581, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.3110154074122025\n",
      "Step - 19387, Loss - 0.7353016295009318, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.058191510680617\n",
      "Step - 19388, Loss - 0.7286372277409784, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5123318297362334\n",
      "Step - 19389, Loss - 0.7415046608090206, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.009576967583607\n",
      "Step - 19390, Loss - 0.7406837319788475, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3950496481012689\n",
      "Step - 19391, Loss - 0.6180519481824154, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6754549616486345\n",
      "Step - 19392, Loss - 0.735504199368905, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4352780868957374\n",
      "Step - 19393, Loss - 0.5127599772005174, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1630512608240622\n",
      "Step - 19394, Loss - 0.7305056505301626, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6891959096137471\n",
      "Step - 19395, Loss - 0.7666407608319716, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.981480715550071\n",
      "Step - 19396, Loss - 0.7683058351020075, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8986052154133621\n",
      "Step - 19397, Loss - 0.7597257635971479, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3283898898696618\n",
      "Step - 19398, Loss - 0.9894184043593386, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5207385256172705\n",
      "Step - 19399, Loss - 0.7267162975371941, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.059119693243994\n",
      "Step - 19400, Loss - 0.577499604987479, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.39921644361971803\n",
      "Step - 19401, Loss - 0.5276428670969474, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1963217199860796\n",
      "Step - 19402, Loss - 0.6645609173806913, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.357664966226299\n",
      "Step - 19403, Loss - 0.6805204075425545, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0226758982209847\n",
      "Step - 19404, Loss - 0.6903175211671584, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7961457441757049\n",
      "Step - 19405, Loss - 0.7261568237345722, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.865393181305291\n",
      "Step - 19406, Loss - 0.66705295042333, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6992431556862518\n",
      "Step - 19407, Loss - 0.8477436808293615, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9835019732891604\n",
      "Step - 19408, Loss - 0.7621138917466463, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7603065262349744\n",
      "Step - 19409, Loss - 0.7287157572071092, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6302389239609958\n",
      "Step - 19410, Loss - 0.6734771334851699, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.343449334387673\n",
      "Step - 19411, Loss - 0.6842682442953868, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8646148854256968\n",
      "Step - 19412, Loss - 0.7818186403044741, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4391462646425492\n",
      "Step - 19413, Loss - 0.6233933678040111, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7384035527749346\n",
      "Step - 19414, Loss - 0.7073920013766684, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.816529847586152\n",
      "Step - 19415, Loss - 0.6551885041587329, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.921229347010429\n",
      "Step - 19416, Loss - 0.5848801874214752, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.367917552096373\n",
      "Step - 19417, Loss - 0.6334115562498104, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7177163464694581\n",
      "Step - 19418, Loss - 0.7712223374908835, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4937193295190088\n",
      "Step - 19419, Loss - 0.8054664417404491, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5364952089424617\n",
      "Step - 19420, Loss - 0.5457019278441273, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7340778819574885\n",
      "Step - 19421, Loss - 0.6917898736731153, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4427589204332478\n",
      "Step - 19422, Loss - 0.6841377206373344, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3871355506252991\n",
      "Step - 19423, Loss - 0.8395464786692688, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.589548974157452\n",
      "Step - 19424, Loss - 0.6171072109261322, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8479379189149381\n",
      "Step - 19425, Loss - 0.6726726859523607, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.089743475394792\n",
      "Step - 19426, Loss - 0.6878578019250817, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7128605369263867\n",
      "Step - 19427, Loss - 0.7848167273071176, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7046800401013037\n",
      "Step - 19428, Loss - 0.714005663700999, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2130517037526538\n",
      "Step - 19429, Loss - 0.7679193530255948, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7032519828833987\n",
      "Step - 19430, Loss - 0.8873247775773898, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4491593488585564\n",
      "Step - 19431, Loss - 0.7704197193649496, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8503181292972382\n",
      "Step - 19432, Loss - 0.7864757133635059, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3611079353628068\n",
      "Step - 19433, Loss - 0.832325093586859, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9073906790808625\n",
      "Step - 19434, Loss - 0.8561202347931193, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9405433640594423\n",
      "Step - 19435, Loss - 0.453043538058542, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2832385730483384\n",
      "Step - 19436, Loss - 0.733400617212774, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7032718397857967\n",
      "Step - 19437, Loss - 0.5289302426066889, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.239261175721994\n",
      "Step - 19438, Loss - 0.8155262788198452, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7523330924703968\n",
      "Step - 19439, Loss - 0.7394307549466432, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1661039043229582\n",
      "Step - 19440, Loss - 0.6219944958589133, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5287198431454452\n",
      "Step - 19441, Loss - 0.7391221629268473, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2805578051378872\n",
      "Step - 19442, Loss - 0.7138494335109642, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9365412886241468\n",
      "Step - 19443, Loss - 0.8515297448000242, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4247638185623979\n",
      "Step - 19444, Loss - 0.5912345220685511, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.2685838676969237\n",
      "Step - 19445, Loss - 0.665572832038355, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.989000892760207\n",
      "Step - 19446, Loss - 0.7425176123687451, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.321031755950297\n",
      "Step - 19447, Loss - 0.5908403153136905, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.3221917130198713\n",
      "Step - 19448, Loss - 0.8582353161440299, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.20066490307820087\n",
      "Step - 19449, Loss - 0.735603667262569, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7153923409617263\n",
      "Step - 19450, Loss - 0.5893407349638022, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8075197835595214\n",
      "Step - 19451, Loss - 0.5570197847767997, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6237142129083186\n",
      "Step - 19452, Loss - 0.5575867461995186, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.971128289179342\n",
      "Step - 19453, Loss - 0.7367193983931115, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7029583687411356\n",
      "Step - 19454, Loss - 0.9035389638580521, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.134582536275065\n",
      "Step - 19455, Loss - 0.7418728550571131, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1716659684215336\n",
      "Step - 19456, Loss - 0.6472077922484879, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2026432367992796\n",
      "Step - 19457, Loss - 0.6528221826143531, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1552570243300806\n",
      "Step - 19458, Loss - 0.7820533652422752, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3857420029777217\n",
      "Step - 19459, Loss - 0.9096918972837724, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.638279803301493\n",
      "Step - 19460, Loss - 0.6151968946776011, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8262266758468295\n",
      "Step - 19461, Loss - 0.8298552561748213, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9719736757427688\n",
      "Step - 19462, Loss - 0.7281465812002954, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0750971076393478\n",
      "Step - 19463, Loss - 0.7063870711467234, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.225687300257325\n",
      "Step - 19464, Loss - 0.8080363197947645, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7673820294289966\n",
      "Step - 19465, Loss - 0.7203214478770377, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6539349711320238\n",
      "Step - 19466, Loss - 0.8078925399991311, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.602768595679168\n",
      "Step - 19467, Loss - 0.7810391855366071, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7143855057951104\n",
      "Step - 19468, Loss - 0.7535172307515997, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5901117550896728\n",
      "Step - 19469, Loss - 0.7587198551522578, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.480285138184469\n",
      "Step - 19470, Loss - 0.7672750845774423, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8684518738581414\n",
      "Step - 19471, Loss - 0.6320169268566904, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2173556476436194\n",
      "Step - 19472, Loss - 0.7095994634112605, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5286858265116832\n",
      "Step - 19473, Loss - 0.7397452364553608, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8465140058486056\n",
      "Step - 19474, Loss - 0.785154170303402, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1895521960779387\n",
      "Step - 19475, Loss - 0.7041445117611806, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.22007040243131\n",
      "Step - 19476, Loss - 0.687401294127215, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2375712177803153\n",
      "Step - 19477, Loss - 0.7576778699614857, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5985930431506982\n",
      "Step - 19478, Loss - 0.5840126919608629, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8496607510098255\n",
      "Step - 19479, Loss - 0.8731130699148326, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.529757315611608\n",
      "Step - 19480, Loss - 0.7045664207015989, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2255852192221746\n",
      "Step - 19481, Loss - 0.60764829193617, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8450887478748426\n",
      "Step - 19482, Loss - 0.7579173522799221, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6385065479783596\n",
      "Step - 19483, Loss - 0.7015444520921127, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1601977448542888\n",
      "Step - 19484, Loss - 0.83719835905544, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3758419053373536\n",
      "Step - 19485, Loss - 0.7249005259951893, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1370658018186737\n",
      "Step - 19486, Loss - 0.6426077196898816, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0339480955525602\n",
      "Step - 19487, Loss - 0.7156829984018767, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8239739888415953\n",
      "Step - 19488, Loss - 0.7373937131491858, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6371553465944039\n",
      "Step - 19489, Loss - 0.7186816089283509, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7326377263665416\n",
      "Step - 19490, Loss - 0.5462462964800392, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7725353285715781\n",
      "Step - 19491, Loss - 0.5345023743070032, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8826228945242758\n",
      "Step - 19492, Loss - 0.666065552475927, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.949498376130467\n",
      "Step - 19493, Loss - 0.7472599479696481, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.821159784974562\n",
      "Step - 19494, Loss - 0.6952422966032598, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0267932030427802\n",
      "Step - 19495, Loss - 0.7713792933463501, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8077796361512181\n",
      "Step - 19496, Loss - 0.5202377521363399, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3915152759246259\n",
      "Step - 19497, Loss - 0.5899484633870294, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.064076559047924\n",
      "Step - 19498, Loss - 0.5455977835388589, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.0131663844633727\n",
      "Step - 19499, Loss - 0.8394125392981403, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8474891827584541\n",
      "Step - 19500, Loss - 0.6208213129710692, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.529632656493551\n",
      "Step - 19501, Loss - 0.5033777120908426, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4110279735191373\n",
      "Step - 19502, Loss - 0.6594875952402501, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6718155703633242\n",
      "Step - 19503, Loss - 0.6226174901974018, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4436088114100046\n",
      "Step - 19504, Loss - 0.7469380555883844, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1019127284172279\n",
      "Step - 19505, Loss - 0.6572780338032173, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.773575439193615\n",
      "Step - 19506, Loss - 0.7170436824726085, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1360644033392582\n",
      "Step - 19507, Loss - 0.872440421594438, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8437426472292769\n",
      "Step - 19508, Loss - 0.6835543854144428, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9771116923893333\n",
      "Step - 19509, Loss - 0.5861701930331453, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.252395259024881\n",
      "Step - 19510, Loss - 0.7637404748509805, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9925958648853295\n",
      "Step - 19511, Loss - 0.8628597785742015, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.571182227009974\n",
      "Step - 19512, Loss - 0.5354410936753804, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8010941745228364\n",
      "Step - 19513, Loss - 0.6650552812674846, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9829606395408056\n",
      "Step - 19514, Loss - 0.5641514567332748, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5930154862111022\n",
      "Step - 19515, Loss - 0.8668798541420228, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.416712766664581\n",
      "Step - 19516, Loss - 0.7743001010222983, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4319131362252746\n",
      "Step - 19517, Loss - 0.5359025197920672, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.305618265324447\n",
      "Step - 19518, Loss - 0.7182416333861614, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2897239470216117\n",
      "Step - 19519, Loss - 0.8462982559934686, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5961111514335153\n",
      "Step - 19520, Loss - 0.655824843835556, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3687521999534613\n",
      "Step - 19521, Loss - 0.7172803493135693, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7924957464181316\n",
      "Step - 19522, Loss - 0.6826468562466729, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9339915692246413\n",
      "Step - 19523, Loss - 0.6488770966263865, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0881206716125913\n",
      "Step - 19524, Loss - 0.8311716388072155, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4186695524621569\n",
      "Step - 19525, Loss - 0.8015512530150425, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5801728350866797\n",
      "Step - 19526, Loss - 0.8077870296002192, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.242994876057103\n",
      "Step - 19527, Loss - 0.49842710426469117, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9089802586572515\n",
      "Step - 19528, Loss - 0.7823341121223408, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6047863159917994\n",
      "Step - 19529, Loss - 0.7323355793671322, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5636748648167271\n",
      "Step - 19530, Loss - 0.5800596810778615, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1014332927233614\n",
      "Step - 19531, Loss - 0.7372949382700809, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9295820764245405\n",
      "Step - 19532, Loss - 0.683690231548963, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7839008983756337\n",
      "Step - 19533, Loss - 0.5533764482556834, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0102527519080888\n",
      "Step - 19534, Loss - 0.785371949043304, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.1506232503385907\n",
      "Step - 19535, Loss - 0.6738302043737814, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9440671157767246\n",
      "Step - 19536, Loss - 0.6876688512050425, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7701107793326906\n",
      "Step - 19537, Loss - 0.7636656905409936, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8885075825767526\n",
      "Step - 19538, Loss - 0.739208391108261, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7446467724149038\n",
      "Step - 19539, Loss - 0.757784481895099, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2034211129460861\n",
      "Step - 19540, Loss - 0.7699269008167653, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3199068565223646\n",
      "Step - 19541, Loss - 0.7960521295603697, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.189150530492946\n",
      "Step - 19542, Loss - 0.8413059733970216, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.387899702825592\n",
      "Step - 19543, Loss - 0.7122328989638, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0143437057255948\n",
      "Step - 19544, Loss - 0.7017375368374003, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4783312447875732\n",
      "Step - 19545, Loss - 0.6442648047236996, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.775124760243033\n",
      "Step - 19546, Loss - 0.7104119850021445, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4800376862061962\n",
      "Step - 19547, Loss - 0.8133837735884475, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7454414604012096\n",
      "Step - 19548, Loss - 0.7951155327270218, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4770348468580989\n",
      "Step - 19549, Loss - 0.6848971454181344, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5539141878848052\n",
      "Step - 19550, Loss - 0.6893241168910352, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1509886517474004\n",
      "Step - 19551, Loss - 0.9411747413330386, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.609224279821654\n",
      "Step - 19552, Loss - 0.7700614880478329, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5604223354622544\n",
      "Step - 19553, Loss - 0.7595250947711882, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8729659791127972\n",
      "Step - 19554, Loss - 0.6594026386547543, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9283122486010528\n",
      "Step - 19555, Loss - 0.4965189226226032, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5494705435858962\n",
      "Step - 19556, Loss - 0.6328704783186918, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.34904510059351773\n",
      "Step - 19557, Loss - 0.9588287714764216, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.869795775840955\n",
      "Step - 19558, Loss - 0.7990675012106093, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0243262916079865\n",
      "Step - 19559, Loss - 0.5838069043940969, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1435102404154505\n",
      "Step - 19560, Loss - 0.5936430863899163, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9331126733420647\n",
      "Step - 19561, Loss - 0.46911580000120356, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9411529980712023\n",
      "Step - 19562, Loss - 0.6493244527809845, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6813432412167073\n",
      "Step - 19563, Loss - 0.7997093674184513, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.968606936418982\n",
      "Step - 19564, Loss - 0.7639008271194954, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.843892297476181\n",
      "Step - 19565, Loss - 0.6689144410314063, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3268329181620662\n",
      "Step - 19566, Loss - 0.7212967154784209, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0935946674235646\n",
      "Step - 19567, Loss - 0.5665700517668458, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2087645088442662\n",
      "Step - 19568, Loss - 0.82024499388791, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1416297140869331\n",
      "Step - 19569, Loss - 0.7997265953250899, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6169759892070719\n",
      "Step - 19570, Loss - 0.8856715703298937, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5921453686121527\n",
      "Step - 19571, Loss - 0.6301880158553574, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9401358376642315\n",
      "Step - 19572, Loss - 0.5572115064793423, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7275812399428883\n",
      "Step - 19573, Loss - 0.8004632313305506, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.385159664460195\n",
      "Step - 19574, Loss - 1.000730097655266, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0476062517523617\n",
      "Step - 19575, Loss - 0.7852117626017687, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6264575584604881\n",
      "Step - 19576, Loss - 0.733233377528877, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.0819194198117814\n",
      "Step - 19577, Loss - 0.7875007257616635, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2129362872218594\n",
      "Step - 19578, Loss - 0.688552377445294, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8286978898641923\n",
      "Step - 19579, Loss - 0.5524313446871982, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5641467922775365\n",
      "Step - 19580, Loss - 0.8910094475430667, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.2987468893267793\n",
      "Step - 19581, Loss - 0.538342506783651, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8048361856618653\n",
      "Step - 19582, Loss - 0.6909603773700858, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5912135474760312\n",
      "Step - 19583, Loss - 0.602395177456793, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8488191180847326\n",
      "Step - 19584, Loss - 0.6167885153543005, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0660630074816722\n",
      "Step - 19585, Loss - 0.7151362203331415, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7442810926599936\n",
      "Step - 19586, Loss - 0.7120171282596347, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2782880481440526\n",
      "Step - 19587, Loss - 0.509295225500267, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8819210566209937\n",
      "Step - 19588, Loss - 0.714286418798019, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1018205879602223\n",
      "Step - 19589, Loss - 0.5841819742192149, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.214475712750037\n",
      "Step - 19590, Loss - 0.7129064087182546, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1172931856679404\n",
      "Step - 19591, Loss - 0.7256884777477648, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.0081243278672467\n",
      "Step - 19592, Loss - 0.7423837369231064, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7113370763313295\n",
      "Step - 19593, Loss - 0.7721183872360584, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7200307883476816\n",
      "Step - 19594, Loss - 0.7926002568719897, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3390287428268626\n",
      "Step - 19595, Loss - 0.8367471457709504, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7575504487098424\n",
      "Step - 19596, Loss - 0.8757613239315388, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3108742013721422\n",
      "Step - 19597, Loss - 0.6284364775859012, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9500084810867124\n",
      "Step - 19598, Loss - 0.7285918807645402, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3828405935802335\n",
      "Step - 19599, Loss - 0.838851589635806, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3855435329255168\n",
      "Step - 19600, Loss - 0.7631454435895618, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3521067004108702\n",
      "Step - 19601, Loss - 0.7027299200230193, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4893024599737792\n",
      "Step - 19602, Loss - 0.7434878292085468, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6695345107007498\n",
      "Step - 19603, Loss - 0.6625694260053774, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2408223547124113\n",
      "Step - 19604, Loss - 0.7743586812654756, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7739777487515256\n",
      "Step - 19605, Loss - 0.5146928963499948, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1240144254766207\n",
      "Step - 19606, Loss - 0.7219382506622097, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.3773806522681772\n",
      "Step - 19607, Loss - 0.8086297073433641, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1160992603663007\n",
      "Step - 19608, Loss - 0.6721300702475093, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.1394376290902963\n",
      "Step - 19609, Loss - 0.8615048157770582, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 3.335330889620013\n",
      "Step - 19610, Loss - 0.616456476917386, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6409863083495527\n",
      "Step - 19611, Loss - 0.9014740317402166, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3804702696128905\n",
      "Step - 19612, Loss - 0.6219464079700965, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9601388324043483\n",
      "Step - 19613, Loss - 0.872620689744411, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9346456032383088\n",
      "Step - 19614, Loss - 0.9221301191707678, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2444163171290696\n",
      "Step - 19615, Loss - 0.6003611783713406, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0229378307244774\n",
      "Step - 19616, Loss - 0.9014355830533483, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1603947186701344\n",
      "Step - 19617, Loss - 0.5769792374110553, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.409063282321969\n",
      "Step - 19618, Loss - 0.7750415980083776, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8293863978496949\n",
      "Step - 19619, Loss - 0.818601859682141, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4784135202588089\n",
      "Step - 19620, Loss - 0.863993565048238, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3265356850661558\n",
      "Step - 19621, Loss - 0.7255955540176312, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8125669364279791\n",
      "Step - 19622, Loss - 0.8001519364618506, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.3059410973078727\n",
      "Step - 19623, Loss - 0.739104776769522, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1374941114688548\n",
      "Step - 19624, Loss - 0.7756646071398476, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.869623605635626\n",
      "Step - 19625, Loss - 0.6924111042852765, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3073176308923566\n",
      "Step - 19626, Loss - 0.6874768461385496, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4259017925140133\n",
      "Step - 19627, Loss - 0.6563682074268307, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9468624622143238\n",
      "Step - 19628, Loss - 0.5557252404150668, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1807443638128314\n",
      "Step - 19629, Loss - 0.6567848061188073, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8516534769668002\n",
      "Step - 19630, Loss - 0.590883659233511, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5165071781008312\n",
      "Step - 19631, Loss - 0.7800297378712242, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2789058974116998\n",
      "Step - 19632, Loss - 0.7385719772059398, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0791139530711615\n",
      "Step - 19633, Loss - 0.7714920107387101, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.2214425293602953\n",
      "Step - 19634, Loss - 0.762511357356059, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.0642687028176034\n",
      "Step - 19635, Loss - 0.7619021362941405, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4988825256026068\n",
      "Step - 19636, Loss - 0.8038806434539936, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0604714521818186\n",
      "Step - 19637, Loss - 0.719595215787627, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8767198460965808\n",
      "Step - 19638, Loss - 0.6089443320048897, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3970478981731753\n",
      "Step - 19639, Loss - 0.713415853242907, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5765155593046906\n",
      "Step - 19640, Loss - 0.7804265582639818, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9228393753562374\n",
      "Step - 19641, Loss - 0.9434256993222185, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5906262669316935\n",
      "Step - 19642, Loss - 0.7476727853278945, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.504208348160765\n",
      "Step - 19643, Loss - 0.8183322926463863, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.9020714978045943\n",
      "Step - 19644, Loss - 0.8478776479532899, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9172355756681182\n",
      "Step - 19645, Loss - 0.7435503107059018, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3555075905487357\n",
      "Step - 19646, Loss - 0.7711802512292718, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9045892194518746\n",
      "Step - 19647, Loss - 0.5202301436452208, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4015597685812624\n",
      "Step - 19648, Loss - 0.7720066248064961, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0332129435230992\n",
      "Step - 19649, Loss - 0.8571078904092025, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3230233941359206\n",
      "Step - 19650, Loss - 0.722738961197823, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1774942066502099\n",
      "Step - 19651, Loss - 0.7187844512778296, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6288056265056479\n",
      "Step - 19652, Loss - 0.6478393768995343, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0522184591522308\n",
      "Step - 19653, Loss - 0.6207860045351352, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3348088842231134\n",
      "Step - 19654, Loss - 0.5153526073605397, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4584844391632834\n",
      "Step - 19655, Loss - 0.6404896493781134, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7013315696287428\n",
      "Step - 19656, Loss - 0.6845556688263323, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5746169645995656\n",
      "Step - 19657, Loss - 0.8226677343682789, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2784682180360152\n",
      "Step - 19658, Loss - 0.7628956627083112, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9600799500614894\n",
      "Step - 19659, Loss - 0.6853164694195355, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.46523560523356944\n",
      "Step - 19660, Loss - 0.6446392966232417, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5984670053772096\n",
      "Step - 19661, Loss - 0.6999137157219648, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4645948167818667\n",
      "Step - 19662, Loss - 0.6043284022660176, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0093022821180504\n",
      "Step - 19663, Loss - 0.7939989786503705, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.3609205249641905\n",
      "Step - 19664, Loss - 0.7176254980428467, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3861747836580363\n",
      "Step - 19665, Loss - 0.7901410516786631, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6534281834724458\n",
      "Step - 19666, Loss - 0.7669268959465371, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0792548716790709\n",
      "Step - 19667, Loss - 0.6734573126098794, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5980947672156535\n",
      "Step - 19668, Loss - 0.5832458042889713, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2430065028345336\n",
      "Step - 19669, Loss - 0.8813095428662827, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8223139579384406\n",
      "Step - 19670, Loss - 0.7364615412710013, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.028250351936728\n",
      "Step - 19671, Loss - 0.7201061569198426, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5892364943659336\n",
      "Step - 19672, Loss - 0.6150237703949653, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.885792184751359\n",
      "Step - 19673, Loss - 0.8333994565574931, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5010000560047496\n",
      "Step - 19674, Loss - 0.6628283705741445, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8537876520582832\n",
      "Step - 19675, Loss - 0.7417152550793913, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7628322024170845\n",
      "Step - 19676, Loss - 0.596000758558138, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9638097786436158\n",
      "Step - 19677, Loss - 0.6684416093236453, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0951046451293336\n",
      "Step - 19678, Loss - 0.6890664501482722, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1803298644607796\n",
      "Step - 19679, Loss - 0.5466554168868104, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7812221956028175\n",
      "Step - 19680, Loss - 0.6746630835100953, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0356786544385845\n",
      "Step - 19681, Loss - 0.6212928248278221, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0946671391576597\n",
      "Step - 19682, Loss - 0.553125452332121, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5811476159105006\n",
      "Step - 19683, Loss - 0.6674344971684234, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.268441175833423\n",
      "Step - 19684, Loss - 0.7251423365852121, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8691860671501295\n",
      "Step - 19685, Loss - 0.654433177604081, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.019080404258088\n",
      "Step - 19686, Loss - 0.7841060471953343, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.0188323715312335\n",
      "Step - 19687, Loss - 0.6130907179050005, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4056761546196742\n",
      "Step - 19688, Loss - 0.6984814476832635, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 3.1290818334670867\n",
      "Step - 19689, Loss - 0.8331494036572393, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7940840357909793\n",
      "Step - 19690, Loss - 0.4745305094443955, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0678561575119199\n",
      "Step - 19691, Loss - 0.899769567076651, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5009743320366415\n",
      "Step - 19692, Loss - 0.8033802798174793, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9017194986922582\n",
      "Step - 19693, Loss - 0.741632787299887, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2735658527080396\n",
      "Step - 19694, Loss - 0.8089203557980168, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6868717559222635\n",
      "Step - 19695, Loss - 0.7202700206556313, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.771690383171375\n",
      "Step - 19696, Loss - 0.5967129990082952, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7351021677479358\n",
      "Step - 19697, Loss - 0.8020707159202617, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4941234070312468\n",
      "Step - 19698, Loss - 0.5949214714051694, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.804862122044639\n",
      "Step - 19699, Loss - 0.7397104922525256, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.470996324885205\n",
      "Step - 19700, Loss - 0.6029282283818169, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9982192740983495\n",
      "Step - 19701, Loss - 0.4817683634351123, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1061380815513522\n",
      "Step - 19702, Loss - 0.7124882386254838, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6018583175983405\n",
      "Step - 19703, Loss - 0.8370047230965076, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8920089294986062\n",
      "Step - 19704, Loss - 0.9592453967069047, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.364738763634357\n",
      "Step - 19705, Loss - 0.8284673893994186, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3462680538242207\n",
      "Step - 19706, Loss - 0.8110588912852553, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9922002993464013\n",
      "Step - 19707, Loss - 0.65347363018129, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5155600842550021\n",
      "Step - 19708, Loss - 0.7714206861698187, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.267782332095987\n",
      "Step - 19709, Loss - 0.6191701520084711, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.949381331131679\n",
      "Step - 19710, Loss - 0.8594353875196674, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7391277131204224\n",
      "Step - 19711, Loss - 0.5693283055674837, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.0757869340681463\n",
      "Step - 19712, Loss - 0.7641544826695301, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5657747752905153\n",
      "Step - 19713, Loss - 0.7911422043254206, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6557385464457499\n",
      "Step - 19714, Loss - 0.8711432127855471, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9271208616238176\n",
      "Step - 19715, Loss - 0.8375771650626965, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.862953233006751\n",
      "Step - 19716, Loss - 0.7606251227451505, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5532555192833941\n",
      "Step - 19717, Loss - 0.8078889445641725, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4718566705920264\n",
      "Step - 19718, Loss - 0.7044584019492957, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1404950405865153\n",
      "Step - 19719, Loss - 0.8776217127798808, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3126123214733736\n",
      "Step - 19720, Loss - 0.6010333471308453, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4425262882857393\n",
      "Step - 19721, Loss - 0.528818915450008, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7922008968838221\n",
      "Step - 19722, Loss - 0.6439806169334241, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.478983610801306\n",
      "Step - 19723, Loss - 0.673604726597195, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9205992309819102\n",
      "Step - 19724, Loss - 0.4678101565459766, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9295192282741823\n",
      "Step - 19725, Loss - 0.7937356611284906, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6248701197737675\n",
      "Step - 19726, Loss - 0.8116470850704776, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.790453008147759\n",
      "Step - 19727, Loss - 0.7959829682710868, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2294044768235823\n",
      "Step - 19728, Loss - 0.7803121970660856, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4715016521529014\n",
      "Step - 19729, Loss - 0.6427849678653096, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9374246338339376\n",
      "Step - 19730, Loss - 0.5628872813466603, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7318724375859966\n",
      "Step - 19731, Loss - 0.6936754540780933, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0328314972710873\n",
      "Step - 19732, Loss - 0.5808095532965687, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8080911880511759\n",
      "Step - 19733, Loss - 0.6316427993693623, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.091174934404627\n",
      "Step - 19734, Loss - 0.8975616805894151, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8550301474320909\n",
      "Step - 19735, Loss - 0.5914489168697085, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.871462123573743\n",
      "Step - 19736, Loss - 0.5705736983020655, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6874382159303615\n",
      "Step - 19737, Loss - 0.502878897208747, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8934737141192992\n",
      "Step - 19738, Loss - 0.8318362993263726, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.0094068009532764\n",
      "Step - 19739, Loss - 0.6640675948007049, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7210021476366336\n",
      "Step - 19740, Loss - 0.6401611426226379, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0416013946248528\n",
      "Step - 19741, Loss - 0.5879119112694031, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7751547264792897\n",
      "Step - 19742, Loss - 0.5981087536590253, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9721950561729348\n",
      "Step - 19743, Loss - 0.8117511061426846, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3220799058036043\n",
      "Step - 19744, Loss - 0.923546974420526, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5686681271828218\n",
      "Step - 19745, Loss - 0.86846658858821, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.2678703031776615\n",
      "Step - 19746, Loss - 0.7277087616978848, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8102579287211613\n",
      "Step - 19747, Loss - 0.7382834100953346, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9991157500349087\n",
      "Step - 19748, Loss - 0.8729173544164976, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.031676861290754\n",
      "Step - 19749, Loss - 0.8403582374584939, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0268608073362255\n",
      "Step - 19750, Loss - 0.877337858023969, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.042724289139755\n",
      "Step - 19751, Loss - 0.766770404613691, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7926680710335567\n",
      "Step - 19752, Loss - 0.6261632338328826, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4630950166095447\n",
      "Step - 19753, Loss - 0.7864069330839739, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9329884074733679\n",
      "Step - 19754, Loss - 0.7047746775524112, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7180974953111504\n",
      "Step - 19755, Loss - 0.5013662339372698, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8501390598080523\n",
      "Step - 19756, Loss - 0.9445474731395097, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6700528298317443\n",
      "Step - 19757, Loss - 0.7014590912220078, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3348167348847073\n",
      "Step - 19758, Loss - 0.769915366167885, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.817243480113971\n",
      "Step - 19759, Loss - 0.6437126481410983, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.13869404295110282\n",
      "Step - 19760, Loss - 0.5723523113998635, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5825688860485576\n",
      "Step - 19761, Loss - 0.7374734785659401, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9392736536603742\n",
      "Step - 19762, Loss - 0.7583239028772804, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.5707587091869457\n",
      "Step - 19763, Loss - 0.8856440572841763, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4083222909927025\n",
      "Step - 19764, Loss - 0.7781738414893465, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.3613280479392773\n",
      "Step - 19765, Loss - 0.6688354675259071, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.446648990960016\n",
      "Step - 19766, Loss - 0.6420670981206315, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7736029169893265\n",
      "Step - 19767, Loss - 0.9014391040586855, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7869833718644723\n",
      "Step - 19768, Loss - 0.6868979888428344, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8921576625557621\n",
      "Step - 19769, Loss - 0.954838368844891, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7925096364967209\n",
      "Step - 19770, Loss - 0.7913555351513629, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.631818062250657\n",
      "Step - 19771, Loss - 0.7583562366448426, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7901031660603878\n",
      "Step - 19772, Loss - 0.7214625348834569, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6974985166165987\n",
      "Step - 19773, Loss - 0.4297777214975488, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9269957179278716\n",
      "Step - 19774, Loss - 0.7246527405175981, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.3780861848256066\n",
      "Step - 19775, Loss - 0.7459366061955562, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2012455764655152\n",
      "Step - 19776, Loss - 0.6548576768705577, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6680261895202462\n",
      "Step - 19777, Loss - 0.6882893371568175, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7685194972869795\n",
      "Step - 19778, Loss - 0.6935999581847834, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5692557340632594\n",
      "Step - 19779, Loss - 0.6431286125219894, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2668054926352226\n",
      "Step - 19780, Loss - 0.6821819097262856, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9049677457276366\n",
      "Step - 19781, Loss - 0.6094119618389715, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6833698589864269\n",
      "Step - 19782, Loss - 0.6851376277991639, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8612612167210187\n",
      "Step - 19783, Loss - 0.7616066386659686, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6451460736136859\n",
      "Step - 19784, Loss - 0.6313796242252643, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1100988765180193\n",
      "Step - 19785, Loss - 0.5624304721110482, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7345653281766761\n",
      "Step - 19786, Loss - 0.7016706378864527, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.906492106835922\n",
      "Step - 19787, Loss - 0.7102731449138172, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2962765861174794\n",
      "Step - 19788, Loss - 0.5737001808548721, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.296977388263513\n",
      "Step - 19789, Loss - 0.6562446627967987, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.37935281936901116\n",
      "Step - 19790, Loss - 0.6865174914638164, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4915060728066856\n",
      "Step - 19791, Loss - 0.6276947444526689, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9076426242604666\n",
      "Step - 19792, Loss - 0.699112534100184, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2433783533248794\n",
      "Step - 19793, Loss - 0.8574399948007747, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8833428836922508\n",
      "Step - 19794, Loss - 1.007895064523403, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.920253440411037\n",
      "Step - 19795, Loss - 0.8833808804135764, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8635993431574605\n",
      "Step - 19796, Loss - 0.7944531539393329, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9500110412774054\n",
      "Step - 19797, Loss - 0.9431704928068451, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.0577786900195356\n",
      "Step - 19798, Loss - 0.751706819023788, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5698936077360623\n",
      "Step - 19799, Loss - 0.8451969596791493, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5210543888964703\n",
      "Step - 19800, Loss - 0.7241423623223993, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2224513822216356\n",
      "Step - 19801, Loss - 0.5819075257950332, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7888008095610343\n",
      "Step - 19802, Loss - 0.7470547320832237, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.4547290922207066\n",
      "Step - 19803, Loss - 0.6515856388579322, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.031475362615049\n",
      "Step - 19804, Loss - 0.8290005321416238, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.014785287793187\n",
      "Step - 19805, Loss - 0.8234335667052249, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4165884260324022\n",
      "Step - 19806, Loss - 0.7760217539223309, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.262311628213471\n",
      "Step - 19807, Loss - 0.5109882878781733, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7398149392673545\n",
      "Step - 19808, Loss - 0.5256965191425454, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3271510940733815\n",
      "Step - 19809, Loss - 0.6867170454426849, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4830200673782297\n",
      "Step - 19810, Loss - 0.647748607197743, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.914282249223614\n",
      "Step - 19811, Loss - 0.6677263147509662, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6152232748603156\n",
      "Step - 19812, Loss - 0.6513161928053631, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.934590191070887\n",
      "Step - 19813, Loss - 0.7589868918971319, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.166572248017411\n",
      "Step - 19814, Loss - 0.8128072319149737, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.168495814834747\n",
      "Step - 19815, Loss - 0.5663395678187843, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4178954205374765\n",
      "Step - 19816, Loss - 0.6724753361440681, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2448119470689816\n",
      "Step - 19817, Loss - 0.7815511395246666, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.632229825635137\n",
      "Step - 19818, Loss - 0.6093285506115571, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5127668270255201\n",
      "Step - 19819, Loss - 0.793379949550528, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6339625958426105\n",
      "Step - 19820, Loss - 0.8293715048145934, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7002663866067216\n",
      "Step - 19821, Loss - 0.729956474787282, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5158142153249952\n",
      "Step - 19822, Loss - 0.5280548050675217, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.662473201603635\n",
      "Step - 19823, Loss - 0.5729533698548084, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7472287657524053\n",
      "Step - 19824, Loss - 0.7038800791397675, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5209384732177977\n",
      "Step - 19825, Loss - 0.6842686223170891, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0029328036262022\n",
      "Step - 19826, Loss - 0.60504610425087, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.1053604195776154\n",
      "Step - 19827, Loss - 0.7758514194822803, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1799106221806346\n",
      "Step - 19828, Loss - 0.6719662300488487, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6182308303210267\n",
      "Step - 19829, Loss - 0.5373706318376806, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.7576109898621772\n",
      "Step - 19830, Loss - 0.721732411538097, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.131501623126999\n",
      "Step - 19831, Loss - 0.9021163980062339, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3179037280636487\n",
      "Step - 19832, Loss - 0.5130389973703245, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.119930837812448\n",
      "Step - 19833, Loss - 0.6720113756597832, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6567714858232092\n",
      "Step - 19834, Loss - 0.7064528599807463, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.908813097673108\n",
      "Step - 19835, Loss - 0.5398919849451721, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1880721912978822\n",
      "Step - 19836, Loss - 0.5836684414027495, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0229387732769317\n",
      "Step - 19837, Loss - 0.851417380832427, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.2574250232197257\n",
      "Step - 19838, Loss - 0.7868789830164056, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.2729041130198273\n",
      "Step - 19839, Loss - 0.7550202070947291, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8811850529270246\n",
      "Step - 19840, Loss - 0.6251605493071449, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.711978154965956\n",
      "Step - 19841, Loss - 0.9137300026566799, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5329642487560535\n",
      "Step - 19842, Loss - 0.842249433483707, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7416497452259926\n",
      "Step - 19843, Loss - 0.5586046733226565, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5875152948041363\n",
      "Step - 19844, Loss - 0.6291781765157789, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5166191608218442\n",
      "Step - 19845, Loss - 0.7923947430902892, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.1480750388939436\n",
      "Step - 19846, Loss - 0.5765127570952299, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9520942776312775\n",
      "Step - 19847, Loss - 0.6772584222048196, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.2953898688298641\n",
      "Step - 19848, Loss - 0.6663629259347911, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8378601216268449\n",
      "Step - 19849, Loss - 0.7445256379130688, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9429060504747322\n",
      "Step - 19850, Loss - 0.8047802795140341, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9456394355547435\n",
      "Step - 19851, Loss - 0.8217682203329533, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6493935457605582\n",
      "Step - 19852, Loss - 0.5933069396568185, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.7227627410143973\n",
      "Step - 19853, Loss - 0.7235254215813376, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9010545500338362\n",
      "Step - 19854, Loss - 0.6719831148331528, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0317008237202814\n",
      "Step - 19855, Loss - 0.9417216326008566, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.356945764946406\n",
      "Step - 19856, Loss - 0.7453645659095458, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1232875529620556\n",
      "Step - 19857, Loss - 0.6475860157671283, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7341652447700209\n",
      "Step - 19858, Loss - 0.6831200510978103, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8130464693268912\n",
      "Step - 19859, Loss - 0.728707793310752, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8116184146384897\n",
      "Step - 19860, Loss - 0.746568871241753, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5461466994077967\n",
      "Step - 19861, Loss - 0.7511601820326854, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7776537943149575\n",
      "Step - 19862, Loss - 0.5865600265230074, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4117315566781068\n",
      "Step - 19863, Loss - 0.7646801154917618, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6982959269386602\n",
      "Step - 19864, Loss - 0.6725200530054575, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6056429322684507\n",
      "Step - 19865, Loss - 0.8151906136296148, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0700217559484784\n",
      "Step - 19866, Loss - 0.7998490643428331, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9026661313488368\n",
      "Step - 19867, Loss - 0.749315837613005, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.196334735761205\n",
      "Step - 19868, Loss - 0.718941472884222, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5847214313545785\n",
      "Step - 19869, Loss - 0.9465402437354552, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9070023757084821\n",
      "Step - 19870, Loss - 0.6375418601697149, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8931975310259122\n",
      "Step - 19871, Loss - 0.6370382812511608, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0610235266732044\n",
      "Step - 19872, Loss - 0.6033680578996757, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9143390234449362\n",
      "Step - 19873, Loss - 0.8358796540634366, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2308441483032122\n",
      "Step - 19874, Loss - 0.6546883454293693, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2687720345936448\n",
      "Step - 19875, Loss - 0.7617819109643439, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7058573974390996\n",
      "Step - 19876, Loss - 0.789714272881236, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.981099541654714\n",
      "Step - 19877, Loss - 0.47331546325681817, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3346953453594335\n",
      "Step - 19878, Loss - 0.5928185699796479, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6456510373790187\n",
      "Step - 19879, Loss - 0.7145291064621733, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3996135901868099\n",
      "Step - 19880, Loss - 0.7448791303288793, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9875101396843109\n",
      "Step - 19881, Loss - 0.715022161350959, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5788442795369709\n",
      "Step - 19882, Loss - 0.7657557077965377, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9439041916122868\n",
      "Step - 19883, Loss - 0.6174525505677491, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8295457423426884\n",
      "Step - 19884, Loss - 0.6186765156127076, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.0143711474280805\n",
      "Step - 19885, Loss - 0.720763865701279, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.40848564335775667\n",
      "Step - 19886, Loss - 0.6831590441328037, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1527755429591497\n",
      "Step - 19887, Loss - 0.6591954111261727, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9696318446860208\n",
      "Step - 19888, Loss - 0.5597613436843534, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8057097259611352\n",
      "Step - 19889, Loss - 0.7293775923498911, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0968962983046076\n",
      "Step - 19890, Loss - 0.5522257027950055, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5474114271778272\n",
      "Step - 19891, Loss - 0.642033281848156, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9167755153751134\n",
      "Step - 19892, Loss - 0.7265748997645676, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2559481314708743\n",
      "Step - 19893, Loss - 0.5424096127710751, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2046916066248556\n",
      "Step - 19894, Loss - 0.653202253215212, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5502318404361243\n",
      "Step - 19895, Loss - 0.7678299391282709, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8546167129196256\n",
      "Step - 19896, Loss - 0.5712969985330856, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6888127299765898\n",
      "Step - 19897, Loss - 0.5683324238972272, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8477870797693902\n",
      "Step - 19898, Loss - 0.8736271701250631, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6497544726904674\n",
      "Step - 19899, Loss - 0.6615526377100078, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8332804901647604\n",
      "Step - 19900, Loss - 0.7202630902587632, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4263762954614847\n",
      "Step - 19901, Loss - 0.8396105754398757, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2334378530107957\n",
      "Step - 19902, Loss - 0.7347448826478171, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.998060867691795\n",
      "Step - 19903, Loss - 0.9814812502644426, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8162782876705106\n",
      "Step - 19904, Loss - 0.8730650599452566, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0948560594144328\n",
      "Step - 19905, Loss - 0.6380665556238518, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.056691504942153\n",
      "Step - 19906, Loss - 0.8959524719544399, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0976344290489142\n",
      "Step - 19907, Loss - 0.5688496772217748, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7269127830294341\n",
      "Step - 19908, Loss - 0.7917624186948344, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5627605793050204\n",
      "Step - 19909, Loss - 0.7561929969291676, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5858424354596299\n",
      "Step - 19910, Loss - 0.6521105111728638, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9953594870177409\n",
      "Step - 19911, Loss - 0.6082161425840102, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6530363462828235\n",
      "Step - 19912, Loss - 0.6147808539024029, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6297963322155749\n",
      "Step - 19913, Loss - 0.6409082589139287, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7393955346426674\n",
      "Step - 19914, Loss - 0.6137861784432785, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7549045801336278\n",
      "Step - 19915, Loss - 0.6132823478555657, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2399464213782752\n",
      "Step - 19916, Loss - 0.7311303024897166, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3201989174124649\n",
      "Step - 19917, Loss - 0.7822347641118306, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5400585548348533\n",
      "Step - 19918, Loss - 0.7241028710951621, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.890917610399393\n",
      "Step - 19919, Loss - 0.6434847363567381, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4114721126737666\n",
      "Step - 19920, Loss - 0.5797666740089418, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7670803505680475\n",
      "Step - 19921, Loss - 0.7027393380839191, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9961593201930486\n",
      "Step - 19922, Loss - 0.8035212837456994, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7735112269583737\n",
      "Step - 19923, Loss - 0.6453247950080272, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8911354053256042\n",
      "Step - 19924, Loss - 0.6121131489525095, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7591994227806291\n",
      "Step - 19925, Loss - 0.7931153468885643, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3693373748435\n",
      "Step - 19926, Loss - 0.8064875415659093, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0283883611069624\n",
      "Step - 19927, Loss - 0.6624040728276939, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 3.3372138443440917\n",
      "Step - 19928, Loss - 0.8074068647989797, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9880778002435939\n",
      "Step - 19929, Loss - 0.6752082438213332, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.30136511994912324\n",
      "Step - 19930, Loss - 0.9058690055277061, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.412083513143289\n",
      "Step - 19931, Loss - 0.6809214275881107, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.532263946126507\n",
      "Step - 19932, Loss - 0.532676416114322, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.245082050053955\n",
      "Step - 19933, Loss - 0.7909022464677835, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6942412134947695\n",
      "Step - 19934, Loss - 0.6075322823157988, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.8892084945103038\n",
      "Step - 19935, Loss - 0.6495808722083447, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0241839108555462\n",
      "Step - 19936, Loss - 0.7530480261602908, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5923556283499092\n",
      "Step - 19937, Loss - 0.6280742077737897, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7109729930304985\n",
      "Step - 19938, Loss - 0.6180613957239313, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7531791867940306\n",
      "Step - 19939, Loss - 0.8543056896227176, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.107597415361835\n",
      "Step - 19940, Loss - 0.5277633315338369, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0903532251126893\n",
      "Step - 19941, Loss - 0.5676520682681131, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7894616381500958\n",
      "Step - 19942, Loss - 0.8218915325624516, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5350508543686094\n",
      "Step - 19943, Loss - 0.8602777578437589, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3928272400723485\n",
      "Step - 19944, Loss - 0.5272241774702016, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8726727256977589\n",
      "Step - 19945, Loss - 0.6759162006114042, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.623741034143493\n",
      "Step - 19946, Loss - 0.5728930769238068, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5824015017660552\n",
      "Step - 19947, Loss - 0.5152830763890895, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4739036673240784\n",
      "Step - 19948, Loss - 0.7199823707348837, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.171425308939062\n",
      "Step - 19949, Loss - 0.5739712489636715, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9851541612819474\n",
      "Step - 19950, Loss - 0.754736193519786, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1140347986841737\n",
      "Step - 19951, Loss - 0.459557179572573, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5394632343317065\n",
      "Step - 19952, Loss - 0.8288694528684298, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2513986316292467\n",
      "Step - 19953, Loss - 0.6596905224499365, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3644090239661262\n",
      "Step - 19954, Loss - 0.6050779383378677, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0073424026970252\n",
      "Step - 19955, Loss - 0.502089723661922, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2086804103880873\n",
      "Step - 19956, Loss - 0.8266199857610516, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3411285669105213\n",
      "Step - 19957, Loss - 0.647398056429326, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.47274255618819655\n",
      "Step - 19958, Loss - 0.680135695061066, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.2963431606508724\n",
      "Step - 19959, Loss - 0.716143929897287, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0929210062821468\n",
      "Step - 19960, Loss - 0.7930406078051587, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9934513351676861\n",
      "Step - 19961, Loss - 0.937279491840901, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8566158409081714\n",
      "Step - 19962, Loss - 0.9175324619325066, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.6093140825089258\n",
      "Step - 19963, Loss - 0.7135351331863705, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.35250274374602064\n",
      "Step - 19964, Loss - 0.5219488499207858, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7115554198295249\n",
      "Step - 19965, Loss - 0.5508664881520167, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5636584879725166\n",
      "Step - 19966, Loss - 0.7318192894911075, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.485603515459696\n",
      "Step - 19967, Loss - 1.0267349317458978, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.7383316666939961\n",
      "Step - 19968, Loss - 0.7023823033568416, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.705435980579526\n",
      "Step - 19969, Loss - 0.6046948057792056, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.7077388736331842\n",
      "Step - 19970, Loss - 0.6494035272601024, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0078846374220587\n",
      "Step - 19971, Loss - 0.5947237358644105, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.3206877435713875\n",
      "Step - 19972, Loss - 0.7219484344179501, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.538659411297984\n",
      "Step - 19973, Loss - 0.7408638697813391, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.8771048174706089\n",
      "Step - 19974, Loss - 0.6195098064889357, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0864706061424956\n",
      "Step - 19975, Loss - 0.8072033293491618, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.0868872039275448\n",
      "Step - 19976, Loss - 0.7125955639908441, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.419319272508684\n",
      "Step - 19977, Loss - 0.41032958229854277, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4806085476787059\n",
      "Step - 19978, Loss - 0.7412514793043845, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9115863402514928\n",
      "Step - 19979, Loss - 0.662369599317503, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2739181858398654\n",
      "Step - 19980, Loss - 0.5409055709621177, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5971385884277954\n",
      "Step - 19981, Loss - 0.6554999237403698, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.315265433831989\n",
      "Step - 19982, Loss - 0.830318261834446, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6201896914095748\n",
      "Step - 19983, Loss - 0.7288242711063175, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.5809710920494747\n",
      "Step - 19984, Loss - 0.6585424965926788, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 2.648876074174796\n",
      "Step - 19985, Loss - 0.7673830354624094, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.6938856857775823\n",
      "Step - 19986, Loss - 0.854142623229014, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2792189046895015\n",
      "Step - 19987, Loss - 0.7296691176055348, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.4533658820636941\n",
      "Step - 19988, Loss - 0.7425119081222837, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9161605312888753\n",
      "Step - 19989, Loss - 0.7414680496142874, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9974666932997764\n",
      "Step - 19990, Loss - 0.8779312070304662, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.1745659470482954\n",
      "Step - 19991, Loss - 0.7687656167000545, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.9070094433998293\n",
      "Step - 19992, Loss - 0.5086432615877737, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.5349980683339737\n",
      "Step - 19993, Loss - 0.672565541712471, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 0.9420365340753927\n",
      "Step - 19994, Loss - 0.6778525566998237, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2813594237437793\n",
      "Step - 19995, Loss - 0.7999880028344162, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4212391960162143\n",
      "Step - 19996, Loss - 0.8624543486382622, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.417870223147051\n",
      "Step - 19997, Loss - 0.7361630554097796, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4238874658209133\n",
      "Step - 19998, Loss - 0.9224101119898922, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.4547256637212664\n",
      "Step - 19999, Loss - 0.784968706301366, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.2092220049881681\n",
      "Step - 20000, Loss - 0.6117399441718927, Learning Rate - 9.5367431640625e-08, magnitude of gradient - 1.461695815076948\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.random([num_labels,num_attri])\n",
    "num_steps = 20000\n",
    "learning_rate = 0.1\n",
    "records = []\n",
    "for iind in range(num_steps):\n",
    "    selected_data = np.random.randint(0,num_rec,100)\n",
    "    loss,gradient=LossFunctionGradRegularization(attri[selected_data,:],label[selected_data],weights,1)\n",
    "    old_weights = weights.copy()\n",
    "    \n",
    "    grad_mag = np.linalg.norm(gradient)\n",
    "    \n",
    "    if(iind%1000 == 0):\n",
    "        learning_rate = learning_rate*0.5\n",
    "    \n",
    "    weights = weights-learning_rate*gradient/grad_mag\n",
    "    \n",
    "    records.append([iind+1,loss,learning_rate,grad_mag])\n",
    "\n",
    "    print(f'Step - {iind+1}, Loss - {loss}, Learning Rate - {learning_rate}, magnitude of gradient - {grad_mag}')\n",
    "    \n",
    "    if(grad_mag <1E-6):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16496559f40>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=pd.DataFrame.from_records(records,columns=['Step','Loss','Learning Rate','Gradient_Magnitude'])\n",
    "fig = plt.figure(figsize=[10,5])\n",
    "fig.patch.set_facecolor(color=\"white\")\n",
    "plt.plot(df['Step'],df['Loss'],'-r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(weights,attri):\n",
    "    p_score = weights @ attri\n",
    "    labels=np.argmax(p_score,axis=0)\n",
    "    return labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-b3182f9d1011>:5: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.\n",
      "  plt.pcolormesh(xx,yy,zz)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attrib = np.c_[xx_row,yy_row].T\n",
    "zz_row=prediction(weights,attrib)\n",
    "zz=zz_row.reshape(xx.shape)\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.pcolormesh(xx,yy,zz)\n",
    "sns.scatterplot(data=points,x='x',y='y',hue='label')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossFunctionGradientWithBias(data,label,weights,bias,delta,eps=1E-3):\n",
    "    \n",
    "    num_rec = data.shape[0]\n",
    "    scores = weights @ data.T + np.c_[bias]\n",
    "    margins = np.maximum(0,scores-scores[label,np.arange(num_rec)]+delta)\n",
    "    margins[label,np.arange(num_rec)] = 0 \n",
    "    loss = np.sum(margins)/num_rec\n",
    "    \n",
    "    num_pixels = weights.shape[1]  \n",
    "    num_classes = weights.shape[0]\n",
    "  \n",
    "    wgradient = np.zeros(weights.shape)\n",
    "    for jind in range(num_pixels): \n",
    "        pert = eps*data.T[jind,:]\n",
    "        for iind in range(num_classes):    \n",
    "            newScores = scores.copy()\n",
    "            newScores[iind,:] = scores[iind,:]+pert\n",
    "            newmargins = np.maximum(0,newScores-newScores[label,np.arange(num_rec)]+delta)\n",
    "            newmargins[label,np.arange(num_rec)]=0\n",
    "            newloss = np.sum(newmargins)/num_rec\n",
    "            wgradient[iind,jind]=(newloss - loss)/eps\n",
    "    \n",
    "    pert=eps\n",
    "    bgradient = np.zeros(bias.shape)\n",
    "    for iind in range(num_classes):\n",
    "        newScores = scores.copy()\n",
    "        newScores[iind] = newScores[iind]+eps\n",
    "        newmargins = np.maximum(0,newScores-newScores[label,np.arange(num_rec)]+delta)\n",
    "        newmargins[label,np.arange(num_rec)]=0\n",
    "        newloss = np.sum(newmargins)/num_rec\n",
    "        bgradient[iind] = (newloss - loss)/eps\n",
    "        \n",
    "    return loss,wgradient,bgradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step - 1, Loss - 8.620019402146516, Learning Rate - 0.15, magnitude of gradient of weights - 24.688023236396074, magnitude of gradient of bias - 2.1145685139051897\n",
      "Step - 2, Loss - 5.645775117631704, Learning Rate - 0.15, magnitude of gradient of weights - 18.444813350515894, magnitude of gradient of bias - 1.6899704139421592\n",
      "Step - 3, Loss - 3.106410801155795, Learning Rate - 0.15, magnitude of gradient of weights - 7.771329359396958, magnitude of gradient of bias - 0.8105553651665761\n",
      "Step - 4, Loss - 2.261648213602218, Learning Rate - 0.15, magnitude of gradient of weights - 5.691743869220668, magnitude of gradient of bias - 0.7270678796190377\n",
      "Step - 5, Loss - 1.6755406022304384, Learning Rate - 0.15, magnitude of gradient of weights - 3.838865561523271, magnitude of gradient of bias - 0.5446099521674862\n",
      "Step - 6, Loss - 1.0761337306824532, Learning Rate - 0.15, magnitude of gradient of weights - 7.845703197969972, magnitude of gradient of bias - 0.7736924453550553\n",
      "Step - 7, Loss - 1.488210467453738, Learning Rate - 0.15, magnitude of gradient of weights - 4.737675694109217, magnitude of gradient of bias - 0.5073460357587267\n",
      "Step - 8, Loss - 0.8415914882310983, Learning Rate - 0.15, magnitude of gradient of weights - 6.637572015724428, magnitude of gradient of bias - 0.7947058967223244\n",
      "Step - 9, Loss - 1.087141385433271, Learning Rate - 0.15, magnitude of gradient of weights - 4.2707513854786905, magnitude of gradient of bias - 0.3717526059087131\n",
      "Step - 10, Loss - 0.6249488795437361, Learning Rate - 0.15, magnitude of gradient of weights - 5.831407321302254, magnitude of gradient of bias - 0.47064601925129107\n",
      "Step - 11, Loss - 0.8507851839720579, Learning Rate - 0.15, magnitude of gradient of weights - 3.6226411383840285, magnitude of gradient of bias - 0.41158231254514327\n",
      "Step - 12, Loss - 0.9444871095839562, Learning Rate - 0.15, magnitude of gradient of weights - 8.44238301211235, magnitude of gradient of bias - 0.7962411694955245\n",
      "Step - 13, Loss - 1.0739852575278444, Learning Rate - 0.15, magnitude of gradient of weights - 5.889756346509674, magnitude of gradient of bias - 0.47434164902498466\n",
      "Step - 14, Loss - 0.7675990202449892, Learning Rate - 0.15, magnitude of gradient of weights - 4.472077408802327, magnitude of gradient of bias - 0.5319774431307965\n",
      "Step - 15, Loss - 0.6028729989249012, Learning Rate - 0.15, magnitude of gradient of weights - 2.6991774023828787, magnitude of gradient of bias - 0.35042830935871294\n",
      "Step - 16, Loss - 0.6099594905587289, Learning Rate - 0.15, magnitude of gradient of weights - 8.276581227176173, magnitude of gradient of bias - 0.6816157275177491\n",
      "Step - 17, Loss - 0.7697475561338947, Learning Rate - 0.15, magnitude of gradient of weights - 2.6518020882780107, magnitude of gradient of bias - 0.5560575509782708\n",
      "Step - 18, Loss - 0.6285451948708803, Learning Rate - 0.15, magnitude of gradient of weights - 5.395785889318397, magnitude of gradient of bias - 0.2842534080707285\n",
      "Step - 19, Loss - 0.6414572978381122, Learning Rate - 0.15, magnitude of gradient of weights - 3.068798249932037, magnitude of gradient of bias - 0.4939635614091473\n",
      "Step - 20, Loss - 0.5975259777753934, Learning Rate - 0.15, magnitude of gradient of weights - 5.537873982192667, magnitude of gradient of bias - 0.3089340327589564\n",
      "Step - 21, Loss - 0.5761456391959001, Learning Rate - 0.15, magnitude of gradient of weights - 4.559185244930265, magnitude of gradient of bias - 0.5279847906217482\n",
      "Step - 22, Loss - 0.4722826916608563, Learning Rate - 0.15, magnitude of gradient of weights - 2.5444677591119387, magnitude of gradient of bias - 0.3974921382870842\n",
      "Step - 23, Loss - 0.713906073165234, Learning Rate - 0.15, magnitude of gradient of weights - 9.444575540541724, magnitude of gradient of bias - 0.7308898685844787\n",
      "Step - 24, Loss - 0.4127201125525093, Learning Rate - 0.15, magnitude of gradient of weights - 2.0848104713950106, magnitude of gradient of bias - 0.3277193921634525\n",
      "Step - 25, Loss - 0.6004128770617994, Learning Rate - 0.15, magnitude of gradient of weights - 8.748153078532846, magnitude of gradient of bias - 0.7619711280616378\n",
      "Step - 26, Loss - 0.6186248846736712, Learning Rate - 0.15, magnitude of gradient of weights - 5.512767347070617, magnitude of gradient of bias - 0.5089204259999425\n",
      "Step - 27, Loss - 0.3986931996016102, Learning Rate - 0.15, magnitude of gradient of weights - 5.654661813257082, magnitude of gradient of bias - 0.57148928248906\n",
      "Step - 28, Loss - 0.4215257714743316, Learning Rate - 0.15, magnitude of gradient of weights - 2.9650858935604085, magnitude of gradient of bias - 0.24939927826662503\n",
      "Step - 29, Loss - 0.6584257351399413, Learning Rate - 0.15, magnitude of gradient of weights - 6.474272849788229, magnitude of gradient of bias - 0.6706713054841681\n",
      "Step - 30, Loss - 0.4615857929960308, Learning Rate - 0.15, magnitude of gradient of weights - 4.098675089612408, magnitude of gradient of bias - 0.3765634076751919\n",
      "Step - 31, Loss - 0.4279216956251804, Learning Rate - 0.15, magnitude of gradient of weights - 4.391650636649595, magnitude of gradient of bias - 0.452327315115874\n",
      "Step - 32, Loss - 0.41697431497107273, Learning Rate - 0.15, magnitude of gradient of weights - 4.00289759739924, magnitude of gradient of bias - 0.30430248109394537\n",
      "Step - 33, Loss - 0.4667087140228127, Learning Rate - 0.15, magnitude of gradient of weights - 5.51354921133083, magnitude of gradient of bias - 0.5268775948926533\n",
      "Step - 34, Loss - 0.41267457298346616, Learning Rate - 0.15, magnitude of gradient of weights - 3.5388667077051608, magnitude of gradient of bias - 0.36331804249169564\n",
      "Step - 35, Loss - 0.33712772525131823, Learning Rate - 0.15, magnitude of gradient of weights - 4.208187242435826, magnitude of gradient of bias - 0.4166533331198956\n",
      "Step - 36, Loss - 0.35785240751112224, Learning Rate - 0.15, magnitude of gradient of weights - 2.168196820753532, magnitude of gradient of bias - 0.20736441353320081\n",
      "Step - 37, Loss - 0.43419407103559393, Learning Rate - 0.15, magnitude of gradient of weights - 5.479705506237254, magnitude of gradient of bias - 0.568748473755889\n",
      "Step - 38, Loss - 0.5783422126777011, Learning Rate - 0.15, magnitude of gradient of weights - 7.0140421191510285, magnitude of gradient of bias - 0.5247856705359736\n",
      "Step - 39, Loss - 0.3627771433388631, Learning Rate - 0.15, magnitude of gradient of weights - 3.7923865068968365, magnitude of gradient of bias - 0.48641546028051225\n",
      "Step - 40, Loss - 0.5249448901911836, Learning Rate - 0.15, magnitude of gradient of weights - 5.234826379693553, magnitude of gradient of bias - 0.39370039370028215\n",
      "Step - 41, Loss - 0.3152528157955686, Learning Rate - 0.15, magnitude of gradient of weights - 2.71421918675452, magnitude of gradient of bias - 0.3343650699459739\n",
      "Step - 42, Loss - 0.4057374527677569, Learning Rate - 0.15, magnitude of gradient of weights - 3.432111169083893, magnitude of gradient of bias - 0.2753179979586506\n",
      "Step - 43, Loss - 0.3270215780938657, Learning Rate - 0.15, magnitude of gradient of weights - 3.469607479162447, magnitude of gradient of bias - 0.3597221149719881\n",
      "Step - 44, Loss - 0.26435430017015554, Learning Rate - 0.15, magnitude of gradient of weights - 3.0175118574118853, magnitude of gradient of bias - 0.2353720459186615\n",
      "Step - 45, Loss - 0.28606026641740184, Learning Rate - 0.15, magnitude of gradient of weights - 4.229356591653891, magnitude of gradient of bias - 0.42071367935913634\n",
      "Step - 46, Loss - 0.4168477337221064, Learning Rate - 0.15, magnitude of gradient of weights - 3.4172083069475145, magnitude of gradient of bias - 0.29966648127537615\n",
      "Step - 47, Loss - 0.3494799981615643, Learning Rate - 0.15, magnitude of gradient of weights - 3.566220951649678, magnitude of gradient of bias - 0.4703375789287321\n",
      "Step - 48, Loss - 0.5294830820231686, Learning Rate - 0.15, magnitude of gradient of weights - 6.3097116815687135, magnitude of gradient of bias - 0.49295030175448934\n",
      "Step - 49, Loss - 0.34276907680275853, Learning Rate - 0.15, magnitude of gradient of weights - 3.2664200111835697, magnitude of gradient of bias - 0.31080540535831\n",
      "Step - 50, Loss - 0.31974071705089935, Learning Rate - 0.15, magnitude of gradient of weights - 4.032769807243467, magnitude of gradient of bias - 0.289482296522382\n",
      "Step - 51, Loss - 0.43818310748091227, Learning Rate - 0.15, magnitude of gradient of weights - 4.93383045287239, magnitude of gradient of bias - 0.5007372850126156\n",
      "Step - 52, Loss - 0.371448453412278, Learning Rate - 0.15, magnitude of gradient of weights - 4.137015213135234, magnitude of gradient of bias - 0.3515679166248405\n",
      "Step - 53, Loss - 0.32410402813575573, Learning Rate - 0.15, magnitude of gradient of weights - 4.375084927546934, magnitude of gradient of bias - 0.46070302993608664\n",
      "Step - 54, Loss - 0.3157377075088607, Learning Rate - 0.15, magnitude of gradient of weights - 2.822831204517628, magnitude of gradient of bias - 0.252389396833194\n",
      "Step - 55, Loss - 0.3180967051150756, Learning Rate - 0.15, magnitude of gradient of weights - 3.1904280470870146, magnitude of gradient of bias - 0.3343650699459855\n",
      "Step - 56, Loss - 0.3969136076771714, Learning Rate - 0.15, magnitude of gradient of weights - 4.991355755387084, magnitude of gradient of bias - 0.4002499219236452\n",
      "Step - 57, Loss - 0.3796402612559494, Learning Rate - 0.15, magnitude of gradient of weights - 4.127100259483099, magnitude of gradient of bias - 0.4310452412449363\n",
      "Step - 58, Loss - 0.3723007440275163, Learning Rate - 0.15, magnitude of gradient of weights - 3.3437568545973684, magnitude of gradient of bias - 0.2629035416611451\n",
      "Step - 59, Loss - 0.3566205155768116, Learning Rate - 0.15, magnitude of gradient of weights - 4.604359412797591, magnitude of gradient of bias - 0.45585085280152693\n",
      "Step - 60, Loss - 0.33327788135166103, Learning Rate - 0.15, magnitude of gradient of weights - 4.917927383411034, magnitude of gradient of bias - 0.3885871845543033\n",
      "Step - 61, Loss - 0.2305425896249454, Learning Rate - 0.15, magnitude of gradient of weights - 2.985316341841398, magnitude of gradient of bias - 0.3003331483535382\n",
      "Step - 62, Loss - 0.37807746228315525, Learning Rate - 0.15, magnitude of gradient of weights - 3.0766696117107593, magnitude of gradient of bias - 0.2481482666727796\n",
      "Step - 63, Loss - 0.25708533448079407, Learning Rate - 0.15, magnitude of gradient of weights - 3.5550824078447687, magnitude of gradient of bias - 0.38444765573472534\n",
      "Step - 64, Loss - 0.30417910990787955, Learning Rate - 0.15, magnitude of gradient of weights - 3.104116400369376, magnitude of gradient of bias - 0.24454038521262197\n",
      "Step - 65, Loss - 0.2671173419834062, Learning Rate - 0.15, magnitude of gradient of weights - 2.3772457567568255, magnitude of gradient of bias - 0.2572936066052778\n",
      "Step - 66, Loss - 0.38589551979394715, Learning Rate - 0.15, magnitude of gradient of weights - 4.6513894990741615, magnitude of gradient of bias - 0.3569313659513884\n",
      "Step - 67, Loss - 0.2733212697065783, Learning Rate - 0.15, magnitude of gradient of weights - 2.9511372332765355, magnitude of gradient of bias - 0.34698703145788695\n",
      "Step - 68, Loss - 0.22576147218481843, Learning Rate - 0.15, magnitude of gradient of weights - 2.561652046356652, magnitude of gradient of bias - 0.19697715603582178\n",
      "Step - 69, Loss - 0.3058187251170132, Learning Rate - 0.15, magnitude of gradient of weights - 2.7103213428030264, magnitude of gradient of bias - 0.2918903903864876\n",
      "Step - 70, Loss - 0.21625065011808506, Learning Rate - 0.15, magnitude of gradient of weights - 2.370515974112776, magnitude of gradient of bias - 0.22671568097501335\n",
      "Step - 71, Loss - 0.2602710707204257, Learning Rate - 0.15, magnitude of gradient of weights - 2.3079295981138883, magnitude of gradient of bias - 0.23874672772622618\n",
      "Step - 72, Loss - 0.4361127210524667, Learning Rate - 0.15, magnitude of gradient of weights - 5.713646552635348, magnitude of gradient of bias - 0.44474711915861737\n",
      "Step - 73, Loss - 0.3026576220041142, Learning Rate - 0.15, magnitude of gradient of weights - 3.466535577710889, magnitude of gradient of bias - 0.34467375879216905\n",
      "Step - 74, Loss - 0.2826024106292982, Learning Rate - 0.15, magnitude of gradient of weights - 2.3640087963222842, magnitude of gradient of bias - 0.17663521732648219\n",
      "Step - 75, Loss - 0.24183545962486513, Learning Rate - 0.15, magnitude of gradient of weights - 2.390312097279353, magnitude of gradient of bias - 0.26720778431772935\n",
      "Step - 76, Loss - 0.3471185033153103, Learning Rate - 0.15, magnitude of gradient of weights - 5.203825417379156, magnitude of gradient of bias - 0.4115823125449396\n",
      "Step - 77, Loss - 0.2927334663424819, Learning Rate - 0.15, magnitude of gradient of weights - 3.2332743791993575, magnitude of gradient of bias - 0.33970575502917383\n",
      "Step - 78, Loss - 0.27997306911848646, Learning Rate - 0.15, magnitude of gradient of weights - 3.616913721230656, magnitude of gradient of bias - 0.2908607914448679\n",
      "Step - 79, Loss - 0.24649165538399714, Learning Rate - 0.15, magnitude of gradient of weights - 2.4456463060882188, magnitude of gradient of bias - 0.28142494558938547\n",
      "Step - 80, Loss - 0.2532301810535318, Learning Rate - 0.15, magnitude of gradient of weights - 2.5985325799367995, magnitude of gradient of bias - 0.2130727575265204\n",
      "Step - 81, Loss - 0.24415007069257114, Learning Rate - 0.15, magnitude of gradient of weights - 3.118800673369972, magnitude of gradient of bias - 0.3240370349203342\n",
      "Step - 82, Loss - 0.3741777087511838, Learning Rate - 0.15, magnitude of gradient of weights - 5.027911586418225, magnitude of gradient of bias - 0.40249223594975236\n",
      "Step - 83, Loss - 0.24436277657897398, Learning Rate - 0.15, magnitude of gradient of weights - 2.463844306009531, magnitude of gradient of bias - 0.2428991560297408\n",
      "Step - 84, Loss - 0.3225407851315227, Learning Rate - 0.15, magnitude of gradient of weights - 4.461580500536918, magnitude of gradient of bias - 0.3501428280000859\n",
      "Step - 85, Loss - 0.2622736231202713, Learning Rate - 0.15, magnitude of gradient of weights - 3.7135986432173573, magnitude of gradient of bias - 0.3847076812333095\n",
      "Step - 86, Loss - 0.2219385158782056, Learning Rate - 0.15, magnitude of gradient of weights - 1.9556385140412693, magnitude of gradient of bias - 0.18325511235538355\n",
      "Step - 87, Loss - 0.3125826432233492, Learning Rate - 0.15, magnitude of gradient of weights - 2.7975537204871763, magnitude of gradient of bias - 0.2599547499416869\n",
      "Step - 88, Loss - 0.23379663208080706, Learning Rate - 0.15, magnitude of gradient of weights - 2.2055105982857097, magnitude of gradient of bias - 0.1783255450011994\n",
      "Step - 89, Loss - 0.2560554037468178, Learning Rate - 0.15, magnitude of gradient of weights - 2.559710397296084, magnitude of gradient of bias - 0.2445403852127014\n",
      "Step - 90, Loss - 0.22324269820514558, Learning Rate - 0.15, magnitude of gradient of weights - 1.9802487909571522, magnitude of gradient of bias - 0.1643167672514625\n",
      "Step - 91, Loss - 0.19368589639403497, Learning Rate - 0.15, magnitude of gradient of weights - 2.2343357823776917, magnitude of gradient of bias - 0.24535688292767643\n",
      "Step - 92, Loss - 0.3324411857409382, Learning Rate - 0.15, magnitude of gradient of weights - 3.5436864602073577, magnitude of gradient of bias - 0.2774887385101656\n",
      "Step - 93, Loss - 0.3110319490129486, Learning Rate - 0.15, magnitude of gradient of weights - 3.6083926129375747, magnitude of gradient of bias - 0.3277193921634034\n",
      "Step - 94, Loss - 0.3002007558364609, Learning Rate - 0.15, magnitude of gradient of weights - 2.9780107657678903, magnitude of gradient of bias - 0.2580697580112078\n",
      "Step - 95, Loss - 0.19330032052344007, Learning Rate - 0.15, magnitude of gradient of weights - 2.315747299484116, magnitude of gradient of bias - 0.21213203435588202\n",
      "Step - 96, Loss - 0.1872394555622118, Learning Rate - 0.15, magnitude of gradient of weights - 2.688493546541013, magnitude of gradient of bias - 0.21400934559026866\n",
      "Step - 97, Loss - 0.1945996166699845, Learning Rate - 0.15, magnitude of gradient of weights - 2.2604663055069842, magnitude of gradient of bias - 0.23832750575623018\n",
      "Step - 98, Loss - 0.2350302709787386, Learning Rate - 0.15, magnitude of gradient of weights - 1.7777782508610636, magnitude of gradient of bias - 0.1407124727946424\n",
      "Step - 99, Loss - 0.23823722912280196, Learning Rate - 0.15, magnitude of gradient of weights - 2.2516602236615877, magnitude of gradient of bias - 0.22494443758398472\n",
      "Step - 100, Loss - 0.31974630084626887, Learning Rate - 0.15, magnitude of gradient of weights - 4.032116218377262, magnitude of gradient of bias - 0.3289376840678253\n",
      "Step - 101, Loss - 0.25773455508581494, Learning Rate - 0.15, magnitude of gradient of weights - 1.8615571131826503, magnitude of gradient of bias - 0.16911534525284325\n",
      "Step - 102, Loss - 0.17312428043363715, Learning Rate - 0.15, magnitude of gradient of weights - 2.2821214345901035, magnitude of gradient of bias - 0.17378147196969002\n",
      "Step - 103, Loss - 0.35893121311531495, Learning Rate - 0.15, magnitude of gradient of weights - 3.7193801516250318, magnitude of gradient of bias - 0.37656340767524354\n",
      "Step - 104, Loss - 0.13882984535329823, Learning Rate - 0.15, magnitude of gradient of weights - 0.6593146630766786, magnitude of gradient of bias - 0.050990195135918966\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.random([num_labels,num_attri])\n",
    "bias = np.random.random(num_labels)\n",
    "num_steps = 2000\n",
    "learning_rate = 0.3\n",
    "records = []\n",
    "\n",
    "for iind in range(num_steps):\n",
    "\n",
    "    selected_data = np.random.randint(0,num_rec,100)\n",
    "    loss,wgradient,bgradient=LossFunctionGradientWithBias(attri[selected_data,:],label[selected_data],weights,bias,1)\n",
    "    \n",
    "    old_weights = weights.copy()\n",
    "    old_bias = bias.copy()\n",
    "    \n",
    "    wgrad_mag = np.linalg.norm(wgradient)\n",
    "    bgrad_mag = np.linalg.norm(bgradient)\n",
    "    \n",
    "    if(iind%1000 == 0):\n",
    "        learning_rate = learning_rate*0.5\n",
    "    \n",
    "    weights = weights-learning_rate*wgradient/wgrad_mag\n",
    "    bias = bias - learning_rate*bgradient/bgrad_mag\n",
    "    \n",
    "    records.append([iind+1,loss,learning_rate,wgrad_mag,bgrad_mag])\n",
    "\n",
    "    print(f'Step - {iind+1}, Loss - {loss}, Learning Rate - {learning_rate}, magnitude of gradient of weights - {wgrad_mag}, magnitude of gradient of bias - {bgrad_mag}')\n",
    "    \n",
    "    if(wgrad_mag <1E-1 or bgrad_mag<1E-1):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(weights,bias,attri):\n",
    "    p_score = weights @ attri + np.c_[bias]\n",
    "    labels=np.argmax(p_score,axis=0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-9c163a1ee01c>:5: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.\n",
      "  plt.pcolormesh(xx,yy,zz)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attrib = np.c_[xx_row,yy_row].T\n",
    "zz_row=prediction(weights,bias,attrib)\n",
    "zz=zz_row.reshape(xx.shape)\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.pcolormesh(xx,yy,zz)\n",
    "sns.scatterplot(data=points,x='x',y='y',hue='label')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
